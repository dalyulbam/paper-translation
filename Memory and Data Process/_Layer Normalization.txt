3.1 층 정규화된 순환 신경망

최근의 시퀀스 대 시퀀스 모델 [Sutskever et al., 2014]은 자연어 처리에서 순차적 예측 문제를 해결하기 위해 간결한 순환 신경망을 사용합니다. NLP 작업에서는 서로 다른 학습 사례에 대해 서로 다른 문장 길이를 가지는 것이 일반적입니다. 이는 RNN에서 처리하기 쉽습니다. 왜냐하면 동일한 가중치가 모든 시간 단계에서 사용되기 때문입니다. 그러나 RNN에 배치 정규화를 적용할 때는 각 시간 단계별로 별도의 통계를 계산하고 저장해야 합니다. 이는 테스트 시퀀스가 학습 시퀀스 중 가장 긴 경우 문제가 됩니다. 레이어 정규화는 현재 시간 단계에서 레이어에 대한 입력의 합에만 의존하는 정규화 용어를 가지고 있으므로 이러한 문제가 없습니다. 또한 모든 시간 단계에서 공유되는 하나의 게인 및 바이어스 매개변수 세트만 가지고 있습니다.

표준 RNN에서 순환층의 합산 입력은 현재 입력 xt와 이전 숨겨진 상태 벡터 ht−1로부터 계산됩니다. 이는 at = Whhht−1 +Wxhxt로 계산됩니다. 레이어 정규화된 순환층은 추가 정규화 항을 사용하여 활성화를 재중심화하고 재스케일링합니다. 이는 식 (3)과 유사합니다.

ht = f (cid:104) g
σt
(cid:12) (cid:0) at − µt(cid:1) + b(cid:105) µt = 1

ht = f (cid:104) g
σt
(cid:12) (cid:0) at − µt(cid:1) + b(cid:105) µt = 1

안녕하세요
안녕하세요 (cid:88)

나는 1이다.
at
i
σt = (시그마) (시그마) (시그마) 1
H
H (X)

나는 1이다.
(at
i
− µt)2 (4)

Whh는 재귀적으로 숨겨진 가중치이고, Wxh는 하향식 입력에서 숨겨진 가중치입니다. (cid:12)는 두 벡터 간의 요소별 곱셈입니다. b와 g는 ht와 동일한 차원의 편향 및 이득 매개변수로 정의됩니다.

표준 RNN에서는 재귀 유닛에 대한 합산 입력의 평균 크기가 시간 단계마다 증가하거나 감소하는 경향이 있어 그라디언트가 폭발하거나 소멸하는 현상이 발생합니다. 레이어 정규화된 RNN에서는 정규화 항목들이 레이어의 모든 합산 입력의 재조정에 불변하게 만들어져 숨겨진-숨겨진 동적을 훨씬 안정적으로 유지합니다.

4 관련 연구

배치 정규화는 이전에 순환 신경망에도 확장되었습니다 [Laurent et al., 2015, Amodei et al., 2015, Cooijmans et al., 2016]. 이전 연구 [Cooijmans et al., 2016]는 각 시간 단계마다 독립적인 정규화 통계를 유지함으로써 순환 배치 정규화의 최상의 성능을 얻을 수 있다고 제안합니다. 저자들은 순환 배치 정규화 레이어의 게인 매개변수를 0.1로 초기화하는 것이 모델의 최종 성능에 큰 차이를 만든다고 보여줍니다. 우리의 연구는 또한 가중치 정규화 [Salimans and Kingma, 2016]와 관련이 있습니다. 가중치 정규화에서는 분산 대신 들어오는 가중치의 L2 노름을 사용하여 뉴런에 대한 합산 입력을 정규화합니다. 기대 통계를 사용하여 가중치 정규화 또는 배치 정규화를 적용하는 것은 원래의 피드포워드 신경망의 다른 매개변수화와 동등합니다. ReLU 네트워크에서의 재매개변화는 Path-normalized SGD [Neyshabur et al., 2015]에서 연구되었습니다. 그러나 우리가 제안하는 레이어 정규화 방법은 원래의 신경망의 재매개변화가 아닙니다. 따라서 레이어 정규화된 모델은 다른 불변성 특성을 가지며, 이를 다음 섹션에서 연구할 것입니다.

5 분석

이 섹션에서는 다른 정규화 방법의 불변성 속성을 조사합니다.

5.1 가중치와 데이터 변환에 대한 불변성

제안된 레이어 정규화는 배치 정규화와 가중치 정규화와 관련이 있습니다. 비록 정규화 스칼라가 다르게 계산되지만, 이러한 방법들은 뉴런에 대한 입력 ai를 두 개의 스칼라 µ와 σ를 통해 정규화하는 것으로 요약될 수 있습니다. 또한, 정규화 후 각 뉴런에 대해 적응적인 편향 b와 이득 g를 학습합니다.

안녕하세요 = hi
f(gi
σi
(ai − µi) + bi) = f(giσi(ai − µi) + bi) (5)

레이어 정규화와 배치 정규화의 경우, µ와 σ는 식 2와 3에 따라 계산됩니다. 가중치 정규화에서는 µ는 0이고, σ = (cid:107)w(cid:107)2입니다.

3
가중치 행렬 가중치 행렬 가중치 벡터 데이터셋 데이터셋 단일 훈련 케이스
재조정 재중심화 재조정 재조정 재중심화 재조정

배치 정규화 불변 불변 불변 불변 불변
가중치 정규화 불변 불변 불변 불변 불변
레이어 정규화 불변 불변 불변 불변 불변

표 1: 정규화 방법에 대한 불변성 특성.

표 1은 세 가지 정규화 방법에 대한 불변성 결과를 강조합니다.

무게 재조정 및 재중심화: 먼저, 배치 정규화와 가중치 정규화에서 단일 뉴런의 입력 가중치 wi에 대한 재조정은 뉴런의 정규화된 합산 입력에 영향을 미치지 않음을 관찰할 수 있습니다. 정확히 말하면, 배치 및 가중치 정규화에서 가중치 벡터가 δ로 스케일링되면 두 개의 스칼라 µ와 σ도 δ로 스케일링됩니다. 정규화된 합산 입력은 스케일링 전후로 동일합니다. 따라서 배치 및 가중치 정규화는 가중치의 재조정에 불변합니다. 반면에 레이어 정규화는 단일 가중치 벡터의 개별 스케일링에는 불변하지 않습니다. 대신, 레이어 정규화는 전체 가중치 행렬의 스케일링 및 가중치 행렬의 모든 입력 가중치에 대한 이동에는 불변합니다. 모델 파라미터 θ, θ(cid:48)의 두 개의 세트가 있고, 가중치 행렬 W와 W(cid:48)가 스케일링 요인 δ로 다르며, W(cid:48)의 모든 입력 가중치도 상수 벡터 γ로 이동된다고 가정해 봅시다. 즉, W(cid:48) = δW + 1γ(cid:62)입니다. 레이어 정규화에서 두 모델은 실제로 동일한 출력을 계산합니다.

h(0) = f(g(σ(0)(W(0)x - µ(0)) + b)) = f(g(σ(0)(W(0)x - µ(0)) + b))

σ(σ(Wx − µ) + b) = h. (6)

입력에 대해서만 정규화가 적용되면, 모델은 가중치의 재조정과 재중심화에 대해 불변하지 않을 것임을 유의하세요.

데이터 재조정 및 재중심화: 모든 정규화 방법이 데이터셋의 재조정에 대해 불변함을 보일 수 있습니다. 이는 뉴런의 입력값의 합이 변화하지 않음을 확인하여 입증할 수 있습니다. 또한, 레이어 정규화는 개별 학습 사례의 재조정에 대해 불변합니다. 왜냐하면 방정식 (3)의 정규화 스칼라 µ와 σ는 현재 입력 데이터에만 의존하기 때문입니다. x(cid:48)를 δ로 x를 재조정하여 얻은 새로운 데이터 포인트라고 가정해봅시다. 그러면 다음과 같습니다.

안녕하세요
f(기)
σ(기) = 0
위의 식을 번역해주세요.

δσ
(cid:0) δw(cid:62)
i
x − δµ(cid:1) + bi) = hi. (7)

δσ
(cid:0) δw(cid:62)
i
x − δµ(cid:1) + bi) = hi. (7)

개별 데이터 포인트의 재조정이 레이어 정규화에서 모델의 예측을 변경하지 않는 것은 쉽게 알 수 있습니다. 레이어 정규화에서 가중치 행렬의 재중심화와 유사하게, 배치 정규화도 데이터셋의 재중심화에 대해 불변성을 보일 수 있습니다.

5.2 학습 중 매개변수 공간의 기하학

우리는 모델의 예측이 매개변수의 재중심화와 재조정에 대해 불변성을 가지는지 조사했습니다. 그러나 학습은 매개변수화에 따라 매우 다르게 동작할 수 있으며, 모델은 동일한 기저 함수를 표현하더라도 다른 매개변수화에서 다른 학습 동작을 보일 수 있습니다. 이 섹션에서는 매개변수 공간의 기하학과 다양체를 통해 학습 동작을 분석합니다. 우리는 정규화 스칼라 σ가 학습 속도를 암묵적으로 감소시키고 학습을 더 안정적으로 만들 수 있다는 것을 보여줍니다.

5.2.1 리만계 측도

통계 모델의 학습 가능한 매개변수는 모델의 모든 가능한 입력-출력 관계로 구성된 매끄러운 매니폴드를 형성합니다. 출력이 확률 분포인 모델의 경우, 이 매니폴드 상에서 두 점의 분리 정도를 측정하는 자연스러운 방법은 Kullback-Leibler 발산입니다. KL 발산 측정 기준에 따르면, 매개변수 공간은 리만 매니폴드입니다.

정리공간의 Riemannian 곡률은 완전히 Riemannian metric에 의해 포착되며, 이의 이차형식은 ds2로 표기된다. 이는 매개변수 공간의 한 점에서 접선 공간에서의 미소 거리를 나타낸다. 직관적으로, 이는 접선 방향을 따라 매개변수 공간에서 모델 출력의 변화를 측정한다. Riemannian metric은 이전에 KL에 따라 연구되었으며 [Amari, 1998] Fisher의 2차 테일러 전개를 사용하여 잘 근사됨을 보였다.

4
정보 행렬:
ds2 = DKL(cid:2) P(y |x; θ)(cid:107)P(y |x; θ + δ)(cid:3) ≈ 1 2δ(cid:62)F(θ)δ, (8)

F(θ) = E
x∼P(x),y∼P(y|x)
∂ logP(y |x; θ)
∂θ
∂ logP(y |x; θ)
∂θ
(cid:62)(cid:35)
,       (9)

δ는 매개변수에 대한 작은 변화입니다. 위의 리만계량학적 측도는 매개변수 공간의 기하학적인 관점을 제시합니다. 리만계량학적 측도에 대한 다음 분석은 정규화 방법이 신경망 훈련에 어떻게 도움이 될 수 있는지에 대한 통찰력을 제공합니다.

5.2.2 정규화된 일반화 선형 모델의 기하학

우리는 일반화된 선형 모델에 대한 기하학적 분석에 초점을 맞추고 있습니다. 다음 분석 결과는 피셔 정보 행렬에 대한 블록 대각 근사를 사용하여 깊은 신경망을 이해하는 데 쉽게 적용할 수 있습니다. 각 블록은 단일 뉴런의 매개변수에 해당합니다.

일반화된 선형 모델(GLM)은 가중치 벡터 w와 편향 스칼라 b를 사용하여 지수 패밀리에서 출력 분포를 매개변수화하는 것으로 볼 수 있습니다. 이전 섹션과 일관성을 유지하기 위해 GLM의 로그 우도는 다음과 같이 합산된 입력 a를 사용하여 작성될 수 있습니다.

logP(y |x; w,b) = 로그 P(y |x; w,b) =

(a + b)y − η(a + b)
φ
+ c(y,φ),        (10)

(a + b)y - η(a + b)φ + c(y,φ), (10)

E[y |x] = f(a + b) = f(w*x + b), Var[y |x] = φf^2(a + b), (11)

f(·)은 신경망의 비선형성에 해당하는 전달 함수이고, f'(·)은 전달 함수의 도함수이다. η(·)은 실수값 함수이고, c(·)은 로그 파티션 함수이다. φ는 출력 분산을 조절하는 상수이다. H-차원 출력 벡터 y = [y1,y2,··· ,yH]는 H개의 독립적인 GLM을 사용하여 모델링되며, logP(y|x; W,b) = (cid:80)H i=1 logP(yi |x; wi,bi)이다. W는 각 GLM의 가중치 벡터가 행으로 구성된 가중치 행렬이고, b는 길이가 H인 편향 벡터를 나타낸다. vec(·)은 크로네커 벡터 연산자를 나타낸다. 매개변수 θ = [w(cid:62) 1 ,b1,··· ,w(cid:62) H ,bH](cid:62) = vec([W,b](cid:62))에 대한 다차원 GLM의 피셔 정보 행렬은 단순히 데이터 특징과 출력 공분산 행렬의 기댓값인 크로네커 곱으로 정의된다.

F(θ) = E
x∼P(x)(cid:20)Cov[y|x]
φ2
⊗
(cid:20) xx(cid:62) x
x(cid:62)
1(cid:21)(cid:21)
.            (12)

F(θ) = E
x∼P(x)(cid:20)Cov[y|x]
φ2
⊗
(cid:20) xx(cid:62) x
x(cid:62)
1(cid:21)(cid:21)
.            (12)

우리는 정규화된 GLM을 원래 모델의 합산 입력 a에 µ와 σ를 통해 정규화 방법을 적용하여 얻습니다. 일반성을 잃지 않고, 우리는 추가 이득 매개변수 θ = vec([W,b,g])를 가진 정규화된 다차원 GLM에서 Fisher 정보 행렬 ¯ F를 나타냅니다.

¯ F(θ) = 
¯ F(θ) =

  
¯ F11 ··· ¯ F1H
... ... ...
¯ FH1 ··· ¯
FHH
  , ¯ Fij = E
x∼P(x)
  Cov[yi, yj |x] φ2


  
¯ F11 ··· ¯ F1H
... ... ...
¯ FH1 ··· ¯
FHH
  , ¯ Fij = E
x∼P(x)
  Cov[yi, yj |x] φ2


기계 학습 알고리즘
지
알고리즘을
시
알고리즘을(평균-분산)
시시 지 시 1 평균-분산 시
알고리즘을(평균-분산) 시시 (평균-분산)(평균-분산) 시시

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.

아래의 문장을 한국어로 번역해주세요. 번역 외에는 작성하지 마세요.

(13) 13번째입니다.

χi = x - ∂µi/∂wi - ai - µi/σi ∂σi/∂wi. (14)

가중치 벡터의 성장을 통한 암묵적 학습률 감소: 표준 GLM과 비교할 때, 가중치 벡터 wi 방향의 블록 ¯ Fij는 이득 매개변수와 정규화 스칼라 σi에 의해 스케일링됨을 주목하십시오. 가중치 벡터 wi의 노름이 두 배로 커져도 모델의 출력은 동일하더라도 피셔 정보 행렬은 다를 것입니다. wi 방향의 곡률은 1의 인자로 변경될 것입니다.

2
σi도 두 배로 커질 것이기 때문입니다. 결과적으로, 정규화된 모델에서 동일한 매개변수 업데이트에 대해 가중치 벡터의 노름은 가중치 벡터의 학습 속도를 효과적으로 제어합니다. 학습 중에는 노름이 큰 가중치 벡터의 방향을 변경하기가 더 어렵습니다. 따라서 정규화 방법은

5
0 50 100 150 200 250 300
반복 x 300
34
35 36 37 38 39 40 41 42
43

이미지 검색 (검증)

주문-임베딩 + LN 주문-임베딩

(a) 리콜@1

0 50 100 150 200 250 300
반복 x 300
71
72 73 74 75 76 77
78

이미지 검색 (검증)

주문-임베딩 + LN 주문-임베딩

(b) 리콜@5

0 50 100 150 200 250 300
반복 x 300
84
85 86 87 88 89
90

이미지 검색 (검증)

주문-임베딩 + LN 주문-임베딩

(c) 리콜@10

그림 1: 레이어 정규화를 사용한 순서 임베딩을 이용한 Recall@K 곡선.

MSCOCO
캡션 검색 이미지 검색
모델 R@1 R@5 R@10 평균 r R@1 R@5 R@10 평균 r

Sym [Vendrov et al., 2016] 45.4 88.7 5.8 36.3 85.8 9.0
OE [Vendrov et al., 2016] 46.7 88.9 5.7 37.9 85.9 8.1
OE (ours)       46.6 79.3 89.1 5.2 37.8 73.6 85.7 7.9
OE + LN         48.5 80.6 89.8 5.1 38.9 74.3 86.3 7.6

표 2: 캡션 및 이미지 검색에 대한 5개의 테스트 분할의 평균 결과. R@K는 K에 대한 리콜을 의미합니다 (높을수록 좋음). 평균 r은 평균 순위를 의미합니다 (낮을수록 좋음). Sym은 대칭 기준선을 나타내며, OE는 순서 임베딩을 나타냅니다.

가중치 벡터에 암묵적인 "조기 중단" 효과를 가지며 학습을 수렴 방향으로 안정화하는 데 도움이 됩니다.

들어오는 가중치의 크기를 학습하는 것: 정규화된 모델에서는 들어오는 가중치의 크기가 명시적으로 이득 매개변수에 의해 매개화됩니다. 우리는 정규화된 GLM에서 이득 매개변수를 업데이트하는 것과 원래 매개화에서 동등한 가중치의 크기를 업데이트하는 것 사이에서 모델 출력이 어떻게 변하는지 비교합니다. ¯ F에서 이득 매개변수를 따라가는 방향은 들어오는 가중치의 크기에 대한 기하학을 포착합니다. 우리는 표준 GLM의 들어오는 가중치의 크기에 대한 리만 메트릭이 입력의 노름에 의해 스케일링된다는 것을 보여줍니다. 반면에 배치 정규화 및 레이어 정규화 모델의 이득 매개변수를 학습하는 것은 예측 오차의 크기에만 의존합니다. 정규화된 모델에서 들어오는 가중치의 크기를 학습하는 것은 따라서 입력과 매개변수의 스케일링에 대해 더 견고합니다. 자세한 유도 내용은 부록을 참조하십시오.

6 실험 결과

우리는 6가지 작업에서 레이어 정규화를 사용하여 실험을 진행합니다. 특히 순환 신경망에 초점을 맞추고 있습니다. 이미지-문장 순위 매기기, 질문-답변, 문맥 언어 모델링, 생성 모델링, 필기 순서 생성 및 MNIST 분류입니다. 그 외에 별도로 언급되지 않는 한, 레이어 정규화의 기본 초기화는 실험에서 적응적인 게인을 1로 설정하고 편향을 0으로 설정하는 것입니다.

6.1 이미지와 언어의 순서 임베딩

이 실험에서는 Vendrov et al. [2016]의 최근 제안된 order-embeddings 모델에 layer normalization을 적용합니다. 이미지와 문장의 공동 임베딩 공간을 학습하기 위해 Vendrov et al. [2016]과 동일한 실험 프로토콜을 따르며, layer normalization을 포함하기 위해 그들의 공개 코드를 수정합니다. 이는 Theano [Team et al., 2016]을 사용합니다. Microsoft COCO 데이터셋 [Lin et al., 2014]에서 이미지와 문장을 공통 벡터 공간에 임베딩하며, GRU [Cho et al., 2014]를 사용하여 문장을 인코딩하고, 사전 훈련된 VGG ConvNet [Simonyan and Zisserman, 2015] (10-crop)의 출력을 사용하여 이미지를 인코딩합니다. order-embedding 모델은 이미지와 문장을 2단계 부분 순서로 나타내며, Kiros et al. [2014]에서 사용된 코사인 유사도 점수 함수를 비대칭 함수로 대체합니다.

1. https://github.com/ivendrov/order-embedding
1. https://github.com/ivendrov/order-embedding

6
0    100  200   300  400  500   600  700  800

훈련 단계 (수천 개)
0.4
0.5
0.6
0.7
0.8
0.9
1.0

유효성 오류율

주의 깊은 독자

LSTM

BN-LSTM

BN-어디든지

LN-LSTM

그림 2: 주의 깊은 독자 모델에 대한 검증 곡선. BN 결과는 [Cooijmans et al., 2016]에서 가져왔습니다.

우리는 두 개의 모델을 훈련시켰습니다: 기준 순서 임베딩 모델과 GRU에 레이어 정규화를 적용한 동일한 모델입니다. 매 300번의 반복마다 유효성 검증 세트에서 Recall@K (R@K) 값을 계산하고 R@K가 향상될 때마다 모델을 저장합니다. 가장 성능이 우수한 모델은 1000개의 이미지와 5000개의 캡션을 포함한 5개의 별도의 테스트 세트에서 평가되며, 평균 결과가 보고됩니다. 두 모델 모두 Adam [Kingma and Ba, 2014]을 사용하며 초기 하이퍼파라미터도 동일하게 설정되었으며, Vendrov et al. [2016]에서 사용된 동일한 구조적 선택사항을 사용하여 훈련되었습니다. GRU에 레이어 정규화가 어떻게 적용되는지에 대한 설명은 부록을 참조하십시오.

그림 1은 레이어 정규화를 사용한 모델과 그렇지 않은 모델의 유효성 곡선을 보여줍니다. 이미지 검색 작업에서 R@1, R@5 및 R@10을 그래프로 나타냅니다. 우리는 모든 지표에서 레이어 정규화가 반복마다 속도 향상을 제공하고 기준 모델이 수행하는 시간의 60%에서 최상의 유효성 모델에 수렴하는 것을 관찰합니다. 테이블 2에서는 테스트 세트 결과를 보고하는데, 여기서 우리는 레이어 정규화가 원래 모델보다 개선된 일반화를 가져온다는 것을 관찰합니다. 우리가 보고하는 결과는 RNN 임베딩 모델에 대한 최첨단 기술입니다. Wang et al. [2016]의 구조 보존 모델만이 이 작업에서 더 나은 결과를 보고합니다. 그러나 그들은 다른 조건에서 (평균 대신 1개의 테스트 세트) 평가되므로 직접 비교할 수 없습니다.

6.2 기계에게 읽고 이해하는 법 가르치기

최근 제안된 순환 배치 정규화 [Cooijmans et al., 2016]와 레이어 정규화를 비교하기 위해, 우리는 Hermann et al. [2015]에 의해 소개된 CNN 말뭉치에서 단방향 주의 리더 모델을 훈련시킵니다. 이는 패시지에 대한 쿼리 설명을 빈칸에 채워 답변해야 하는 질문-답변 작업입니다. 데이터는 엔티티가 비정상적인 해결책을 방지하기 위해 무작위 토큰으로 주어지도록 익명화되며, 훈련 및 평가 중에 일관되게 순열이 바뀝니다. 우리는 Cooijmans et al. [2016]과 동일한 실험 프로토콜을 따르고, Theano [Team et al., 2016]을 사용하는 레이어 정규화를 포함하기 위해 그들의 공개 코드를 수정했습니다. 우리는 Cooijmans et al. [2016]이 사용한 전처리된 데이터셋을 얻었으며, 이는 Hermann et al. [2015]의 원래 실험과 다르게 각 패시지가 4개의 문장으로 제한되었습니다. Cooijmans et al. [2016]에서는 순환 배치 정규화의 두 가지 변형이 사용되는데, 하나는 LSTM에만 BN이 적용되고 다른 하나는 모델 전체에 BN이 적용됩니다. 우리의 실험에서는 LSTM 내에서만 레이어 정규화를 적용합니다.

이 실험의 결과는 그림 2에 나와 있습니다. 우리는 레이어 정규화가 기준선과 BN 변형에 비해 더 빠르게 훈련되고 더 좋은 검증 결과에 수렴한다는 것을 관찰합니다. Cooijmans et al. [2016]에서는 BN의 스케일 매개변수가 신중하게 선택되어야 하며, 그들의 실험에서는 0.1로 설정되었다고 주장합니다. 우리는 1.0 및 0.1 스케일 초기화에 대해 레이어 정규화를 실험해 보았고, 전자 모델이 훨씬 더 우수한 성능을 발휘했습니다. 이는 레이어 정규화가 재귀적인 BN과는 달리 초기 스케일에 민감하지 않음을 보여줍니다. 3

6.3 스킵-생각 벡터

Skip-thoughts [Kiros et al., 2015]은 skip-gram 모델 [Mikolov et al., 2013]의 일반화된 형태입니다. 이 모델은 비지도 학습을 통해 문장의 분산 표현을 학습합니다. 연속된 텍스트가 주어지면, 문장은

2https://github.com/cooijmanstim/Attentive_reader/tree/bn
3우리는 Cooijmans et al. [2016]의 경우와 마찬가지로 검증 세트에서만 결과를 생성합니다.

7
5  10  15 20
반복 x 50000
82.0
82.5 83.0 83.5 84.0 84.5 85.0 85.5
86.0

피어슨 x 100 스킵-스로츠 + LN 스킵-스로츠 오리지널
(a) SICK(r)

5 10 15 20
반복 x 50000
27
28 29 30 31 32 33
34
M S E x 1 0 0 스킵-생각 + LN 스킵-생각 오리지널

아픔(MSE)

5 10 15 20
반복 x 50000
70
72 74 76 78 80
82

정확도 Skip-Thoughts + LN Skip-Thoughts 원본

(c) MR

5 10 15 20
반복 x 50000
74
76 78 80 82
84
86

정확도 Skip-Thoughts + LN Skip-Thoughts 원본

CR

5  10  15 20
반복 x 50000
90.0
90.5 91.0 91.5 92.0 92.5 93.0
93.5
94.0
94.5

정확도 Skip-Thoughts + LN Skip-Thoughts 원본

(e) 주제

5 10 15 20
반복 x 50000
83
84 85 86 87 88
89
90
91

정확도 Skip-Thoughts + LN Skip-Thoughts 원본

(f) MPQA

그림 3: 훈련 반복 횟수에 따른 다운스트림 작업에서 레이어 정규화를 사용한 스킵-생각 벡터의 성능. 원래 선은 [Kiros et al., 2015]에서 보고된 결과입니다. 오류가 있는 그래프는 10-fold 교차 검증을 사용합니다. 색상으로 가장 잘 보입니다.

방법          SICK(r) SICK(ρ) SICK(MSE) MR CR SUBJ MPQA

[Kiros et al., 2015] 0.848 0.778 0.287 75.5 79.3 92.1 86.9

[Kiros et al., 2015] 0.848 0.778 0.287 75.5 79.3 92.1 86.9

우리들            0.842  0.767  0.298 77.3 81.8 92.6 87.9
우리들 + LN       0.854  0.785  0.277 79.5 82.6 93.4 89.0
우리들 + LN †     0.858  0.788  0.270 79.4 83.1 93.7 89.3

표 3: Skip-thoughts 결과. 첫 두 평가 열은 피어슨 상관계수와 스피어만 상관계수를 나타내며, 세 번째는 평균 제곱 오차이고 나머지는 분류 정확도를 나타냅니다. MSE를 제외한 모든 평가에서 높은 값이 더 좋습니다. 저희 모델은 100만 번의 반복 학습을 거쳤으며, (†)는 1개월 동안 약 170만 번의 반복 학습을 거쳤습니다.

인코더 RNN과 디코더 RNN을 사용하여 주변 문장을 예측하는 모델로 인코딩되었습니다.
Kiros et al. [2015]는 이 모델이 세부 조정 없이도 여러 작업에서 잘 수행되는 일반적인 문장 표현을 생성할 수 있다고 보여주었습니다.
하지만, 이 모델을 훈련하는 데에는 시간이 많이 소요되며 의미 있는 결과를 얻기 위해서는 몇 일 동안의 훈련이 필요합니다.

이 실험에서는 레이어 정규화가 훈련 속도를 어떻게 높일 수 있는지를 확인합니다. Kiros et al. [2015]의 공개 코드를 사용하여 BookCorpus 데이터셋 [Zhu et al., 2015]에서 두 개의 모델을 훈련시킵니다. 하나는 레이어 정규화를 사용하고 다른 하나는 사용하지 않습니다. 이 실험은 Theano [Team et al., 2016]을 사용하여 수행됩니다. 우리는 Kiros et al. [2015]에서 사용한 실험 설정을 따르며, 동일한 하이퍼파라미터로 2400차원의 문장 인코더를 훈련시킵니다. 사용된 상태의 크기를 고려하면, 레이어 정규화를 사용하면 반복당 업데이트 속도가 느려질 수 있다고 생각됩니다. 그러나, CNMeM 5를 사용하면 두 모델 간에 유의미한 차이가 없음을 발견했습니다. 우리는 50,000번의 반복마다 두 모델을 체크포인트로 저장하고, semantic-relatedness (SICK) [Marelli et al., 2014], movie review sentiment (MR) [Pang and Lee, 2005], customer product reviews (CR) [Hu and Liu, 2004], subjectivity/objectivity classification (SUBJ) [Pang and Lee, 2004] 및 opinion polarity (MPQA) [Wiebe et al., 2005] 다섯 가지 작업에서 성능을 평가합니다. 모든 작업에 대해 각 체크포인트에서 두 모델의 성능을 그래프로 나타내어 LN으로 성능을 향상시킬 수 있는지 확인합니다.

실험 결과는 그림 3에 설명되어 있습니다. 우리는 레이어 정규화를 적용하면 기준선 대비 속도 향상과 1백만 번의 반복 후 더 나은 최종 결과가 나타남을 관찰합니다. 표 3에 나와 있는 대로. 우리는 또한 레이어 정규화를 적용한 모델을 한 달 동안 훈련시켜 성능을 더욱 향상시켰습니다. 한 가지 작업을 제외하고 모든 작업에서 성능이 향상되었습니다. 우리는 성능을 주목합니다.

4. https://github.com/ryankiros/skip-thoughts
5. https://github.com/NVIDIA/cnmem

4. https://github.com/ryankiros/skip-thoughts
5. https://github.com/NVIDIA/cnmem

8
백
백일
백이
백삼

업데이트 x 200
-900
-800 -700 -600
-500 -400 -300 -200
-1000

부정적
로그 가능도

기준선 테스트

기준선 훈련 LN 테스트 LN 훈련

그림 5: 레이어 정규화를 사용한 필기 순서 생성 모델의 음의 로그 우도와 레이어 정규화를 사용하지 않은 모델의 음의 로그 우도. 모델은 미니 배치 크기 8과 시퀀스 길이 500으로 훈련되었습니다.

원래 보고된 결과와 우리의 결과 사이의 차이는 공개된 코드가 디코더의 각 타임스텝에서 조건을 걸지 않기 때문일 가능성이 높습니다. 반면 원래 모델은 조건을 걸고 있습니다.

6.4 DRAW를 사용하여 이진화된 MNIST 모델링하기

0  20 40 60 80 100
에포크
80
85
90
95
100

시험
변분
경계

기준선
WN
LN

그림 4: 레이어 정규화를 사용한 DRAW 모델 테스트 음의 로그 우도
또한, 우리는 MNIST 데이터셋에서 생성 모델링에 대한 실험도 진행했습니다. Deep Recurrent Attention Writer (DRAW) [Gregor et al., 2015]는 이전에 MNIST 숫자의 분포를 모델링하는 데 있어 최고의 성능을 달성했습니다. 이 모델은 차별적인 어텐션 메커니즘과 순차적으로 이미지 조각을 생성하기 위한 순환 신경망을 사용합니다. 우리는 64개의 시선과 256개의 LSTM 은닉 유닛을 사용한 DRAW 모델에서 레이어 정규화의 효과를 평가했습니다. 이 모델은 Adam [Kingma and Ba, 2014] 옵티마이저의 기본 설정과 미니배치 크기 128로 훈련되었습니다. 이전의 이진화된 MNIST에 대한 출판물들은 데이터셋을 생성하기 위해 다양한 훈련 프로토콜을 사용했습니다. 이 실험에서는 Larochelle과 Murray [2011]의 고정 이진화를 사용했습니다. 데이터셋은 50,000개의 훈련 이미지, 10,000개의 검증 이미지, 10,000개의 테스트 이미지로 분할되었습니다.

그림 4는 처음 100 epoch에 대한 테스트 변분 경계를 보여줍니다. 이는 레이어 정규화를 적용한 DRAW가 기준 모델보다 거의 2배 빠르게 수렴하는 속도 향상 이점을 강조합니다.
200 epoch 이후에는 기준 모델이 테스트 데이터에서 82.36 nats의 변분 로그 우도로 수렴하고, 레이어 정규화 모델은 82.09 nats를 얻습니다.

6.5 필기 순서 생성

이전 실험들은 대부분 길이가 10에서 40 사이인 NLP 작업에서 RNN을 조사했습니다. 더 긴 시퀀스에서 레이어 정규화의 효과를 보여주기 위해, 우리는 IAM 온라인 필기 데이터베이스 [Liwicki and Bunke, 2005]를 사용하여 필기 생성 작업을 수행했습니다. IAM-OnDB는 221명의 다른 작가로부터 수집된 필기 줄들로 구성되어 있습니다. 입력 문자열이 주어지면, 목표는 해당 필기 줄의 x와 y 펜 좌표의 시퀀스를 예측하는 것입니다. 총 12179개의 필기 줄 시퀀스가 있습니다. 입력 문자열은 일반적으로 25자 이상이며, 평균 필기 줄의 길이는 약 700입니다.

우리는 Graves [2013]의 섹션 (5.2)와 동일한 모델 아키텍처를 사용했습니다. 모델 아키텍처는 400개의 LSTM 셀로 이루어진 세 개의 은닉층으로 구성되어 있으며, 출력층에서는 20개의 이변량 가우시안 혼합 구성요소를 생성하고 입력층은 크기 3입니다. 문자열 시퀀스는 원-핫 벡터로 인코딩되었으며, 따라서 윈도우 벡터의 크기는 57입니다. 윈도우 매개변수에는 10개의 가우시안 함수 혼합이 사용되어 크기 30의 매개변수 벡터가 필요합니다. 총 가중치 수는 약 3.7M으로 증가되었습니다. 모델은 크기 8의 미니 배치를 사용하여 Adam [Kingma and Ba, 2014] 옵티마이저를 사용하여 훈련되었습니다.

작은 미니 배치 크기와 아주 긴 시퀀스의 조합은 매우 안정적인 숨겨진 동적을 가져야 하는 중요성을 갖습니다. 그림 5는 레이어 정규화가 기준 모델과 비교 가능한 로그 우도에 수렴하지만 훨씬 빠르다는 것을 보여줍니다.

9
0 10 20 30 40 50 60
에포크
10-7
10-6
10-5
10-4 10-3
10-2
10-1
100

기차 NLL

배치 정규화 bz128

베이스라인 bz128 레이어 정규화 bz128

0 10 20 30 40 50 60
에포크
0.005
0.010
0.015
0.020
0.025

테스트 에러.

BatchNorm bz128 기준선 bz128
LayerNorm bz128
0  10 20 30 40 50 60
에포크
10-7
10-6
10-5
10-4 10-3
10-2
10-1
100

기차 NLL

레이어 정규화 bz4

베이스라인 bz4 배치노말라이제이션 bz4

0  10 20 30 40 50 60
에포크
0.005
0.010
0.015
0.020
0.025

테스트 에러.

LayerNorm bz4 기준선 bz4
BatchNorm bz4

그림 6: 순열 불변 MNIST 784-1000-1000-10 모델의 음의 로그 우도와 테스트 오류, 레이어 정규화 및 배치 정규화와 함께. (왼쪽) 모델은 배치 크기 128로 훈련됩니다. (오른쪽) 모델은 배치 크기 4로 훈련됩니다.

6.6 순열 불변 MNIST

RNN 이외에도, 우리는 feed-forward 네트워크에서 layer normalization을 조사했습니다. 우리는 잘 연구된 순열 불변 MNIST 분류 문제에서 layer normalization이 batch normalization과 비교되는 방식을 보여줍니다. 이전 분석에서, layer normalization은 입력 재조정에 불변하며, 이는 내부 은닉층에 대해 바람직합니다. 그러나 이는 로짓 출력에는 불필요하며, 예측 신뢰도는 로짓의 스케일에 의해 결정됩니다. 우리는 layer normalization을 마지막 softmax 층을 제외한 완전히 연결된 은닉층에만 적용합니다.

모든 모델은 55000개의 훈련 데이터 포인트와 Adam [Kingma and Ba, 2014] 옵티마이저를 사용하여 훈련되었습니다. 작은 배치 크기의 경우, 배치 정규화의 분산 항은 편향되지 않은 추정치를 사용하여 계산됩니다. Figure 6의 실험 결과는 레이어 정규화가 배치 크기에 강건하며 모든 레이어에 적용되는 배치 정규화와 비교하여 더 빠른 훈련 수렴을 보여줍니다.

6.7 합성곱 신경망

우리는 합성곱 신경망에 대해서도 실험을 진행해 보았습니다. 예비 실험에서 우리는 레이어 정규화가 정규화 없는 기준 모델보다 속도를 향상시키는 것을 관찰했지만, 배치 정규화가 다른 방법들보다 우수한 성능을 보였습니다. 완전 연결층에서는, 한 층의 모든 은닉 유닛들이 최종 예측에 유사한 기여를 하는 경향이 있으며, 한 층의 합산 입력을 재센터링하고 재스케일링하는 것이 잘 작동합니다. 그러나, 합성곱 신경망에서는 유사한 기여를 하는 가정이 더 이상 참이 아닙니다. 이미지 경계에 위치한 수많은 은닉 유닛들은 거의 활성화되지 않으며, 따라서 동일한 층 내의 다른 은닉 유닛들과는 매우 다른 통계를 가지고 있습니다. 우리는 레이어 정규화가 ConvNets에서 잘 작동하려면 추가적인 연구가 필요하다고 생각합니다.

7 결론

이 논문에서는 신경망의 훈련 속도를 높이기 위해 레이어 정규화를 소개했습니다. 레이어 정규화의 불변성 특성을 배치 정규화와 가중치 정규화와 비교하는 이론적 분석을 제공했습니다. 우리는 레이어 정규화가 훈련 사례 특성의 이동과 스케일링에 대해 불변성을 가지는 것을 보였습니다.

경험적으로 우리는 재귀 신경망이 제안된 방법에서 가장 큰 이점을 얻는다는 것을 보였다. 특히 긴 시퀀스와 작은 미니 배치에 대해서는 더욱 그렇다.

감사의 말씀

이 연구는 NSERC, CFI 및 Google의 장학금으로 지원되었습니다.

10
참고문헌

알렉스 크리즈헤브스키, 일리야 숫크에버, 그리고 제프리 힌튼. 딥 컨볼루션 신경망을 이용한 이미지넷 분류. NIPS, 2012.

Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath 등. 음성 인식에서 음향 모델링을 위한 심층 신경망: 네 개의 연구 그룹의 공유된 견해. IEEE, 2012.

제프리 딘, 그렉 코라도, 라자트 몽가, 카이 첸, 마티유 데빈, 마크 마오, 앤드류 세니어, 폴 터커, 케 양, 쿼크 르 등. 대규모 분산 딥 네트워크. NIPS, 2012.

세르게이 이오프와 크리스티안 제게디. 배치 정규화: 내부 공변량 변화를 줄여 깊은 신경망 훈련 가속화. ICML, 2015.

이리야 수츠케버, 오리올 비냐르스, 그리고 쿼크 브이 레. 신경망을 이용한 시퀀스 대 시퀀스 학습. Advances in neural information processing systems, 2014, 3104-3112쪽.

C´ esar Laurent, Gabriel Pereyra, Phil´ emon Brakel, Ying Zhang, and Yoshua Bengio. 배치 정규화된 순환 신경망. arXiv 사전 인쇄 arXiv:1510.01378, 2015.

다리오 아모데이, 리시타 아누바이, 에릭 배튼버그, 칼 케이스, 제어드 캐스퍼, 브라이언 카탄자로, 징동 천,
마이크 크잔로스키, 아담 코츠, 그레그 디아모스 등. 딥 스피치 2: 영어와 중국어의 엔드 투 엔드 음성 인식. arXiv 사전 인쇄 arXiv:1512.02595, 2015.

팀 쿠이맨스, 니콜라스 발라스, 세자르 로랑, 그리고 아론 쿠르빌. 반복 배치 정규화. arXiv 사전 인쇄 arXiv:1603.09025, 2016.

팀 살리만스와 디에더릭 피 킹마. 가중치 정규화: 깊은 신경망의 훈련 가속화를 위한 간단한 재매개화. arXiv 사전 인쇄 arXiv:1602.07868, 2016.

Behnam Neyshabur, Ruslan Salakhutdinov, and Nati Srebro. Path-sgd: 깊은 신경망에서 경로 정규화된 최적화. Advances in Neural Information Processing Systems, 페이지 2413-2421, 2015.

쇤이치 아마리. 자연 그래디언트는 학습에 효과적으로 작용합니다. 신경 계산, 1998.

이반 벤드로프, 라이언 키로스, 산자 피들러, 그리고 라켈 우르타순. 이미지와 언어의 순서 임베딩. ICLR, 2016.

The Theano Development Team, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller,
DzmitryBahdanau,NicolasBallas,Fr´ ed´ ericBastien,JustinBayer,AnatolyBelikov,etal. Theano: Apython
framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.

Theano 개발팀, Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller,
DzmitryBahdanau,NicolasBallas,Fr´ ed´ ericBastien,JustinBayer,AnatolyBelikov,etal. Theano: 수학식의 빠른 계산을 위한 파이썬 프레임워크. arXiv 사전 인쇄 arXiv:1605.02688, 2016.

쯔옹이 린, 마이클 마이어, 세르지 벨롱지, 제임스 헤이스, 피에트로 페로나, 데바 라마난, 피오트르 돌라르, 그리고 C 로렌스 지트닉. 마이크로소프트 코코: 문맥 속의 공통 객체. ECCV, 2014.

경현 조, 바트 반 메리엔버, 카글라르 굴체레, 드미트리 바드하나우, 페티 부가레, 홀거 슈벵, 그리고 요슈아 벤지오. 통계 기계 번역을 위한 RNN 인코더-디코더를 사용한 구문 표현 학습. EMNLP, 2014.

Karen Simonyan과 Andrew Zisserman. 대규모 이미지 인식을 위한 매우 깊은 합성곱 신경망. ICLR, 2015.

라이언 키로스, 루슬란 살라후딘노프, 그리고 리처드 S 제멜. 다중 모달 신경 언어 모델을 사용한 시각-의미 임베딩 통합. arXiv 사전 인쇄 arXiv:1411.2539, 2014.

D. Kingma와 J. L. Ba. Adam: 확률적 최적화를 위한 방법. ICLR, 2014. arXiv:1412.6980.

리웨이 왕, 인 리, 스베틀라나 라제브닉. 깊은 구조 보존 이미지-텍스트 임베딩 학습. CVPR, 2016.

칼 모리츠 헤르만, 토마스 코치스키, 에드워드 그레펜스테트, 라세 에스페홀트, 윌 케이, 무스타파 술레이만, 필 블런솜. 기계에게 읽고 이해하는 법 가르치기. NIPS, 2015.

RyanKiros, YukunZhu, RuslanRSalakhutdinov, RichardZemel, RaquelUrtasun, AntonioTorralba, and Sanja Fidler. Skip-thought vectors. In NIPS, 2015.
라이언 키로스, 유쿤 주, 루슬란 R. 살라후딘노프, 리처드 제멜, 라퀼 우르타선, 안토니오 토랄바, 그리고 산자 피들러. 스킵-생각 벡터. NIPS, 2015.

토마스 미콜로프, 카이 첸, 그렉 코라도, 제프리 딘. 벡터 공간에서 단어 표현의 효율적인 추정. arXiv 사전 인쇄 arXiv:1301.3781, 2013.

유쿤 주, 라이언 키로스, 리치 제멜, 루슬란 살라후트디노프, 라퀼 우르타순, 안토니오 토랄바, 그리고 산자 피들러. 책과 영화의 조화: 영화 시청과 책 읽기를 통한 이야기 같은 시각적 설명을 위해. ICCV, 2015.

Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli.
Semeval-2014 과제 1: 문맥적 분포 의미 모델의 문장 전체에 대한 의미 관련성과 텍스트 함의성 평가. SemEval-2014, 2014.

11
Bo Pang과 Lillian Lee. 별을 보다: 등급 척도에 대한 감성 분류를 위한 클래스 관계의 활용. ACL에서, 페이지 115-124, 2005년.

민칭 후와 빙 리우. 고객 리뷰의 채굴과 요약. 지식 발견과 데이터 마이닝에 관한 제10회 ACM SIGKDD 국제 컨퍼런스 논문집, 2004년.

보 팡과 릴리안 리. 감성 교육: 최소 절단을 기반으로 한 주관성 요약을 사용한 감성 분석. ACL, 2004.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. 언어에서 의견과 감정의 표현을 주석으로 달기. 언어 자원과 평가, 2005.

K. Gregor, I. Danihelka, A. Graves, 그리고 D. Wierstra. DRAW: 이미지 생성을 위한 순환 신경망.
arXiv:1502.04623, 2015.

휴고 라로셀과 아이언 머레이. 신경 자기 회귀 분포 추정기. AISTATS에서, 6권, 622쪽, 2011년.

마르쿠스 리위키와 호르스트 분케. Iam-ondb는 손글씨로 쓴 텍스트를 기반으로 한 온라인 영어 문장 데이터베이스입니다. ICDAR, 2005에서 소개되었습니다.

알렉스 그레이브스. 순환 신경망을 사용하여 시퀀스 생성하기. arXiv 사전 인쇄물 arXiv:1308.0850, 2013.

12
보충 자료

각 실험에 레이어 정규화 적용

이 섹션에서는 각 논문의 실험에 레이어 정규화가 적용되는 방법에 대해 설명합니다. 표기의 편의를 위해, 우리는 레이어 정규화를 함수로 정의하며, 이 함수는 두 개의 적응형 매개변수인 gains α와 biases β와 매핑하는 함수 LN : RD → RD로 정의합니다.

LN(z;α,β) = (z − µ) / σ√(2π) α + β

µ = 1/D^2

i=1
zi, σ = 
평균 µ와 표준편차 σ를 가지는
i=1(zi − µ)2,         (16)

zi는 벡터 z의 i번째 요소입니다.

기계에게 읽고 이해하며 필기 순서 생성을 가르치는 것

이 실험에 사용된 기본 LSTM 방정식은 다음과 같습니다.

아무도 와서 물어보지 않았어요.
그녀는 항상 웃고 있어요.
나는 한국 음식을 좋아해요.
우리는 함께 여행하고 싶어요.
오늘은 날씨가 좋아요.
나는 영화를 보는 것을 좋아해요.
그는 매일 운동을 해요.
나는 음악을 듣는 것을 좋아해요.
이 책은 정말 재미있어요.
나는 친구들과 함께 시간을 보내는 것을 좋아해요.


ft
it
ot
gt


= 이전 시간 단계의 Whht + 현재 시간 단계의 Wxxt + b             (17)

ct = σ(ft) * ct−1 + σ(it) * tanh(gt)
ht = σ(ot) * tanh(ct)

레이어 정규화를 포함한 버전은 다음과 같이 수정되었습니다.

아무도 와서 물어보지 않았어요.
그녀는 항상 웃고 있어요.
나는 한국 음식을 좋아해요.
우리는 함께 여행하고 싶어요.
오늘은 날씨가 좋아요.
나는 영화를 보는 것을 좋아해요.
그는 매일 운동을 해요.
나는 음악을 듣는 것을 좋아해요.
이 책은 정말 재미있어요.
나는 친구들과 함께 시간을 보내는 것을 좋아해요.


ft
it
ot
gt


= LN(이전 은닉 상태;α1,β1) + LN(현재 입력;α2,β2) + b (20)

ct = σ(ft) * ct−1 + σ(it) * tanh(gt)
ht = σ(ot) * tanh(LN(ct;α3,β3))

αi, βi는 각각 덧셈 및 곱셈 매개 변수입니다. 각 αi는 0 벡터로 초기화되고 각 βi는 1 벡터로 초기화됩니다.

주문 임베딩과 스킵-생각

이 실험들은 다음과 같이 정의된 게이트된 순환 유닛의 변형을 활용합니다.

(cid:18)
zt
rt(cid:19)
=  Whht−1 + Wxxt                 (23)

ˆ ht = tanh(Wxt + σ(rt) ⊙ (Uht−1)) (24)
ht = (1 − σ(zt))ht−1 + σ(zt)ˆ ht   (25)

레이어 정규화는 다음과 같이 적용됩니다.

= LN(이전 hidden state;α1,β1) + LN(현재 input;α2,β2)    (26)

ˆ ht = tanh(LN(Wxt;α3,β3) + σ(rt) (cid:12) LN(Uht−1;α4,β4)) (27)
ht = (1 − σ(zt))ht−1 + σ(zt)ˆ ht          (28)

ˆ ht = tanh(LN(Wxt;α3,β3) + σ(rt) (cid:12) LN(Uht−1;α4,β4)) (27)
ht = (1 − σ(zt))ht−1 + σ(zt)ˆ ht          (28)

이전과 마찬가지로, αi는 0 벡터로 초기화되고 각 βi는 1 벡터로 초기화됩니다.

13
DRAW를 사용하여 이진화된 MNIST 모델링

이 실험에서는 레이어 정규화가 LSTM 은닉 상태의 출력에만 적용됩니다.

레이어 정규화를 포함한 버전은 다음과 같이 수정되었습니다.

아무도 와서 물어보지 않았어요.
그녀는 항상 웃고 있어요.
나는 한국 음식을 좋아해요.
우리는 함께 여행하고 싶어요.
오늘은 날씨가 좋아요.
나는 영화를 보는 것을 좋아해요.
그는 매일 운동을 해요.
나는 음악을 듣는 것을 좋아해요.
이 책은 정말 재미있어요.
나는 친구들과 함께 시간을 보내는 것을 좋아해요.


ft
it
ot
gt


= 이전 시간 단계의 Whht + 현재 시간 단계의 Wxxt + b             (29)

ct = σ(ft) (cid:12) ct−1 + σ(it) (cid:12) tanh(gt) (30)
ht = σ(ot) (cid:12) tanh(LN(ct;α,β)) (31)
α, β는 각각 덧셈 및 곱셈 매개 변수입니다. α는 0 벡터로 초기화되고, β는 1 벡터로 초기화됩니다.

들어오는 가중치의 크기를 배우는 것

우리는 이제 정규화된 GLM과 원래의 매개변수화 사이에서 등가 가중치의 변화량을 비교합니다. 가중치의 크기는 정규화된 모델에서 이득 매개변수를 사용하여 명시적으로 매개변수화됩니다. 가중치 벡터의 노름을 δg만큼 변경하는 그래디언트 업데이트가 있다고 가정해 봅시다. 우리는 그래디언트 업데이트를 일반 GLM의 가중치 벡터에 투영할 수 있습니다. 정규화된 모델의 KL 지표, 즉 그래디언트 업데이트가 모델 예측을 얼마나 변경하는지는 예측 오차의 크기에만 의존합니다. 구체적으로는,

배치 정규화를 사용하면:

ds2 = 
1
2
vec([0,0,δg])(cid:62) ¯ F(vec([W,b,g])(cid:62))vec([0,0,δg]) =

1. 1
2. 2δ
3. g
4. E
5. x∼P(x) Cov[y|x]
6. φ2
7. δg.

(32)
(32)

하위 계층 정규화:

ds2 = 1
2
vec([0,0,δg])의 F 벡터 = vec([W,b,g])vec([0,0,δg])

=1 2δ(cid:62) g 1 φ2 E
x∼P(x)
  
Cov(y1,y1 |x)(a1−µ)2
σ2
··· Cov(y1,yH |x)(a1−µ)(aH−µ)

=1 2δ(cid:62) g 1 φ2 E
x∼P(x)
  
Cov(y1,y1 |x)(a1−µ)2
σ2
··· Cov(y1,yH |x)(a1−µ)(aH−µ)

σ2
...       ...        ...
Cov(yH,y1 |x)(aH−µ)(a1−µ)

σ2
··· Cov(yH,yH |x)(aH−µ)2

σ2
··· Cov(yH,yH |x)(aH−µ)2

σ2

  δg
아래의 문장을 한국어로 번역해주세요.

(33)
(33)

체중 정규화를 적용하면:

ds2 = 1
2
vec([0,0,δg])의 F 벡터 = vec([W,b,g])vec([0,0,δg])

=1 2δ(cid:62)
g
1
φ2
E
x∼P(x)
 


Cov(y1,y1 |x) a2 1
(cid:107)w1(cid:107)2
2
··· Cov(y1,yH |x) a1aH

=1 2델타 크게
g
1
파이2
E
x∼P(x)
 


Cov(y1,y1 |x) a2 1
케이w1케이2
2
··· Cov(y1,yH |x) a1aH

(cid:107)w1(cid:107)2(cid:107)wH(cid:107)2
...       ...       ...
Cov(yH,y1 |x) aHa1
(cid:107)wH(cid:107)2(cid:107)w1(cid:107)2
···  Cov(yH,yH |x) a2 H

(cid:107)wH(cid:107)2
2


 

δg. (34)

 

δg. (34)

그러나 표준 GLM에서 KL 지표는 활동 ai = w와 관련이 있습니다.

나는
x, 그것은 의존한다
현재 가중치와 입력 데이터에. 우리는 기울기 업데이트를 이용하여 이진 뉴런의 이득 매개 변수 δgi에 투영합니다
i번째 뉴런의 가중치 벡터로 δgi wi

표준 GLM 모델에서:

1. 1
2. 2
벡터([δgi
wi
행렬([wi^2, δgj
wj
wj^2])(>)F([w
i
,bi,w
j
,bj])(>)벡터([δgi
wi
행렬([wi^2, δgj
wj
wj^2]))
=δgiδgj
2φ2
E
x∼P(x)(>)Cov(yi,yj |x)
aiaj (wi^2)(wj^2)

(35)
35) 이 책은 내가 읽은 가장 재미있는 책 중 하나였다.

배치 정규화와 레이어 정규화 모델은 표준 모델보다 입력과 파라미터의 스케일링에 대해 더 견고합니다.

14

