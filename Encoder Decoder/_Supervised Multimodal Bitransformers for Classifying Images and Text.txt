이미지와 텍스트를 분류하기 위한 감독형 다중 모달 비트랜스포머

Douwe Kiela†, Suvrat Bhooshan†, Hamed Firooz†, Ethan Perez‡, Davide Testuggine†

도우 키엘라†, 수브랏 부산†, 하메드 피루즈†, 이단 페레즈‡, 다비데 테스투지네†

†페이스북 인공지능; ‡뉴욕 대학교
{dkiela, sbh, mhfirooz, davidet}@fb.com, perez@nyu.edu

요약

셀프-지도학습 양방향 트랜스포머 모델인 BERT는 다양한 텍스트 분류 작업에서 극적인 개선을 이끌어냈습니다. 하지만 현대의 디지털 세계는 점점 다중모달이 되고 있으며, 텍스트 정보와 함께 이미지와 같은 다른 모달리티가 자주 동반됩니다. 우리는 다중모달 BERT와 유사한 아키텍처를 위한 간단하면서도 효과적인 기준선을 소개합니다. 이는 텍스트와 이미지 인코더를 유니모달로 사전학습한 후, 이미지 임베딩을 텍스트 토큰 공간으로 투영하여 함께 미세조정하는 지도학습 다중모달 이중트랜스포머입니다. 우리의 방법은 다중모달 성능을 측정하기 위해 특별히 설계된 어려운 테스트 세트를 포함하여 여러 텍스트 중심의 다중모달 분류 작업에서 최신 정확도에 근접하거나 능가하는 성과를 보입니다. 놀랍게도, 우리의 방법은 자기지도학습 다중모달 "비전과 언어를 위한 BERT"인 ViLBERT와 경쟁력을 유지하면서 훨씬 간단하고 확장성이 높습니다.

1 소개

현대 디지털 세계에서 우리가 직면하는 많은 분류 문제들은 다중모드적인 성격을 가지고 있다. 웹상의 텍스트 정보는 거의 혼자 나타나지 않고 종종 이미지, 소리, 비디오 또는 다른 모드와 함께 나타난다. BERT (Devlin et al., 2019)와 같은 자연어 처리를 위한 표현 학습의 최근 발전은 텍스트만을 다루는 분류 문제에서 극적인 개선을 이끌어냈다. BERT의 성공을 따라서, ViLBERT (Lu et al., 2019), VisualBERT (Li et al., 2019), LXMERT (Tan and Bansal, 2019), VL-BERT (Su et al., 2019) 및 여러 다른 다중모드 아키텍처들이 제안되었는데, 이들은 손에 있는 다중모드 작업에 대한 사전학습이나 대리 다중모드 작업을 통한 세밀조정을 주장한다.

이 작업에서는 BERT와 유사한 다중모달 아키텍처에 대해 간단하지만 매우 효과적인 기준선 아키텍처를 설명합니다. 우리는 단일모달 사전훈련 구성요소를 가진 지도형 양방향 트랜스포머가 다중모달 퓨전을 수행하는 데 우수하며 다양한 대체 퓨전 기술보다 우수한 성능을 보여준다는 것을 입증합니다. 더욱이, 우리는 다중모달 사전훈련된 ViLBERT 모델과 경쟁력 있는 성능을 보이며 다양한 다중모달 분류 작업에서 이를 능가할 수 있다는 것을 발견합니다.
우리가 제안하는 접근법은 여러 가지 이점을 제공합니다. 단일모달 사전훈련된 모델은 단순하며 단일모달 발전에 적응하기 쉽습니다. 즉, 텍스트 또는 이미지 인코더를 더 나은 대안으로 교체하고 직접 세밀하게 조정할 수 있으며 다중모달 재훈련이 필요하지 않습니다. 더욱이, 우리의 방법은 특정한 특징 추출 파이프라인에 의존하지 않으며, 예를 들어 영역 또는 경계 상자 제안을 필요로하지 않으며, 모달에 중립적입니다. 밀집 벡터의 임의의 시퀀스에 대해 작동합니다. 따라서 이미지의 원시 특징을 계산하는 데 사용할 수 있으며, 사전 추출 대신 전체 인코더를 통해 역전파 할 수 있습니다.
구체적으로, 우리의 모델은 BERT-first이며, 밀집 다중모달 특징을 BERT의 토큰 임베딩 공간으로 매핑하는 방법을 학습합니다. 우리는 이 접근법이 텍스트 중심의 다중모달 분류 작업인 MM-IMDB (Arevalo et al., 2017), Food101 (Wang et al., 2015) 및 V-SNLI (Vu et al., 2018)에서 잘 작동하는 것을 보여줍니다. 이러한 작업을 평가하는 것은 여러 가지 이점을 제공합니다. 인터넷 데이터의 많은 실제 다중모달 작업은 텍스트가 주요 모달리티이고 단일 분류 레이블을 예측하는 것이 목표인 경우가 많으며, 질문에 대답하는 것이 목표가 아닙니다. 중요한 것은 VQA (Antol et al., 2015)와 달리 이러한 작업이 다중모달 트랜스포머 문헌에서 아직 충분히 연구되지 않았다는 것입니다. 따라서 우리의 연구는 VQA에서의 다중모달 발전이 다른 작업에도 적용되는지 확인할 수 있습니다.

→
w1 토큰 w2 w3 w4 w5 w6 w7 w8 I1 I2 I3 I4 I5 I6 I7
0 위치 1 2 3 4 5 6 7  0   1  2  3   4  5  6
0 세그먼트 0 0 0 0 0 0  0  1   1  1   1  1  1
트랜스포머

레이블 1
레이블 2
레이블 3

그림 1: 다중 모달 비트랜스포머 아키텍처의 그림 설명.

이러한 작업들. 마침내, 다중모달 모델의 원하는 특성은 고품질의 다중모달 정보가 제공되는 경우에 개선된 성능을 보여줍니다. 즉, 전체는 엄격하게 각 부분의 합보다 우수한 성능을 발휘해야 합니다. 우리는 이러한 작업들을 사용하여 시스템의 다중모달 성능을 측정하기 위해 특별히 설계된 새로운 어려운 테스트 세트를 구성합니다. 이 테스트 세트는 단일모달 방법으로 올바르게 분류하지 못하는 예제들로 이루어져 있습니다.

우리의 연구 결과는 제안된 지도학습 다중모달 이진 변환 모델이 다양한 경쟁적인 퓨전 기술보다 우수한 성능을 보여준다는 것을 나타냅니다. 심지어 더 많은 매개변수를 제공해도 그렇습니다. 우리는 이것이 다중모달 이진 변환 모델이 양 모달에 대해 동시에 자기-주의를 사용하여 이전보다 더 일찍 그리고 더 세밀한 다중모달 퓨전을 제공할 수 있는 능력 때문이라고 주장합니다. 우리는 우리의 간단한 방법이 우리의 작업에서 다중모달로 사전 훈련된 ViLBERT 모델과 비슷한 성능을 보인다는 것을 발견했습니다. 다시 말해, 다중모달 사전 훈련 없이도 다중모달로 사전 훈련된 모델의 성능을 맞출 수 있습니다. 이 결과들은 제안된 방법이 다중모달 분류에 대한 미래 연구의 강력한 기준선을 형성한다는 것을 보여줍니다. 왜냐하면 이 방법은 구현하기 간단하며, 다른 모달이나 다른 인코더로 확장하기 쉽고, 더 정교한 방법들과 경쟁력 있는 성능을 발휘합니다.

2개의 다중 모달 비트랜스포머

There is a long history, both in natural language processing and computer vision, of transfer learning from pre-trained representations. Self-supervised word and sentence embeddings (Collobert and Weston, 2008; Mikolov et al., 2013; Le and Mikolov, 2014) have been successfully used as a starting point for various downstream tasks.

스톤, 2008; Mikolov et al., 2013; Kiros et al., 2015)은 자연어 처리에서 널리 사용되고 있다. 컴퓨터 비전에서는 지도 학습된 ImageNet 특징들을 전이하는 것이 사실상의 표준이다 (Oquab et al., 2014; Razavian et al., 2014).
NLP에서의 지도 학습 데이터는 범용 문장 표현에도 유용하게 사용되었지만 (Conneau et al., 2017), 자기 지도 학습 언어 모델링 시스템의 개념에 의해 혁신적인 발전을 이루었다 (Dai and Le, 2015). 언어 모델링은 시스템이 문맥화된 방식으로 임베딩을 학습할 수 있게 하여 다양한 작업에서 성능을 향상시킨다 (Peters et al., 2018; Howard and Ruder, 2018). 대량의 데이터에 대해 transformers (Vaswani et al., 2017)를 훈련시키면 더 좋은 결과를 얻을 수 있다 (Radford et al., 2018).
BERT (Devlin et al., 2019)는 transformers를 양방향으로 훈련시키고 (우리는 이를 bitransformers라고 부른다) 목적을 마스킹으로 변경하여 많은 작업에서 최첨단 성능을 달성하였다.
우리는 자연어 처리의 자기 지도 학습 표현과 컴퓨터 비전의 최첨단 합성곱 신경망 구조를 결합한 직관적이면서도 매우 효과적인 다중 모달 bitransformer 모델을 소개한다. 아키텍처에 대한 그림은 Figure 1을 참조하라. 이어지는 내용에서는 더 자세한 설명을 제공한다.

2.1 이미지 인코더

컴퓨터 비전에서는 사전 훈련된 합성곱 신경망의 최종 완전 연결 레이어를 전이하는 것이 일반적입니다 (Razavian et al., 2014). 데이터셋 소스 유형 훈련 개발 테스트 입력 개수 클래스 개수

MM-IMDB (Arevalo et al., 2017) 다중 레이블 15552 2608 7799 2 23
FOOD101 (Wang et al., 2015) 다중 클래스 60101 5000 21695 2 101
V-SNLI (Vu et al., 2018) 다중 클래스 545620 9842 9842 3 3

테이블 1: 성능 평가에 사용되는 평가 과제.

출력은 종종 풀링 작업의 결과입니다. 그러나 다중 모달 비트랜스포머의 경우, 이러한 풀링은 필요하지 않습니다. 왜냐하면 임의의 개수의 밀집 입력을 처리할 수 있기 때문입니다. 따라서 우리는 풀링이 하나의 단일 출력 벡터가 아닌 N개의 별도의 이미지 임베딩을 생성하도록 실험합니다. 이는 일반적인 합성곱 신경망과는 달리 ResNet-152 (He et al., 2016)를 사용합니다. 이미지에서 K×M 그리드를 통해 평균 풀링을 수행하여 N = KM 개의 2048 차원 출력 벡터를 생성합니다. 이미지는 크기 조정, 중앙 자르기 및 정규화됩니다.

2.2 다중 모달 트랜스포머 입력 레이어

우리는 사전 훈련된 BERT 가중치로 초기화된 양방향 트랜스포머 모델을 사용합니다. 이 아키텍처는 문맥 임베딩을 입력으로 사용하며, 각 문맥 임베딩은 별도의 D-차원 세그먼트, 위치 및 토큰 임베딩의 합으로 계산됩니다. 우리는 각 N개의 이미지 임베딩을 D-차원 토큰 입력 임베딩 공간으로 투영하기 위해 Wn ∈ RP×D 가중치를 학습합니다.

나는 W nf(img,n)와 같다. (1)

f(·,n)은 이미지 인코더의 마지막 풀링 작업의 n번째 출력입니다.

단일 텍스트와 단일 이미지 입력으로 구성된 작업의 경우, 텍스트에는 하나의 세그먼트 ID를 할당하고 이미지 임베딩에는 다른 세그먼트 ID를 할당합니다. 우리는 0부터 세기 시작하는 0-인덱스 위치 부호화를 사용합니다. 아키텍처는 임의의 수의 모달리티에 대해 직관적으로 일반화될 수 있습니다. 이를 V-SNLI 작업에 대해 보여줍니다. V-SNLI 작업은 세 개의 입력으로 구성됩니다. 사전 훈련된 BERT 자체는 두 개의 세그먼트 임베딩만 가지고 있으므로, 이러한 경우에는 추가적인 세그먼트 임베딩을 다음과 같이 초기화합니다. 여기서 s i 는 i ≥ 2에 대한 세그먼트 임베딩이고 (cid:15) ∼ N(0,1e−2)입니다. 우리의 방법은 각 예제에 모든 모달리티가 존재하지 않는 시나리오와 호환됨을 유의하십시오 (즉, 텍스트만 있는 경우 또는 이미지만 있는 경우).

2.3 분류

우리는 최종 레이어의 첫 번째 출력을 입력으로 사용합니다.
분류 레이어 clf(x) = Wx + b를 사용하여
W ∈ RD×C, 여기서 D는 변환기 차원이고 C는 클래스의 수입니다. 다중 레이블 작업의 경우, 여러 개의 올바른 답이 있을 수 있으므로
로짓에 시그모이드를 적용하고 각 출력 클래스에 대해 이진 교차 엔트로피 손실로 훈련합니다 (추론 시에는 임계값을 0.5로 설정합니다). 다중 클래스 작업의 경우 로짓에 소프트맥스를 적용하고 정규 교차 엔트로피 손실로 훈련합니다.

2.4 사전 훈련

이미지 인코더는 ImageNet에서 사전 훈련되었습니다 (Deng et al., 2009). 우리는 ResNet-152 (He et al., 2016) 구현과 가중치를 PyTorch (Paszke et al., 2017)의 torchvision을 통해 사용합니다. 우리는 BERT (Devlin et al., 2019)를 위해 사전 훈련된 12개 레이어 768차원의 base-uncased 모델을 사용하며, 이는 영어 위키피디아에서 훈련되었습니다.

2.5 세밀 조정

우리의 아키텍처는 사전 훈련된 구성 요소와 무작위로 초기화된 구성 요소의 혼합으로 이루어져 있습니다. NLP에서는 BERT가 일반적으로 전체적으로 세밀하게 조정되며, 고정 매개변수로 인코더로 전달되지 않습니다. 예를 들어 SkipThought (Kiros et al., 2015)와 InferSent (Conneau et al., 2017)의 경우와 같았던 것과 달리. 컴퓨터 비전에서는 합성곱 신경망이 일반적으로 고정되어 있습니다 (Razavian et al., 2014), 하지만 합성곱 신경망을 훈련의 나중 단계에서 해제하면 이미지 캡션 검색 (Faghri et al., 2017)에서 유의한 개선이 있음이 밝혀졌습니다.
다중모달 최적화는 간단하지 않습니다 (Wang et al., 2019). 우리의 모델에서는 이미지 임베딩을 BERT의 토큰 공간으로 매핑하기 위해 일련의 무작위로 초기화된 매핑 W n을 사용합니다. 여기서 우리는 다중 모달 간 최적화에 대한 간단한 해결책을 탐구합니다. 즉, 다른 단계에서 인코딩 구성 요소를 고정 및 해제하는 것을 하이퍼파라미터로 취급합니다. 우리가 먼저 이미지 임베딩을 텍스트 인코더의 입력 공간의 적절한 부분 공간으로 매핑하는 것을 배우면 네트워크가 개선될 것으로 기대할 수 있습니다.
데이터셋  레이블   이미지  텍스트

MM-IMDB 코미디. 브라이언은 크리스마스에 어린이를 낳은 말장에 태어났다. 그리고 그 옆에는 당신이 아시는 분이 있었다.

현명한 사람들이 나타나서 선물을 나눠줍니다. 별은 더 멀리 움직이기 때문에 그들은 모든 것을 다시 가져가고 떠납니다. 이것이 브라이언의 삶의 흐름입니다. 그는 유대인 인민전선에 가입하며, 실제로 아무것도 하지 않지만 로마인들을 정말로 싫어하는 수십 개의 분리주의 그룹 중 하나입니다. 이것은 예수에 대한 이야기는 아니지만, 예수의 메시지를 듣기에 시간이나 관심이 없는 사람들에 대한 이야기입니다. 많은 정치적 및 사회적 코멘트가 있습니다.

FOOD101 컵케이크는 간단하면서도 맛있어서 생일 축하에 잘 어울리는 기본적인 케이크입니다.

트리트는 24가지 재료로 만들어집니다. 200g의 녹인 무염 버터, 1 티스푼 바닐라 추출물, 1컵의 설탕, 3개의 달걀, 2 1/2컵의 셀프 레이징 밀가루를 사용합니다. 15~17분 동안 굽거나, 1 테이블스푼 용량의 미니 머핀 팬에 1 테이블스푼의 혼합물을 사용하여 10~12분 동안 굽습니다. 4개의 스탠드 케이크를 팬에 2분 동안 굽고, 와이어 랙에 옮겨 식힙니다. 파티 테마에 맞게 장식하세요.

V-SNLI   함축

전제: 카메라에 웃으며 손을 흔드는 아이들이 있다.
가설: 아이들이 존재한다.

테이블 2: 각 데이터셋의 예시 데이터.

다른 경우보다 시각적 정보를 더욱 활용하도록 하기 위해. 텍스트 형식이 우세할 가능성이 있으므로, 시각적 형식에 기회를 주고자 합니다.

3 접근 방식

이 섹션에서는 데이터셋, 기준선 및 기타 실험 세부사항을 설명합니다.

3.1 평가

우리는 다양한 다중모달 분류 작업에 대해 평가합니다. 우리는 (Kiela et al., 2018)에서도 사용된 두 가지 작업인 MM-IMDB (Arevalo et al., 2017)와 FOOD101 (Wang et al., 2015)와 비교합니다. 아키텍처가 두 가지 입력 유형을 초과하여 일반화되는 것을 보여주기 위해 우리는 또한 (premise, hypothesis, image) 세트로 구성된 V-SNLI (Vu et al., 2018)에서도 평가합니다. 데이터셋 통계는 테이블 1을 참조하고 예시는 테이블 2를 참조하십시오.

MM-IMDB 데이터셋은 영화 줄거리와 영화 포스터로 구성되어 있습니다. 목표는 각 영화를 장르별로 분류하는 것입니다. 이는 다중 레이블 예측 문제로, 한 영화에는 여러 장르가 있을 수 있습니다. 이 데이터셋은 고품질의 다중모달 분류 데이터셋의 상대적 부족을 해결하기 위해 (Arevalo 등, 2017)에 의해 특별히 소개되었습니다.

FOOD101 UPMC FOOD101 데이터셋(Wang et al., 2015)은 101개의 음식 레이블에 대한 텍스트 레시피 설명을 포함하고 있습니다. 이 레시피들은 웹 페이지에서 스크랩되었으며, 이후 텍스트를 추출하기 위해 정리되었습니다.

데이터. 각 페이지는 단일 이미지와 일치되었으며, 이미지는 주어진 (가능한 노이즈가 있는) 카테고리에 대해 Google 이미지 검색을 통해 얻었습니다. 목표는 각 레시피-이미지 조합에 대한 해당 음식 라벨을 찾는 것입니다.

V-SNLI 데이터셋은 SNLI 데이터셋(Bowman et al., 2015)을 기반으로 합니다. 목적은 전제와 가설, 관련 이미지를 세 가지 범주 중 하나로 분류하는 것입니다: 함의, 중립 또는 모순. SNLI 데이터셋은 Flickr30k 데이터셋(Young et al., 2014)의 캡션에서 파생된 전제에 대한 가설을 Turkers가 제공하여 생성되었습니다. (Vu et al., 2018)은 원본 이미지와 전제-가설 쌍을 다시 결합하여 V-SNLI라는 기반 함의 작업을 만들었습니다. V-SNLI는 또한 SNLI를 위해 원래 생성된 테스트 세트의 어려운 하위 집합도 함께 제공됩니다. 이 하위 집합에서는 가설만으로 분류기가 실패합니다(Gururangan et al., 2018).

3.2 기준선

우리는 강력한 단일 모달 기준선과 경쟁력 있는, 더 정교한 다중 모달 퓨전 방법과 비교합니다. 모든 경우에 우리는 단일 선형 분류기를 사용하며, 전체 모델을 미세 조정하여 엔드 투 엔드로 진행합니다. 우리는 각각의 기준선을 설명합니다.

단어 가방 (Bow) 우리는 텍스트의 모든 단어에 대해 300차원의 GloVe 임베딩 (Pennington et al., 2014) (Com- mon Crawl)을 합산하고 시각적 특징을 무시하고 이를 분류기에 입력합니다.
텍스트 전용 BERT (Bert) 우리는 사전 훈련된 base-uncased BERT 모델의 최종 레이어의 첫 번째 출력을 가져와 분류기에 입력합니다.

이미지만 (Img) 우리는 평균 풀링을 사용하여 표준 사전 훈련된 ResNet-152를 적용하여 각 이미지에 대해 2048 차원 벡터를 생성하고, 다른 시스템과 동일한 방식으로 분류합니다.

ConcatBow + Img (ConcatBow) 우리는 Bow와 Img 기준의 출력을 연결합니다. 연결은 다중모달 방법에서 강력한 기준으로 자주 사용됩니다. 이 경우, 분류기의 입력은 2048+300 차원입니다.

레이트 퓨전은 우리의 최고의 버트와 이미지 분류기의 점수를 평균하여 최종 예측을 얻습니다.

FiLMBert는 FiLM (Perez et al., 2018)와 BERT를 결합합니다. 여기서 BERT 모델은 ConvNet 분류기를 위해 특성별 이득과 편향을 예측합니다. 우리는 ConvNet의 입력으로 고정된 ResNet-152 특성을 사용합니다. 이는 Perez et al. (2018)와 유사합니다.

ConcatBert는 Bert와 Img 기준선의 출력을 연결합니다. 이 경우, 분류기의 입력은 2048+768 차원입니다. 이는 각 모드의 최상의 인코더를 결합하여 분류기가 인코더 출력에 직접 액세스 할 수 있도록 하는 경쟁력 있는 기준선입니다.

3.3 문제를 더 어렵게 만들기

우리는 다양한 다중모달 분류 작업을 평가하고 있지만, 실제로 이러한 유형의 고품질 작업은 놀랍게도 매우 적습니다. 많은 경우에 텍스트 모드가 지나치게 우세합니다 (이는 VQA에서도 문제가 됩니다; Goyal 등, 2019 참조). 이로 인해 다중모달 방법 간의 차이를 명확히 구분하거나 다중모달 정보를 통합하는 것이 실제로 가치 있는지 확인하기가 어렵습니다. 이전에 관찰한 것처럼, Gururangan 등 (2018)은 SNLI 데이터셋의 어려운 하위 집합을 생성하여 가설만 있는 기준선이 예제를 올바르게 분류하지 못하는 문제를 수정했습니다. 여기서 우리는 비슷한 접근 방식을 따르고, 다른 두 작업에 대해 어려운 다중모달 테스트 세트를 생성합니다.
우리는 테스트 세트에서 Bert와 Img 분류기 예측이 실제 클래스와 가장 다른 예제를 선택하여 어려운 테스트 세트를 구성합니다. 즉, p(a (cid:54)= t|I)p(a (cid:54)= t|T)를 최대화하는 예제를 선택합니다. 여기서 I와 T는 각각 이미지와 텍스트 정보이고, a는 예측입니다.

MM-IMDB: MM-IMDB는 영화 리뷰 데이터셋입니다.
FOOD-101: FOOD-101은 음식 이미지 데이터셋입니다.
V-SNLI: V-SNLI는 시각적 자연어 추론 데이터셋입니다.

GMU 51.4/63.0 - -
CentralNet 56.1/63.9 - -
W+V - 85.1 -
BG - /62.3 90.8 -
V-BiMPM - - 86.99

활     38.1±.2/45.6±.2 72.4±.3 48.6±.3
이미지     32.5±.7/44.4±.3 63.2±.6 33.8±.3
버트    59.9±.3/65.4±.1 87.2±.1 90.1±.3

레이트 퓨전 59.4±.1/66.2±.0 91.1±.1 90.1±.0
콘캣보우 43.8±.4/53.6±.4 79.0±.9 49.5±.1
필럼버트 59.7±.4/65.1±.2 90.2±.3 90.2±.2
콘캣버트 60.5±.3/65.9±.2 90.0±.6 90.2±.4
MMBT    61.6±.2/66.8±.1 92.1±.1 90.4±.1

표 3: 주요 결과. MM-IMDB는 Macro F1 / Micro F1이며, 다른 것들은 정확도입니다. GMU (Arevalo et al., 2017), CentralNet (Vielzeuf et al., 2018), Word2vec+VGGNet (W+V) (Wang et al., 2015), Bilinear-gated (BG) (Kiela et al., 2018) 및 V-BiMPM (Vu et al., 2018)과 비교했습니다.

예측된 답변과 t가 정답입니다. 우리는 가장 다른 예시들 중 상위 10%를 새로운 테스트 세트의 어려운 케이스로 취급합니다. 이 아이디어는 이러한 예시들이 더 세련된 다중모달 추론을 필요로 하며, 다중모달 특정 성능을 더 잘 검토할 수 있도록 하는 것입니다.

3.4 기타 구현 세부 사항

모든 모델에 대해 학습률을 1e−4, 5e−5로 스윕하고 다중 클래스 데이터셋의 검증 정확도 및 다중 레이블 데이터셋의 Micro-F1에 대해 조기 중단을 수행합니다. 또한 텍스트 및 비주얼 인코더를 고정한 상태에서 에포크 수와 이미지 임베딩의 수에 대해 스윕합니다. Bert 모델의 경우, BertAdam (Devlin et al., 2019)을 사용하며 워마업 비율은 0.1입니다. 다른 모델의 경우 일반적인 Adam (Kingma and Ba, 2014)을 사용합니다. 데이터셋이 균형을 이루지 않는 경우, 클래스 레이블을 역 빈도로 가중치를 부여합니다. 코드와 모델은 온라인에서 사용할 수 있습니다.

4 결과

Table 3에서 주요 결과를 찾을 수 있습니다. 각 경우에 대해, 우리는 무작위 시드로 5회 실행한 평균 성능과 표준 편차를 함께 보여줍니다. 우리는 MM-IMDB와 FOOD101에서 (Kiela et al., 2018)의 결과와 비교합니다. 그들은 이중 선형 게이트 모델이 가장 잘 작동한다는 것을 발견했습니다. 즉, 두 입력 모드 중 하나가 중요하다는 것을 의미합니다.

1. https://github.com/facebookresearch/mmbt
그림 2: 사전 훈련된 텍스트와 이미지 구성 요소를 N 에포크 동안 동결하여 분석합니다.

모이드된 다음에 다른 입력에 대해 게이트를 적용합니다. 이를 위해 외적을 사용합니다. 주의할 점은 우리의 경우, 2048차원의 ResNet 출력과 768차원의 Bert 출력이 있으므로, 이중 선형 게이트는 2048×768×101차원의 출력 레이어가 필요합니다 (상단의 분류기만을 위해 약 158M개의 매개변수가 필요합니다). 이는 실용적이지 않습니다.

MM-IMDB에서는 Gated Multimodal Units (Arevalo et al., 2017)와 비교합니다. 이는 다른 모달리티 게이트를 가진 특수한 순환 유닛으로, 다중모달 퓨전을 위해 특별히 설계되었습니다. 또한, 이 데이터셋에서 현재 최고 성능을 보유하고 있는 다중레이어 접근 방식인 CentralNet (Vielzeuf et al., 2018)과도 비교합니다. FOOD101에서는 원래 논문 (Wang et al., 2015)의 결과를 포함합니다. 이는 word2vec과 VGGNet 특징을 연결하고 분류한 결과입니다. V-SNLI에서는 (Vu et al., 2018)의 최첨단 Visual Bilateral Multi-Perspective Matching (V-BiMPM) 모델과 비교합니다.

우리는 다중모달 비트랜스포머(MMBT)가 기준 모델들보다 큰 폭으로 성능이 우수하다는 것을 발견했습니다. 늦은 퓨전, FiLMBert, 그리고 ConcatBert는 유사한 성능을 보입니다. 우리는 MMBT가 ConcatBert보다 우수한 이유는 서로 다른 모달리티의 정보가 최종 레이어뿐만 아니라 자기 어텐션을 통해 서로 상호작용할 수 있는 능력 때문이라고 추측합니다. 성능 향상의 일부는 Bert의 우수한 성능에서 비롯되었지만 (텍스트의 우세성을 고려하면 이해할 수 있습니다), 그래도 MMBT는 MM-IMDB Macro-F1에서 약 3% 정도, Food101에서는 놀라운 6% 정도 (즉, 추가적인 1300개의 예시) Bert보다 성능이 향상되었습니다. 모든 경우에서, 다중모달 모델은 직접적인 단일모달 모델보다 우수한 성능을 보입니다.

MM-IMDB: MM-IMDB
FOOD-101: FOOD-101
V-SNLI: V-SNLI

활   50.6±.4 / 54.7±.4 72.7±.5 27.2±.2
이미지   39.1±.9 / 48.2±.9 63.4±.6 32.3±.3
버트  64.7±.5 / 67.0±.3 87.3±.2 79.7±.4

늦게 61.7±.9 / 66.4±.5 91.3±.5 79.6±.4
연결 64.9±.4 / 67.2±.2 90.4±.3 79.9±.9
MMBT 65.3±.4 / 68.6±.4 92.4±.3 80.3±.1

표 4: 하드 서브셋 (†로 표시됨). Late는 Late Fusion을 의미합니다.
Concat은 ConcatBert입니다. MM-IMDB는 Macro F1 / Micro F1이며,
나머지는 정확도입니다.

4.1 어려운 시험 세트

표 4는 어려운 테스트 세트에 대한 결과를 보고합니다. 이는 단일 모달 (Bert 및 Img) 분류기가 실제 결과와 가장 다른 예제를 선택하여 생성되었음을 기억하십시오. 이러한 결과는 실제 다중 모달 성능에 대한 통찰력을 제공합니다. 또한 VSNLIhard (Gururangan et al., 2018)에 대한 결과도 보고합니다.
MMBT가 대안들보다 우수한 성능을 보이는 주요 결과와 유사한 패턴을 관찰합니다. V-SNLIhard에서는 Vu et al. (2018)이 최고 성능 아키텍처에 대해 73.75의 점수를 보고한 반면, 우리는 80.4의 점수를 보고하고 있습니다. 또한 해당 어려운 테스트 세트에서 이미지만 분류기가 텍스트만 분류기보다 우수한 성능을 보이는 것도 흥미롭게 관찰됩니다. 이는 일반적인 (비-어려운) V-SNLI 테스트 세트의 경우와는 확실히 다른 경우입니다.

4.2 동결 전략

우리는 처음에 다른 사전 훈련된 구성 요소를 동결하는 것이 도움이 되는지에 대한 분석을 수행합니다.
동결은 시각 공간에서 transformer의 예상 토큰 입력 공간으로 매핑하는 학습에 도움이 될 수 있습니다. 다시 말해, 무작위로 초기화된 구성 요소를 먼저 훈련시킬 수 있습니다. 그런 다음 이미지 인코더를 해동시켜 이미지 정보를 최대한 유용하게 만든 후에 해동시킬 수 있습니다.
MM-IMDB - 어려움 FOOD-101 - 어려움

MMBT 61.6±.2 / 66.8±.1 65.3±.4 / 68.6±.4 92.1±.1 92.4±.5
MMBT-Large 63.2±.2 / 68.0±.2 68.2±.5 / 70.3±.4 93.2±.1 93.4±.3

MMBT 61.6±.2 / 66.8±.1 65.3±.4 / 68.6±.4 92.1±.1 92.4±.5
MMBT-Large 63.2±.2 / 68.0±.2 68.2±.5 / 70.3±.4 93.2±.1 93.4±.3

ViLBert-VQA 60.0±.3 / 66.4±.2 62.7±.6 / 66.2±.4 92.1±.1 92.4±.3
ViLBert-VCR 61.6±.3 / 67.6±.2 63.4±.9 / 66.9±.4 92.1±.1 92.1±.3
ViLBert-Refcoco 61.4±.3 / 67.7±.1 63.4±.5 / 67.1±.4 92.2±.1 92.1±.3
ViLBert-Flickr30k 61.4±.3 / 67.8±.1 63.4±.9 / 67.0±.5 92.2±.1 92.2±.3
ViLBert    63.0±.2 / 68.6±.1 65.4±1. / 68.6±.4 92.9±.1 92.9±.3

표 5: MM-IMDB 및 FOOD-101에서 MMBT와 ViLBert의 비교.

비트랜스포머는 전체 시스템을 조정하는 데 사용됩니다. 그림 2는 결과를 보여주며, 실제로 구성 요소를 먼저 배우고 이미지 인코더를 해제한 다음 사전 훈련된 비트랜스포머를 해제하는 것이 유용하다는 직관을 뒷받침합니다. 최적의 에포크 수는 작업에 따라 다르며, 이미지 인코더를 일찍 해제하는 것이 가장 잘 작동합니다.

4.3 매개변수의 수

멀티모달 비트랜스포머가 ConcatBert보다 우수한 성능을 보이는 가능한 이유는 약간 더 많은 파라미터를 가지고 있기 때문일 수 있습니다 (즉, 임베딩 차원 D와 클래스 수 N에 대해 추가적인 2048×D 대비 2048×N, 여기서 D는 임베딩 차원이고 N은 클래스 수입니다). 그러나 차이는 작습니다: 168M 대 170M 파라미터입니다. 이를 조사하기 위해, 우리는 또한 MMBT의 단일 계층 로지스틱 회귀 대신 상단에 2층 및 3층 다층 퍼셉트론 (MLP) 분류기를 가진 ConcatBert와 비교합니다. 이들은 각각 174M 및 175M 파라미터를 가지고 있습니다. MM-IMDB의 경우, ConcatBert-2와 ConcatBert-3는 Macro-F1이 각각 60.21 ± .5 및 59.71 ± .4이고 Micro-F1이 각각 65.08 ± .3 및 64.82 ± .2입니다. Food101의 경우, 이들은 각각 91.13 ± .2 및 90.27 ± .2를 얻습니다. 이는 (표 3 참조) 이미 경쟁력 있는 기준선에 더 많은 파라미터와 더 깊은 분류기를 제공해도 MMBT가 ConcatBert보다 우수함을 명확히 보여줍니다. 결과는 ConcatBert가 과적합에 더 취약하다는 것을 시사합니다.

4.4 누락된 모달리티에 대한 견고성

우리는 ConcatBert와 MMBT를 데이터셋의 일부분에만 이미지가 있는 설정에서 비교합니다. 우리의 지식으로는 이러한 설정은 문헌에서 철저히 탐구되지 않았습니다. 어느 모델이 이 데이터 체제에 대해 더 견고할지는 사전에 알 수 없으며, 이 실험은 중간 수준의 비교에 유용한 추가적인 차원을 제공합니다.

결과는 더 많은 이미지 임베딩과 함께 동일했습니다.

그림 3: MM-IMDB에서 이미지를 훈련 세트의 일부분에 대해 제외하고, 누락된 이미지에 대한 견고성을 측정할 때의 성능 (MicroF1).

MMBT가 제공하는 더 정교한 융합과의 융합.
그림 3은 이미지가 적을수록 성능이 하락하는 것을 보여줍니다.
ConcatBert보다 MMBT가 누락된 이미지에 대해 훨씬 더 견고한 것을 관찰하는 것은 흥미로운 점입니다.

4.5 ViLBERT와의 비교

우리는 단일 모드 사전 훈련된 구성 요소를 융합하는 효과를 검토하여 자기 지도 다중 모드 사전 훈련된 모델과 비교합니다. 우리는 ViLBERT (Lu et al., 2019)를 그런 종류의 모델의 규범적인 예로 삼습니다. ViLBERT는 이미지와 캡션을 다중 모드로 훈련하고 "비전과 언어의 BERT"로 알려져 있습니다. 훈련 중에 고정된 상태로 유지되는 Faster RCNN에서 추출된 경계 상자를 사용합니다. 우리는 이러한 약간 비정상적인 작업에 초점을 맞추어 이제 이러한 모델들을 공정한 기준으로 비교할 수 있습니다.
표 5는 결과를 보여줍니다. 우리는 다양한 ViLBert 모델과 비교하며, 표준 사전 훈련된 버전과 VQA와 같은 특정 작업에 대해 세밀하게 조정된 버전을 모두 비교합니다. 후자의 접근 방식은 원래 ViLBert 논문에서 제안되지 않았지만, 유사한 "두 단계 사전 훈련" 접근 방식은 단일 모드 작업에 대한 BERT의 세밀한 조정에 효과적으로 증명되었습니다 (Phang et al., 2018). 우리는 그 논문에서 사용된 하이퍼파라미터 세트를 사용하여 튜닝합니다: (배치 크기, 학습률) ∈ {(64,2e−5),(256,4e−5)}. 우리는 우리의 직관적인 MMBT 모델이 놀랍도록 경쟁력이 있다는 것을 관찰합니다. MM-IMDB에서는 Macro-F1에서 작업별 ViLBERT 모델과 일치합니다. 이 데이터셋의 Hard 하위 집합에서는 다중 모드 성능을 더 정확하게 측정하며, MMBT는 ViLBert의 성능과 일치합니다. FOOD-101의 경우, 성능이 매우 유사하며, 때로는 작업별 모델을 능가하는 경우도 있습니다, 특히 Hard 하위 집합에서. 우리의 결과는 자기 지도 다중 모드 사전 훈련에는 더 많은 개선 여지가 있으며, 단일 모드 사전 훈련된 구성 요소의 지도 퓨전은 놀랍도록 경쟁력이 있다는 것을 시사합니다.
우리의 방법은 제약 조건에 따라 더 선호될 수 있습니다: 매달 새로운 모델이 출시되면 이를 아키텍처에 쉽게 통합할 수 있습니다. 이 점을 설명하기 위해 (분명히 공정한 비교는 아닙니다), 우리는 BERT-Large 모델을 사용하여 MMBT가 ViLBERT를 능가하도록 만듭니다. 이는 우리의 설정에서는 간단하지만, ViLBERT의 경우 처음부터 다시 훈련해야 합니다.

5 관련 연구

트랜스포머(Vaswani et al., 2017)는 언어 모델링이나 언어 마스킹을 위해 사전 훈련된 후 세부 조정(Radford et al., 2018; Devlin et al., 2019)을 통해 순차적 데이터를 분류하기 위해 성공적으로 사용되었습니다. 다중 모달 퓨전의 효과적인 방법에 대한 문제는 오랜 역사를 가지고 있습니다(Baltruˇ saitis et al., 2019). 연결(concatenation)은 기본적인 방법으로 간주될 수 있지만, 다른 퓨전 방법들이 탐구되었습니다. 예를 들어, 어휘 표현 학습을 위한(Bruni et al., 2014; Lazaridou et al., 2015) 퓨전 방법입니다. 분류에서, Kiela et al. (2018)은 사전 훈련된 고정 표현에 대한 다양한 퓨전 방법을 조사하고, 데이터의 이중 선형 조합이 가장 잘 작동한다는 것을 발견했습니다. 우리의 지도 다중 모달 이중 트랜스포머는 다양한 레이어를 통해 모달 간의 퓨전을 자기-주의(self-attention)를 통해 수행합니다.

다중 모달 NLP의 응용 분야는 분류에서부터 교차 모달 검색(Weston et al., 2011; Frome et al., 2013; Socher et al., 2013), 이미지 캡션(Bernardi et al., 2016), 시각적 질문 응답(Antol et al., 2015) 및 다중 모달 기계 번역(Elliott et al., 2017)에 이르기까지 다양합니다. 다중 모달 정보는 인간과 유사한 의미 표현 학습에도 유용합니다(Baroni, 2016; Kiela, 2017). 다중 모달 이중 트랜스포머는 사실상 깊은 퓨전 방법을 제공합니다. 관련된

깊은 융합 방법에는 다중 모달 변환기(Tsai et al., 2019), CentralNet(Vielzeuf et al., 2018), MFAS(P´ erez-R´ ua et al., 2019) 및 Tensor Fusion Networks(Zadeh et al., 2017)가 포함됩니다.
최근에는 많은 자기 지도 다중 모달 아키텍처가 발표되었습니다. 예를 들어 ViLBERT(Lu et al., 2019), VisualBERT(Li et al., 2019), LXMERT(Tan and Bansal, 2019), VL-BERT(Su et al., 2019), VideoBERT(Sun et al., 2019) 등이 있습니다. 우리의 모델은 이러한 자기 지도 아키텍처와 다릅니다. 개별 구성 요소는 단일 모달로만 사전 훈련됩니다. 이는 장단점이 있습니다. 우리의 방법은 직관적이고 직접적이며, 기존의 자기 지도 인코더에도 쉽게 구현할 수 있으며, 인상적인 개선을 얻을 수 있습니다. 새로운 더 나은 텍스트 또는 비전 모델이 나오면 구성 요소를 교체하는 것은 간단합니다. 반면, 자기 지도 사전 훈련 중에 다중 모달 정보를 완전히 활용할 수는 없습니다. 그렇지만, 단일 모달 데이터에 대한 압도적인 양의 정보에 접근할 수 있습니다. 다시 말해, 이러한 지도 다중 모달 이진 변환기는 자기 지도 다중 모달 사전 훈련이 실제로 얼마나 도움이 되는지를 평가하는 강력한 기준을 제공해야 합니다.

6 결론

이 작업에서는 지도 학습 다중 모달 비트랜스포머 모델을 소개했습니다. 우리는 다양한 작업을 포함하여 여러 기준과 비교했으며, 특히 다중 모달 성능을 검토하기 위해 특별히 생성된 어려운 테스트 세트에서 비교했습니다 (즉, 단일 모달 성능이 실패하는 경우). 우리는 제안된 아키텍처가 기존의 최첨단 기술과 강력한 기준을 크게 능가한다는 것을 발견했습니다. 그런 다음, 얼림/해동 전략을 탐색하고 매개변수 수를 살펴보는 다중 모달 최적화 분석을 수행했으며, 매개변수가 더 많고 더 깊은 분류기를 가진 강력한 기준조차도 능가되었습니다.
우리의 아키텍처는 단일 모달 작업으로 개별적으로 사전 훈련된 구성 요소로 구성되어 있으며, 이미 대안에 비해 큰 개선을 보였습니다. 다중 모달 자기 지도 모델이 일반적으로 유용할지 아직은 명확하지 않습니다. 우리는 ViLBERT와 비교하고 제안된 모델이 경쟁력을 갖으면서 훨씬 간단하다는 것을 보였습니다. 여기에서 제시된 방법은 자기 지도 다중 모달 모델의 성능을 측정하기 위한 유용하고 강력한 기준으로 사용될 수 있어야 합니다. 지도 학습 다중 모달 비트랜스포머는 직관적이고 직관적이며, 중요한 것은 기존의 자기 지도 인코더에 대해서도 쉽게 구현할 수 있습니다.

참고문헌

스타니슬라프 안톨, 아이쉬와리야 아그라왈, 지아센 루, 마가렛 미첼, 드루브 바트라, C 로렌스 지트닉, 그리고 데비 파리크. 2015년. Vqa: 시각적 질문 응답. IEEE 국제 컴퓨터 비전 학회 논문집, 2425-2433쪽.

존 아레발로, 타마르 솔로리오, 마누엘 몬테스-이 고메스, 그리고 파비오 A 곤잘레스. 2017년. 정보 융합을 위한 게이트드 멀티모달 유닛. arXiv 사전 인쇄 arXiv:1702.01992.

타다스 발트루사이티스, 차이타냐 아후자, 그리고 루이스 필립 모렌시. 2019년. 다중모달 기계학습: 조사 및 분류. IEEE 패턴 분석 및 기계지능 트랜잭션, 41(2):423–443.

마르코 바로니. 2016. 시각 세계에서 분포 의미론을 기반으로 한 지식의 근거화. 언어학과 언어학 컴패스, 10(1):3-13.

라파엘라 베르나르디, 루케트 차키치, 데즈몬드 엘리엇,
아이쿠트 에르덤, 에르쿠트 에르덤, 나즐리 이키즐러-친비스,
프랭크 켈러, 에이드리언 뮤스캣, 그리고 바바라 플랭크.
2016년. 이미지로부터의 자동 설명 생성: 모델, 데이터셋, 그리고 평가 척도에 대한 조사. 인공지능 연구 저널, 55:409–442.

사무엘 R 보우먼, 가보르 안젤리, 크리스토퍼 포츠,
그리고 크리스토퍼 D 매닝. 2015. 자연어 추론 학습을 위한 대규모 주석 첨부 말뭉치. 
arXiv 사전 인쇄 arXiv:1508.05326.

엘리아 브루니, 남칸 트란, 그리고 마르코 바로니. 2014년.
다중모달 분포 의미론. 인공지능 연구 저널, 49:1-47.

Ronan Collobert와 Jason Weston. 2008. 자연어 처리를 위한 통합 아키텍처: 다중 작업 학습을 하는 심층 신경망. 제25회 국제 기계 학습 컨퍼런스 논문집, 160-167쪽. ACM.

알렉시스 코너, 도우 키엘라, 홀거 슈벤크, 로이크 바로, 그리고 안토완 보르드. 2017년. 자연어 추론 데이터로부터의 범용 문장 표현의 지도 학습. arXiv 사전 인쇄 arXiv:1705.02364.

앤드류 M 다이와 꽉 레. 2015. 반지도 시퀀스 학습. 신경 정보 처리 시스템의 진보, 페이지 3079-3087.

지아 덩, 웨이 동, 리처드 소처, 리지아 리, 카이 리, 그리고 리 페이페이. 2009. 이미지넷: 대규모 계층적 이미지 데이터베이스. 2009년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 페이지 248-255. IEEE.

제이콥 데블린, 민위 창, 켄튼 리, 그리고 크리스티나 투타노바. 2019. BERT: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. NAACL 학회 논문집.

데즈몬드 엘리엇, 스텔라 프랭크, 로이크 바로, 페티 부가레스, 그리고 루시아 스페시아. 2017년. 다중모달 기계 번역 및 다국어 이미지 설명에 관한 두 번째 공유 작업 결과. 머신 번역 제2회 컨퍼런스 논문집, 2권: 공유 작업 논문, 215-233쪽, 덴마크 코펜하겐.

파르타시 파그리, 데이비드 J 플릿, 제이미 라이언 키로스, 그리고 산자 피들러. 2017년. Vse++: 어려운 부정적 요소를 사용하여 시각-의미 임베딩 개선하기. arXiv 사전 인쇄 arXiv:1707.05612.

안드레아 프롬, 그렉 S 코라도, 존 셀렌스, 샘이 벤지오, 제프 딘, 토마스 미콜로프 등. 2013. 디바이스: 깊은 시각-의미 임베딩 모델. 신경 정보 처리 시스템의 발전, 페이지 2121-2129.

야쉬 고얼, 테자스 코트, 아이쉬와리야 아그라왈, 더글라스 서머스-스테이, 드루브 바트라, 그리고 데비 파리크. 2019년. VQA에서 v의 중요성: 시각적 질문 응답에서 이미지 이해의 역할 강화. Int. J. Comput. Vision, 127(4):398–414.

수친 구루랑간, 스와바 스와야믹디타, 오머 레비, 로이 슈왈츠, 사무엘 R 보우먼, 그리고 노아 A 스미스. 2018. 자연어 추론 데이터에서의 주석 오류. arXiv 사전 인쇄 arXiv:1803.02324.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. 이미지 인식을 위한 깊은 잔여 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 770-778쪽.

제레미 하워드와 세바스찬 루더. 2018. 텍스트 분류를 위한 범용 언어 모델 파인튜닝. ACL 학회 논문집.

도우베 키엘라. 2017. 깊은 구체성: 지각적 형태에서 의미를 뿌리내리다. 박사학위 논문, 케임브리지 대학교, 컴퓨터 연구소.

Douwe Kiela, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2018. 효율적인 대규모 다중 모달 분류. 서른 두 번째 AAAI 인공지능 학회.

디에더릭 P 킹마와 지미 바. 2014. Adam: 확률적 최적화를 위한 방법. arXiv 사전 인쇄 arXiv:1412.6980.

라이언 키로스, 유쿤 주, 루슬란 R 살라후딘노프,
리처드 제멜, 라퀼 우르타순, 안토니오 토랄바,
그리고 산자 피들러. 2015. 스킵-생각 벡터. 
신경 정보 처리 시스템에서의 진보, 
페이지 3294-3302.

안젤리키 라자리두, 니아 펌, 그리고 마르코 바로니. 
2015. 언어와 시각을 결합한 
다중 모달 스킵-그램 모델. 
arXiv 사전 인쇄 arXiv:1501.02598.

리우니안 할드 리, 마크 야츠카르, 다 인, 조주이 씨, 그리고 카이-웨이 창. 2019년. Visualbert: 시각과 언어를 위한 간단하고 성능이 우수한 기준선. arXiv 사전 인쇄 arXiv:1908.03557.

지아센 루, 드루브 바트라, 데비 파리크, 스테판 리. 2019. ViLBERT: 사전 훈련 작업에 중립적인 시각언어 표현을 위한 시각-언어 과제. arXiv 사전 인쇄 arXiv:1908.02265.

토마스 미콜로프, 일리야 숫스케버, 카이 첸, 그레그 S 코라도, 그리고 제프 딘. 2013년. 단어와 구문의 분산 표현과 그들의 조합성. 신경 정보 처리 시스템의 발전에서, 페이지 3111-3119.

막심 오쿠압, 레옹 보투, 이반 랩테프, 요제프 시빅. 2014. 합성곱 신경망을 사용한 중간 수준 이미지 표현의 학습과 전이. IEEE 컴퓨터 비전 및 패턴 인식 학회 논문집, 페이지 1717-1724.

Adam Paszke, Sam Gross, Soumith Chintala, 그리고 Gregory Chanan. 2017. PyTorch: 강력한 GPU 가속을 갖춘 파이썬에서의 텐서와 동적 신경망. 기술 보고서, PyTorch.

제프리 페닝턴, 리처드 소처, 그리고 크리스토퍼 매닝. 2014년. Glove: 단어 표현을 위한 글로벌 벡터. 2014년 자연어 처리에 대한 경험적 방법에 관한 회의(EMNLP) 논문집, 1532-1543쪽.

이단 페레즈, 플로리안 스트룹, 하름 드 브리스, 빈센트 뒤무랭, 그리고 아론 쿠르빌. 2018년. 영화: 일반 조건 계층을 사용한 시각적 추론. AAAI 학회 논문집에서 발표.

Juan-Manuel P´ erez-R´ ua, Valentin Vielzeuf, St´ ephane Pateux, Moez Baccouche, and Fr´ ed´ eric Jurie. 2019. MFAS: 다중 모달 퓨전 아키텍처 검색. CVPR 논문집, arxiv 사전 인쇄 1903.06496.

매튜 E 피터스, 마크 뉴만, 모히트 이예르, 매트 가드너, 크리스토퍼 클락, 켄튼 리, 그리고 루크 제틀모이어. 2018년. 깊은 문맥화된 단어 표현. NAACL 학회 논문집.

제이슨 팡, 티보 프에브리, 그리고 사무엘 R. 보우먼.
2018년. 스틸트 위의 문장 인코더: 중간 레이블 데이터 작업에 대한 보충 훈련. CoRR,
abs/1811.01088.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. 언어 이해력 향상을 위한 생성적 사전 훈련. 기술 보고서, OpenAI.

알리 샤리프 라자비안, 호세인 아지즈푸어, 조세핀 설리번, 스테판 칼슨. 2014. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 806–813.

리처드 소처, 밀린드 간주, 크리스토퍼 D 매닝, 그리고 앤드류 엔지. 2013. 교차 모달 전이를 통한 제로샷 학습. 신경 정보 처리 시스템 발전에서, 페이지 935-943.

위제 수, 시저 주, 유에 차오, 빈 리, 류 류웨이, 후루 웨이, 그리고 지펑 다이. 2019. Vl-bert: 일반적인 시각-언어 표현의 사전 훈련. arXiv 사전 인쇄 arXiv:1908.08530.

천 선, 오스틴 마이어스, 칼 본드릭, 케빈 머피, 그리고 코르델리아 슈미드. 2019년. Videobert: 비디오와 언어 표현 학습을 위한 공동 모델. arXiv 사전 인쇄 arXiv:1904.01766.

Hao Tan과 Mohit Bansal. 2019. Lxmert: 트랜스포머로부터의 교차 모달리티 인코더 표현 학습. arXiv 사전 인쇄 arXiv:1908.07490.

야오-훙 휴버트 총, 샤오지에 바이, 폴 푸 리앙,
J. 지코 콜터, 루이스 필립 모렌시, 그리고 루슬란
살라후트디노프. 2019. 비정렬된 다중 모달 언어 시퀀스를 위한 다중 모달 트랜스포머. arxiv
사전 인쇄물 1906.00295.

아시쉬 바스와니, 노암 샤지어, 니키 파마르, 야코브 우스코레이트, 리온 존스, 에이단 엔 고메즈, 우카시 카이저, 그리고 일리아 폴로수킨. 2017년. 주의는 당신이 필요한 모든 것이다. 신경 정보 처리 시스템 발전에서, 페이지 5998-6008.

발랑틴 비엘조프, 알렉시스 르셰르비, 스테판 파토, 프레데릭 주리. 2018. Centralnet: 다중 모달 퓨전을 위한 다층 접근 방식. 유럽 컴퓨터 비전 학회(ECCV) 워크샵에서 발표.

호아 트롱 부, 클라우디오 그레코, 알리야 에로페에바, 소마예 자파리타제안, 기도 린더스, 마크 탄티, 알베르토 테스토니, 라파엘라 베르나르디, 알버트 갓. 2018. 기반 텍스트 함의. COLING 논문집, 2354-2368쪽.

위야오 왕, 두 트란, 매트 페이즐리. 2019. 훈련 멀티모달 네트워크를 어렵게 만드는 요소는 무엇인가? arXiv 사전 인쇄 arXiv:1905.12681.

신 왕, 데빈더 쿠마르, 니콜라스 톰, 마티유 코르드, 그리고 프레데릭 프레시오소. 2015년. 대규모 다중모달 식품 데이터셋을 이용한 레시피 인식. 2015 IEEE 국제 멀티미디어 및 엑스포 워크샵 (ICMEW) 논문집, 1-6쪽. IEEE.

제이슨 웨스트턴, 새미 벵지오, 니콜라 우수니에.
2011년. Wsabie: 대용량 어휘 이미지 주석에 대한 확장. 제22회 국제 인공지능 학회.
피터 영, 앨리스 라이, 마이카 호도시, 줄리아 호크마이어. 2014년. 이미지 설명에서 시각적 표현으로: 의미 추론을 위한 새로운 유사도 측정법. 연합 컴퓨터 언어학회 트랜잭션, 2:67-78.

아미르 자데, 민하이 첸, 소우자냐 포리아, 에릭 캠브리아, 그리고 루이스 필립 모렌시. 2017. 다중 모달 감성 분석을 위한 텐서 퓨전 네트워크. arXiv 사전 인쇄 arXiv:1707.07250.

