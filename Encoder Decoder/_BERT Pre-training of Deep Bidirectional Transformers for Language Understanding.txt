keyword

1024, contextual token, self-attention, ##, Position
Embeddings, MLM, (Taylor, 1953),  [MASK], A and B, Segment Embedding

Language Understanding

BERT: 깊은 양방향 트랜스포머의 사전 훈련을 위한 언어 이해

언어 이해

제이콥 데블린, 민웨이 창, 켄튼 리, 크리스티나 투타노바

구글 AI 언어
{jacobdevlin,mingweichang,kentonl,kristout}@google.com

요약

우리는 BERT라고 불리는 새로운 언어 표현 모델을 소개합니다. BERT는 Transformers에서 온 양방향 인코더 표현입니다. 최근의 언어 표현 모델과는 달리 (Peters et al., 2018a; Radford et al., 2018), BERT는 모든 레이어에서 왼쪽과 오른쪽 문맥을 동시에 고려하여 레이블이 없는 텍스트로부터 깊은 양방향 표현을 사전 훈련합니다. 결과적으로, 사전 훈련된 BERT 모델은 추가적인 출력 레이어 하나만으로도 최첨단 모델을 만들 수 있으며, 질문 응답 및 언어 추론과 같은 다양한 작업에 대해 특정 작업 아키텍처 수정 없이 성능을 높일 수 있습니다.

BERT는 개념적으로 간단하면서 경험적으로 강력합니다. GLUE 점수를 80.5%로 끌어올려 새로운 최고 성능을 달성하였으며(7.7% 절대 개선), MultiNLI 정확도를 86.7%로 향상시켰습니다(4.6% 절대 개선). 또한, SQuAD v1.1 질문 응답 테스트 F1을 93.2로 향상시켰으며(1.5 절대 개선), SQuAD v2.0 테스트 F1을 83.1로 향상시켰습니다(5.1 절대 개선).

소개

언어 모델 사전 훈련은 많은 자연어 처리 작업의 효과적인 개선 방법으로 입증되었습니다 (Dai and Le, 2015; Peters et al., 2018a; Radford et al., 2018; Howard and Ruder, 2018). 이에는 문장 수준 작업인 자연어 추론 (Bowman et al., 2015; Williams et al., 2018) 및 문장 간 관계를 종합적으로 분석하여 예측하는 패러프레이징 (Dolan and Brockett, 2005)과 같은 작업뿐만 아니라, 개체명 인식 및 질문 응답과 같은 토큰 수준 작업도 포함됩니다. 이러한 작업에서는 모델이 토큰 수준에서 세밀한 출력을 생성해야 합니다 (Tjong Kim Sang and De Meulder, 2003; Rajpurkar et al., 2016).

LLM에 대한 내용입니다.

사전 훈련된 언어 표현을 하위 작업에 적용하는 두 가지 전략이 있습니다: 특징 기반 및 세밀 조정. 특징 기반 접근 방식인 ELMo (Peters et al., 2018a)와 같은 것은 사전 훈련된 표현을 추가 기능으로 포함하는 작업 특정 아키텍처를 사용합니다. 세밀 조정 접근 방식인 Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018)와 같은 것은 최소한의 작업 특정 매개변수를 도입하고, 사전 훈련된 모든 매개변수를 단순히 세밀하게 조정하여 하위 작업에서 훈련됩니다. 두 가지 접근 방식은 사전 훈련 중에 동일한 목적 함수를 공유하며, 단방향 언어 모델을 사용하여 일반 언어 표현을 학습합니다.

우리는 현재 기술이 사전 훈련된 표현의 성능을 제한한다고 주장합니다, 특히 세밀 조정 접근 방식의 경우입니다. 주요 제한은 표준 언어 모델이 단방향이라는 것이며, 이는 사전 훈련 중에 사용할 수 있는 아키텍처의 선택을 제한합니다. 예를 들어, OpenAI GPT에서 저자들은 왼쪽에서 오른쪽으로의 아키텍처를 사용하며, Transformer의 자기 어텐션 레이어에서 각 토큰은 이전 토큰에만 주의를 기울일 수 있습니다 (Vaswani et al., 2017). 이러한 제한은 문장 수준 작업에는 최적화되어 있지 않으며, 질문 응답과 같은 토큰 수준 작업에 세밀 조정 기반 접근 방식을 적용할 때 매우 해로울 수 있습니다. 이러한 작업에서는 양방향 컨텍스트를 통합하는 것이 중요합니다.

본 논문에서는 BERT: 양방향 인코더 표현을 제안하여 세밀 조정 기반 접근 방식을 개선합니다. BERT는 "마스크된 언어 모델" (MLM) 사전 훈련 목적을 사용하여 이전에 언급한 단방향성 제약을 완화합니다. 이는 Cloze 작업 (Taylor, 1953)에서 영감을 받았습니다. 마스크된 언어 모델은 입력에서 일부 토큰을 무작위로 마스킹하고, 목적은 마스크된 단어의 원래 어휘 ID를 컨텍스트를 기반으로 예측하는 것입니다. 왼쪽에서 오른쪽 언어 모델 사전 훈련과 달리, MLM 목적은 표현이 왼쪽과 오른쪽 컨텍스트를 퓨즈할 수 있도록 하여 깊은 양방향 Transformer를 사전 훈련할 수 있게 합니다. 마스크된 언어 모델 외에도, 우리는 텍스트 쌍 표현을 공동으로 사전 훈련하는 "다음 문장 예측" 작업도 사용합니다. 본 논문의 기여는 다음과 같습니다:

• 우리는 양방향 사전 훈련의 중요성을 보여줍니다. Radford et al. (2018)가 단방향 언어 모델을 사용하는 반면, BERT는 마스크된 언어 모델을 사용하여 사전 훈련된 깊은 양방향 표현을 가능하게 합니다. 이는 또한 Peters et al. (2018a)와 대조적인데, 그들은 독립적으로 훈련된 좌에서 우로 및 우에서 좌로 언어 모델의 얕은 연결을 사용합니다.

우리는 사전 훈련된 표현이 많은 공학적인 작업 특정 아키텍처의 필요성을 줄인다는 것을 보여줍니다. BERT는 대규모의 문장 수준 및 토큰 수준 작업에서 최첨단 성능을 달성하는 첫 번째 fine-tuning 기반 표현 모델로, 많은 작업 특정 아키텍처를 능가합니다.

BERT는 11가지 NLP 작업에 대한 최신 기술을 제공합니다. 코드와 사전 훈련된 모델은 https://github.com/google-research/bert에서 사용할 수 있습니다.

2 관련 연구

일반 언어 표현 사전 훈련에는 긴 역사가 있으며, 이 섹션에서는 가장 널리 사용되는 접근 방식을 간단히 검토합니다.

2.1 비지도 학습 기반 특징 추출 방법

단어의 널리 적용 가능한 표현을 학습하는 것은 수십 년 동안 활발히 연구되어 왔으며, 비신경망 (Brown et al., 1992; Ando and Zhang, 2005; Blitzer et al., 2006) 및 신경망 (Mikolov et al., 2013; Pennington et al., 2014) 방법이 포함되어 있습니다. 사전 훈련된 단어 임베딩은 현대 자연어 처리 시스템의 중요한 부분이며, 처음부터 학습된 임베딩에 비해 상당한 개선을 제공합니다 (Turian et al., 2010). 단어 임베딩 벡터를 사전 훈련하기 위해 왼쪽에서 오른쪽으로 언어 모델링 목적이 사용되었으며 (Mnih and Hinton, 2009), 또한 왼쪽과 오른쪽 문맥에서 올바른 단어와 잘못된 단어를 구별하는 목적도 사용되었습니다 (Mikolov et al., 2013).

이러한 접근 방식은 문장 임베딩(Kiros et al., 2015; Logeswaran and Lee, 2018)이나 단락 임베딩(Le and Mikolov, 2014)과 같은 더 거친 단위로 일반화되었습니다. 문장 표현을 훈련하기 위해 이전 문장의 표현을 기반으로 다음 문장 단어를 생성하는 좌에서 우로의 방향으로 훈련하는 방법(Jernite et al., 2017; Logeswaran and Lee, 2018)이나 이전 문장의 표현을 사용하여 다음 문장의 단어 후보를 순위 매기는 목적 함수(Kiros et al., 2015) 또는 노이즈 제거 오토인코더 기반 목적 함수(Hill et al., 2016)가 사용되었습니다. ELMo와 그 전신인 모델(Peters et al., 2017, 2018a)은 전통적인 단어 임베딩 연구를 다른 차원에서 일반화합니다. ELMo는 좌에서 우로와 우에서 좌로 언어 모델에서 문맥에 민감한 특징을 추출합니다. 각 토큰의 문맥 표현은 좌에서 우로와 우에서 좌로의 표현의 연결입니다. ELMo와 기존의 작업 특정 아키텍처를 통합할 때, ELMo는 질문 응답(Rajpurkar et al., 2016), 감성 분석(Socher et al., 2013) 및 개체명 인식(Tjong Kim Sang and De Meulder, 2003)을 포함한 여러 주요 NLP 벤치마크에서 최신 기술을 제공합니다. Melamud et al. (2016)은 LSTMs를 사용하여 좌우 문맥에서 단일 단어를 예측하는 작업을 통해 문맥적 표현을 학습하는 것을 제안했습니다. ELMo와 유사하게, 그들의 모델은 특징 기반이며 깊게 양방향적이지 않습니다. Fedus et al. (2018)은 클로즈 태스크가 텍스트 생성 모델의 견고성을 향상시키는 데 사용될 수 있다는 것을 보여줍니다.

2.2 비지도 세부 조정 접근 방식

기능 기반 접근법과 마찬가지로, 첫 번째로는 미분류된 텍스트로부터 사전 훈련된 단어 임베딩 매개변수만 사용하여 작동합니다 (Collobert and Weston, 2008).

보다 최근에는 문장이나 문서 인코더가 미분류된 텍스트로부터 사전 훈련되고 감독 지도 하의 하류 작업을 위해 세부 조정되는 문맥 토큰 표현을 생성합니다 (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018). 이러한 접근법의 장점은 처음부터 학습해야 할 매개변수가 적다는 것입니다. 이러한 장점으로 인해 OpenAI GPT (Radford et al., 2018)는 GLUE 벤치마크 (Wang et al., 2018a)의 많은 문장 수준 작업에서 이전에 최고 성능을 달성했습니다.

왼쪽에서 오른쪽으로 언어 모델 BERT BERT

E[CLS] E1   E[SEP] ... EN E1’ ... EM’
C  T1      T[SEP] ... TN T1’ ... TM’

E[CLS] E1   E[SEP] ... EN E1’ ... EM’
C  T1      T[SEP] ... TN T1’ ... TM’

[CLS] 토큰 1 [SEP] ... 토큰 N 토큰 1 ... 토큰 M

질문: LLM에 대해 설명해주세요.

답변: LLM은 법학 석사 학위로, 법률 분야에서 깊은 지식과 전문성을 갖춘 전문가를 양성하는 학위입니다.

시작/끝 구간

BERT (Bidirectional Encoder Representations from Transformers)는 자연어 처리를 위한 사전 훈련된 언어 모델로, 양방향으로 문맥을 이해하고 단어의 의미를 파악하는 능력을 갖추고 있습니다.

E[CLS] E1  E[SEP] ... EN E1’ ... EM’
C  T1     T[SEP] ... TN T1’ ... TM’

E[CLS] E1  E[SEP] ... EN E1’ ... EM’
C  T1     T[SEP] ... TN T1’ ... TM’

[CLS] 토큰 1 [SEP] ... 토큰 N 토큰 1 ... 토큰 M

마스크된 문장 A
마스크된 문장 B

사전 훈련                       세부 튜닝
NSP  마스크 언어 모델   마스크 언어 모델

라벨이 없는 문장 A와 B 쌍

SQuAD

질문 대답 쌍
NER MNLI

그림 1: BERT의 전체 사전 훈련 및 세부 조정 절차. 출력 레이어를 제외한 모든 구조는 사전 훈련 및 세부 조정에 사용됩니다. 동일한 사전 훈련된 모델 매개변수는 다른 하향 작업을 위해 모델을 초기화하는 데 사용됩니다. 세부 조정 중에는 모든 매개변수가 세부 조정됩니다. [CLS]는 각 입력 예제 앞에 추가된 특수 기호이며, [SEP]은 특수 구분자 토큰입니다 (예: 질문/답변 구분).

인그 및 오토인코더 목표는 이러한 모델의 사전 훈련에 사용되었습니다 (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015).

2.3 지도 학습 데이터로부터의 전이 학습

지도 학습된 대규모 데이터셋을 사용한 작업에서의 효과적인 전이 학습에 대한 연구도 있었는데, 예를 들어 자연어 추론 (Conneau et al., 2017) 및 기계 번역 (McCann et al., 2017)과 같은 작업에서 전이 학습의 효과가 입증되었습니다. 컴퓨터 비전 연구에서도 대규모 사전 훈련된 모델로부터의 전이 학습의 중요성이 입증되었는데, 효과적인 방법은 ImageNet으로 사전 훈련된 모델을 세밀하게 조정하는 것입니다 (Deng et al., 2009; Yosinski et al., 2014).

3 BERT

3 BERT는 Bidirectional Encoder Representations from Transformers의 약자로, 양방향으로 동작하는 Transformer 기반의 인코더 모델입니다.

이 섹션에서는 BERT와 그의 상세한 구현을 소개합니다. 우리의 프레임워크에는 두 단계가 있습니다: 사전 훈련과 세부 조정. 사전 훈련 단계에서는 모델이 레이블이 지정되지 않은 데이터를 사용하여 훈련됩니다. 세부 조정에서는 BERT 모델이 사전 훈련된 매개변수로 초기화되고, 모든 매개변수가 하향 작업에서 레이블이 지정된 데이터를 사용하여 세부 조정됩니다. 각 하향 작업은 별도의 세부 조정된 모델을 가지고 있으며, 그러나 모두 동일한 사전 훈련된 매개변수로 초기화됩니다. Figure 1의 질문-답변 예제는 이 섹션에서 실행 예제로 사용될 것입니다.
BERT의 독특한 특징은 다른 작업들 간에 통합된 아키텍처입니다. 작업마다 별도의 세부 조정된 모델이 있습니다.

사전 훈련된 아키텍처와 최종 하향 아키텍처 사이의 말 차이.

모델 아키텍처 BERT의 모델 아키텍처는 Vaswani et al. (2017)에서 설명된 원래의 구현을 기반으로 한 다중 레이어 양방향 Transformer 인코더입니다. 이 구현은 tensor2tensor 라이브러리에서 공개되었습니다. Transformers의 사용이 흔해지고 우리의 구현이 원래와 거의 동일하기 때문에, 모델 아키텍처에 대한 상세한 배경 설명은 생략하고 Vaswani et al. (2017) 및 "The Annotated Transformer"와 같은 훌륭한 가이드를 참조하도록 하겠습니다.

이 작업에서, 우리는 레이어의 수 (즉, Transformer 블록)를 L, hidden size를 H, self-attention head의 수를 A로 표기합니다. 주로 두 가지 모델 크기에 대한 결과를 보고합니다: BERT BASE (L=12, H=768, A=12, 총 파라미터=110M)와 BERT LARGE (L=24, H=1024, A=16, 총 파라미터=340M).

BERT BASE는 비교를 위해 OpenAI GPT와 동일한 모델 크기로 선택되었습니다. 그러나 BERT Transformer는 양방향 self-attention을 사용하고, GPT Transformer는 제한된 self-attention을 사용하여 각 토큰은 왼쪽의 문맥에만 참여할 수 있습니다.

1https://github.com/tensorflow/tensor2tensor
제한 사항 및 향후 연구

우리의 실험 결과, 분석 및 미련한 영역과 향후 연구를 위해 몇 가지 제한 사항과 영역을 주목했습니다.
개선된 디코딩 전략. Whisper를 확장하면서 큰 모델이 비슷한 소리나는 단어를 혼동하는 것과 같은 인식 관련 오류를 줄이는 데 지속적이고 신뢰할 수 있는 진전을 보였습니다. 그러나 여전히 많은 오류, 특히 장편 전사에서는 더 고집스럽고 명백하게 비인간/지각적인 성격을 가지고 있습니다. 이러한 오류는 seq2seq 모델, 언어 모델 및 텍스트-오디오 정렬의 실패 모드인 반면, 무한 반복 루프에 갇히는 문제, 오디오 세그먼트의 처음이나 마지막 몇 마디를 전사하지 않는 문제 또는 모델이 실제 오디오와 관련이 없는 전사를 출력하는 완전한 환각 등을 포함합니다. 섹션 4.5에서 논의한 디코딩 세부 정보는 큰 도움이 되지만, 우리는 Whisper 모델을 고품질 감독형 데이터셋에 대한 미세 조정하거나 디코딩 성능을 더 직접 최적화하기 위해 강화 학습을 사용하여 이러한 오류를 더 줄일 수 있을 것으로 예상합니다.

낮은 리소스 언어용 훈련 데이터 증가

Figure 3에서 확인할 수 있듯이 Whisper의 음성 인식 성능은 여전히 많은 언어에서 상당히 낮습니다. 동일한 분석은 각 언어의 훈련 데이터 양으로 언어의 성능을 아주 잘 예측한다는 명확한 향상 경로를 제시합니다. 현재 우리의 사전 훈련 데이터셋은 주로 인터넷의 영어 중심 부분에서 가져온 데이터 수집 파이프라인의 편향으로 인해 영어가 매우 중요한 비중을 차지하고 있으므로 대부분의 언어는 1000시간 미만의 훈련 데이터만 가지고 있습니다. 이러한 드문 언어의 데이터 양을 늘리는 것에 대한 목표 지향적 노력은 전체 훈련 데이터셋 크기의 작은 증가만 있어도 평균 음성 인식 성능을 크게 향상시킬 수 있을 것입니다.

미세 조정 연구

이 연구에서는 음성 처리 시스템의 견고성 특성에 중점을 두어 Whisper의 제로샷 전송 성능만 연구했습니다. 이것은 일반적인 신뢰성을 나타내기 때문에 연구해야하는 중요한 설정입니다. 그러나 고품질의 감독형 음성 데이터가 존재하는 많은 도메인에서는 미세 조정을 통해 결과를 더 개선할 수 있을 것으로 예상됩니다. 미세 조정 연구를 연구하는 추가 이점은 훨씬 더 일반적인 평가 설정이기 때문에 직접적인 비교를 허용한다는 것입니다.

로버스트성에 대한 언어 모델의 영향 연구

소개에서 논의한대로, Whisper의 로버스트성은 그 강력한 디코더에 부분적으로 기인한다고 의심합니다. 현재 Whisper의 이점이 어디에서 유래되었는지에 대한 정도는 아직 알려져 있지 않습니다. 이것은 Whisper의 여러 디자인 구성 요소를 삭제하거나 CTC 모델에서 디코더를 훈련시키는 것과 같은 방법으로 연구하거나 언어 모델과 함께 사용될 때 wav2vec 2.0과 같은 기존 음성 인식 인코더의 성능이 어떻게 변하는지 연구함으로써 연구할 수 있습니다.
보조 훈련 목표 추가
Whisper는 최근 대규모 음성 인식 시스템의 상당한 부분과 달리 자율적 사전 훈련이나 자기 교육 기술이 부족한 점에서 주목할만한 차이점이 있습니다. 우리는 좋은 성능을 얻기 위해 이러한 기술이 필요하지 않다고 판단했지만, 이러한 기술을 통합함으로써 결과를 더 개선할 수 있을 것입니다.
결론
Whisper는 음성 인식 연구에서 약점 있는 감독형 사전 훈련의 스케일링이 지금까지 미묘하게 인식되어 왔음을 시사합니다. 우리는 최근 대규모 음성 인식 연구의 핵심이었던 자체 감독 및 자체 훈련 기술이 필요하지 않고, 대규모이고 다양한 지도 데이터셋에서 훈련하고 제로샷 전송에 중점을 두면 음성 인식 시스템의 견고성을 크게 향상시킬 수 있음을 보여주었습니다.
감사의 말
Whisper 데이터 생성에 참여한 수백만 명의 사람들에게 감사드립니다. 또한 이 프로젝트를 영감을 주는 폭포 하이킹에서의 대화를 위해 Nick Ryder, Will Zhuk 및 Andrew Carr에게 감사드립니다. 이 프로젝트에서 사용한 소프트웨어 및 하드웨어 인프라에 대한 중요한 역할을 한 OpenAI의 가속 및 슈퍼컴퓨팅 팀에 감사드립니다. 또한 정책 관점에서 이 프로젝트를 조언해 준 Pamela Mishkin에게 감사드립니다. 마지막으로, 이 프로젝트 전반에 걸쳐 사용된 다양한 소프트웨어 패키지, Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020) 및 scikit-learn (Pedregosa et al., 2011)를 비롯한 소프트웨어 패키지 개발자들에게 감사드립니다.
2http://nlp.seas.harvard.edu/2018/04/03/attention.html
3모든 경우에 대해 피드 포워드/필터 크기를 4H로 설정합니다. 즉, H = 768인 경우 3072이고, H = 1024인 경우 4096입니다.
4문헌에서는 양방향 Trans-Input/Output Representations에 대해 언급합니다. BERT가 다양한 하위 작업을 처리할 수 있도록하기 위해 입력 표현은 하나의 토큰 시퀀스에서 단일 문장과 문장 쌍 (예 : (cid:104) 질문, 답변 (cid:105))을 명확하게 나타낼 수 있습니다. 이 연구에서 "문장"은 실제 언어적 문장이 아닌 임의의 연속 텍스트 범위를 의미합니다. "시퀀스"는 BERT에 대한 입력 토큰 시퀀스를 나타내며, 단일 문장이나 두 문장이 함께 패킹 될 수 있습니다. 우리는 30,000 토큰 어휘를 가진 WordPiece 임베딩 (Wu et al., 2016)을 사용합니다. 모든 시퀀스의 첫 번째 토큰은 항상 특수 분류 토큰 ([CLS])입니다. 이 토큰에 해당하는 최종 숨겨진 상태는 분류 작업을 위한 집계 시퀀스 표현으로 사용됩니다. 문장 쌍은 하나의 시퀀스로 함께 패킹됩니다. 우리는 두 가지 방법으로 문장을 구분합니다. 첫째, 특수 토큰 ([SEP])으로 구분합니다. 둘째, 문장 A 또는 문장 B에 속하는지를 나타내는 학습된 임베딩을 각 토큰에 추가합니다. Figure 1에 표시된대로 입력 임베딩을 E, 특수 [CLS] 토큰에 해당하는 최종 숨겨진 벡터를 C ∈ RH로 표기하고, i번째 입력 토큰에 해당하는 최종 숨겨진 벡터를 T i ∈ RH로 표기합니다. 주어진 토큰의 입력 표현은 해당 토큰, 세그먼트 및 위치 임베딩을 합산하여 구성됩니다. 이 구성의 시각화는 Figure 2에서 볼 수 있습니다.

3.1 BERT 사전 훈련

Peters et al. (2018a) 및 Radford et al. (2018)과 달리, 우리는 전통적인 좌에서 우로 또는 우에서 좌로 언어 모델을 사용하지 않고 BERT를 사전 훈련합니다. 대신, 우리는 이 섹션에서 설명하는 두 가지 비지도 학습 작업을 사용하여 BERT를 사전 훈련합니다. 이 단계는 그림 1의 왼쪽 부분에 제시되어 있습니다.

과제 #1: 가려진 언어 모델 (Masked LM) 직관적으로, 깊은 양방향 모델이 좌에서 우로 모델이나 좌에서 우로 모델과 우에서 좌로 모델의 얕은 연결보다 엄격하게 더 강력하다고 믿는 것은 합리적입니다. 불행하게도, 표준 조건부 언어 모델은 좌에서 우로 또는 우에서 좌로만 훈련될 수 있습니다. 양방향 조건부는 각 단어가 간접적으로 "자신을 볼 수 있게" 하며, 모델이 다중 계층 컨텍스트에서 대상 단어를 쉽게 예측할 수 있게 됩니다.

전자는 종종 "트랜스포머 인코더"로 불리며, 
왼쪽 컨텍스트만을 고려하는 버전은 "트랜스포머 디코더"로 불리며, 텍스트 생성에 사용될 수 있습니다.

깊은 양방향 표현을 훈련시키기 위해, 우리는 단순히 입력 토큰의 일부를 무작위로 마스킹하고, 그 마스킹된 토큰을 예측합니다. 이 절차를 "마스킹된 언어 모델" (MLM)이라고 합니다. 문헌에서는 종종 클로즈 작업(Cloze task)이라고도 부릅니다 (Taylor, 1953). 이 경우, 마스킹된 토큰에 해당하는 최종 은닉 벡터는 표준 언어 모델과 마찬가지로 어휘에 대한 출력 소프트맥스로 전달됩니다. 모든 실험에서 우리는 각 시퀀스의 모든 WordPiece 토큰의 15%를 무작위로 마스킹합니다. Vincent 등 (2008)의 노이즈 제거 오토인코더와는 달리, 우리는 전체 입력을 복원하는 대신 마스킹된 단어만 예측합니다.
이를 통해 우리는 양방향 사전 훈련 모델을 얻을 수 있지만, 단점은 [MASK] 토큰이 fine-tuning 중에 나타나지 않는다는 점에서 사전 훈련과 fine-tuning 사이에 불일치가 발생한다는 것입니다. 이를 완화하기 위해, 우리는 항상 "마스킹된" 단어를 실제 [MASK] 토큰으로 대체하지 않습니다. 훈련 데이터 생성기는 예측을 위해 무작위로 토큰 위치의 15%를 선택합니다. i번째 토큰이 선택된 경우, i번째 토큰을 (1) 80%의 확률로 [MASK] 토큰으로 대체하고, (2) 10%의 확률로 무작위 토큰으로 대체하고, (3) 10%의 확률로 변경되지 않은 i번째 토큰으로 대체합니다. 그런 다음, T i는 원래 토큰을 예측하기 위해 교차 엔트로피 손실과 함께 사용됩니다. 이 절차의 변형을 부록 C.2에서 비교합니다.

과제 #2: 다음 문장 예측 (NSP)
질문 응답 (QA) 및 자연어 추론 (NLI)과 같은 많은 중요한 하위 작업은 두 문장 간의 관계를 이해하는 것에 기반하며, 이는 언어 모델링으로 직접적으로 포착되지 않습니다. 문장 관계를 이해하는 모델을 훈련시키기 위해, 우리는 단일 언어 말뭉치에서 쉽게 생성할 수 있는 이진화된 다음 문장 예측 작업을 사전 훈련합니다. 구체적으로, 각 사전 훈련 예제에 대해 문장 A와 B를 선택할 때, 50%의 경우 B는 A 다음에 오는 실제 다음 문장입니다 (IsNext로 레이블링됨) 그리고 50%의 경우 B는 말뭉치에서 임의의 문장입니다 (NotNext로 레이블링됨). 그림 1에서 보여주는 것처럼, C는 다음 문장 예측 (NSP)에 사용됩니다. 이 간단함에도 불구하고, 우리는 5.1 절에서 이 작업을 향한 사전 훈련이 QA와 NLI 모두에 매우 유익하다는 것을 보여줍니다. 6

5 최종 모델은 NSP에서 97% - 98%의 정확도를 달성합니다.
6 ThevectorC는 NSP로 훈련되었기 때문에 세부 조정 없이는 의미 있는 문장 표현이 아닙니다.
[CLS] 그는 놀기를 좋아합니다 [SEP] 내 개는 귀여워요 [SEP] 입력

E [CLS]                E 그는 E 놀기 E ##을 E 좋아합니다 E [SEP] E 내 E 개 E는 E 귀엽습니다 E [SEP]
토큰
임베딩

LLM은 법학 석사 학위로, 법학 분야에서 깊은 지식과 전문성을 갖춘 전문가를 양성하는 프로그램입니다. 

LLM은 법학 분야에서의 진취적인 연구와 학문적인 교육을 통해 학생들에게 국제적인 법적 문제에 대한 이해력과 해결 능력을 갖추게 합니다. 

LLM 프로그램은 학생들에게 법학 분야에서의 전문적인 경력을 쌓을 수 있는 기회를 제공하며, 다양한 법적 분야에서의 전문 지식을 습득할 수 있도록 합니다. 

LLM은 법학 분야에서의 경력을 쌓고자 하는 학생들에게 매우 유용한 학위입니다. 

LLM 프로그램은 학생들에게 법학 분야에서의 깊은 이해와 전문성을 갖출 수 있는 훌륭한 기회를 제공합니다. 

LLM은 법학 분야에서의 전문적인 경력을 쌓고자 하는 학생들에게 매우 유용한 학위입니다. 

LLM 프로그램은 학생들에게 법학 분야에서의 깊은 이해와 전문성을 갖출 수 있는 훌륭한 기회를 제공합니다.

E
B
E
B
E
B
E
B
E
B
E
A
E
A
E
A
E
A
E
A
세그먼트
임베딩

엘엘엠에 대해 아래 텍스트를 한국어로 번역해 주시겠습니까?

'번역 외에 추가적인 답변은 없습니다.'

번역 후 한 줄을 비워주세요.

E
6
E
7
E
8
E
9
E
10
E
1
E
2
E
3
E
4
E
5
위치
임베딩

그림 2: BERT 입력 표현. 입력 임베딩은 토큰 임베딩, 세그멘테이션 임베딩 및 위치 임베딩의 합입니다.

NSP 작업은 Jernite et al. (2017) 및 Logeswaran and Lee (2018)에서 사용된 표현 학습 목표와 밀접한 관련이 있습니다. 그러나 이전 연구에서는 문장 임베딩만 하위 작업으로 전달되었지만, BERT는 모든 매개변수를 전달하여 최종 작업 모델 매개변수를 초기화합니다.

사전 훈련 데이터 사전 훈련 절차는 주로 언어 모델 사전 훈련에 관한 기존 문헌을 따릅니다. 사전 훈련 말뭉치로는 BooksCorpus (800M 단어) (Zhu et al., 2015)와 영어 위키피디아 (2,500M 단어)를 사용합니다. 위키피디아에서는 목록, 표, 헤더를 무시하고 텍스트 단락만 추출합니다. 연속된 긴 시퀀스를 추출하기 위해 Billion Word Benchmark (Chelba et al., 2013)와 같은 문장 수준의 섞인 말뭉치 대신 문서 수준의 말뭉치를 사용하는 것이 중요합니다.

3.2 BERT의 세부 조정

LLM에 대한 것입니다.

Transformer의 self-attention 메커니즘을 통해 BERT는 적절한 입력과 출력을 교체함으로써 많은 하위 작업을 모델링할 수 있으므로 fine-tuning은 간단합니다. 텍스트 쌍을 포함하는 응용 프로그램의 경우, Parikh et al. (2016); Seo et al. (2017)와 같이 양방향 교차 어텐션을 적용하기 전에 텍스트 쌍을 독립적으로 인코딩하는 것이 일반적인 패턴입니다. 그러나 BERT는 self-attention 메커니즘을 사용하여 이 두 단계를 통합합니다. 즉, self-attention을 사용하여 연결된 텍스트 쌍을 인코딩하면 두 문장 간에 양방향 교차 어텐션이 효과적으로 포함됩니다. 각 작업에 대해 우리는 간단히 작업별 입력과 출력을 BERT에 연결하고 모든 매개변수를 end-to-end로 fine-tuning합니다. 입력에서, 사전 훈련의 문장 A와 문장 B는 (1) 동의어화에서의 문장 쌍, (2) 함의에서의 가설-전제 쌍, (3) 질문 응답에서의 질문-지문 쌍과 유사합니다.

(4) 텍스트 분류나 시퀀스 태깅에서의 비정상적인 텍스트-∅ 쌍입니다. 출력에서 토큰 표현은 토큰 수준 작업(시퀀스 태깅이나 질문 응답과 같은)을 위해 출력 레이어로 전달되고, [CLS] 표현은 분류(엔테일먼트나 감성 분석과 같은)을 위해 출력 레이어로 전달됩니다.
미세 조정은 사전 훈련에 비해 비교적 비용이 적게 듭니다. 논문의 모든 결과는 정확히 동일한 사전 훈련 모델을 기반으로 구글 클라우드 TPU에서 최대 1시간이면 복제할 수 있으며, GPU에서는 몇 시간이 걸립니다. 작업별 세부 정보는 섹션 4의 해당 하위 섹션에서 설명합니다. 추가 세부 정보는 부록 A.5에서 찾을 수 있습니다.

4 실험

이 섹션에서는 11가지 NLP 작업에 대한 BERT 세부 조정 결과를 제시합니다.

4.1 접착제

일반 언어 이해 평가(GLUE) 벤치마크(Wang et al., 2018a)는 다양한 자연어 이해 작업의 모음입니다. GLUE 데이터셋에 대한 자세한 설명은 부록 B.1에 포함되어 있습니다.

GLUE에서 fine-tuning하기 위해 입력 시퀀스(단일 문장 또는 문장 쌍)를 섹션 3에서 설명한대로 표현하고, 첫 번째 입력 토큰([CLS])에 해당하는 최종 숨겨진 벡터 C ∈ RH를 집계 표현으로 사용합니다. fine-tuning 중 도입되는 유일한 새로운 매개변수는 분류 계층 가중치 W ∈ RK×H입니다. 여기서 K는 레이블의 수입니다. C와 W를 사용하여 표준 분류 손실을 계산합니다. 즉, log(softmax(CWT)).

예를 들어, BERT SQuAD 모델은 단일 Cloud TPU에서 약 30분 동안 훈련될 수 있으며 Dev F1 점수 91.0%를 달성할 수 있습니다.

https://gluebenchmark.com/faq에서 (10)을 참조하세요.

System       MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average

392k    363k 108k 67k  8.5k 5.7k  3.5k 2.5k  -
Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0
BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0
OpenAI GPT    82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1
BERTBASE      84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6
BERTLARGE     86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1

표 1: 평가 서버(https://gluebenchmark.com/leaderboard)에 의해 평가된 GLUE 테스트 결과입니다.
각 작업 아래의 숫자는 훈련 예제의 수를 나타냅니다. "평균" 열은 공식 GLUE 점수와 약간 다릅니다. WNLI 세트를 제외하기 때문입니다. BERT와 OpenAI GPT는 단일 모델, 단일 작업입니다. QQP와 MRPC에 대해서는 F1 점수를, STS-B에 대해서는 Spearman 상관관계를, 그 외 작업에 대해서는 정확도 점수를 보고합니다. BERT를 구성 요소 중 하나로 사용하는 항목은 제외합니다.

우리는 모든 GLUE 작업에 대해 데이터를 3번 에포크 동안 32 배치 크기로 세밀 조정합니다. 각 작업에 대해 우리는 Dev 세트에서 최상의 세밀 조정 학습률 (5e-5, 4e-5, 3e-5 및 2e-5 중에서)을 선택했습니다. 또한, BERTLARGE의 경우 작은 데이터셋에서 세밀 조정이 때때로 불안정한 경우가 있어 여러 번의 무작위 재시작을 실행하고 Dev 세트에서 최상의 모델을 선택했습니다. 무작위 재시작을 사용할 때는 동일한 사전 훈련된 체크포인트를 사용하지만 세밀 조정 데이터 섞기와 분류기 레이어 초기화를 다르게 수행합니다.

결과는 표 1에 제시되었습니다. BERTBASE와 BERTLARGE는 모든 작업에서 모든 시스템을 큰 폭으로 능가하여 이전 최고 성능 대비 4.5%와 7.0%의 평균 정확도 향상을 얻었습니다. BERTBASE와 OpenAI GPT는 모델 아키텍처를 제외하고는 거의 동일하며, 가장 크고 가장 널리 보고된 GLUE 작업인 MNLI에서 BERT는 4.6%의 절대 정확도 향상을 얻습니다. 공식 GLUE 리더보드에서 BERTLARGE는 80.5의 점수를 얻었으며, 글 작성일 기준으로 OpenAI GPT는 72.8의 점수를 얻었습니다.

우리는 BERTLARGE가 특히 훈련 데이터가 매우 적은 작업을 포함하여 모든 작업에서 BERTBASE보다 큰 성능을 발휘한다는 것을 발견했습니다. 모델 크기의 영향은 섹션 5.2에서 더 자세히 탐구됩니다.

4.2 SQuAD v1.1

4.2 SQuAD v1.1

스탠포드 질문 응답 데이터셋(SQuAD v1.1)은 10만 개의 크라우드소싱 질문/답변 쌍(Rajpurkar et al., 2016)의 모음입니다. 질문과 단락이 주어졌을 때, 이 데이터셋은 기계가 정확한 답변을 추출할 수 있는지를 평가하는 데 사용됩니다.

GLUE 데이터 세트 배포에는 테스트 라벨이 포함되어 있지 않으며, BERTBASE와 BERTLARGE 각각에 대해 단일 GLUE 평가 서버 제출만 수행했습니다.

https://gluebenchmark.com/leaderboard

위키피디아에는 답이 포함되어 있으며, 과제는 텍스트 스팬에서 답을 예측하는 것입니다.
그림 1에 나와 있는 것처럼, 질문 응답 과제에서는 입력 질문과 텍스트를 하나의 패킹된 시퀀스로 표현합니다. 질문은 A 임베딩을 사용하고 텍스트는 B 임베딩을 사용합니다. 우리는 미세 조정 중에 시작 벡터 S ∈ RH와 끝 벡터 E ∈ RH를 도입합니다. 단어 i가 답 스팬의 시작인 확률은 T i와 S의 내적을 계산한 후 단락의 모든 단어에 대해 소프트맥스를 적용하여 계산됩니다: P i = eS·Ti (cid:80)

j
eS·Tj .
유사한 공식은 답변 범위의 끝에 사용됩니다. 위치 i에서 위치 j까지의 후보 범위의 점수는 S·T i + E·T j로 정의되며, j ≥ i인 최대 점수 범위가 예측으로 사용됩니다. 훈련 목표는 올바른 시작 위치와 끝 위치의 로그 우도의 합입니다. 학습률은 5e-5이고 배치 크기는 32로 3개의 epoch 동안 미세 조정합니다.
테이블 2는 최고의 리더보드 항목과 최고의 게시된 시스템 결과 (Seo et al., 2017; Clark and Gardner, 2018; Peters et al., 2018a; Hu et al., 2018)을 보여줍니다. SQuAD 리더보드의 최고 결과는 최신의 공개 시스템 설명이 없으며, 시스템을 훈련할 때 공개 데이터를 사용할 수 있습니다. 따라서 우리는 TriviaQA (Joshi et al., 2017)에서 먼저 미세 조정한 후 SQuAD에서 미세 조정하여 우리의 시스템에 적당한 데이터 증강을 사용합니다.
우리의 최고 성능 시스템은 앙상블에서 +1.5 F1, 단일 시스템에서 +1.3 F1로 최고의 리더보드 시스템을 능가합니다. 실제로, 우리의 단일 BERT 모델은 F1 점수 측면에서 최고의 앙상블 시스템을 능가합니다. TriviaQA 미세 조정 없이.

11QANet는 Yu et al. (2018)에서 설명되었지만, 해당 시스템은 출판 이후 크게 개선되었습니다.
시스템   개발   테스트
EM   F1   EM   F1

상위 리더보드 시스템 (2018년 12월 10일)
인간            -  - 82.3 91.2
#1 앙상블 - nlnet - - 86.0 91.7
#2 앙상블 - QANet - - 84.5 90.5

게시됨
BiDAF+ELMo (단일) - 85.6 - 85.8
R.M. Reader (앙상블) 81.2 87.9 82.3 88.5

우리들
BERTBASE (단일) 80.8 88.5 - -
BERTLARGE (단일) 84.1 90.9 - -
BERTLARGE (앙상블) 85.8 91.8 - -
BERTLARGE (단일+TriviaQA) 84.2 91.1 85.1 91.8
BERTLARGE (앙상블+TriviaQA) 86.2 92.2 87.4 93.2

표 2: SQuAD 1.1 결과. BERT 앙상블은 서로 다른 사전 훈련 체크포인트와 세부 조정 시드를 사용하는 7개의 시스템입니다.

시스템 개발 테스트
EM F1 EM F1

상위 리더보드 시스템 (2018년 12월 10일)
인간           86.3 89.0 86.9 89.5
1위 개인 - MIR-MRC (F-Net) - - 74.8 78.0
2위 개인 - nlnet - -  74.2 77.1

게시됨
unet (앙상블) - 71.4 74.9
SLQA+ (단일) - 71.4 74.4

우리들
BERTLARGE (단일) 78.7 81.9 80.0 83.1

표 3: SQuAD 2.0 결과. BERT를 구성 요소 중 하나로 사용하는 항목은 제외합니다.

튜닝 데이터를 사용하면, 우리는 0.1-0.4 F1만큼 손실하지만, 여전히 기존 시스템들을 크게 앞서고 있습니다.12

4.3 SQuAD v2.0

4.3 SQuAD v2.0

SQuAD 2.0 작업은 SQuAD 1.1 문제 정의를 확장하여 제공된 문단에 짧은 답변이 없을 수 있는 가능성을 허용하여 문제를 더 현실적으로 만듭니다.
이 작업을 위해 SQuAD v1.1 BERT 모델을 확장하기 위해 간단한 접근 방식을 사용합니다. 답변이 없는 질문은 [CLS] 토큰에서 시작하고 끝나는 답변 범위를 가진 것으로 처리합니다. 시작과 끝 답변 범위의 확률 공간은 [CLS] 토큰의 위치를 포함하여 확장됩니다. 예측을 위해 우리는 답변이 없는 범위의 점수인 s null = S·C + E·C를 최상의 비-NULL 범위의 점수와 비교합니다.

우리가 사용한 TriviaQA 데이터는 적어도 하나의 제공된 가능한 답변을 포함하는 문서의 처음 400 토큰으로 구성된 TriviaQA-Wiki 단락으로 이루어져 있습니다.

시스템 개발 테스트

ESIM+GloVe   51.9 52.7
ESIM+ELMo    59.1 59.2
OpenAI GPT    - 78.0

BERTBASE     81.6 -
BERTLARGE    86.6 86.3

BERTBASE     81.6 -
BERTLARGE    86.6 86.3

인간 (전문가)† - 85.0
인간 (5개의 주석)† - 88.0

표 4: SWAG 개발 및 테스트 정확도. †인간의 성능은 SWAG 논문에서 보고된 것과 같이 100개의 샘플로 측정됩니다.

ˆ s i,j = maxj≥iS·T i + E·T j. 우리는 ˆ s i,j > s null + τ 일 때, 비어있지 않은 답변을 예측합니다. 여기서 임계값 τ는 F1을 최대화하기 위해 개발 세트에서 선택됩니다.
이 모델에는 TriviaQA 데이터를 사용하지 않았습니다. 학습률은 5e-5이고 배치 크기는 48로 2 에포크 동안 미세 조정했습니다.
이전 리더보드 항목 및 최고의 출판물(Sun et al., 2018; Wang et al., 2018b)과 비교한 결과는 표 3에 나와 있습니다. BERT를 구성 요소 중 하나로 사용하는 시스템은 제외되었습니다. 이전 최고 시스템 대비 +5.1 F1 향상을 관찰하였습니다.

4.4 SWAG
4.4 SWAG

적대적 생성과 관련된 상황(SWAG) 데이터셋은 113,000개의 문장 쌍 완성 예제를 포함하며, 이는 논리적인 상식 추론을 평가합니다(Zellers et al., 2018). 주어진 문장에 대해, 고르기 가장 타당한 연속 중 하나를 선택하는 것이 과제입니다.
SWAG 데이터셋에서 fine-tuning을 할 때, 우리는 네 개의 입력 시퀀스를 구성합니다. 각각은 주어진 문장 (문장 A)과 가능한 연속 (문장 B)의 연결을 포함합니다. 유일하게 도입된 작업 특정 매개변수는 [CLS] 토큰 표현 C와의 내적이 각 선택에 대한 점수를 나타내며, 소프트맥스 레이어로 정규화됩니다.
우리는 학습률이 2e-5이고 배치 크기가 16인 조건에서 모델을 3 에포크 동안 fine-tuning합니다. 결과는 표 4에 제시되어 있습니다. BERTLARGE는 저자의 기준선인 ESIM+ELMo 시스템보다 +27.1% 더 우수하며, OpenAI GPT보다 8.3% 우수한 성능을 보입니다.

5. 제거 연구

이 섹션에서는 BERT의 여러 측면에 대한 소성 실험을 수행하여 그들의 상대적 중요성을 더 잘 이해하기 위해 합니다.

추가적인 개발 세트 작업
MNLI-m QNLI MRPC SST-2 SQuAD
(Acc) (Acc) (Acc) (Acc) (F1)

BERTBASE 84.4 88.4 86.7 92.7 88.5
NSP 없음 83.9 84.9 86.5 92.6 87.9
LTR 및 NSP 없음 82.1 84.3 77.5 92.1 77.8
+ BiLSTM 82.1 84.1 75.7 91.6 84.9

표 5: BERTBASE 아키텍처를 사용한 사전 훈련 작업에 대한 제거 실험 결과. "No NSP"는 다음 문장 예측 작업 없이 훈련된 모델입니다. "LTR & No NSP"는 다음 문장 예측 없이 왼쪽에서 오른쪽으로 언어 모델로 훈련된 모델로, OpenAI GPT와 유사합니다. "+ BiLSTM"은 "LTR + No NSP" 모델 위에 무작위로 초기화된 BiLSTM을 추가하여 세밀 조정하는 모델입니다.

추출 연구는 부록 C에서 찾을 수 있습니다.

5.1 사전 훈련 작업의 효과

우리는 BERT의 깊은 양방향성의 중요성을 입증하기 위해 BERTBASE와 동일한 사전 훈련 데이터, 세부 조정 방법 및 하이퍼파라미터를 사용하여 두 가지 사전 훈련 목표를 평가합니다.

NSP 없음: "마스크된 언어 모델" (MLM)을 사용하여 훈련된 양방향 모델이지만 "다음 문장 예측" (NSP) 작업은 수행하지 않음.
LTR & NSP 없음: 표준적인 좌측에서 우측으로 (LTR) 언어 모델을 사용하여 훈련된 좌측 컨텍스트만을 고려하는 모델. 좌측 제약 조건은 세부 튜닝에서도 적용되었으며, 이를 제거하면 사전 훈련/세부 튜닝 불일치가 발생하여 하류 성능이 저하됩니다. 또한, 이 모델은 NSP 작업 없이 사전 훈련되었습니다. 이는 OpenAI GPT와 직접 비교할 수 있으며, 우리의 더 큰 교육 데이터 세트, 입력 표현 및 세부 튜닝 방식을 사용합니다.
먼저 NSP 작업이 가져온 영향을 살펴봅니다. 표 5에서 NSP를 제거하면 QNLI, MNLI 및 SQuAD 1.1에서 성능이 크게 저하되는 것을 보여줍니다. 다음으로, 양방향 표현을 훈련시키는 것의 영향을 "NSP 없음"과 "LTR & NSP 없음"을 비교하여 평가합니다. LTR 모델은 모든 작업에서 MLM 모델보다 성능이 떨어지며, MRPC와 SQuAD에서 크게 하락합니다. SQuAD의 경우, 토큰 수준의 숨겨진 상태에 오른쪽 컨텍스트가 없으므로 LTR 모델은 토큰 예측에서 성능이 좋지 않을 것이 직관적으로 명확합니다. LTR 시스템을 강화하기 위해 무작위로 초기화된 BiLSTM을 추가하면 SQuAD에서 결과가 상당히 향상되지만,

결과는 여전히 사전 훈련된 양방향 모델의 결과보다 훨씬 나쁘다. BiLSTM은 GLUE 작업의 성능을 저하시킨다.
우리는 LTR과 RTL 모델을 별도로 훈련하고 각 토큰을 두 모델의 연결로 표현하는 것도 가능하다는 것을 인식한다. ELMo와 같다. 그러나: (a) 이는 단일 양방향 모델보다 두 배로 비용이 많이 든다. (b) 이는 QA와 같은 작업에 대해 직관적이지 않다. RTL 모델은 질문에 대한 답변을 조건으로 설정할 수 없다. (c) 이는 깊은 양방향 모델보다 엄격하게 덜 강력하다. 왜냐하면 각 레이어에서 왼쪽과 오른쪽 컨텍스트를 모두 사용할 수 있기 때문이다.

5.2 모델 크기의 효과

이 섹션에서는 모델 크기가 세밀 조정 작업 정확도에 미치는 영향을 탐색합니다. 우리는 레이어, 은닉 유닛 및 어텐션 헤드의 수가 다른 여러 개의 BERT 모델을 훈련시켰으며, 그 외에는 이전에 설명한 것과 동일한 하이퍼파라미터와 훈련 절차를 사용했습니다.

선택한 GLUE 작업의 결과는 표 6에 나와 있습니다. 이 표에서는 세밀 조정의 5개의 무작위 재시작으로부터 얻은 평균 Dev Set 정확도를 보고합니다. 우리는 더 큰 모델이 모든 네 가지 데이터셋에서 엄격한 정확도 향상을 이끌어냄을 볼 수 있습니다. 심지어 MRPC는 레이블이 지정된 훈련 예제가 3,600개뿐이며 사전 훈련 작업과는 매우 다릅니다. 또한 이미 기존 문헌에 비해 상당히 큰 모델 위에 이러한 중요한 개선을 달성할 수 있다는 점도 놀랍습니다. 예를 들어, Vaswani et al. (2017)에서 탐색한 가장 큰 Transformer는 (L=6, H=1024, A=16)로 인코더에 1억 개의 매개변수를 가지고 있으며, 우리가 문헌에서 찾은 가장 큰 Transformer는 (L=64, H=512, A=2)로 2억 3천 5백만 개의 매개변수를 가지고 있습니다 (Al-Rfou et al., 2018). 대조적으로, BERTBASE는 1억 1천만 개의 매개변수를 포함하고 BERTLARGE는 3억 4천만 개의 매개변수를 포함합니다.

모델 크기를 증가시키면 기계 번역 및 언어 모델링과 같은 대규모 작업에서 지속적인 개선이 이루어질 것이라는 것은 오랫동안 알려져 왔습니다. 이는 표 6에 나와 있는 보유된 훈련 데이터의 언어 모델 퍼플렉서티로도 입증되었습니다. 그러나 우리는 모델이 충분히 사전 훈련되었다면 극단적인 모델 크기로 확장하는 것이 매우 작은 규모의 작업에서도 큰 개선을 가져온다는 것을 설득력 있게 처음으로 입증한 작업이라고 믿습니다. Peters et al. (2018b)는 사전 훈련된 bi-LM 크기를 두 개에서 네 개로 증가시키는 것이 하류 작업에 미치는 영향에 대해 혼합된 결과를 제시했으며, Melamud et al. (2016)는 200에서 600으로 숨겨진 차원 크기를 증가시키는 것이 도움이 되었지만, 1,000으로 더 증가시키는 것은 추가 개선을 가져오지 않았다고 언급했습니다. 이전 작업들은 특징 기반 접근법을 사용했는데, 우리는 모델이 하류 작업에서 직접 세밀 조정되며 무작위로 초기화된 추가 매개변수의 수가 매우 적을 때, 작업별 모델은 매우 작은 규모의 하류 작업 데이터에서도 더 크고 표현력이 높은 사전 훈련된 표현을 활용할 수 있다고 가설을 세웁니다.

5.3 BERT를 활용한 특징 기반 접근 방식

지금까지 제시된 모든 BERT 결과는 fine-tuning 접근 방식을 사용했습니다. 여기서는 사전 훈련된 모델에 간단한 분류 계층을 추가하고 모든 매개변수를 동시에 fine-tuning하는 방식입니다. 그러나 feature-based 접근 방식은 사전 훈련된 모델에서 고정된 특징을 추출하는 방식으로 특정 이점이 있습니다. 첫째, 모든 작업이 Transformer 인코더 아키텍처로 쉽게 표현될 수 있는 것은 아니며, 따라서 작업별로 특정 모델 아키텍처를 추가해야 합니다. 둘째, 비용이 많이 드는 훈련 데이터의 표현을 미리 계산하고 이 표현 위에 저렴한 모델로 여러 실험을 실행하는 것에는 주요한 계산상의 이점이 있습니다. 이 섹션에서는 BERT를 CoNLL-2003 Named Entity Recognition (NER) 작업에 적용하여 두 가지 접근 방식을 비교합니다. BERT의 입력으로는 case-preserving WordPiece 모델을 사용하고 데이터에서 제공되는 최대 문서 컨텍스트를 포함합니다. 표준적인 방법을 따라 이를 태깅 작업으로 정의하지만 CRF는 사용하지 않습니다.

하이퍼파라미터    개발 세트 정확도

#L #H #A LM (사람들) MNLI-m MRPC SST-2

3 768 12 5.84 77.9 79.8 88.4
6 768 3 5.24 80.6 82.2 90.7
6 768 12 4.68 81.9 84.8 91.3
12 768 12 3.99 84.4 86.7 92.9
12 1024 16 3.54 85.7 86.9 93.3
24 1024 16 3.23 86.6 87.8 93.7

표 6: BERT 모델 크기에 따른 제거 효과. #L = 레이어 수; #H = 은닉 크기; #A = 어텐션 헤드 수. "LM (ppl)"은 보류 중인 훈련 데이터의 마스크된 LM 퍼플렉서티입니다.

시스템 개발 F1 테스트 F1

ELMo (Peters et al., 2018a) 95.7 92.2
CVT (Clark et al., 2018) - 92.6
CSE (Akbik et al., 2018) - 93.1

ELMo (Peters et al., 2018a) 95.7 92.2
CVT (Clark et al., 2018) - 92.6
CSE (Akbik et al., 2018) - 93.1

세밀 조정 접근 방식
BERTLARGE          96.6 92.8
BERTBASE           96.4 92.4

기능 기반 접근법 (BERTBASE)
임베딩 91.0 -
두 번째로 마지막 숨겨진 95.6 -
마지막 숨겨진 94.9 -
마지막 네 숨겨진의 가중 합 95.9 -
마지막 네 숨겨진의 연결 96.1 -
모든 12개 레이어의 가중 합 95.5 -

표 7: CoNLL-2003 Named Entity Recognition 결과. 하이퍼파라미터는 Dev 세트를 사용하여 선택되었습니다. 보고된 Dev 및 Test 점수는 해당 하이퍼파라미터를 사용하여 5번의 무작위 재시작을 통해 평균화되었습니다.

출력에 레이어를 추가합니다. 우리는 첫 번째 서브 토큰의 표현을 NER 레이블 세트에 대한 토큰 수준 분류기의 입력으로 사용합니다.

미세 조정 접근 방식을 제거하기 위해, 우리는 BERT의 어떤 매개변수도 미세 조정하지 않고 하나 이상의 레이어에서 활성화를 추출하는 특징 기반 접근 방식을 적용합니다. 이러한 문맥 임베딩은 분류 레이어 이전에 무작위로 초기화된 2층 768차원 BiLSTM의 입력으로 사용됩니다.

결과는 표 7에 제시되었습니다. BERTLARGE는 최첨단 방법들과 경쟁력을 갖습니다. 가장 성능이 우수한 방법은 사전 훈련된 Transformer의 상위 네 개의 은닉 레이어에서 토큰 표현을 연결하는 것인데, 이는 전체 모델을 fine-tuning하는 것과 0.3 F1만 차이가 납니다. 이는 BERT가 fine-tuning 및 특징 기반 접근 방식 모두에 효과적임을 보여줍니다.

결론

최근 언어 모델과의 전이 학습을 통한 경험적 개선은 풍부한 비지도 사전 훈련이 많은 언어 이해 시스템의 필수 요소임을 입증하였습니다. 특히, 이러한 결과는 저자원 작업조차도 깊은 단방향 아키텍처의 이점을 누릴 수 있도록 합니다. 우리의 주요 기여는 이러한 결과를 깊은 양방향 아키텍처로 일반화하여 동일한 사전 훈련된 모델이 다양한 NLP 작업을 성공적으로 수행할 수 있도록 하는 것입니다. 

참고문헌

알란 악빅, 던컨 블라이스, 롤랜드 폴그라프.
2018년. 시퀀스 라벨링을 위한 문맥적 문자열 임베딩. 제27회 국제 계산 언어학 회의 논문집, 1638-1649쪽.

라미 알-르푸, 도쿠크 초이, 노아 콘스턴트, 만디 구오, 그리고 리온 존스. 2018년. 더 깊은 셀프 어텐션을 사용한 문자 수준 언어 모델링. arXiv 사전 인쇄물 arXiv:1808.04444.

리에쿠보타안도와 장통. 2005. 다중 작업과 라벨이 없는 데이터로부터 예측 구조를 학습하기 위한 프레임워크. 기계 학습 연구 저널, 6(Nov):1817–1853.

Luisa Bentivogli, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The fifth PASCAL recognizing textual entailment
challenge. In TAC. NIST.

루이사 벤티보리, 베르나르도 마그니니, 이도 다간,
호아 트랑 당, 그리고 다닐로 임피콜로. 2009년.
제5회 PASCAL 텍스트 함의 인식 챌린지. TAC에서. NIST.

존 블리처, 라이언 맥도날드, 페르난도 페레이라.
2006년. 구조적 일치 학습을 이용한 도메인 적응. 2006년 자연어 처리에 대한 경험적 방법 회의 논문집, 120-128쪽. Association for Computational Linguistics.

사무엘 R. 보우먼, 가보르 안젤리, 크리스토퍼 포츠, 그리고 크리스토퍼 D. 매닝. 2015. 자연어 추론 학습을 위한 대규모 주석 말뭉치. EMNLP에서 발표. 계산언어학 협회.

피터 F 브라운, 피터 V 데소우자, 로버트 L 머서, 빈센트 J 델라 피에트라, 그리고 제니퍼 C 라이. 1992. 자연어의 클래스 기반 n-gram 모델. 계산언어학, 18(4):467–479.

다니엘 세르, 모나 디압, 에네코 아기레, 이니고 로페스-가지오, 그리고 루시아 스페시아. 2017년. Semeval-2017 과제 1: 다국어 및 교차언어 의미적 텍스트 유사성 평가. 제11회 국제 의미 평가 워크샵 (SemEval-2017) 논문집, 1-14쪽, 캐나다 밴쿠버. 계산언어학 협회.

CiprianChelba, TomasMikolov, MikeSchuster, QiGe,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2013. 통계 언어 모델링의 진전을 측정하기 위한 10억 단어 벤치마크. arXiv
논문 arXiv:1312.3005.

Z. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.
Quora 질문 쌍.

크리스토퍼 클락과 매트 가드너. 2018. 간단하고 효과적인 다단락 독해. ACL에서.

케빈 클락, 민 탕 루옹, 크리스토퍼 D 매닝, 그리고 쿼크 레. 2018. 교차보기 훈련을 통한 준지도 시퀀스 모델링. 2018년 자연어 처리에 대한 경험적 방법 회의 논문집, 1914-1925쪽.

Ronan Collobert와 Jason Weston. 2008. 자연어 처리를 위한 통합 아키텍처: 다중 작업 학습을 갖춘 심층 신경망. 제25회 국제 기계 학습 컨퍼런스 논문집, 160-167쪽. ACM.

알렉시스 코너, 두위 키엘라, 홀거 슈벵크, 로이크 바로, 안토완 보르드. 2017. 자연어 추론 데이터로부터의 범용 문장 표현의 지도 학습. 2017년 자연어 처리 기법에 관한 학회 논문집, 670-680쪽, 코펜하겐, 덴마크. 계산언어학 협회.

앤드류 M 다이와 쿼크 V 레. 2015. 반지도 학습 시퀀스 학습. 신경 정보 처리 시스템 발전, 페이지 3079-3087.

J.Deng, W.Dong, R.Socher, L.-J.Li, K.Li, and L.Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.

J.Deng, W.Dong, R.Socher, L.-J.Li, K.Li, 그리고 L.Fei-Fei. 2009. ImageNet: 대규모 계층적 이미지 데이터베이스. CVPR09에서.

윌리엄 B 돌란과 크리스 브로켓. 2005. 문장의 패러프레이즈를 자동으로 구축하는 방법. 제3회 국제 패러프레이징 워크샵 논문집(IWP2005)에서 발표.

윌리엄 페더스, 이안 굿펠로우, 그리고 앤드류 M 다이.
2018. Maskgan: 빈칸 채우기를 통한 더 나은 텍스트 생성.
arXiv 사전 인쇄 arXiv:1801.07736.

Dan Hendrycks와 Kevin Gimpel. 2016. 가우시안 오차 선형 유닛을 사용하여 비선형성과 확률적 정규화기를 극복하기. CoRR, abs/1606.08415.

FelixHill, KyunghyunCho, andAnnaKorhonen.2016.
라벨이 없는 데이터로부터 문장의 분산 표현 학습하기.
2016년 북미 협회 컨퍼런스: 인간 언어 기술을 위한 연구논문집에서 발표됨.
Association for Computational Linguistics.

제레미 하워드와 세바스찬 루더. 2018. 범용 언어 모델 세밀 조정을 위한 텍스트 분류. ACL. 컴퓨터 언어학 협회.

민하오 후, 유싱 펑, 젠 황, 시펑 치우,
푸루 웨이, 그리고 민 주. 2018년. 기계 독해를 위한 강화된 기억력 리더. IJCAI에서.

야신 제르니트, 사무엘 R. 보우먼, 그리고 데이비드 손타그. 2017. 빠른 비지도 학습을 위한 담론 기반 목표 문장 표현 학습. CoRR, abs/1705.00557.
만다르 조시, 은솔 최, 다니엘 S 웰드, 그리고 루크 제틀레모이어. 2017. Triviaqa: 독해를 위한 대규모 원격 감독 도전 데이터셋. ACL에서.

라이언 키로스, 유쿤 주, 루슬란 R 살라후딘노프, 리처드 제멜, 라퀼 우르타순, 안토니오 토랄바, 그리고 산자 피들러. 2015년. Skip-thought 벡터. Advances in neural information processing systems에서, 페이지 3294-3302.

쿼크 레와 토마스 미코로프. 2014. 문장과 문서의 분산 표현. 기계 학습 국제 학회에서, 페이지 1188-1196.

헥터 J 르베스크, 어니스트 데이비스, 그리고 레오라 모르겐스턴. 2011년. 윈로그라드 스키마 도전. Aaai 봄 심포지엄: 상식적 추론의 논리적 형식화, 46권, 47쪽.

라자누겐 로게스와란과 홍락 리. 2018. 문장 표현을 학습하기 위한 효율적인 프레임워크. 학습 표현에 대한 국제 학회에서 발표.

브라이언 맥캔, 제임스 브래드베리, 카이밍 셩, 그리고 리처드 소처. 2017년. 번역에서 배운 것: 맥락화된 단어 벡터. NIPS에서 발표.

오렌 멜라무드, 야코브 골드버거, 이도 다간.
2016년. context2vec: 양방향 LSTM을 사용하여 일반적인 문맥 임베딩 학습. CoNLL에서.

토마스 미콜로프, 일리야 숫스케버, 카이 첸, 그레그 S 코라도, 그리고 제프 딘. 2013. 단어와 구문의 분산 표현과 그들의 조합성. 신경 정보 처리 시스템 진보 26, 페이지 3111-3119. Curran Associates, Inc.

안드리 Mnih와 Geoffrey E Hinton. 2009. 확장 가능한 계층 분산 언어 모델. D. Koller, D. Schuurmans, Y. Bengio, L. Bottou 편집, Advances in Neural Information Processing Systems 21, 1081-1088쪽. Curran Associates, Inc.

앙쿠르 P 파리크, 오스카 T 택스트롬, 디판잔 다스, 그리고
야코브 우스코레이트. 2016. 자연어 추론을 위한 분해 가능한 어텐션 모델. EMNLP에서.

제프리 페닝턴, 리처드 소처, 그리고 크리스토퍼 D. 매닝. 2014. Glove: 단어 표현을 위한 글로벌 벡터. Empirical Methods in Natural Language Processing (EMNLP)에서, 1532-1543쪽.

매튜 피터스, 왈리드 암마르, 찬드라 바가바툴라, 러셀 파워. 2017. 양방향 언어 모델을 사용한 준지도 시퀀스 태깅. ACL에서.

매튜 피터스, 마크 뉴먼, 모히트 이어, 매트 가드너, 크리스토퍼 클락, 켄튼 리, 그리고 루크 제틀모이어. 2018a. 깊은 문맥화된 단어 표현. NAACL에서.

매튜 피터스, 마크 뉴먼, 루크 제틀모이어, 그리고 이 웬타우. 2018b. 문맥적 단어 임베딩의 해부학: 아키텍처와 표현. 2018 자연어 처리에 대한 경험적 방법 회의 논문집, 페이지 1499-1509.

알렉 라드포드, 카르틱 나라시만, 팀 살리만스, 그리고 이리야 숫크에버. 2018년. 비지도 학습을 통한 언어 이해력 향상. 기술 보고서, OpenAI.

PranavRajpurkar, JianZhang, KonstantinLopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Nat-
ural Language Processing, pages 2383–2392.

프라나브 라즈푸르카, 지안 장, 콘스탄틴 로피레프, 그리고 퍼시 량. 2016. Squad: 텍스트 기계 이해를 위한 100,000개 이상의 질문. 2016년 자연어 처리에 대한 경험적 방법 컨퍼런스 논문집, 2383-2392쪽.

민준 서, 아니루다 켐바비, 알리 파르하디, 그리고 한나네 하지시르지. 2017년. 기계 이해를 위한 양방향 주의 흐름. ICLR에서.

리처드 소처, 알렉스 페렐리진, 전 우, 제이슨 차앙, 크리스토퍼 D 매닝, 앤드류 엔, 그리고 크리스토퍼 포츠. 2013년. 감성 트리뱅크를 통한 의미 합성에 대한 재귀적인 딥 모델. 2013년 자연어 처리에 대한 경험적인 방법에 관한 학회 논문집, 페이지 1631-1642.

푸 선, 리냥 리, 시펑 취, 양 리우.
2018. U-net: 답변할 수 없는 질문과 함께하는 기계 독해. arXiv 사전인쇄
arXiv:1810.06638.

윌슨 L 테일러. 1953. 클로즈 절차: 가독성 측정을 위한 새로운 도구. 저널리즘 소식지, 30(4):415–433.

에릭 F 통 김 상과 피엔 데 뮐더.
2003년. conll-2003 공유 작업 소개:
언어 독립적인 명명된 개체 인식. CoNLL에서.

Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
단어 표현: 반지도 학습을 위한 간단하고 일반적인 방법.
Association for Computational Linguistics의 48회 연례 회의 논문집, ACL '10, 384-394쪽.

아시쉬 바스와니, 노암 샤지어, 니키 파마르, 야코브 우스코레이트, 리온 존스, 에이단 엔 고메즈, 루카시 카이저, 그리고 일리아 폴로수킨. 2017년. 주의는 당신이 필요한 모든 것이다. 신경 정보 처리 시스템에서의 진보, 페이지 6000-6010.

파스칼 빈센트, 휴고 라로셀, 요슈아 벤지오, 그리고 피에르-앙투안 마자골. 2008. 노이즈 제거 오토인코더를 사용하여 견고한 특징 추출 및 조합하기. 제25회 국제 기계 학습 컨퍼런스 논문집, 1096-1103쪽. ACM.

알렉스 왕, 아만프리트 싱, 줄리안 마이클, 펠릭스 힐, 오머 레비, 그리고 사무엘 보우먼. 2018a. Glue: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. 2018 EMNLP 워크샵 BlackboxNLP: NLP를 위한 신경망 분석과 해석, 페이지 353-355.

Wei Wang, Ming Yan, and Chen Wu. 2018b. 다중 단계 계층적 주의 퓨전 네트워크를 이용한 독해와 질문 답변. 제56회 연례 컴퓨터 언어학 협회 학술대회 논문집 (1권: 장문). 컴퓨터 언어학 협회.

알렉스 워스타트, 아만프리트 싱, 그리고 사무엘 R 보우먼. 2018. 신경망 수용 가능성 판단. arXiv 사전 인쇄 arXiv:1805.12471.

아디나 윌리엄스, 니키타 낭기아, 그리고 사무엘 R 보우먼. 2018년. 추론을 통한 문장 이해를 위한 광범위한 커버리지의 도전 말뭉치. NAACL에서.

용희 우, 마이크 슈스터, 지펑 천, 쿼크 르, 모하마드 노루지, 볼프강 마허리, 막심 크리쿤, 유안 차오, 친 가오, 클라우스 마허리 등. 2016년. 구글의 신경 기계 번역 시스템: 인간과 기계 번역 사이의 간극을 좁히다. arXiv 사전 인쇄 arXiv:1609.08144.

제이슨 요신스키, 제프 클룬, 요슈아 벤지오, 그리고 호드 리프슨. 2014년. 심층 신경망에서 특징이 얼마나 이전 가능한가? 신경정보처리 시스템 발전에서, 페이지 3320-3328.

AdamsWeiYu, DavidDohan, Minh-ThangLuong, Rui
Zhao, Kai Chen, Mohammad Norouzi, and Quoc V
Le. 2018. QANet: 지역 컨볼루션과 전역 셀프 어텐션을 결합한 독해를 위한 모델. ICLR에서 발표.

로완 젤러스, 요나탄 비스크, 로이 슈왈츠, 예진 최. 2018. Swag: 대규모 적대적 데이터셋을 통한 기반 상식 추론. 2018 자연어 처리 기법 (EMNLP) 학회 논문집.

유쿤 주, 라이언 키로스, 리치 제멜, 루슬란 살라후티노프, 라켈 우르타순, 안토니오 토랄바, 그리고 산자 피들러. 2015년. 책과 영화의 조화: 영화 시청과 책 읽기를 통한 이야기 같은 시각적 설명을 위해. IEEE 국제 컴퓨터 비전 학회 논문집, 페이지 19-27.

“BERT: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련”에 대한 부록

우리는 부록을 세 가지 섹션으로 구성합니다:

BERT에 대한 추가 구현 세부 사항은 부록 A에서 제시됩니다.

우리 실험에 대한 추가적인 세부사항은 부록 B에서 제시되었습니다.



• 부록 C에서 추가적인 제거 실험 결과가 제시되었습니다.

BERT에 대한 추가적인 제거 연구를 제시합니다.

- 훈련 단계 수의 효과;

- 다른 마스킹 절차를 위한 제거.

BERT에 대한 추가 세부 정보

A.1 사전 훈련 작업의 설명

다음에는 사전 훈련 과제의 예시를 제공합니다.

마스크된 LM과 마스킹 절차는 라벨이 없는 문장이 "내 개는 털이 많다"라고 가정하고, 무작위 마스킹 절차 중 4번째 토큰(털이)을 선택한 경우, 우리의 마스킹 절차는 다음과 같이 더 자세히 설명될 수 있다.

• 80%의 경우: 단어를 [MASK] 토큰으로 대체합니다. 예를 들어, 내 개는 털이 많다 → 내 개는 [MASK]입니다.

• 10%의 시간 동안: 단어를 무작위 단어로 대체합니다. 예를 들어, 내 개는 털이 많다 → 내 개는 사과입니다.

• 10%의 시간 동안: 단어를 변경하지 않고 유지합니다. 예를 들어, 내 개는 털이 많다 → 내 개는 털이 많다. 이것의 목적은 실제 관찰된 단어를 중심으로 표현을 편향시키는 것입니다.

이 절차의 장점은 Transformer 인코더가 예측해야 할 단어나 무작위 단어로 대체된 단어를 알지 못하기 때문에, 모든 입력 토큰의 분포적 문맥 표현을 유지하기 위해 강제된다는 것입니다. 또한, 무작위 대체는 전체 토큰의 1.5%에 해당하는 것으로만 발생하므로 (즉, 15%의 10%), 이는 모델의 언어 이해 능력에 해를 끼치지 않는 것으로 보입니다. C.2 절에서는 이 절차의 영향을 평가합니다.
표준 언어 모델 훈련과 비교하여, 마스크된 LM은 각 배치의 토큰 중 15%에 대해서만 예측을 수행하므로, 모델에 더 많은 사전 훈련 단계가 필요할 것으로 추정됩니다. (BERT (우리들) 모델)

트름 트름 트름

트림 트림 트림

...
트름 트름 트름

번역을 제외한 추가 답변은 없습니다.

트름 트름 트름

'LLM에 관한 내용입니다.'
...

오픈AI GPT

LSTM (Long Short-Term Memory)은 순환 신경망의 한 종류입니다.

ELMo는 "Embeddings from Language Models"의 약자로, 사전 훈련된 언어 모델을 사용하여 단어의 의미를 표현하는 방법입니다.

LSTM LSTM

Lstm Lstm Lstm

Lstm Lstm Lstm

Lstm Lstm Lstm
T1 T2     TN ...

LLM은 법학 석사 학위입니다.

LLM은 법학 석사 학위입니다.

LLM은 법학 석사 학위입니다.

LLM은 법학 석사 학위입니다.

E1 E2     EN ...
T1 T2    TN ...

E1 E2    EN ...

번역 이외의 추가 답변은 없습니다.

T1 T2    TN ...

E1 E2     EN ...

E1 E2     EN ...

그림 3: 사전 훈련 모델 아키텍처의 차이점. BERT는 양방향 Transformer를 사용합니다. OpenAI GPT는 좌에서 우로 Transformer를 사용합니다. ELMo는 독립적으로 훈련된 좌에서 우로 및 우에서 좌로 LSTMs의 연결을 사용하여 하위 작업에 대한 특징을 생성합니다. 이 세 가지 중에서 BERT 표현만이 모든 레이어에서 좌 및 우 컨텍스트에 대해 공동으로 조건이 됩니다. 아키텍처의 차이 외에도, BERT와 OpenAI GPT는 세밀 조정 접근 방식이며, ELMo는 특징 기반 접근 방식입니다.

수렴하다. C.1 섹션에서는 MLM이 왼쪽에서 오른쪽으로 모든 토큰을 예측하는 모델보다 약간 느리게 수렴한다는 것을 보여줍니다. 그러나 MLM 모델의 경험적인 개선은 훈련 비용의 증가를 훨씬 상쇄시킵니다.

다음 문장 예측 다음 문장 예측 작업은 다음과 같은 예를 통해 설명할 수 있습니다.

입력 = [CLS] 그 남자는 [MASK] 가게에 갔다 [SEP]

그는 한 갤런의 우유를 샀다.

라벨 = 다음 것

입력 = [CLS] 그 남자는 상점에 [MASK] [SEP]

펭귄은 비행하지 않는 조류입니다.

라벨 = 다음 것이 아님

A.2 사전 훈련 절차

각 훈련 입력 시퀀스를 생성하기 위해 우리는 말뭉치에서 두 개의 텍스트 구간을 샘플링합니다. 이를 "문장"이라고 부르지만, 일반적으로 단일 문장보다 훨씬 길지만 때로는 더 짧을 수도 있습니다. 첫 번째 문장은 A 임베딩을 받고, 두 번째 문장은 B 임베딩을 받습니다. B가 A 다음에 오는 실제 다음 문장인 경우와 무작위 문장인 경우를 50%씩 샘플링하여 "다음 문장 예측" 작업을 수행합니다. 이들은 결합된 길이가 512 토큰 이하가 되도록 샘플링됩니다. LM 마스킹은 WordPiece 토큰화 후에 15%의 균일한 마스킹 비율로 적용되며, 부분 단어 조각에 대한 특별한 고려는 주어지지 않습니다.
우리는 256 시퀀스 (256 시퀀스 * 512 토큰 = 128,000 토큰/배치)의 배치 크기로 1,000,000 단계 동안 훈련을 진행하며, 이는 대략 40

3.3 billion 단어 말뭉치에 대해 에포크를 수행합니다. 학습률이 1e-4, β 1 = 0.9, β 2 = 0.999, L2 가중치 감쇠가 0.01, 처음 10,000 단계 동안 학습률 워마업, 학습률의 선형 감소를 사용하여 Adam을 사용합니다. 모든 레이어에 대해 드롭아웃 확률은 0.1을 사용합니다. OpenAI GPT를 따라 표준 relu 대신 gelu 활성화(Hendrycks and Gimpel, 2016)를 사용합니다. 훈련 손실은 평균 마스크된 LM 가능성과 평균 다음 문장 예측 가능성의 합입니다.
BERTBASE의 훈련은 4개의 Cloud TPU를 사용하여 Pod 구성에서 수행되었습니다(총 16개의 TPU 칩). BERTLARGE의 훈련은 16개의 Cloud TPU(총 64개의 TPU 칩)를 사용하여 수행되었습니다. 각 사전 훈련은 4일이 소요되었습니다.
더 긴 시퀀스는 시퀀스 길이에 비례하여 비용이 증가합니다. 실험에서 사전 훈련 속도를 높이기 위해 90%의 단계에서 시퀀스 길이가 128인 모델을 사전 훈련하고, 나머지 10%의 단계에서 시퀀스 길이가 512인 모델을 훈련하여 위치 임베딩을 학습합니다.

A.3 세부 조정 절차

미세 조정을 위해 대부분의 모델 하이퍼파라미터는 사전 훈련과 동일하며, 배치 크기, 학습률 및 훈련 에포크 수를 제외하고는 동일합니다. 드롭아웃 확률은 항상 0.1로 유지되었습니다. 최적의 하이퍼파라미터 값은 과제에 따라 다르지만, 다음 범위의 가능한 값이 모든 과제에서 잘 작동하는 것으로 확인되었습니다.

일괄 처리 크기: 16, 32

LLM에 관한 내용입니다.
13https://cloudplatform.googleblog.com/2018/06/Cloud-
TPU-now-offers-preemptible-pricing-and-global-
availability.html
• 학습률 (Adam): 5e-5, 3e-5, 2e-5
• 에폭 수: 2, 3, 4

우리는 또한 대규모 데이터 세트 (예 : 100k 이상의 레이블이 지정된 훈련 예제)는 작은 데이터 세트보다 하이퍼파라미터 선택에 훨씬 덜 민감하다는 것을 관찰했습니다. 세밀 조정은 일반적으로 매우 빠르므로 위의 매개변수에 대해 완전 탐색을 단순히 실행하고 개발 세트에서 가장 우수한 성능을 발휘하는 모델을 선택하는 것이 합리적입니다.

A.4 BERT, ELMo 및 OpenAI GPT의 비교

여기에서는 ELMo, OpenAI GPT 및 BERT와 같은 최근 인기있는 표현 학습 모델의 차이점을 연구했습니다. 모델 아키텍처 간의 비교는 그림 3에서 시각적으로 보여집니다. 아키텍처 차이 외에도, BERT와 OpenAI GPT는 세부 조정 접근 방식이며, ELMo는 특징 기반 접근 방식입니다.

BERT와 가장 유사한 기존 사전 훈련 방법은 OpenAI GPT입니다. OpenAI GPT는 대규모 텍스트 말뭉치에서 좌에서 우로 Transformer 언어 모델을 훈련시킵니다. 사실, BERT의 많은 설계 결정은 최소한의 비교를 위해 GPT와 가능한 한 유사하게 만들기 위해 의도적으로 이루어졌습니다. 이 연구의 핵심 주장은 양방향성과 3.1절에서 제시된 두 가지 사전 훈련 작업이 경험적 개선의 대부분을 설명한다는 것입니다. 그러나 BERT와 GPT의 훈련 방식 사이에는 몇 가지 다른 차이점이 있음을 언급해야 합니다.

GPT는 BooksCorpus (8억 단어)로 훈련되었으며, BERT는 BooksCorpus (8억 단어)와 Wikipedia (25억 단어)로 훈련되었습니다.

GPT는 fine-tuning 시에만 도입되는 문장 구분자([SEP])와 분류자 토큰([CLS])을 사용하며, BERT는 사전 훈련 과정에서 [SEP], [CLS] 및 문장 A/B 임베딩을 학습합니다.

GPT는 32,000 단어의 배치 크기로 1M 단계를 훈련했고, BERT는 128,000 단어의 배치 크기로 1M 단계를 훈련했습니다.

GPT는 모든 세부 조정 실험에 대해 동일한 학습률 5e-5를 사용했으며, BERT는 개발 세트에서 가장 우수한 성능을 보이는 작업별 세부 조정 학습률을 선택합니다.

이러한 차이의 영향을 분리하기 위해, 우리는 섹션 5.1에서 제거 실험을 수행합니다. 이 실험은 대부분의 개선이 사실상 두 가지 사전 훈련 작업과 그들이 가능하게 하는 양방향성에서 나오는 것을 보여줍니다.

다른 작업에서의 세부 조정에 대한 예시들

BERT를 다른 작업에 대해 세밀하게 조정하는 방법에 대한 그림은 그림 4에서 볼 수 있습니다. 우리의 작업별 모델은 BERT와 추가 출력 레이어를 통합하여 형성되므로 최소한의 매개변수만 처음부터 학습해야 합니다. (a)와 (b)는 시퀀스 수준 작업이고, (c)와 (d)는 토큰 수준 작업입니다. 그림에서 E는 입력 임베딩을 나타내고, T i는 토큰 i의 문맥적 표현을 나타냅니다. [CLS]는 분류 출력을 위한 특수 기호이고, [SEP]은 비연속적인 토큰 시퀀스를 구분하기 위한 특수 기호입니다.

B  상세한 실험 설정

GLUE 벤치마크 실험에 대한 자세한 설명.

표 1에서 우리의 GLUE 결과는 https://gluebenchmark.com/ 리더보드와 https://blog.openai.com/language-unsupervised에서 얻었습니다. GLUE 벤치마크에는 원래 Wang et al. (2018a)에서 요약된 다음 데이터셋이 포함되어 있습니다.

MNLI  Multi-Genre Natural Language Inference은 대규모로 집단지성을 활용한 함의 분류 작업입니다 (Williams et al., 2018). 두 문장 쌍이 주어지면, 첫 번째 문장과 관련하여 두 번째 문장이 함의, 모순 또는 중립인지 예측하는 것이 목표입니다.

QQP (Quora Question Pairs)는 이진 분류 작업으로, Quora에서 두 개의 질문이 의미적으로 동등한지 여부를 결정하는 것을 목표로 합니다 (Chen et al., 2018).

QNLI 질문 자연어 추론은 Stanford 질문 응답 데이터셋(Rajpurkar et al., 2016)의 이진 분류 작업으로 변환된 것입니다(Wang et al., 2018a). 긍정적인 예는 올바른 답변을 포함하는 (질문, 문장) 쌍이고, 부정적인 예는 답변을 포함하지 않는 동일한 단락의 (질문, 문장)입니다.

E[CLS] E1  E[SEP] ... EN E1’ ... EM’
C  T1      T[SEP] ... TN T1’ ... TM’

E[CLS] E1  E[SEP] ... EN E1’ ... EM’
C  T1      T[SEP] ... TN T1’ ... TM’

[CLS] 토큰 1 [SEP] ... 토큰 N 토큰 1 ... 토큰 M

BERT는 자연어 처리를 위한 사전 훈련된 언어 모델입니다. 이 모델은 Transformer 아키텍처를 기반으로 하며, 문장의 문맥을 이해하고 다양한 자연어 처리 작업에 유연하게 적용할 수 있습니다. BERT는 최근 자연어 처리 분야에서 많은 주목을 받고 있으며, 다양한 언어에 대한 사전 훈련 모델이 제공되고 있습니다. 이러한 사전 훈련된 모델을 fine-tuning하여 특정 자연어 처리 작업에 적용할 수 있습니다. BERT는 문장의 문맥을 파악하는 능력이 뛰어나기 때문에, 기존의 자연어 처리 모델보다 더 좋은 성능을 보여줄 수 있습니다.

E[CLS] E1 E2      EN
C  T1  T2        TN

E[CLS] E1 E2      EN
C  T1  T2        TN

단일 문장

BERT

토큰 1 토큰 2 토큰 N ... [CLS]
E[CLS] E1 E2       EN
C  T1  T2         TN

단일 문장

B-PER O    O
...

... E[CLS] E1 E[SEP]
클래스
레이블

... KR   E1’ ... EM’
C  T1     T[SEP] ... TN T1’ ... TM’

시작/끝 스팬

수업
레이블

BERT (Bidirectional Encoder Representations from Transformers)는 자연어 처리를 위한 사전 훈련된 모델로, 양방향으로 문맥을 이해하고 단어의 의미를 파악하는 능력을 갖추고 있습니다.

토큰 1 토큰 2 토큰 N ... [CLS] 토큰 1 [CLS] [CLS] 토큰 1 [SEP] ... 토큰 N 토큰 1 ... 토큰 M

문장 1

...

그림 4: 다른 작업에서 BERT의 세부 조정 예시.

SST-2 스탠포드 감성 트리뱅크는 영화 리뷰에서 추출된 문장들에 대한 인간의 감성 주석을 포함한 이진 단일 문장 분류 작업입니다 (Socher 등, 2013).

CoLA 언어 수용성 코퍼스는 영어 문장이 언어적으로 "수용 가능"한지 여부를 예측하는 이진 단일 문장 분류 작업입니다 (Warstadt 등, 2018).

STS-B 시맨틱 텍스트 유사성 벤치마크는 뉴스 헤드라인 및 기타 소스에서 추출된 문장 쌍의 모음입니다 (Cer et al., 2017). 이들은 의미적 의미 측면에서 두 문장이 얼마나 유사한지를 나타내는 1에서 5까지의 점수로 주석이 달렸습니다.

MRPC (Microsoft Research Paraphrase Corpus)는 온라인 뉴스 소스에서 자동으로 추출된 문장 쌍으로 구성되어 있으며, 인간 주석이 추가되었습니다.

문장 쌍이 의미적으로 동등한지 여부를 판단하기 위해 (Dolan and Brockett, 2005)에서 사용되었습니다.

RTE 텍스트 인용은 MNLI와 유사한 이진 함의 작업입니다. 그러나 훈련 데이터가 훨씬 적습니다 (Bentivogli et al., 2009).14

WNLI  Winograd NLI는 작은 자연어 추론 데이터셋입니다 (Levesque et al., 2011).
GLUE 웹페이지는 이 데이터셋의 구성에 문제가 있다고 언급하며, GLUE에 제출된 모든 훈련된 시스템은 대다수 클래스를 예측하는 65.1 기준 정확도보다 성능이 떨어졌습니다. 따라서 우리는 OpenAI GPT에게 공정하게 대우하기 위해 이 데이터셋을 제외합니다. GLUE 제출을 위해 우리는 항상 대다수 클래스를 예측했습니다.

이 논문에서는 단일 작업 세부 조정 결과만 보고한다는 점을 유의하십시오. 다중 작업 세부 조정 접근 방식은 성능을 더욱 향상시킬 수 있습니다. 예를 들어, MNLI와의 다중 작업 훈련으로 RTE에서 상당한 개선을 관찰했습니다.

https://gluebenchmark.com/faq

추가적인 소작용 연구

C.1 훈련 단계 수의 영향

그림 5는 k 단계로 사전 훈련된 체크포인트에서 세밀 조정 후 MNLI Dev 정확도를 보여줍니다. 이를 통해 다음 질문에 답할 수 있습니다:

질문: BERT는 실제로 높은 세밀 조정 정확도를 달성하기 위해 이렇게 많은 양의 사전 훈련(128,000 단어/배치 * 1,000,000 단계)이 필요한가요?
답변: 네, BERTBASE는 500k 단계 대비 1M 단계로 훈련시킬 때 MNLI에서 거의 1.0%의 추가 정확도를 달성합니다.

질문: MLM 사전 훈련은 LTR 사전 훈련보다 수렴이 느린가요? 각 배치에서 예측되는 단어의 비율이 모든 단어가 아닌 15%이기 때문에.
답변: MLM 모델은 LTR 모델보다 약간 느리게 수렴합니다. 그러나 절대 정확도 측면에서는 MLM 모델이 거의 즉시 LTR 모델을 능가하기 시작합니다.

C.2 다른 마스킹 절차에 대한 소작용

3.1절에서는 BERT가 마스크된 언어 모델(MLM) 목적으로 사전 훈련할 때 대상 토큰을 마스킹하는 데 혼합 전략을 사용한다고 언급합니다. 다음은 다른 마스킹 전략의 효과를 평가하기 위한 제거 연구입니다.

200  400 600  800 1,000
76
78
80
82
84

200  400 600  800 1,000
76
78
80
82
84

사전 훈련 단계(천 개)
M
N
L I
D e v
정확도

BERTBASE (마스크된 언어 모델)
BERTBASE (왼쪽에서 오른쪽으로)

그림 5: 훈련 단계 수에 따른 제거 효과. 이는 미리 훈련된 모델 매개변수를 시작으로 세밀 조정한 후의 MNLI 정확도를 보여줍니다. x축은 k의 값입니다.

마스킹 전략의 목적은 사전 훈련과 세부 조정 사이의 불일치를 줄이는 것입니다. [MASK] 기호는 세부 조정 단계에서 나타나지 않으므로, 우리는 MNLI와 NER의 Dev 결과를 보고합니다. NER의 경우, 세부 조정 및 특징 기반 접근 방식을 모두 보고합니다. 특징 기반 접근 방식에서는 모델이 표현을 조정할 기회가 없으므로 불일치가 증폭될 것으로 예상됩니다.

마스킹 비율  개발 세트 결과

마스크는 동일한 RND MNLI NER입니다.

세부 조정 기능 기반

80% 10% 10%  84.2 95.4   94.9
80% 10% 10%  84.2 95.4   94.9

100% 0%  0%  84.3 94.9   94.0
100% 0%  0%  84.3 94.9   94.0

80%  0% 20%  84.1 95.2   94.6
80%  0% 20%  84.1 95.2   94.6

80% 20%  0%  84.4 95.2   94.7
80% 20%  0%  84.4 95.2   94.7

0% 20% 80%  83.7 94.8   94.6
0% 20% 80%  83.7 94.8   94.6

0%  0% 100% 83.6 94.9   94.6
0%  0% 100% 83.6 94.9   94.6

표 8: 다른 마스킹 전략에 대한 제거 효과.

결과는 표 8에 제시되었습니다. 표에서 MASK는 MLM을 위해 대상 토큰을 [MASK] 기호로 대체하는 것을 의미하며, SAME은 대상 토큰을 그대로 유지하는 것을 의미합니다. RND는 대상 토큰을 다른 임의의 토큰으로 대체하는 것을 의미합니다.
표의 왼쪽 부분의 숫자는 MLM 사전 훈련 중 사용된 특정 전략의 확률을 나타냅니다 (BERT는 80%, 10%, 10%를 사용합니다). 논문의 오른쪽 부분은 Dev 세트 결과를 나타냅니다. 특징 기반 접근 방식의 경우, BERT의 마지막 4개 레이어를 특징으로 연결합니다. 이는 섹션 5.3에서 최상의 접근 방식으로 입증되었습니다.
표에서는 세밀 조정이 다양한 마스킹 전략에 대해 놀랍게도 견고하다는 것을 알 수 있습니다. 그러나 예상대로, NER에 특징 기반 접근 방식을 적용할 때 MASK 전략만 사용하는 것은 문제가 있었습니다. 흥미롭게도, 우리의 전략보다는 RND 전략만 사용하는 것이 훨씬 성능이 나쁩니다.

