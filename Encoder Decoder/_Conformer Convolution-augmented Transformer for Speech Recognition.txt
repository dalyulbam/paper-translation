컨포머: 음성 인식을 위한 합성곱 증강 트랜스포머

안몰 구라티, 제임스 친, 정청 치우, 니키 파마르, 유 장, 지아희 유, 웨이 한, 시보

왕, 정동 장, 용희 우, 루오밍 팡

구글 주식회사. {anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,

용희, rpang}@google.com

요약

최근에는 Transformer와 Convolutional Neural Network (CNN) 기반 모델이 자동 음성 인식 (ASR)에서 유망한 결과를 보여주며, 순환 신경망 (RNN)을 능가하고 있습니다. Transformer 모델은 콘텐츠 기반의 전역 상호작용을 잘 포착하는 반면, CNN은 지역적인 특징을 효과적으로 활용합니다. 이 연구에서는 오디오 시퀀스의 지역 및 전역 의존성을 파라미터 효율적인 방식으로 모델링하기 위해 CNN과 Transformer를 결합하는 방법을 연구하여 최상의 결과를 얻었습니다. 이와 관련하여, 우리는 음성 인식을 위한 컨볼루션 보강 Transformer인 Conformer를 제안합니다. Conformer는 이전의 Transformer 및 CNN 기반 모델보다 우수한 성능을 보여주며 최신 기술의 정확도를 달성합니다. 널리 사용되는 LibriSpeech 벤치마크에서, 우리 모델은 언어 모델을 사용하지 않고 테스트/테스트 기타에서 2.1%/4.3%의 WER을 달성하며, 외부 언어 모델을 사용하면 1.9%/3.9%의 정확도를 달성합니다. 또한, 10M 파라미터만으로 작은 모델의 경쟁력 있는 성능인 2.7%/6.3%를 관찰할 수 있습니다. IndexTerms: 음성 인식, 어텐션, 컨볼루션 신경망, Transformer, end-to-end

1. 소개

최근 몇 년 동안, 신경망을 기반으로 한 엔드 투 엔드 자동 음성 인식 (ASR) 시스템은 큰 발전을 이루었습니다. 재귀 신경망 (RNN)은 ASR에서 사실상의 선택으로 여겨져 왔으며, 오디오 시퀀스의 시간적 의존성을 효과적으로 모델링할 수 있습니다. 최근에는 자기 주의(self-attention)에 기반한 Transformer 아키텍처가 시퀀스 모델링을 위해 널리 채택되었으며, 먼 거리 상호작용과 높은 훈련 효율성을 캡처할 수 있는 능력으로 인해 인기를 끌고 있습니다. 또한, 합성곱은 ASR에서도 성공적으로 사용되었으며, 계층별로 지역 수용 영역을 통해 점진적으로 지역 컨텍스트를 캡처합니다.

그러나, 자기 주의 또는 합성곱을 사용한 모델은 각각 제한 사항이 있습니다. Transformer는 장거리 전역 컨텍스트를 모델링하는 데 우수하지만, 세밀한 지역 특징 패턴을 추출하는 능력은 떨어집니다. 반면, 합성곱 신경망 (CNN)은 지역 정보를 활용하며, 시각에서는 사실상의 계산 블록으로 사용됩니다. 이들은 지역 창에서 공유 위치 기반 커널을 학습하여 변환 등변성을 유지하고, 가장자리와 모양과 같은 특징을 캡처합니다. 지역 연결성을 사용하는 한 가지 제한은 전역 정보를 캡처하기 위해 많은 수의 레이어 또는 매개변수가 필요하다는 것입니다. 이 문제를 해결하기 위해 최근 연구인 ContextNet은 각 잔여 블록에 스퀴즈-앤-익스사이트 모듈을 도입하여 더 긴 컨텍스트를 캡처합니다. 그러나, 전체 시퀀스에 대해 전역 평균화만 적용하기 때문에 동적인 전역 컨텍스트를 캡처하는 데는 여전히 한계가 있습니다.

최근 연구들은 합성곱과 자기 주의를 결합하는 것이 ASR에서 유용하다는 것을 보여주고 있습니다.

피드 포워드 모듈
멀티헤드 셀프 어텐션 모듈
컨볼루션 모듈

1/2 곱하기 x
1/2 곱하기 x

피드 포워드 모듈

중퇴
형태 일치 블록

선형

스펙올각
10ms 비율

10 밀리초 비율
합성곱
서브샘플링
40 밀리초 비율

x N

40 밀리초 비율
40 밀리초 비율

레이어노름

그림 1: 콘포머 인코더 모델 아키텍처. 콘포머는 반스텝 잔여 연결로 이루어진 두 개의 마카롱과 같은 피드포워드 레이어로 시작하여, 멀티헤드 셀프 어텐션과 컨볼루션 모듈을 사이에 두고 있습니다. 이후에는 포스트 레이어노멀라이제이션이 이어집니다.

자기 주의는 개별적으로 사용하는 것보다 향상된다 [14]. 함께 사용하면 위치별 지역적 특징을 학습하고 콘텐츠 기반의 전역적 상호작용을 사용할 수 있다. 동시에 [15, 16]와 같은 논문들은 상대적인 위치 기반 정보를 사용하여 자기 주의를 보강하였다. Wu 등 [17]은 입력을 자기 주의와 합성곱으로 나누고 출력을 연결하는 다중 분기 아키텍처를 제안했다. 그들의 연구는 모바일 애플리케이션을 대상으로 하며 기계 번역 작업에서의 개선을 보였다. 본 연구에서는 ASR 모델에서 합성곱과 자기 주의를 유기적으로 결합하는 방법을 연구한다. 우리는 전역적 상호작용과 지역적 상대적 상관관계를 효율적으로 포착하기 위해 자기 주의와 합성곱의 혼합을 제안한다. Wu 등 [17, 18]의 영감을 받아, 그림 1에 나와 있는 것처럼 자기 주의와 합성곱의 혼합을 소개한다. 우리가 제안하는 모델인 Conformer는 LibriSpeech에서 최고 성능을 달성하여 이전에 발표된 최고의 Transformer Transducer [7]보다 15% 상대적 개선을 이룩한다.

그림 2: 합성곱 모듈. 합성곱 모듈은 확장 계수가 2인 점별 합성곱을 포함하며 GLU 활성화 계층을 통해 채널 수를 투영합니다. 이후 1-D 깊이별 합성곱이 이어집니다. 1-D 깊이별 합성곱 다음에는 배치 정규화 및 스위시 활성화 계층이 이어집니다.

외부 언어 모델을 사용하여 테스트 기타 데이터셋에서 모델을 평가했습니다.
10M, 30M 및 118M의 모델 매개변수 제한 조건을 기반으로 세 가지 모델을 제시합니다. 우리의 10M 모델은 유사한 크기의 동시 작업[10]과 비교했을 때 2.7%/6.3%의 개선을 보여줍니다. 중간 크기인 30M 매개변수 모델은 이미 139M 모델 매개변수를 사용하는 transformer transducer[7]보다 우수한 성능을 보입니다. 큰 118M 매개변수 모델로는 언어 모델을 사용하지 않고 2.1%/4.3%의 성능을 달성할 수 있으며, 외부 언어 모델을 사용하면 1.9%/3.9%의 성능을 얻을 수 있습니다.

우리는 또한 주의 헤드의 수, 컨볼루션 커널 크기, 활성화 함수, 피드포워드 레이어의 배치, 그리고 Transformer 기반 네트워크에 컨볼루션 모듈을 추가하는 다양한 전략의 효과를 주의 깊게 연구하고, 각각이 정확도 향상에 어떻게 기여하는지 밝힙니다.

2. 형태 변환 인코더

우리의 오디오 인코더는 먼저 입력을 컨볼루션 서브샘플링 레이어로 처리한 다음 Figure1에 나와 있는 것처럼 여러 개의 Conformer 블록으로 처리합니다. 우리 모델의 특징은 Transformer 블록 대신 Conformer 블록을 사용한다는 것입니다 [7, 19].

구성자 블록은 네 개의 모듈이 쌓여 있습니다. 즉, 피드 포워드 모듈, 셀프 어텐션 모듈, 컨볼루션 모듈 및 마지막으로 두 번째 피드 포워드 모듈로 구성됩니다. 2.1, 1 및 2.3 절에서는 각각 셀프 어텐션, 컨볼루션 및 피드 포워드 모듈을 소개합니다. 마지막으로, 2.4에서는 이러한 하위 블록이 어떻게 결합되는지 설명합니다.

2.1. 멀티 헤드 셀프 어텐션 모듈

우리는 Transformer-XL [20]에서 중요한 기술인 상대적인 사인 함수 위치 인코딩 체계를 통합하면서 다중 헤드 자기 어텐션(MHSA)을 사용합니다. 상대적인 위치 인코딩은 자기 어텐션 모듈이 다양한 입력 길이에 대해 더 잘 일반화되고, 결과적으로 인코더가 발화 길이의 변동에 더 견고해질 수 있도록 합니다. 우리는 드롭아웃과 함께 사전 정규화 잔차 유닛 [21, 22]을 사용하여 깊은 모델을 훈련하고 정규화하는 데 도움을 줍니다. 아래 그림 3은 다중 헤드 자기 어텐션 블록을 설명합니다.

레이어 정규화
상대적 위치 임베딩을 사용한 멀티헤드 어텐션
드롭아웃 추가

그림 3: 다중 헤드 셀프 어텐션 모듈. 우리는 사전-노름 잔차 유닛에서 상대적 위치 임베딩과 함께 다중 헤드 셀프 어텐션을 사용합니다.

2.2. 합성곱 모듈

[17]에 영감을 받아, 컨볼루션 모듈은 게이팅 메커니즘 [23]으로 시작합니다 - 포인트와이즈 컨볼루션과 게이트드 리니어 유닛 (GLU)으로 구성됩니다. 이후에는 단일 1-D 깊이별 컨볼루션 레이어가 이어집니다. 깊은 모델의 훈련을 돕기 위해 컨볼루션 이후에 배치 정규화가 적용됩니다. 그림 2는 컨볼루션 블록을 보여줍니다.

2.3. 피드 포워드 모듈

[6]에서 제안된 Transformer 아키텍처는 MHSA 레이어 뒤에 피드포워드 모듈을 배치하며, 두 개의 선형 변환과 비선형 활성화로 구성됩니다. 피드포워드 레이어 위에 잔차 연결이 추가되고, 그 뒤에 레이어 정규화가 이루어집니다. 이 구조는 Transformer ASR 모델 [7, 24]에서도 채택되었습니다.
저희는 사전 정규화 잔차 유닛 [21, 22]을 따르고, 첫 번째 선형 레이어 이전에 잔차 유닛 내부와 입력에 대해 레이어 정규화를 적용합니다. 또한 Swish 활성화 [25]와 드롭아웃을 적용하여 네트워크를 정규화하는 데 도움이 됩니다. 그림 4는 피드포워드 (FFN) 모듈을 설명합니다.

2.4. 콘포머 블록

우리가 제안한 Conformer 블록은 Figure 1에 나와 있는 것처럼 Multi-Headed Self-Attention 모듈과 Convolution 모듈을 사이에 두 개의 Feed Forward 모듈로 구성되어 있습니다. 이 샌드위치 구조는 Macaron-Net [18]에서 영감을 받았으며, Transformer 블록의 원래 피드포워드 레이어를 양쪽으로 나눈 두 개의 반스텝 피드포워드 레이어로 대체하는 것을 제안합니다. Macron-Net과 마찬가지로, 우리는 피드포워드 (FFN) 모듈에서 반스텝 잔차 가중치를 사용합니다. 두 번째 피드포워드 모듈은 최종 레이어노멀라이제이션 레이어에 의해 따라옵니다. 수학적으로 말하면, Conformer 블록 i의 입력 xi에 대한 출력 yi는 다음과 같습니다.

˜ xi = xi + 1/2FFN(xi)
x0 i = ˜ xi + MHSA(˜ xi)
x00 i = x0 i + Conv(x0 i)
yi = Layernorm(x00 i + 1/2FFN(x00 i))

(1) 저는 한국에 살고 있어요.

FFN은 피드 포워드 모듈을 나타내며, MHSA는 멀티 헤드 셀프 어텐션 모듈을 나타내며, Conv는 이전 섹션에서 설명한 컨볼루션 모듈을 나타냅니다. 
3.4.3 절에서 논의된 우리의 제거 연구는 이전 작업에서 사용된 바닐라 FFN과 맥카롱 스타일 반스텝 FFN을 비교합니다. 
우리는 어텐션과 컨볼루션 모듈 사이에 반스텝 잔여 연결을 가진 맥카롱 넷 스타일 피드 포워드 레이어 두 개를 가지는 것이 Conformer 아키텍처에서 단일 피드 포워드 모듈을 가지는 것보다 큰 개선을 제공한다는 것을 발견했습니다. 
컨볼루션과 셀프 어텐션의 조합은 이전에 연구되었으며, 많은 방법으로 달성할 수 있다고 상상할 수 있습니다. 
레이어 정규화 선형 레이어 드롭아웃 선형 레이어 스위시 활성화 드롭아웃 +

그림 4: 피드 포워드 모듈. 첫 번째 선형 레이어는 확장 계수 4를 사용하고 두 번째 선형 레이어는 모델 차원으로 다시 투영합니다. 피드 포워드 모듈에서는 스위시 활성화 함수와 사전 정규화 잔차 유닛을 사용합니다.

그. 자가-주의를 이용한 합성곱의 다양한 옵션들은 3.4.2절에서 연구되었습니다. 우리는 자가-주의 모듈 뒤에 합성곱 모듈을 쌓는 것이 음성 인식에 가장 잘 작동한다는 것을 발견했습니다.

3. 실험
3.1. 데이터

우리는 제안된 모델을 LibriSpeech [26] 데이터셋에서 평가했습니다. 이 데이터셋은 970시간의 레이블이 지정된 음성과 언어 모델 구축을 위한 추가적인 800M 단어 토큰 텍스트 전용 말뭉치로 구성되어 있습니다. 우리는 25ms 창과 10ms 간격으로 계산된 80채널 필터뱅크 특징을 추출했습니다. 우리는 SpecAugment [27, 28]를 사용하며 마스크 매개변수 (F = 27)와 최대 시간-마스크 비율 (pS = 0.05)인 10개의 시간 마스크를 사용했습니다. 여기서 시간 마스크의 최대 크기는 발화의 길이에 대해 pS 배로 설정되었습니다.

3.2. 형태 변환기

우리는 네트워크 깊이, 모델 차원, 어텐션 헤드 수의 다양한 조합을 통해 작은, 중간, 큰 세 가지 모델을 식별하였습니다. 각각의 파라미터는 10M, 30M, 118M입니다. 모델 파라미터 크기 제약 내에서 가장 성능이 우수한 모델을 선택하였습니다. 모든 모델에서는 단일 LSTM 레이어 디코더를 사용합니다. 표 1은 그들의 아키텍처 하이퍼파라미터를 설명합니다.
정규화를 위해, 우리는 컨포머의 각 잔차 유닛에 드롭아웃 [29]을 적용합니다. 즉, 각 모듈의 출력에 모듈 입력 전에 드롭아웃을 적용합니다. 우리는 Pdrop = 0.1의 비율을 사용합니다. 변동 노이즈 [5, 30]는 정규화로 모델에 도입됩니다. (cid:96)2 정규화는 1e−6의 가중치로 네트워크의 모든 학습 가능한 가중치에 추가됩니다. 우리는 Adam 옵티마이저 [31]를 사용하여 모델을 훈련시킵니다. 이 때, β1 = 0.9, β2 = 0.98, (cid:15) = 10−9로 설정하고, transformer 학습률 스케줄 [6]을 사용합니다. 이 때, 10k의 웜업 단계와 최대 학습률을 가지고 있습니다.

0.05/√
d는 conformer 인코더의 모델 차원입니다.
우리는 LibriSpeech 언어 모델 말뭉치에서 훈련된 폭이 4096인 3층 LSTM 언어 모델(LM)을 사용합니다. 이는 LibriSpeech960h 트랜스크립트를 추가하여 토큰화한 1k WPM으로 구성되었습니다. LM은 개발 세트 트랜스크립트에서 단어 수준의 퍼플렉서티 63.9를 가지고 있습니다. 얕은 융합을 위한 LM 가중치 λ는 그리드 서치를 통해 개발 세트에서 조정되었습니다. 모든 모델은 Lingvo 툴킷 [32]을 사용하여 구현되었습니다.

3.3. LibriSpeech에서의 결과

오늘은 테이블 2에서 우리 모델의 (WER) 결과를 LibriSpeech test-clean/test-other에서 몇 가지 최첨단 모델과 비교합니다. 이 모델들은 ContextNet [10], Transformer transducer [7], 그리고 QuartzNet [9]를 포함합니다. 모든 평가 결과는 소수점 이하 1자리까지 반올림됩니다.
언어 모델 없이도, 우리의 중간 모델은 이미 test/testother에서 2.3/5.0의 경쟁력 있는 결과를 달성하며, 최고의 알려진 Transformer, LSTM 기반 모델 또는 유사한 크기의 컨볼루션 모델을 능가합니다. 언어 모델을 추가하면, 우리 모델은 가장 낮은 단어 오류율을 달성합니다.

표 1: Conformer S, M 및 L 모델의 모델 하이퍼파라미터. 다양한 조합을 시도하고 매개변수 제한 내에서 가장 성능이 우수한 모델을 선택하여 찾았습니다.

모델
컨포머
(S)
컨포머
(M)
컨포머
(L)

Num Params (M) 10.3 30.7  118.8
인코더 레이어 16  16     17
인코더 차원 144    256    512
어텐션 헤드 4   4      8
컨벌루션 커널 크기 32 32    32
디코더 레이어 1    1      1
디코더 차원 320    640    640

표 2: 최근 게시된 모델과의 Conformer 비교. 우리 모델은 다양한 모델 매개변수 크기 제약 조건에서 일관적으로 개선되었습니다. 10.3M 매개변수에서 우리 모델은 테스트 기준으로 0.7% 더 우수한 성능을 보여줍니다. 이는 최신 작업인 ContextNet(S)[10]와 비교했을 때입니다. 30.7M 모델 매개변수에서는 우리 모델이 이전에 게시된 TransformerTransducer[7]의 139M 매개변수보다 이미 현저히 우수한 성능을 보입니다.

방법   #매개변수(M) LM없는 WER LM있는 WER

테스트클린 테스트오더 테스트클린 테스트오더

하이브리드
트랜스포머[33] - - - 2.26 4.85
CTC
퀄츠넷[9] 19 3.90 11.28 2.69 7.25
LAS
트랜스포머[34] 270 2.89 6.98 2.33 5.17
트랜스포머[19] - 2.2 5.6 2.6 5.7 LSTM 360 2.6 6.0 2.2 5.2
트랜스듀서
트랜스포머[7] 139 2.4 5.6 2.0 4.6
컨텍스트넷(S)[10] 10.8 2.9 7.0 2.3 5.5
컨텍스트넷(M)[10] 31.4 2.4 5.4 2.0 4.5
컨텍스트넷(L)[10] 112.7 2.1 4.6 1.9 4.1

컨포머(우리)
컨포머(S) 10.3 2.7 6.3 2.1 5.0
컨포머(M) 30.7 2.3 5.0 2.0 4.3
컨포머(L) 118.8 2.1 4.3 1.9 3.9

모든 기존 모델들 사이의 오류율. 이는 Transformer와 convolution을 단일 신경망에 결합하는 효과를 명확히 보여줍니다.

3.4. 제거 연구

3.4.1. 콘포머 블록 대 트랜스포머 블록

컨포머 블록은 트랜스포머 블록과 여러 가지 방식으로 다릅니다. 특히, 컨볼루션 블록의 포함과 맥카롱 스타일에서 블록을 둘러싼 FFN의 쌍이 있습니다. 아래에서는 이러한 차이의 영향을 연구하기 위해 컨포머 블록을 트랜스포머 블록으로 변이시키면서 전체 매개변수 수를 유지합니다. 표 3은 컨포머 블록에 대한 각 변경 사항의 영향을 보여줍니다. 모든 차이 중에서 컨볼루션 서브 블록이 가장 중요한 특징이며, 맥카롱 스타일 FFN 쌍이 동일한 매개변수 수의 단일 FFN보다 효과적입니다. 스위시 활성화를 사용하면 컨포머 모델의 수렴이 더 빨랐습니다.

표 3: Conformer의 분리. Conformer 블록에서 시작하여 그 특징을 제거하고 일반적인 Transformer 블록으로 이동합니다: (1) SWISH를 ReLU로 대체; (2) 합성곱 하위 블록 제거; (3) Macaron-style FFN 쌍을 단일 FFN으로 대체; (4) self-attention을 상대적 위치 임베딩 [20]과 일반적인 self-attention 레이어 [6]로 대체. 모든 실험 결과는 외부 LM 없이 평가됩니다.

모델
건축

개발
청소
개발
다른
테스트
청소
테스트
다른

컨포머 모델 1.9 4.4 2.1 4.3
- SWISH + ReLU 1.9 4.4 2.0 4.5
- 컨볼루션 블록 2.1 4.8 2.1 4.9
- 마카롱 FFN 2.1 5.1 2.1 5.0
- 상대적 위치 임베딩 2.3 5.8 2.4 5.6

3.4.2. 합성곱과 트랜스포머 모듈의 조합

우리는 다양한 다른 방식으로 다중 헤드 셀프 어텐션 (MHSA) 모듈을 컨볼루션 모듈과 결합하는 효과를 연구합니다. 먼저, 컨볼루션 모듈의 깊이별 컨볼루션을 경량 컨볼루션 [35]으로 대체해보고, 특히 dev-other 데이터셋에서 성능이 크게 저하되는 것을 확인합니다. 둘째, 우리의 Conformer 모델에서 컨볼루션 모듈을 MHSA 모듈 앞에 배치하는 것을 연구하고, dev-other에서 결과가 0.1 저하되는 것을 발견합니다. 또 다른 가능한 아키텍처는 입력을 병렬로 분할하여 다중 헤드 셀프 어텐션 모듈과 컨볼루션 모듈을 포함하여 출력을 연결하는 것입니다 [17]에서 제안한 대로. 우리의 제안된 아키텍처와 비교했을 때, 이는 성능을 악화시킵니다. 표 4의 이러한 결과는 Conformer 블록에서 셀프 어텐션 모듈 뒤에 컨볼루션 모듈을 배치하는 장점을 시사합니다.

표 4: Conformer Attention Convolution 블록의 제거 연구. 다중 헤드 셀프 어텐션과 컨볼루션 블록의 조합을 다양하게 변화시킴: (1) Conformer 아키텍처; (2) Conformer의 컨볼루션 블록에서 깊이별 컨볼루션 대신 경량 컨볼루션 사용; (3) 다중 헤드 셀프 어텐션 이전에 컨볼루션; (4) 컨볼루션과 MHSA를 병렬로 실행하고 그 출력을 연결함 [17].

모델 아키텍처

개발
청소
개발
다른

컨포머 1.9 4.4
- 깊이 방향 합성곱 + 가벼운 합성곱 2.0 4.8
MHSA 이전의 합성곱 블록 1.9 4.5
병렬 MHSA와 합성곱 2.0 4.9

3.4.3. 마카롱 피드 포워드 모듈

트랜스포머 모델에서처럼 어텐션 블록 이후에 단일 피드포워드 모듈(FFN) 대신에, 컨포머 블록은 셀프 어텐션과 컨볼루션 모듈을 사이에 맥카롱과 같은 피드포워드 모듈 한 쌍으로 구성되어 있습니다. 더 나아가, 컨포머 피드포워드 모듈은 반스텝 잔차와 함께 사용됩니다. 테이블 5는 컨포머 블록을 단일 FFN이나 풀스텝 잔차를 사용하도록 변경했을 때의 영향을 보여줍니다.

표 5: Macaron-net 피드 포워드 모듈의 제거 연구. Conformer 피드 포워드 모듈과 Transformer 모델에서 사용되는 단일 FFN 간의 차이 제거: (1) Conformer; (2) 피드 포워드 모듈에 완전한 스텝 잔차를 가진 Conformer; (3) Macaron-style FFN 쌍을 단일 FFN으로 대체.

모델
건축
개발
청소
개발
기타
테스트
청소
테스트
기타

일치자 1.9 4.4 2.1 4.3
단일 FFN 1.9 4.5 2.1 4.5
전체 단계 잔차 1.9 4.5 2.1 4.5

3.4.4. 관심 헤드의 수

셀프 어텐션에서 각 어텐션 헤드는 입력의 다른 부분에 초점을 맞추는 것을 학습하여 단순한 가중 평균을 넘어 예측을 개선할 수 있게 합니다. 우리는 큰 모델에서 어텐션 헤드의 개수를 4에서 32까지 변화시키는 실험을 수행하였으며, 모든 레이어에서 동일한 개수의 헤드를 사용하였습니다. 우리는 어텐션 헤드를 16까지 증가시키는 것이 특히 devother 데이터셋을 포함한 정확도를 향상시킨다는 것을 발견하였습니다. 표 6에서 확인할 수 있습니다.

테이블 6: 다중 헤드 자기 주의에서 주의 헤드에 대한 제거 연구.

주의
머리
희미한 사람
머리
개발자
청소
개발자
다른
테스트
청소
테스트
다른

4     128  1.9 4.6 2.0 4.5
8     64   1.9 4.4 2.1 4.3
16    32   2.0 4.3 2.2 4.4
32    16   1.9 4.4 2.1 4.5

4     128  1.9 4.6 2.0 4.5
8     64   1.9 4.4 2.1 4.3
16    32   2.0 4.3 2.2 4.4
32    16   1.9 4.4 2.1 4.5

3.4.5. 합성곱 커널 크기

깊이별 합성곱에서 커널 크기의 영향을 연구하기 위해, 우리는 큰 모델의 커널 크기를 {3,7,17,32,65}로 변화시키고 모든 레이어에 동일한 커널 크기를 사용합니다. 우리는 커널 크기가 17과 32일 때까지 커널 크기가 클수록 성능이 향상되지만, 커널 크기가 65인 경우에는 성능이 저하되는 것을 발견했습니다. 표 7에 나와 있는 것처럼, dev WER의 두 번째 소수점을 비교해보면, 커널 크기 32가 나머지보다 더 좋은 성능을 보인다는 것을 알 수 있습니다.

테이블 7: 깊이별 컨볼루션 커널 크기에 대한 제거 연구.

커널
크기
개발
정리
개발
다른
테스트
정리
테스트
다른

3   1.88 4.41 1.99 4.39
7   1.88 4.30 2.02 4.44
17  1.87 4.31 2.04 4.38
32  1.83 4.30 2.03 4.29
65  1.89 4.47 1.98 4.46

3   1.88 4.41 1.99 4.39
7   1.88 4.30 2.02 4.44
17  1.87 4.31 2.04 4.38
32  1.83 4.30 2.03 4.29
65  1.89 4.47 1.98 4.46

결론

이 작업에서는 Conformer라는 아키텍처를 소개하였습니다. 이 아키텍처는 CNN과 Transformer의 구성 요소를 통합하여 엔드 투 엔드 음성 인식을 수행합니다. 우리는 각 구성 요소의 중요성을 연구하였으며, 컨볼루션 모듈의 포함이 Conformer 모델의 성능에 중요한 역할을 한다는 것을 입증하였습니다. 이 모델은 LibriSpeech 데이터셋에서 이전 작업보다 적은 매개변수로 더 높은 정확도를 보여주며, 테스트/테스트기타에서 1.9%/3.9%의 새로운 최고 성능을 달성합니다. 5. 참고 문헌

[1] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina 등, "시퀀스-투-시퀀스 모델을 이용한 최신 음성 인식 기술," 2018년 IEEE 국제 음향, 음성 및 신호 처리 학회(ICASSP) 논문집, IEEE, 2018, pp. 4774-4778.

[2] K. Rao, H. Sak, and R. Prabhavalkar, "rnn-transducer를 사용한 스트리밍 엔드 투 엔드 음성 인식을 위한 아키텍처, 데이터 및 유닛 탐색," 2017년 IEEE 자동 음성 인식 및 이해 워크샵(ASRU)에서, IEEE, 2017, 193-199쪽.

[3] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,
D. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang,
D. Bhatia, Y. Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby,
S.-Y. Chang, K. Rao, and A. Gruenstein, "모바일 기기를 위한 스트리밍 엔드 투 엔드 음성 인식," ICASSP, 2019.

[4] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,
S.-y. Chang, W. Li, R. Alvarez, Z. Chen, 그리고 기타, "서버 측 전통적인 모델의 품질과 지연 시간을 능가하는 스트리밍 장치 내 엔드 투 엔드 모델," ICASSP, 2020년.

[5] A. Graves, "순환 신경망을 이용한 시퀀스 변환," arXiv 사전인쇄 arXiv:1211.3711, 2012.

[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "주의는 당신이 필요한 모든 것입니다," 2017.

[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, 그리고 S. Kumar, "Transformer transducer: Transformer encoders와 RNN-T loss를 사용한 스트리밍 가능한 음성 인식 모델," ICASSP 2020-2020 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP). IEEE, 2020, pp. 7829-7833.

[8] J.Li,V.Lavrukhin,B.Ginsburg,R.Leary,O.Kuchaiev,J.M.Co-
hen, H. Nguyen, and R. T. Gadde, "Jasper: An end-to-end convo-
lutionalneuralacousticmodel,"arXivpreprintarXiv:1904.03288,
2019.

[8] J.Li, V.Lavrukhin, B.Ginsburg, R.Leary, O.Kuchaiev, J.M.Co-
hen, H. Nguyen, 그리고 R. T. Gadde, "Jasper: 엔드 투 엔드 컨보-
루션 신경 음향 모델," arXiv 사전 인쇄 arXiv:1904.03288,
2019.

[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,
V. Lavrukhin, R. Leary, J. Li, and Y. Zhang, "Quartznet: Deep
automaticspeechrecognitionwith1dtime-channelseparablecon-
volutions,"arXivpreprintarXiv:1910.10261,2019.

[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,
V. Lavrukhin, R. Leary, J. Li, and Y. Zhang, "Quartznet: Deep
automaticspeechrecognitionwith1dtime-channelseparablecon-
volutions,"arXivpreprintarXiv:1910.10261,2019.

[10] W. 한, Z. 장, Y. 장, J. 유, C.-C. 츄, J. 퀸, A. 구라티, R. 팡, 그리고 Y. 우, "Contextnet: 전역 컨텍스트를 활용하여 자동 음성 인식을 위한 합성곱 신경망 개선," arXiv 사전 인쇄 arXiv:2005.03191, 2020.

[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, "Deep convolutional neural networks for LVCSR," in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 8614-8618.

[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, 그리고 B. Ramabhadran, "LVCSR을 위한 깊은 합성곱 신경망," 2013년 IEEE 국제 음향, 음성 및 신호 처리 학회에서 발표. IEEE, 2013, pp. 8614-8618.

[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,
and D. Yu, "음성 인식을 위한 합성곱 신경망," IEEE/ACM Transactions on audio, speech, and language processing,vol.22,no.10,pp.1533–1545,2014.

[13] J. Hu, L. Shen, and G. Sun, "Squeeze-and-excitation networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132-7141. 

[13] J. Hu, L. Shen, 그리고 G. Sun, "Squeeze-and-excitation networks," IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2018, pp. 7132-7141.

[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, "Attention augmented convolutional networks," in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 3286–3295.

[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, 그리고 Q. V. Le, "어텐션 증강 컨볼루션 네트워크," IEEE 국제 컴퓨터 비전 학회 논문집, 2019, pp. 3286–3295.

[15] B. 양, L. 왕, D. 웡, L. S. 초, 그리고 Z. 투, "합성곱 자기-주의 네트워크," arXiv 사전 인쇄 arXiv:1904.03107, 2019.

[16] A.W.Yu,D.Dohan,M.-T.Luong,R.Zhao,K.Chen,M.Norouzi,
and Q. V. Le, "Qanet: 지역 컨볼루션과 전역 셀프 어텐션을 결합한 독해를 위한 모델," arXiv 사전 인쇄 arXiv:1804.09541,2018.

[17] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, "Lite transformer with long-short range attention," arXiv 사전 인쇄 arXiv:2004.11886, 2020.

[18] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y. Liu, "다중 입자 동적 시스템 관점에서 Transformer의 이해와 개선," arXiv 사전 인쇄 arXiv:1906.02762, 2019.

[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,
M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang 등, "음성 응용에서의 트랜스포머 대 RNN에 대한 비교 연구," arXivpreprintarXiv:1909.06317, 2019.

[20] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, "Transformer-xl: 고정 길이 컨텍스트를 넘어선 주의력 언어 모델," 2019.

[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao, "기계 번역을 위한 딥 트랜스포머 모델 학습," 제57회 연례 협회 컴퓨터 언어학 회의 논문집. 협회 컴퓨터 언어학, 2019년 7월, 1810-1822쪽.

[22] T. Q. Nguyen과 J. Salazar, "눈물 없는 트랜스포머: 셀프 어텐션의 정규화 개선," arXiv 사전 인쇄 arXiv:1910.05895, 2019.

[23] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, "게이트 컨볼루션 네트워크를 사용한 언어 모델링," 제34회 국제 기계 학습 대회 논문집-Volume 70. JMLR.org, 2017, pp. 933-941.

[24] L.Dong,S.Xu,andB.Xu,“음성 변환기: 음성 인식을 위한 비재귀 시퀀스-시퀀스 모델,” 2018년 IEEE 국제 음향, 음성 및 신호 처리 학회(ICASSP)에서 발표. IEEE, 2018, 5884-5888쪽.

[25] P. Ramachandran, B. Zoph, and Q. V. Le, "활성화 함수를 찾아서," arXiv 사전인쇄 arXiv:1710.05941, 2017.

[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "Librispeech: an asr corpus based on public domain audio books," in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206-5210.

[26] V. Panayotov, G. Chen, D. Povey, 그리고 S. Khudanpur, "Librispeech: 공공 도메인 오디오북을 기반으로 한 asr 말뭉치," 2015년 IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP)에서, IEEE, 2015, pp. 5206-5210.

[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, "Specaugment: A simple data augmentation method for automatic speech recognition," arXiv preprint arXiv:1904.08779, 2019.

[27] 박 D. S., 찬 W., 장 Y., 츄 C.-C., 조프 B., 큐베크 E. D., 그리고 레 Q. V., "Specaugment: 자동 음성 인식을 위한 간단한 데이터 증강 방법," arXiv 사전 인쇄 arXiv:1904.08779, 2019.

[28] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le, and Y. Wu, "대규모 데이터셋에서의 Specaugment," arXiv preprintarXiv:1912.05533, 2019.

[29] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: 신경망이 과적합되는 것을 방지하기 위한 간단한 방법," 기계 학습 연구 저널, 제15권, 제56호, 1929-1958쪽, 2014년.

[30] K.-C. Jim, C. L. Giles, and B. G. Horne, "순환 신경망의 소음 분석: 수렴과 일반화," IEEE 신경망 트랜잭션, 제7권, 제6호, 1424-1438쪽, 1996년.

[31] D. P. Kingma와 J. Ba, "Adam: 확률적 최적화를 위한 방법," arXiv 사전인쇄 arXiv:1412.6980, 2014.

[32] J. Shen, P. Nguyen, Y. Wu, Z. Chen, 그리고 외, "Lingvo: 시퀀스-시퀀스 모델링을 위한 모듈식이며 확장 가능한 프레임워크," 2019.

[33] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,
H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., "하이브리드 음성 인식을 위한 Transformer 기반 음향 모델링," arXiv 사전인쇄arXiv:1910.09799, 2019.

[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,
V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, "End-to-
endasr: fromsupervisedtosemi-supervisedlearningwithmodern
architectures,"2019.

[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,
V. Pratap, A. Sriram, V. Liptchinsky, 그리고 R. Collobert, "End-to-
endasr: 현대 아키텍처를 사용한 지도 학습에서 준지도 학습으로," 2019.

[35] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, "가벼운 동적 컨벌루션으로 더 적은 주의를 기울이기," arXiv 사전인쇄 arXiv:1901.10430, 2019.

