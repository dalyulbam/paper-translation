RNN 인코더-디코더를 사용하여 구문 표현 학습하기

통계 기계 번역을 위해

경현 조
바트 반 메리엔보어 카글라르 굴체레
몬트리올 대학교
firstname.lastname@umontreal.ca

드미트리 바다나우
독일 야콥스 대학교
d.bahdanau@jacobs-university.de

페티 부가레스 홀거 슈벵크
프랑스 메인 대학교
firstname.lastname@lium.univ-lemans.fr

요슈아 벵지오
몬트리올 대학교, CIFAR 고급 연구원
find.me@on.the.web

요약

이 논문에서는 RNN Encoder-Decoder라는 새로운 신경망 모델을 제안합니다. 이 모델은 두 개의 순환 신경망(RNN)으로 구성되어 있습니다. 하나의 RNN은 기호들의 시퀀스를 고정 길이의 벡터 표현으로 인코딩하고, 다른 하나의 RNN은 표현을 다른 기호들의 시퀀스로 디코딩합니다. 제안된 모델의 인코더와 디코더는 소스 시퀀스가 주어졌을 때 타겟 시퀀스의 조건부 확률을 최대화하기 위해 공동으로 훈련됩니다. RNN Encoder-Decoder가 계산한 구문 구조의 조건부 확률을 기존 로그-선형 모델의 추가적인 특징으로 사용함으로써 통계 기계 번역 시스템의 성능이 경험적으로 향상되는 것을 확인하였습니다. 우리는 제안된 모델이 언어적인 구문 구조의 의미론적이고 문법적으로 의미 있는 표현을 학습한다는 것을 질적으로 보여줍니다.

1 소개

딥 신경망은 물체 인식 (예: Krizhevsky et al., 2012) 및 음성 인식 (예: Dahl et al., 2012)과 같은 다양한 응용 분야에서 큰 성공을 보여주었습니다. 또한, 최근의 많은 연구들은 신경망이 자연어 처리 (NLP)의 여러 작업에 성공적으로 사용될 수 있다는 것을 보여주었습니다. 이에는 언어 모델링 (Bengio et al., 2003), 유사 문장 감지 (Socher et al., 2011) 및 단어 임베딩 추출 (Mikolov et al., 2013) 등이 포함됩니다. 통계 기반 기계 번역 (SMT) 분야에서도, 딥 신경망은 유망한 결과를 보이기 시작했습니다. (Schwenk, 2012)은 구문 기반 SMT 시스템에서 피드포워드 신경망의 성공적인 사용을 요약하고 있습니다.

신경망을 사용한 SMT에 대한 이 연구의 연장선으로, 이 논문은 전통적인 구문 기반 SMT 시스템의 일부로 사용될 수 있는 새로운 신경망 구조에 초점을 맞추고 있습니다. 제안된 신경망 구조는 RNN 인코더-디코더로 참조하며, 인코더와 디코더 쌍으로 작동하는 두 개의 순환 신경망(RNN)으로 구성됩니다. 인코더는 가변 길이의 소스 시퀀스를 고정 길이의 벡터로 매핑하고, 디코더는 벡터 표현을 가변 길이의 대상 시퀀스로 매핑합니다. 두 신경망은 소스 시퀀스가 주어졌을 때 대상 시퀀스의 조건부 확률을 최대화하기 위해 공동으로 훈련됩니다. 또한, 우리는 기억 용량과 훈련의 용이성을 개선하기 위해 상당히 복잡한 은닉 유닛을 사용하는 것을 제안합니다.

제안된 RNN 인코더-디코더는 새로운 은닉 유닛을 가지고 있으며, 영어에서 프랑스어로의 번역 작업에서 경험적으로 평가되었습니다. 우리는 모델을 훈련시켜 영어 구문을 해당하는 프랑스어 구문으로 번역하는 확률을 학습시킵니다. 그런 다음 모델은 구문 테이블의 각 구문 쌍을 점수화하여 표준 구문 기반 SMT 시스템의 일부로 사용됩니다. 경험적 평가 결과, RNN 인코더-디코더를 사용하여 구문 쌍에 점수를 매기는 이 접근 방식은 번역 성능을 향상시킵니다.

훈련된 RNN Encoder-Decoder를 정성적으로 분석하여, 기존의 번역 모델이 제공하는 구문 점수와 비교합니다. 정성적 분석 결과, RNN Encoder-Decoder는 구문 테이블의 언어적 규칙성을 더 잘 포착하는 것으로 나타났으며, 이는 전체적인 번역 성능의 정량적 개선을 간접적으로 설명합니다. 모델의 추가 분석 결과, RNN Encoder-Decoder는 구문의 의미론적 및 문법적 구조를 보존하는 구문의 연속적인 공간 표현을 학습한다는 것을 알 수 있습니다.

2.1 예비: 재귀 신경망

순환 신경망(RNN)은 숨겨진 상태 h와 선택적 출력 y로 구성된 신경망입니다. 이는 변수 길이의 시퀀스 x = (x 1,...,x T)에 작용합니다. 각 시간 단계 t마다 RNN의 숨겨진 상태 h (cid:104)t(cid:105)는 다음과 같이 업데이트됩니다.

h(t) = f(h(t-1), xt)      (1)

f는 비선형 활성화 함수입니다. f는 요소별 로지스틱 시그모이드 함수와 같이 간단할 수도 있고, 장단기 메모리(LSTM) 유닛과 같이 복잡할 수도 있습니다 (Hochreiter and Schmidhuber, 1997).
RNN은 시퀀스에서 확률 분포를 학습할 수 있으며, 다음 시퀀스 기호를 예측하기 위해 훈련됩니다. 이 경우, 각 시간 단계 t의 출력은 조건부 분포 p(x t | x t−1,...,x 1)입니다. 예를 들어, 다항 분포 (1-of-K 코딩)는 소프트맥스 활성화 함수를 사용하여 출력될 수 있습니다.

p(x_t,j=1 | x_{t-1},...,x_1) =

exp
w jh
헤이티
피케이
j0=1
exp w j0h
헤이티,
(2)

모든 가능한 심볼 j = 1,...,K에 대해, w j는 가중치 행렬 W의 행들입니다. 이러한 확률들을 결합하여, 우리는 시퀀스 x의 확률을 계산할 수 있습니다.

p(x) = T
t=1p(xt | xt−1,...,x1). (3)
이 학습된 분포를 통해 새로운 시퀀스를 샘플링하는 것은 각 시간 단계에서 심볼을 반복적으로 샘플링하는 것으로 간단합니다.

2.2 RNN 인코더-디코더

이 논문에서는 변수 길이의 시퀀스를 고정 길이의 벡터 표현으로 인코딩하고 주어진 고정 길이의 벡터 표현을 변수 길이의 시퀀스로 디코딩하는 새로운 신경망 구조를 제안합니다. 확률적인 관점에서 이 새로운 모델은 다른 변수 길이의 시퀀스에 조건을 걸어 변수 길이의 시퀀스에 대한 조건부 분포를 학습하는 일반적인 방법입니다. 예를 들어, p(y 1,...,y T(cid:48) | x 1,...,x T)와 같은 형태입니다.

x
1
x
2
x
T
y T'     y 2    y 1

x
1
x
2
x
T
y T'     y 2    y 1

디코더

인코더

그림 1: 제안된 RNN 인코더-디코더의 그림.

입력 및 출력 시퀀스의 길이 T와 T'가 다를 수 있다는 점을 주의해야 합니다.
인코더는 입력 시퀀스 x의 각 기호를 순차적으로 읽는 RNN입니다. 각 기호를 읽을 때마다 RNN의 은닉 상태는 식 (1)에 따라 변경됩니다. 시퀀스의 끝 (종료 기호로 표시)을 읽은 후, RNN의 은닉 상태는 전체 입력 시퀀스의 요약 c입니다.
제안된 모델의 디코더는 다른 RNN으로, 다음 기호 y_t를 예측하여 출력 시퀀스를 생성하는 데 훈련됩니다. 그러나 Sec. 2.1에서 설명한 RNN과 달리, y_t와 h_t는 y_t-1 및 입력 시퀀스의 요약 c에도 의존합니다. 따라서 디코더의 은닉 상태는 시간 t에서 다음과 같이 계산됩니다.

h(t) = f(h(t-1), y(t-1), c)

그리고 마찬가지로, 다음 기호의 조건부 분포는

P(y t|y t−1,y t−2,...,y 1,c) = g
P(y t|y t−1,y t−2,...,y 1,c) = g

안녕하세요,
전 이전 메시지를 받았습니다.

주어진 활성화 함수 f와 g에 대해 (후자는 유효한 확률을 생성해야 함, 예: 소프트맥스를 사용하여)
제안된 모델 구조에 대한 그래픽 표현은 그림 1을 참조하십시오.
제안된 RNN 인코더-디코더의 두 구성 요소는 조건부 로그 우도를 최대화하기 위해 공동으로 훈련됩니다.

최대 θ
1
N
N (cid:88)
n=1logp θ(y n | x n), (4)
여기서 θ는 모델 매개변수의 집합이며
각 (x n, y n)은 훈련 세트에서의 (입력 시퀀스, 출력 시퀀스) 쌍입니다. 우리의 경우,
디코더의 출력은 입력으로부터 시작하여 미분 가능하기 때문에, 모델 매개변수를 추정하기 위해 기울기 기반 알고리즘을 사용할 수 있습니다.
RNN 인코더-디코더가 훈련되면, 모델은 두 가지 방법으로 사용할 수 있습니다. 하나는 입력 시퀀스가 주어졌을 때 대상 시퀀스를 생성하는 데 모델을 사용하는 것입니다. 반면에, 모델은 주어진 입력과 출력 시퀀스 쌍을 점수화하는 데 사용될 수 있으며, 점수는 단순히 Eqs. (3)과 (4)에서의 확률 p θ(y | x)입니다.

2.3 적응적으로 기억하고 잊어버리는 숨겨진 유닛

새로운 모델 아키텍처 외에도, 우리는 계산과 구현이 훨씬 간단한 LSTM 유닛에서 영감을 받은 새로운 유형의 숨겨진 유닛(f in Eq. (1))을 제안합니다. 그림 2는 제안된 숨겨진 유닛의 그래픽 표현을 보여줍니다.
j번째 숨겨진 유닛의 활성화가 어떻게 계산되는지 설명해보겠습니다. 먼저, 리셋 게이트 rj는 다음과 같이 계산됩니다.

r j = σ[W rx] j + U rh t−1 j, (5)

σ는 로지스틱 시그모이드 함수이고, [.]


j번째 요소를 나타냅니다. x와 h t−1은 각각 입력과 이전의 숨겨진 상태입니다. W r과 U r은 학습된 가중치 행렬입니다.
마찬가지로, 업데이트 게이트 z j는 다음과 같이 계산됩니다.

z j = σ([W zx] j + U zh t−1 ) (6)

제안된 단위 h j의 실제 활성화는 다음과 같이 계산됩니다.

h(t)
j
= z jh(t-1)
j
+ (1 - z j)˜ h(t)
j
,   (7)

어디에

˜ h(cid:104)t(cid:105)
j
= φ(cid:16) [Wx]
j
+ (cid:2) U(cid:0) r (cid:12) h (cid:104)t−1(cid:105)(cid:1)(cid:3) j(cid:17) . (8)
이 공식에서, reset 게이트가 0에 가까울 때, 은닉 상태는 이전 은닉 상태를 무시하고 현재 입력과 함께 재설정됩니다.

1. LSTM 유닛은 음성 인식과 같은 여러 응용 분야에서 인상적인 결과를 보여준다. 이 유닛은 메모리 셀과 정보 흐름을 조절하는 적응형 게이팅 유닛 네 개를 가지고 있으며, 제안된 숨겨진 유닛은 게이팅 유닛 두 개만을 가지고 있다. LSTM 네트워크에 대한 자세한 내용은 Graves (2012)를 참조하십시오.

죄송합니다, 하지만 저는 한국어를 번역할 수 없습니다.

r
h          h
~
x

그림 2: 제안된 숨겨진 활성화 함수의 그림. 업데이트 게이트 z는 숨겨진 상태가 새로운 숨겨진 상태 ˜ h로 업데이트되는지 선택합니다. 리셋 게이트 r은 이전 숨겨진 상태가 무시되는지 결정합니다. r, z, h 및 ˜ h의 상세한 방정식은 Eqs. (5)–(8)을 참조하십시오.

만. 이는 숨겨진 상태가 미래에 더 이상 관련이 없다고 판단되는 정보를 삭제할 수 있도록 효과적으로 허용하여 더 간결한 표현을 가능하게 합니다.
반면에, 업데이트 게이트는 이전 숨겨진 상태로부터 현재 숨겨진 상태로 얼마나 많은 정보를 전달할지를 제어합니다. 이는 LSTM 네트워크의 메모리 셀과 유사하게 작동하며 RNN이 장기적인 정보를 기억하는 데 도움을 줍니다. 또한, 이는 누수 통합 유닛의 적응형 변형으로 간주될 수 있습니다 (Bengio et al., 2013).
각 숨겨진 유닛은 별도의 리셋 게이트와 업데이트 게이트를 가지고 있기 때문에 각 숨겨진 유닛은 서로 다른 시간 스케일에서 의존성을 포착하는 것을 학습합니다. 짧은 기간의 의존성을 포착하는 유닛은 자주 활성화되는 리셋 게이트를 가지게 되지만, 장기적인 의존성을 포착하는 유닛은 대부분 활성화되는 업데이트 게이트를 가지게 됩니다.
우리의 예비 실험에서는 이 새로운 유닛을 게이팅 유닛과 함께 사용하는 것이 중요하다는 것을 발견했습니다. 게이팅이 없는 흔히 사용되는 tanh 유닛만으로는 의미 있는 결과를 얻을 수 없었습니다.

3 통계 기계 번역

일반적으로 사용되는 통계 기반 기계 번역 시스템(SMT)에서 시스템(특히 디코더)의 목표는 소스 문장 e가 주어졌을 때 번역 f를 최대화하는 것이다.

p(f | e) ∝ p(e | f)p(f)

오른쪽에 있는 첫 번째 용어는 번역 모델이라고 불리며, 후자는 언어 모델이라고 합니다 (참고: Koehn, 2005). 그러나 실제로는 대부분의 SMT 시스템에서 logp(f | e)를 로그-선형 모델로 모델링하며, 추가적인 특징과 해당 가중치를 가지고 있습니다.

logp(f | e) =
N
(cid:88)
n=1w nf n(f,e) + logZ(e), (9)
여기서 f n과 w n은 n번째 특징과 가중치입니다.
Z(e)는 가중치에 의존하지 않는 정규화 상수입니다.
가중치는 개발 세트에서 BLEU 점수를 최대화하기 위해 종종 최적화됩니다.
(Koehn et al., 2003)와 (Marcu and Wong, 2002)에서 소개된 구문 기반 SMT 프레임워크에서 번역 모델 logp(e | f)은 소스 문장과 대상 문장에서 일치하는 구문의 번역 확률로 분해됩니다.
이러한 확률은 다시 로그 선형 모델의 추가적인 특징으로 간주되며 (식 (9) 참조), BLEU 점수를 최대화하기 위해 적절하게 가중치가 부여됩니다.
(Bengio et al., 2003)에서 제안된 신경망 언어 모델 이후로 신경망은 SMT 시스템에서 널리 사용되고 있습니다.
많은 경우 신경망은 번역 가설 (n-최적 리스트)을 재점수화하는 데 사용되었습니다 (예: (Schwenk et al., 2006)).
그러나 최근에는 소스 문장의 표현을 추가 입력으로 사용하여 번역된 문장 (또는 구문 쌍)을 점수화하는 신경망을 훈련시키는 관심이 있었습니다.
(Schwenk, 2012), (Son et al., 2012) 및 (Zou et al., 2013) 참조.

3.1 RNN을 사용하여 문장 쌍을 점수화하기
인코더-디코더

여기에서는 RNN Encoder-Decoder를 훈련시키기 위해 구문 쌍 테이블을 사용하고, SMT 디코더를 튜닝할 때 로그-선형 모델에서 추가 기능으로 사용합니다.
RNN Encoder-Decoder를 훈련시킬 때, 원본 말뭉치의 각 구문 쌍의 (정규화된) 빈도를 무시합니다. 이 조치는 (1) 정규화된 빈도에 따라 큰 구문 테이블에서 구문 쌍을 무작위로 선택하는 계산 비용을 줄이기 위함과 (2) RNN Encoder-Decoder가 단순히 발생 횟수에 따라 구문 쌍을 순위 매기는 것을 학습하지 않도록 보장하기 위해 취해졌습니다.
이 선택의 근본적인 이유 중 하나는 구문 테이블의 기존 번역 확률이 이미 구문의 빈도를 반영하고 있다는 것입니다.

2. 일반성을 잃지 않고, 이후부터는 각 구문 쌍에 대한 p(e | f)를 번역 모델로 참조합니다.

원본 말뭉치의 쌍들 중에서 학습용 RNN 인코더-디코더의 고정 용량을 가지고, 모델의 대부분 용량이 언어 규칙을 학습하는 데 집중되도록 하려고 노력합니다. 즉, 가능한 번역과 불가능한 번역을 구별하거나 가능한 번역의 "다양체"(확률 집중 영역)를 학습하는 것입니다.
RNN 인코더-디코더가 훈련되면, 각 구문 쌍에 대해 새로운 점수를 기존 구문 테이블에 추가합니다. 이렇게 하면 새로운 점수가 추가 계산 부담 없이 기존 조정 알고리즘에 참여할 수 있습니다.
Schwenk은 (Schwenk, 2012)에서 언급한 대로, 제안된 RNN 인코더-디코더로 기존 구문 테이블을 완전히 대체하는 것이 가능합니다. 이 경우, 주어진 소스 구문에 대해 RNN 인코더-디코더가 (좋은) 대상 구문 목록을 생성해야 합니다. 그러나 이를 위해서는 비용이 많이 드는 샘플링 절차를 반복적으로 수행해야 합니다. 따라서 본 논문에서는 구문 테이블의 구문 쌍에 대해서만 재점수화하는 것을 고려합니다.

3.2 관련 접근 방식: 기계 번역에서의 신경망

실증 결과를 제시하기 전에, 우리는 SMT의 맥락에서 신경망을 사용하는 최근 작업 몇 가지에 대해 논의합니다. (Schwenk, 2012)에서 Schwenk은 구문 쌍의 점수화에 유사한 접근 방식을 제안했습니다. 그는 RNN 기반 신경망 대신 고정 크기의 입력 (7 단어, 짧은 구문에 대한 제로 패딩 포함) 및 고정 크기의 출력 (대상 언어의 7 단어)을 가진 피드포워드 신경망을 사용했습니다. SMT 시스템의 구문에 대한 점수화에 사용될 때, 최대 구문 길이는 종종 작게 선택됩니다. 그러나 구문의 길이가 증가하거나 신경망을 다른 가변 길이 시퀀스 데이터에 적용할 때, 신경망이 가변 길이의 입력과 출력을 처리할 수 있는 것이 중요합니다. 제안된 RNN 인코더-디코더는 이러한 응용에 적합합니다.

(Schwenk, 2012)와 유사하게, Devlin et al. (Devlin et al., 2014)은 번역 모델을 모델링하기 위해 피드포워드 신경망을 사용하는 것을 제안했지만, 한 번에 대상 구문의 한 단어를 예측합니다. 그들은 인상적인 개선을 보고했지만, 그들의 접근 방식은 여전히 입력 구문 (또는 문맥 단어)의 최대 길이가 사전에 고정되어야 한다는 것을 요구합니다.

(Zou et al., 2013)의 저자들은 정확히 신경망을 훈련시키는 것은 아니지만, 단어/구문의 양방향 임베딩을 학습하는 것을 제안했습니다. 그들은 학습된 임베딩을 사용하여 구문 쌍의 추가 점수로 사용되는 구문 쌍 간의 거리를 계산합니다.

(Chandar et al., 2014)에서는 피드포워드 신경망이 입력 구문의 단어 가방 표현에서 출력 구문으로의 매핑을 학습하기 위해 훈련되었습니다. 이는 제안된 RNN 인코더-디코더와 (Schwenk, 2012)에서 제안된 모델과 밀접한 관련이 있지만, 구문의 입력 표현은 단어 가방입니다.

(Gao et al., 2013)에서는 단어 가방 표현을 사용하는 유사한 접근 방식이 제안되었습니다. 이전에는 (Socher et al., 2011)에서 두 개의 재귀 신경망을 사용한 유사한 인코더-디코더 모델이 제안되었지만, 그들의 모델은 단일 언어 설정에 제한되어 있습니다. 최근에는 (Auli et al., 2013)에서 소스 문장 또는 소스 문맥의 표현에 의존하는 RNN을 사용한 또 다른 인코더-디코더 모델이 제안되었습니다.

제안된 RNN 인코더-디코더와 (Zou et al., 2013) 및 (Chandar et al., 2014)의 접근 방식과의 중요한 차이점은 소스 및 대상 구문의 단어 순서를 고려한다는 것입니다. RNN 인코더-디코더는 동일한 단어를 가진 시퀀스를 다른 순서로 자연스럽게 구분하며, 앞서 언급한 접근 방식은 순서 정보를 효과적으로 무시합니다.

제안된 RNN 인코더-디코더와 가장 관련이 있는 접근 방식은 (Kalchbrenner and Blunsom, 2013)에서 제안된 Recurrent Continuous Translation Model (Model 2)입니다. 그들의 논문에서는 인코더와 디코더로 구성된 유사한 모델을 제안했습니다. 우리 모델과의 차이점은 그들이 인코더에 컨볼루션 n-gram 모델 (CGM)을 사용하고 디코더에 역 CGM과 재귀 신경망의 혼합을 사용했다는 것입니다. 그러나 그들은 전통적인 SMT 시스템이 제안한 n-best 목록을 재점수화하고 골드 표준 번역의 퍼플렉서티를 계산하는 데에 그들의 모델을 평가했습니다.

4 실험

우리는 WMT'14 워크샵의 영어/프랑스어 번역 과제에서 우리의 접근 방식을 평가합니다.

4.1 데이터 및 기준 시스템

대량의 자원이 WMT'14 번역 과제의 프레임워크에서 영어/프랑스어 SMT 시스템을 구축하기 위해 사용 가능하다. 이중 언어 말뭉치에는 Europarl (61M 단어), 뉴스 코멘터리 (5.5M), UN (421M) 및 두 개의 크롤된 말뭉치 (각각 90M 및 780M 단어)가 포함되어 있다. 마지막 두 개의 말뭉치는 꽤 노이즈가 있다. 프랑스어 언어 모델을 훈련하기 위해, 비텍스트의 대상 측에 추가로 712M 단어의 크롤된 신문 자료가 사용 가능하다. 모든 단어 수는 토큰화 후의 프랑스어 단어를 의미한다. 이러한 데이터의 연결로 통계 모델을 훈련하는 것이 최적의 성능을 보장하지 않으며, 처리하기 어려운 극도로 큰 모델을 만들어낸다는 것이 일반적으로 인정되고 있다. 대신, 특정 작업에 가장 관련성이 높은 데이터 하위 집합에 초점을 맞추어야 한다. 우리는 (Moore와 Lewis, 2010)에서 제안된 데이터 선택 방법과 이를 이중 언어에 확장한 방법 (Axelrod 등, 2011)을 적용하여 이를 수행했다. 이를 통해 언어 모델링을 위해 2G 단어 이상 중 418M 단어의 하위 집합을 선택하고, RNN Encoder-Decoder를 훈련하기 위해 850M 단어 중 348M 단어의 하위 집합을 선택했다. 우리는 데이터 선택과 MERT를 사용하여 테스트 세트 newstest2012와 2013를 사용하여 가중치 조정을 수행하고, newstest2014를 테스트 세트로 사용했다. 각 세트는 7만 단어 이상과 단일 참조 번역을 가지고 있다.
제안된 RNN Encoder-Decoder를 포함한 신경망을 훈련하기 위해 영어와 프랑스어 모두에서 가장 빈도가 높은 15,000 단어로 소스 및 대상 어휘를 제한했다. 이는 데이터 세트의 약 93%를 커버한다. 모든 사전에 없는 단어는 특수 토큰 ([UNK])으로 매핑되었다.
기준이 되는 구문 기반 SMT 시스템은 기본 설정을 사용하여 Moses를 사용하여 구축되었다. 이 시스템은 개발 및 테스트 세트에서 각각 30.64와 33.3의 BLEU 점수를 달성한다 (표 1 참조).

4.1.1 RNN 인코더-디코더

RNN Encoder-Decoder는 실험에서 1000개의 hidden unit을 사용하였으며, encoder와 decoder에서 제안된 gate를 사용하였습니다. 각 입력 기호 x 사이의 입력 행렬은

안녕하세요
그리고
숨겨진 유닛은 두 개의 낮은 순위 행렬로 근사되며, 출력 행렬은 근사됩니다
모델들

블루
개발  테스트

기준선 30.64 33.30
RNN 31.20 33.87
CSLM + RNN 31.48 34.64
CSLM + RNN + WP 31.50 34.54

표 1: 다양한 접근 방식을 사용하여 개발 및 테스트 세트에서 계산된 BLEU 점수입니다. WP는 단어 벌칙을 나타내며, 우리는 신경망에 알 수 없는 단어의 수를 벌칙으로 적용합니다.

비슷하게. 우리는 rank-100 행렬을 사용했으며, 각 단어에 대한 100차원의 임베딩을 학습하는 것과 동일합니다. 식 (8)에서 ˜ h에 사용된 활성화 함수는 쌍곡 탄젠트 함수입니다. 디코더의 숨겨진 상태에서 출력으로의 계산은 깊은 신경망으로 구현되었으며 (Pascanu et al., 2014), 하나의 중간 레이어에는 500개의 맥스아웃 유닛이 있으며 각각 2개의 입력을 풀링합니다 (Goodfellow et al., 2013).

RNN 인코더-디코더의 모든 가중치 매개변수는 표준 편차가 0.01로 고정된 등방성 영향을 가진 (흰색) 가우시안 분포에서 샘플링하여 초기화되었습니다. 재귀 가중치 매트릭스의 경우, 우리는 먼저 흰색 가우시안 분포에서 샘플링하고, 그 왼쪽 특이 벡터 행렬을 사용했습니다 (Saxe et al., 2014).

우리는 Adadelta와 확률적 경사 하강법을 사용하여 RNN 인코더-디코더를 훈련시켰으며, 하이퍼파라미터 (cid:15) = 10−6 및 ρ = 0.95 (Zeiler, 2012)를 사용했습니다. 각 업데이트마다, 우리는 348M 단어로부터 생성된 구문 테이블에서 무작위로 선택된 64개의 구문 쌍을 사용했습니다. 모델은 약 세 일 동안 훈련되었습니다.

실험에서 사용된 아키텍처의 세부 사항은 부록 자료에서 더 자세히 설명되어 있습니다.

4.1.2 신경 언어 모델

평가의 효과를 평가하기 위해 제안된 RNN Encoder-Decoder로 구문 쌍을 점수화하는 전통적인 방법인 신경망을 사용하여 대상 언어 모델(CSLM)을 학습하는 것도 시도해 보았습니다(Schwenk, 2007). 특히, CSLM을 사용하는 SMT 시스템과 RNN Encoder-Decoder를 사용하여 구문 점수화하는 제안된 방법 간의 비교는 SMT 시스템의 다른 부분에서 여러 신경망의 기여가 명확해질 것입니다.

tem은 더해지거나 중복되는 것입니다.
우리는 대상 말뭉치에서 7-gram에 대해 CSLM 모델을 훈련시켰습니다.
각 입력 단어는 임베딩 공간 R512로 투영되었고, 이들은 연결되어 3072차원 벡터를 형성했습니다.
연결된 벡터는 두 개의 활성화 함수(rectified) 레이어(크기 1536과 1024)를 통과했습니다(Glorot et al., 2011).
출력 레이어는 간단한 소프트맥스 레이어였습니다(식 (2) 참조).
모든 가중치 매개변수는 -0.01과 0.01 사이에서 균등하게 초기화되었으며, 모델은 검증 perplexity가 10 에포크 동안 개선되지 않을 때까지 훈련되었습니다.
훈련 후, 언어 모델의 perplexity는 45.80이었습니다.
검증 세트는 말뭉치의 0.1%를 임의로 선택한 것이었습니다.
모델은 디코딩 과정에서 부분 번역을 점수화하는 데 사용되었으며, 이는 일반적으로 n-best 목록 재점수화보다 BLEU 점수에서 더 큰 이득을 가져옵니다(Vaswani et al., 2013).
디코더에서 CSLM을 사용하는 계산 복잡성을 해결하기 위해 버퍼를 사용하여 디코더가 수행하는 스택 탐색 중에 n-gram을 모으도록 했습니다.
버퍼가 가득 차거나 스택이 제거될 때만 CSLM에 의해 n-gram이 점수화됩니다.
이를 통해 Theano(Bergstra et al., 2010; Bastien et al., 2012)를 사용하여 GPU에서 빠른 행렬-행렬 곱셈을 수행할 수 있습니다.

−60 −50 −40 −30 −20 −10 0 −14
−12
−10
−8 −6
−4
−2
0

마이너스 60 마이너스 50 마이너스 40 마이너스 30 마이너스 20 마이너스 10 0 마이너스 14
마이너스 12
마이너스 10
마이너스 8 마이너스 6
마이너스 4
마이너스 2
0

RNN 점수 (로그)
T
M S 점수 (로그)

그림 3: RNN 인코더-디코더 및 번역 모델에 따른 점수 (로그 확률)에 따른 구문 쌍의 시각화.

4.2 양적 분석

우리는 다음과 같은 조합을 시도해 보았다:

1. 기준 구성
2. 기준 + RNN
3. 기준 + CSLM + RNN
4. 기준 + CSLM + RNN + 단어 벌칙
소스       번역 모델        RNN 인코더-디코더
끝에 [a la fin de la] [´r la fin des ann´ ees] [ˆ etre sup-

prim´ es ` a la fin de la] -> 끝에 프라임이 있습니다.

`마지막에`

처음으로 [r c (cid:13) pour la premir¨ ere fois] [´ et´ e donn´ es pour

처음으로 기념되었습니다.

처음으로
처음으로,
처음으로,

미국에서
그리고

미국에서 발생한 것들과 미국에 개방된 것들, 그리고 미국에서 확인된 것들입니다.

미국에서와 미국의, 미국과, 그리고 미국의
또한, 그리고, 또한, 그리고, 그리고, 그리고
가장 중요한 중 하나로 간주되는

그 중 하나로

[l'un des] [le] [un des]

(a) 긴, 자주 사용되는 출처 구문

, 통신 및 운송 대통령

[통신 및 교통 비서:] [통신 및 교통 비서]

[통신 및 운송 분야 비서] [통신 및 운송 분야 비서 :]
따르지 않았다
the

[의류, 일치하지 않았다]
[언급된 것은 일치하지 않았다]
[제시된 것들은 일치하지 않았다]

[n'ont pas respecté les] [n'était pas conforme aux] [n'ont pas respecté la]

세계의 일부 지역. [세계의 일부 지역.] [세계의 지역들은 다양합니다.]

시드리스. 세계의 지역으로 간주되는 것.

세계의 파티들.
세계의 파티들.
일부 세계의 파티들.
지난 며칠간.
작은 텍스트.
최근 몇 일 동안의 진행.

최근 몇 일 동안.

[이번 몇 일 동안.] [지난 몇 일 동안.] [지난 몇 일 동안의 수업.] 
금요일과 토요일에.

금요일과 토요일에 진행됩니다.
금요일과 토요일에 진행됩니다.

(levendredietlesamedi) 금요일과 토요일
(levendredietsamedi) 금요일과 토요일
(vendredi et samedi) 금요일과 토요일
(b) 긴, 드문 출처 구문

표 2: 번역 모델(직접 번역 확률)과 RNN 인코더-디코더에 따른 소스 구문의 작은 집합에 대한 최고 점수를 받은 대상 구문. 소스 구문은 4개 이상의 단어로 이루어진 구문 중에서 무작위로 선택되었습니다. ?는 불완전한(부분적인) 문자를 나타냅니다. r은 키릴 문자 ghe입니다.

결과는 표 1에 제시되었습니다. 예상대로, 신경망으로 계산된 특징을 추가하는 것은 기준 성능보다 일관적으로 성능을 향상시킵니다.
CSLM과 RNN Encoder-Decoder의 구문 점수를 모두 사용했을 때 가장 우수한 성능을 얻었습니다. 이는 CSLM과 RNN Encoder-Decoder의 기여가 크게 상관되지 않으며, 각 방법을 독립적으로 개선함으로써 더 나은 결과를 기대할 수 있다는 것을 시사합니다. 또한, 신경망에 알려지지 않은 단어의 수를 벌점으로 적용해 보았습니다 (즉, 단어 목록에 없는 단어). 이를 위해 식 (9)의 로그-선형 모델에 알려지지 않은 단어의 수를 추가 기능으로 간단히 추가합니다. 그러나 이 경우에는 성능이 향상되지 않았습니다.

3. 벌점의 효과를 이해하기 위해, 15,000개의 큰 단어 목록 SL에 있는 모든 단어들의 집합을 고려해보십시오. 모든 단어 xi / ∈ SL은 신경망에 점수를 매기기 전에 특수 토큰 [UNK]로 대체됩니다. 따라서, 어떤 xi t / ∈ SL의 조건부 확률은 실제로 모델에 의해 주어집니다.

p(xt = [UNK] | x<t) = p(xt ∈ SL | x<t)
= (cid:88)
xj t/
∈SLp(cid:16) xj t | x<t(cid:17) ≥ p(cid:16) xi t | x<t(cid:17) ,

x<t는 xt−1,...,x1을 나타내는 약식 표기입니다.

테스트 세트에서는 더 좋은 성능을 달성하지 못했지만, 개발 세트에서만 성능이 향상되었습니다.

4.3 질적 분석

성능 향상이 어디서 오는지 이해하기 위해, 우리는 RNN Encoder-Decoder에 의해 계산된 구문 쌍 점수를 번역 모델의 대응하는 p(f | e)와 비교하여 분석합니다. 기존의 번역 모델은 말뭉치의 구문 쌍 통계에만 의존하므로, 우리는 그 점수가 빈번한 구문에 대해서는 더 정확하게 추정되지만 드문 구문에 대해서는 잘못 추정될 것으로 예상합니다. 또한, 우리는 이전에 3.1절에서 언급한 것처럼, 어떠한 빈도 정보도 없이 훈련된 RNN Encoder-Decoder가 구문 쌍의 통계적 발생 빈도보다는 언어적 규칙에 기반하여 구문 쌍에 점수를 매길 것으로 예상합니다.
우리는 소스 구문이 긴 (소스 구문 당 3개 이상의 단어) 그 구문 쌍에 초점을 맞춥니다.

결과적으로, 단축 목록에 없는 단어의 확률은 항상 과대평가됩니다. 이 문제를 해결하기 위해 기존의 단축 목록에 없는 단어를 포함하는 기존 모델로 되돌아갈 수 있습니다 (참조: (Schwenk, 2007)). 그러나 본 논문에서는 대신 단어 패널티를 도입하여 단어 확률의 과대평가를 상쇄시키기로 선택합니다.
소스         RNN 인코더-디코더에서의 샘플
끝에서 [` a la fin de la] (×11)
처음으로 [pour la premi` ere fois] (×24) [pour la premi` ere fois que] (×2)
미국에서 [aux ´ Etats-Unis et] (×6) [dans les ´ Etats-Unis et] (×4)
, 그리고   [, ainsi que] [,] [ainsi que] [, ainsi qu’] [et UNK]
가장 [l’ un des plus] (×9) [l’ un des] (×5) [l’ une des plus] (×2)

(a) 긴, 자주 사용되는 출처 구문

소스: RNN 인코더-디코더에서의 샘플
, 통신 및 운송 장관

[ , 통신 및 운송 장관] (×13)

[n' a pas respecté les] [n' a pas respecté les] (×2) [n' a pas respecté la] (×3)
[parties du monde.] (×11) [des parties du monde.] (×7)
[ces derniers jours.] (×5) [les derniers jours.] (×5) [ces derniers jours.] (×2)
[vendredi et samedi] (×5) [le vendredi et samedi] (×7) [le vendredi et le samedi] (×4)

(b) 긴, 드문 출처 구문

테이블 3: RNN 인코더-디코더에서 생성된 샘플들. 테이블 2에서 사용된 각 소스 구문에 대해 상위 5개의 대상 구문을 보여줍니다. 이들은 RNN 인코더-디코더 점수에 따라 정렬되었습니다.

그림 4: 학습된 단어 표현의 2차원 임베딩. 왼쪽은 전체 임베딩 공간을 보여주고, 오른쪽은 한 영역의 확대된 보기를 보여줍니다 (색상으로 구분됨). 더 많은 그래프는 부록 자료를 참조하세요.

자주. 이러한 소스 구문마다, 우리는 번역 확률 p(f | e) 또는 RNN 인코더-디코더에 의해 높은 점수를 받은 대상 구문을 살펴봅니다. 마찬가지로, 코퍼스에서 소스 구문이 길지만 드문 경우에도 동일한 절차를 수행합니다.

표 2는 번역 모델 또는 RNN 인코더-디코더에 의해 선호되는 소스 구문별 상위 3개의 대상 구문을 나열합니다. 소스 구문은 4개 이상 또는 5개 이상의 단어를 가진 긴 구문 중에서 무작위로 선택되었습니다.

대부분의 경우, RNN 인코더-디코더에 의한 대상 구문의 선택은 실제 또는 문자 그대로의 번역에 더 가깝습니다. 우리는 RNN 인코더-디코더가 일반적으로 더 짧은 구문을 선호하는 것을 관찰할 수 있습니다.

흥미롭게도, 많은 구문 쌍들이 번역 모델과 RNN 인코더-디코더에 의해 유사하게 점수가 매겨졌지만, 같은 수의

다른 문장 쌍들은 극단적으로 다른 점수를 받았다 (그림 3 참조). 이는 RNN 인코더-디코더를 고유한 문장 쌍 집합으로 훈련시키는 제안된 접근 방식에서 비롯될 수 있다. 이는 이전에 설명한대로 말뭉치에서 문장 쌍의 빈도만을 학습하는 것을 방지한다.

뿐만 아니라 표 3에서는 표 2의 각 원문 구문에 대해 RNN 인코더-디코더에서 생성된 샘플을 보여줍니다. 각 원문 구문에 대해 50개의 샘플을 생성하고 점수에 따라 상위 다섯 개의 구문을 보여줍니다. 우리는 RNN 인코더-디코더가 실제 구문 테이블을 보지 않고도 형식에 맞는 목표 구문을 제안할 수 있다는 것을 알 수 있습니다. 더 중요한 것은 생성된 구문이 구문 테이블의 목표 구문과 완전히 겹치지 않는다는 것입니다. 이는 우리에게 구문 테이블 전체 또는 일부를 대체할 가능성을 더 탐구하도록 독려합니다. 그림 5: 학습된 구문 표현의 2차원 임베딩. 왼쪽 상단은 전체 표현 공간을 보여주며 (5000개의 임의로 선택된 점), 다른 세 개의 그림은 특정 영역의 확대된 보기를 색상으로 표시합니다.

제안된 RNN 인코더-디코더와 함께 미래에.

4.4 단어와 구문 표현

제안된 RNN Encoder-Decoder는 기계 번역 작업에만 특별히 설계된 것은 아니므로, 여기서는 훈련된 모델의 특성을 간단히 살펴봅니다.
신경망을 사용한 연속 공간 언어 모델은 의미 있는 임베딩을 학습할 수 있다는 사실은 오랫동안 알려져 왔습니다. (예: Bengio et al., 2003; Mikolov et al., 2013). 제안된 RNN Encoder-Decoder도 단어 시퀀스를 연속 공간 벡터로 투영하고 매핑하기 때문에, 제안된 모델에서도 유사한 특성을 기대할 수 있습니다.
그림 4의 왼쪽 그래프는 RNN Encoder-Decoder가 학습한 단어 임베딩 행렬을 사용하여 단어의 2차원 임베딩을 보여줍니다. 최근 제안된 Barnes-Hut-SNE (van der Maaten, 2013)에 의해 이루어진 투영입니다. 우리는 의미적으로 유사한 단어가 군집되어 있는 것을 명확히 볼 수 있습니다.

서로 (그림 4의 확대된 플롯을 참조하십시오).
제안된 RNN 인코더-디코더는 자연스럽게 구문의 연속 공간 표현을 생성합니다. 이 경우 표현 (그림 1의 c)은 1000차원 벡터입니다. 단어 표현과 마찬가지로, 우리는 4개 이상의 단어로 구성된 구문의 표현을 Barnes-Hut-SNE을 사용하여 시각화합니다 (그림 5).
시각화 결과, RNN 인코더-디코더는 구문 및 의미 구조를 모두 포착합니다. 예를 들어, 왼쪽 아래 플롯에서 대부분의 구문은 시간의 지속에 관한 것이며, 구문적으로 유사한 구문은 함께 클러스터링됩니다. 오른쪽 아래 플롯은 의미적으로 유사한 구문 (국가 또는 지역)의 클러스터를 보여줍니다. 반면, 오른쪽 위 플롯은 구문적으로 유사한 구문을 보여줍니다.

5 결론

이 논문에서는 RNN Encoder-Decoder라는 새로운 신경망 구조를 제안하였다. 이 구조는 임의의 길이의 시퀀스를 다른 집합의 임의의 길이의 시퀀스로 매핑하는 것을 학습할 수 있다. 제안된 RNN Encoder-Decoder는 조건부 확률로 시퀀스 쌍을 평가하거나 소스 시퀀스가 주어졌을 때 대상 시퀀스를 생성할 수 있다. 새로운 구조와 함께, 우리는 시퀀스를 읽거나 생성하는 동안 각 은닉 유닛이 얼마나 기억하거나 잊을지를 조절하는 리셋 게이트와 업데이트 게이트를 포함한 새로운 은닉 유닛을 제안하였다.

우리는 RNN Encoder-Decoder를 사용하여 구문 테이블의 각 구문 쌍을 평가하는 통계 기계 번역 작업으로 제안된 모델을 평가하였다. 질적으로, 우리는 새로운 모델이 구문 쌍의 언어적 규칙성을 잘 포착할 수 있으며, RNN Encoder-Decoder가 잘 형성된 대상 구문을 제안할 수 있다는 것을 보였다.

RNN Encoder-Decoder에 의한 점수는 BLEU 점수를 기준으로 전체 번역 성능을 향상시킨 것으로 나타났다. 또한, RNN Encoder-Decoder의 기여는 기존의 신경망을 사용한 SMT 시스템의 접근과는 상호 독립적이어서, RNN Encoder-Decoder와 신경망 언어 모델을 함께 사용하여 성능을 더욱 향상시킬 수 있다는 것을 발견하였다.

훈련된 모델의 질적 분석 결과, 이 모델은 단어 수준 및 구문 수준에서 언어적 규칙성을 잘 포착한다는 것을 보여준다. 이는 제안된 RNN Encoder-Decoder가 더 많은 자연어 관련 응용 프로그램에 유익할 수 있다는 것을 시사한다.

제안된 구조는 더욱 발전하고 분석할 수 있는 큰 잠재력을 가지고 있다. 여기서 조사되지 않은 한 가지 접근 방법은 RNN Encoder-Decoder가 대상 구문을 제안하도록 구문 테이블 전체 또는 일부를 대체하는 것이다. 또한, 제안된 모델이 쓰여진 언어에만 사용되는 것이 아니라는 점을 고려하면, 제안된 구조를 음성 전사와 같은 다른 응용 프로그램에 적용하는 것이 중요한 미래 연구가 될 것이다.

감사의 말씀

KC, BM, CG, DB, 그리고 YB는 감사의 말씀을 전하고 싶습니다.
NSERC, Calcul Québec, Compute Canada, Canada Research Chairs, 그리고 CIFAR에게 감사의 말씀을 전합니다. 
FB와 HS는 일부분은 유럽 위원회로부터 자금 지원을 받았습니다.

MateCat 프로젝트와 DARPA의 BOLT 프로젝트에서 시온이 개발되었습니다.

참고문헌

[Auli et al.2013] 마이클 아울리, 미셸 갤리, 크리스 퀴르크, 그리고 제프리 즈와이그. 2013. 순환 신경망을 이용한 공동 언어 및 번역 모델링. 자연어 처리에 대한 ACL 컨퍼런스 논문집(EMNLP)에서 발표된 논문, 1044-1054쪽.

[Axelrod et al.2011] Amittai Axelrod, Xiaodong He, 그리고 Jianfeng Gao. 2011. 유사한 도메인 데이터 선택을 통한 도메인 적응. 자연어 처리에 대한 ACL 컨퍼런스의 EMNLP 논문집에서 발표됨, 355-362쪽.

[Bastien et al.2012] 프레데릭 바스티앙, 파스칼 람블랭,
라즈반 패스카누, 제임스 베르그스트라, 이안 J. 굿펠로우,
아르노 베르제롱, 니콜라 부샤르, 그리고 요슈아
벵지오. 2012. Theano: 새로운 기능과 속도 개선.
딥러닝과 비지도 특징 학습 NIPS 2012 워크샵.

[Bengio et al.2003] 요슈아 벵지오, 레장 뒤샤르무, 파스칼 빈센트, 그리고 크리스찬 잔뱅. 2003. 신경 확률 언어 모델. J. Mach. Learn. Res., 3:1137–1155, 3월.

[Bengio et al.2013] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. 2013. 순환 신경망 최적화에 대한 발전. 2013년 38회 국제 음향, 음성 및 신호 처리 학회 논문집(ICASSP 2013)에서 발표. 5월.

[Bergstra et al.2010] 제임스 베르스트라, 올리비에 브로루, 프레데릭 바스티앙, 파스칼 람블랭, 라즈반 패스카누, 기욤 데자르당, 조셉 투리안, 데이비드 워드-팔리, 그리고 요슈아 벵지오. 2010년. Theano: CPU와 GPU 수학 표현 컴파일러. Python for Scientific Computing Conference (SciPy) 논문집, 6월. 구두 발표.

[Chandar et al.2014] 사라트 찬다르, 스타니슬라스 라울리, 휴고 라로셀, 미테시 카프라, 발라라만 라빈드란, 비카스 레이카르, 그리고 암리타 사하. 2014. 이중 언어 단어 표현을 학습하기 위한 오토인코더 접근법. arXiv:1402.1454 [cs.CL], 2월.

[Dahl et al.2012] 조지 E. 달, 동 유, 리 덩,
그리고 알렉스 아세로. 2012. 대용량 어휘 음성 인식을 위한 문맥 의존 사전 훈련된 심층 신경망. IEEE 음향, 음성 및 언어 처리 트랜잭션, 20(1):33–42.

[Devlin et al.2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, , and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proceedings of the ACL 2014 Conference, ACL '14, pages 1370–1380. [Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau Yih, and Li Deng. 2013. Learning semantic representations for the phrase translation model. Technical report, Microsoft Research.

[Devlin et al.2014] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, , 그리고 John Makhoul. 2014. 통계 기계 번역을 위한 빠르고 견고한 신경망 합성 모델. ACL 2014 컨퍼런스 논문집, ACL '14, 페이지 1370-1380. [Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau Yih, 그리고 Li Deng. 2013. 구문 번역 모델을 위한 의미적 표현 학습. 기술 보고서, Microsoft Research.

[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben-
gio. 2011년. 깊은 희소 정류기 신경망. AISTATS’2011에서.

[굿펠로우 외, 2013] 이안 J. 굿펠로우, 데이비드 워드-팔리, 메디 미르자, 아론 쿠르빌, 그리고 요슈아 벤지오. 2013. 맥스아웃 네트워크. ICML'2013에서.

[Graves2012] 알렉스 그레이브스. 2012. 순환 신경망을 이용한 지도 시퀀스 레이블링. 계산 지능 연구. 스프링거.

[Hochreiter and Schmidhuber1997] S. Hochreiter와 J. Schmidhuber. 1997. 장기 단기 기억. 신경 계산, 9(8):1735–1780.

[Kalchbrenner and Blunsom2013] 날 칼크브레너와 필 블런솜. 2013년. 두 개의 반복적인 연속 번역 모델. ACL 자연어 처리 기법에 대한 ACL 컨퍼런스 논문집 (EMNLP)에서 발표된 논문, 1700-1709쪽.

[Koehn et al.2003] Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. 통계적 구문 기반 번역. 2003년 북미 협회 컨퍼런스 논문집, NAACL '03, 48-54쪽.

[Koehn2005] P. Koehn. 2005. Europarl: 통계 기계 번역을 위한 병렬 코퍼스. 기계 번역 정상회의 X, 79-86쪽, 태국 푸켓.

[Krizhevsky et al.2012] 알렉스 크리즈헤브스키, 일리야 수츠케버, 그리고 제프리 힌튼. 2012. 딥 컨볼루션 신경망을 이용한 이미지넷 분류. Advances in Neural Information Processing Systems 25 (NIPS’2012)에서.

[Marcu and Wong2002] 다니엘 마르쿠와 윌리엄 웡. 2002. 통계 기계 번역을 위한 구문 기반, 결합 확률 모델. ACL-02 자연어 처리에 대한 경험적 방법 컨퍼런스 - 10권, EMNLP '02, 페이지 133-139.

[Mikolov et al.2013] 토마스 미콜로프, 일리야 숫스케버, 카이 첸, 그레그 코라도, 그리고 제프 딘. 2013. 단어와 구의 분산 표현과 그들의 조합성. 신경 정보 처리 시스템 26에서의 진보, 페이지 3111-3119.

[Moore and Lewis2010] 로버트 C. 무어와 윌리엄 루이스. 2010. 언어 모델 훈련 데이터의 지능적 선택. ACL 2010 컨퍼런스 단문 논문집에서 발표. ACLShort '10, 220-224쪽, Stroudsburg, PA, 미국.

[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. 2014. 깊은 순환 신경망을 구성하는 방법. 제2회 국제 학습 표현 대회 논문집 (ICLR 2014) 발표문. 4월.

[Saxe et al.2014] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. 2014. Deep linear neural networks에서 학습의 비선형 역학에 대한 정확한 해결책. 제2회 국제 학습 표현 대회 (ICLR 2014) 논문집, 4월.

[Schwenk et al.2006] HolgerSchwenk, MartaR.Costa-Juss` a, and Jos´ e A. R. Fonollosa. 2006. IWSLT에서의 연속 공간 언어 모델. IWSLT, 166-173쪽.

[Schwenk2007] 홀거 슈벵크. 2007. 연속 공간 언어 모델. 컴퓨터 음성 언어, 21(3):492–518, 7월.

[Schwenk2012] Holger Schwenk. 2012. 연속 공간 번역 모델을 위한 구문 기반 통계 기계 번역. Martin Kay와 Christian Boitet 편집자, 제24회 국제 계산 언어학 컨퍼런스 (COLIN) 논문집, 1071-1080쪽.

[Socher et al.2011] 리처드 소처, 에릭 H. 황, 제프리 펜닝턴, 앤드류 Y. 엔그, 그리고 크리스토퍼 D. 매닝. 2011. 패러프레이즈 감지를 위한 동적 풀링 및 펼침 재귀 오토인코더. Advances in Neural Information Processing Systems 24에서.

[손 등 2012] 류하이 손, 알렉산드르 알로젠, 프랑수아 이본. 2012. 신경망을 이용한 연속 공간 번역 모델. 2012년 북미 협회 컨퍼런스 논문집: 인간언어기술, NAACL HLT '12, 39-48쪽, 스트라우즈버그, 미국.

[반 더 마튼2013] 로렌스 반 더 마튼. 2013.
반스-허트-SNE. 제1회 국제 학습 표현 대회 논문집
(ICLR 2013)에서 발표됨, 5월.

[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. 대규모 신경 언어 모델을 사용한 디코딩은 번역을 개선시킵니다. 자연어 처리에 대한 경험적인 방법에 관한 회의 논문, 페이지 1387-1392.

[Zeiler2012] 매튜 D. 제일러. 2012. ADADELTA: 적응형 학습률 방법. 기술 보고서, arXiv 1212.5701.

[Zou et al.2013] 윌 Y. 조우, 리처드 소처, 다니엘 M. 서, 그리고 크리스토퍼 D. 매닝.
2013년. 구문 기반 기계 번역을 위한 이중 언어 단어 임베딩. ACL 자연어 처리 기법에 대한 EMNLP 학회 논문집에서 발표됨, 1393-1398쪽.
RNN 인코더-디코더.

이 문서에서는 실험에서 사용된 RNN Encoder-Decoder의 아키텍처에 대해 자세히 설명합니다.
우리는 소스 구문을 X = (x 1,x 2,...,x N)로 표기하고 대상 구문을 Y = (y 1,y 2,...,y M)로 표기합니다. 각 구문은 K 차원의 원-핫 벡터의 시퀀스이며, 벡터의 요소 중 하나만 1이고 나머지는 모두 0입니다. 활성(1) 요소의 인덱스는 벡터로 표현된 단어를 나타냅니다.

A.1 인코더
원문의 각 단어는 500차원 벡터 공간에 포함됩니다: e(x i) ∈ R500. e(x)는 단어를 시각화하기 위해 섹션 4.4에서 사용됩니다.
인코더의 숨겨진 상태는 1000개의 숨겨진 유닛으로 구성되며, 각각은 시간 t에서 다음과 같이 계산됩니다.

h_t = z_j * h_(t-1) + (1 - z_j) * ˜ h_t

어디에

안녕하세요
저는 [We(x t)]의 tanh입니다

j
+ (cid:2) U(cid:0) r (cid:12) h (cid:104)t−1(cid:105)(cid:1)(cid:3) j(cid:17) ,
z
j
=σ
(cid:16)
[W ze(x t)]
j
+
(cid:2)
U zh
(cid:104)t−1(cid:105)(cid:3) j(cid:17),

j
+ (cid:2) U(cid:0) r (cid:12) h (cid:104)t−1(cid:105)(cid:1)(cid:3) j(cid:17) ,
z
j
=σ
(cid:16)
[W ze(x t)]
j
+
(cid:2)
U zh
(cid:104)t−1(cid:105)(cid:3) j(cid:17),

rj = σ(Wre(xt)j + Urh(t-1)j)

σ와 (cid:12)는 각각 로지스틱 시그모이드 함수와 요소별 곱셈입니다. 식을 깔끔하게 만들기 위해 편향을 생략합니다. 초기 은닉 상태 h(cid:104)0(cid:105)

j는 0으로 고정됩니다.
N 단계(원본 구문의 끝)에서 숨겨진 상태가 계산되면, 원본 구문 c의 표현이 됩니다.

c = 탄젠트(VhNI)

A.1.1 디코더

디코더는 숨겨진 상태를 초기화하여 시작합니다.

h₀ = tanh(V₀c₁)

우리는 디코더의 매개변수와 인코더의 매개변수를 구별하기 위해 ·(cid:48)를 사용할 것입니다.
디코더의 시간 t에서의 숨겨진 상태는 다음과 같이 계산됩니다.

h_t = z_t * h_{t-1} + (1 - z_t) * ˜ h_t

어디에

˜ 안녕하세요
j
=tanh(16)(2) W(y t−1)

j
+ r4
j
은 U4
헷−1
+ Cc3,

zj = σ(θ2Wjzj−1 + θ1y(t−1))

j
+ (cid:2) U(cid:48) zh(cid:48) (cid:104)t−1(cid:105)(cid:3)
j
+ [C zc] j(cid:17) ,
r(cid:48)
j
=σ (cid:16)(cid:2) W(cid:48) re(y t−1)(cid:3)

j
+ (cid:2) U(cid:48) zh(cid:48) (cid:104)t−1(cid:105)(cid:3)
j
+ [C zc] j(cid:17) ,
r(cid:48)
j
=σ (cid:16)(cid:2) W(cid:48) re(y t−1)(cid:3)

j
+ (cid:2) U(cid:48) rh(cid:48) (cid:104)t−1(cid:105)(cid:3)
j
+ [C rc] j(cid:17) ,

j
+ (cid:2) U(cid:48) rh(cid:48) (cid:104)t−1(cid:105)(cid:3)
j
+ [C rc] j(cid:17) ,

그리고 e(y 0)는 모두 0인 벡터입니다. 인코더와 마찬가지로, e(y)는 대상 단어의 임베딩입니다.
소스 구문을 간단히 인코딩하는 것과 달리, 디코더는 대상 구문을 생성하는 방법을 학습합니다. 각 시간 t마다, 디코더는 j번째 단어를 생성할 확률을 계산합니다.

p(yt,j=1 | yt−1,...,y1,X) =

exp
g js
히티
PK
j0=1
exp g j0s
히티,
where the i-element of s

안녕하세요

s(i) = max(s(2i-1), s(2i))

그리고

숫자 = 영 곱하기 영

t-1
+ O cc.

간단히 말해서, s(cid:104)t(cid:105)
i
는 소위 맥스아웃 유닛입니다.
계산 효율성을 위해, 단일 행렬 출력 가중치 G 대신 두 개의 행렬의 곱을 사용합니다.




G l은 RK×500에 속하고, G r은 R500×1000에 속합니다.

B 단어와 구문 표현

여기에서는 그림 4-5에서 단어와 구문 표현의 확대된 플롯을 보여줍니다.
그림 6: 학습된 단어 표현의 2차원 임베딩. 왼쪽 상단은 전체 임베딩 공간을 보여주며, 나머지 세 개의 그림은 특정 영역의 확대된 보기를 보여줍니다 (색상으로 구분).
그림 7: 학습된 구문 표현의 2차원 임베딩. 왼쪽 상단은 전체 표현 공간 (1000개의 임의로 선택된 점)을 보여주며, 나머지 세 개의 그림은 특정 영역의 확대된 보기를 보여줍니다 (색상으로 구분).

