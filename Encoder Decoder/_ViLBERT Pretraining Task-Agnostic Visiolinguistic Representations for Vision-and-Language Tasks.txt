ViLBERT: 비전 및 언어 작업용 과제에 중립적인 사시언어 표현 사전 훈련

초록

우리는 ViLBERT (Vision-and-Language BERT의 약자)라고 불리는 모델을 제시합니다. 이 모델은 이미지 콘텐츠와 자연어의 과제에 중립적인 공동 표현을 학습하는 데 사용됩니다. 우리는 인기 있는 BERT 아키텍처를 멀티모달 이중 스트림 모델로 확장하여 시각적 및 텍스트 입력을 별도의 스트림으로 처리하고 상호 작용하도록 공통 어텐션 트랜스포머 레이어를 사용합니다. 우리는 이 모델을 대규모로 수집된 Conceptual Captions 데이터셋에서 두 가지 프록시 작업을 통해 사전 훈련하고 기본 아키텍처에 작은 수정만을 추가하여 시각적 질문 응답, 시각적 상식 추론, 지칭 표현 및 캡션 기반 이미지 검색과 같은 여러 이미지-언어 작업으로 전이합니다. 기존의 과제별 모델과 비교하여 모든 네 가지 작업에서 최첨단 성능을 달성하며 작업 훈련의 일부로만 시각과 언어 간의 연결을 학습하는 것에서 시각적 연결을 사전 훈련 및 전이 가능한 기능으로 취급하도록 전환하는 작업을 나타냅니다.

... 여름 동안 카메라를 컴퓨터에 연결하고 컴퓨터에게 본 것을 설명하게 하는 것."
1966년 대학생 여름 연구 프로젝트의 목표에 대한 Marvin Minsky의 발언 [1]

이 유명한 야심찬 여름 프로젝트 이후에는 이미지, 비디오 또는 심지어 완전한 3D 환경의 맥락에서 자연어를 생성하거나 응답하여 시각 이해를 시연할 수 있는 시스템으로의 꾸준한 진전이 이루어졌습니다 [2-8]. 이러한 접근법과 해당 작업들은 "비전과 언어"라는 공통 베너 아래로 참조되어 왔습니다. 그러나 자연어와 시각 자극을 조정해야 하는 공통 필요에도 불구하고, 시각과 언어 작업을 위한 시각 기반을 얻기 위한 통합된 기반이 부족합니다. 대신, 지형을 수행하도록 학습하는 주요 전략은 주로 다른 대규모 작업을 위해 사전 훈련된 별도의 언어 및 시각 모델을 사용하여 시작한 다음, 주로 시각-언어 데이터가 제한적이거나 편향된 경우 일반화가 잘되지 않는 막연한 지형을 낳습니다 [9, 10]. 비전과 언어 작업에 대한이 사전 훈련 후 전송 학습 접근법은 대규모 데이터 원본에서 훈련 된 대규모, 공개용 모델의 사용 편리성과 강력한 표현력 덕분에 컴퓨터 비전과 자연어 처리 모두에서 널리 사용되어 왔으므로 자연스럽게 이어진 것입니다 [11-14]. 이러한 도메인에서 사전 훈련 된 모델은 대상 작업에 유용한 정보를 제공 할 수 있으며, 예를 들어 개 품종 민감한 이미지 기능 또는 단어 사이의 적절한 의미적 거리와 같은 것입니다. 이러한 시각적 및 언어적 이해는 물론 "비글"이나 "목자"와 같은 적절한 구문과 관련하지 못한다면 하류 비전-언어 모델이 그것과 관련지지 못한다는 것과 마찬가지로 중요합니다. 따라서 우리는 여러 비전-언어 작업에 널리 적용할 수있는 시각 기반을 사전 훈련하기 위해이 연결을 학습하고 활용할 수있는 공통 모델을 개발하는 데 관심이 있습니다.
이러한 공통 시각 언어 표현을 배우기 위해 우리는 최근의 자기 지도 학습의 성공 사례를 살펴보았습니다. 이러한 자기 지도 작업이라고 불리는 것을 수행하도록 모델을 훈련시켜 대규모, 라벨이 없는 데이터 원본에서 풍부한 의미 및 구조적 정보를 포착했습니다. 이러한 자기 지도 작업은 구조를 활용합니다.
미리보기. 검토 중

그림 1: 우리의 ViLBERT 모델은 시각 (녹색) 및 언어 (보라) 처리를 위한 두 개의 병렬 스트림으로 구성되며, 이러한 스트림은 새로운 공동 어텐션(transformer) 레이어를 통해 상호 작용합니다. 이 구조는 각 모드(모델)에 대한 가변 깊이를 허용하며 공동 어텐션을 통해 희소한 상호 작용이 가능하게 합니다. 곱셈 서브스크립트가 있는 대시된 상자는 레이어 블록의 반복을 나타냅니다.

데이터 내에서 자동으로 지도 학습 작업을 생성하기 위해 구조 내에서 수행하는 작업을 포함한 자동 작업(예: 이미지 색상 칠하기 [20] 또는 텍스트에서 가리킨 단어 재구성 [12])으로부터 풍부한 의미론적 및 구조적 정보를 캡처하기 위한 최근의 성공을 고려해 봅니다. 비전 커뮤니티 내에서의 작업은 약속된 성과를 보여주고 있으며, 최근의 언어 모델인 ELMo [13], BERT [12] 및 GPT [14]와 같은 언어 모델을 통한 자기 지도 학습의 가장 큰 영향은 많은 자연어 처리 (NLP) 작업에 대한 새로운 최고 성과 기록을 세운 것입니다. 비슷한 접근 방식을 통해 시각적인 그라운딩(연결)을 학습하기 위해 시각과 언어 간의 정렬이 가능한 적절한 데이터 원천을 식별해야 합니다. 본 연구에서는 웹의 alt-text를 통해 자동으로 수집된 약하게 연관된 설명적 캡션을 가진 약 330만 개의 이미지로 구성된 Conceptual Captions [24] 데이터셋을 고려합니다.

저희는 이를 "Vision & Language BERT" (ViLBERT)라고 부르며, 짝을 이룬 시각 언어 데이터에서 작업에 중립적인 시각적 그라운딩을 학습하기 위한 공통 모델을 제안합니다. 저희 방법은 최근 개발된 BERT [12] 언어 모델을 텍스트와 이미지를 함께 고려할 수 있도록 확장합니다. 핵심 기술적 혁신은 시각 및 언어 처리를 위한 별도의 스트림을 소개하고, 이러한 스트림이 공동 어텐션(transformer) 레이어를 통해 통신하도록 하는 것입니다. 이 구조는 각 모드(모델)의 다양한 처리 요구 사항을 수용할 수 있으며 다양한 표현 깊이에서 모드 간의 상호 작용을 제공합니다. 저희는 이 구조가 실험에서 단일 스트림 통합 모델보다 우수한 성능을 발휘함을 보입니다. [12]의 교육 작업과 유사하게, 저희 모델을 Conceptual Captions 데이터에서 두 가지 프록시 작업으로 교육합니다. 즉, 가리진 단어와 이미지 영역의 의미를 예측하고 가리지 않은 입력을 제공하며 이미지와 텍스트 세그먼트가 해당하는지 예측합니다. 저희는 사전 교육된 모델을 네 가지 이미지와 언어 작업 - 시각적 질문 응답 [3], 시각적 상식 추론 [25], 참조 표현 [2] 및 캡션 기반 이미지 검색 [26] - 에 기본 모델로 적용하며, 이 네 가지 작업 모두에서 최고 성능을 달성합니다. 저희는 개별로 사전 교육된 비전 및 언어 모델을 사용하는 최첨단 작업별 기준과 비교했을 때 이러한 작업들 각각에서 2에서 10%까지 성능 향상을 찾아냈습니다. 또한, 저희 구조는 이러한 작업 각각에 대한 수정이 간단하며, 여러 비전 및 언어 작업에서 시각적 그라운딩의 공통 기반이 되는 역할을 합니다.


2 접근 방식
이 섹션에서는 먼저 BERT 언어 모델을 간략하게 요약한 다음 어떻게 시각과 언어 데이터를 공동으로 표현하기 위해 확장하는지 설명합니다.

2.1 준비물: 양방향 인코더 표현 (BERT)
[12]에 의해 소개된 BERT 모델은 어텐션 기반 양방향 언어 모델입니다. 대규모 언어 말뭉치에서 사전 교육될 때 BERT는 여러 자연어 처리 작업으로의 전이 학습에 매우 효과적임이 입증되었습니다.

BERT 모델은 단어 토큰 w0, . . . , wT의 시퀀스에서 작동합니다. 이러한 토큰은 학습된 인코딩에 매핑되고 L "인코더 스타일" 트랜스포머 블록 [27]을 통해 최종 표현 h0, . . . , hT를 생성합니다. H(l)을 l-번째 레이어 이후의 중간 표현에 해당하는 행 h(l)0, . . . , h(l)T를 포함하는 행렬로 정의하겠습니다. [27]에서 발견되는 일부 내부 세부 정보를 추상화하면, 단일 인코더 스타일 트랜스포머 블록의 계산을 Fig. 2a에 나타냈습니다. 이 블록은 다중 헤드 어텐션 블록과 작은 완전 연결 네트워크로 구성되어 있으며 둘 다 잔여 더하기로 래핑됩니다. 중간 표현 H(l)을 사용하여 쿼리, 키 및 값에 해당하는 세 개의 행렬 - Q, K 및 V - 을 계산하여 다중 헤드 어텐션 블록을 구동합니다. 구체적으로, 쿼리와 키 간의 점곱 유사성은 값 벡터에 대한 어텐션 분포를 결정합니다. 결과적으로 가중 평균된 값 벡터는 어텐션 블록의 출력을 형성합니다. 나중에 설명할 것처럼 이러한 쿼리 조건 키-값 어텐션 메커니즘을 수정하여 ViLBERT의 다중 모달 공동 어텐션 트랜스포머 모듈을 개발합니다(Fig. 2b).

Figure 2: 우리는 트랜스포머 아키텍처를 기반으로 한 새로운 공동 어텐션 메커니즘을 소개합니다. 다중 헤드 어텐션에서 키-값 쌍을 교환함으로써, 이 구조는 시각 어텐션된 언어 기능을 시각적 표현에 통합할 수 있게 해줍니다(그 반대도 마찬가지입니다).

텍스트 표현. BERT는 어휘 단어와 몇 개의 특수 토큰(SEP, CLS, MASK)으로 구성된 이산 토큰 시퀀스 위에서 작동합니다. 주어진 토큰에 대한 입력 표현은 토큰별 학습된 임베딩 [28]과 위치 (즉, 시퀀스 내 토큰의 인덱스) 및 세그먼트 (즉, 여러 개의 문장이 있는 경우 토큰의 문장 인덱스)에 대한 인코딩의 합입니다.

훈련 작업 및 목표. BERT 모델은 큰 언어 말뭉치에서 두 가지 작업에 대해 end-to-end로 훈련됩니다: 마스크된 언어 모델링 및 다음 문장 예측.

마스크된 언어 모델링 작업에서 입력 토큰은 마스크된 XM 및 관찰된 XO 토큰에 해당하는 분리된 집합으로 무작위로 나눕니다(대략 15%의 토큰이 마스크됨). 마스크된 토큰은 80%의 경우 특수한 MASK 토큰으로 대체되고, 무작위 단어로 10% 대체되며, 변경되지 않은 채로 10% 그대로 유지됩니다. 그런 다음 BERT 모델은 관찰된 집합을 기반으로 이러한 마스크된 토큰을 재구성하기 위해 훈련됩니다. 구체적으로 각 인덱스 (예: hi)의 최종 표현을 어휘에 대한 분포로 매핑하는 선형 레이어가 학습되며, 모델은 교차 엔트로피 손실 하에서 훈련됩니다.

다음 문장 예측에서 BERT 모델은 포맷 {CLS, wA1, . . . , wAT , SEP, wB1, . . . , wBT , SEP}을 따르는 두 개의 텍스트 세그먼트 A와 B가 전달되고, B가 A를 원본 텍스트에서 따르는지 여부를 예측하도록 훈련됩니다. 구체적으로 최종 표현을 기반으로 하는 CLS 토큰 (즉, hCLS)에 작용하는 선형 레이어가 이 레이블에 대한 이진 크로스 엔트로피 손실을 최소화하도록 훈련됩니다.


2.2 ViLBERT: 이미지와 텍스트를 공동으로 표현하기 위한 BERT 확장
BERT의 언어 모델링에서의 성공을 영감으로 하여, 쌍으로 된 데이터에서 언어와 시각적 콘텐츠를 공동으로 표현하기 위한 유사한 모델과 훈련 작업을 개발하고자 합니다. 구체적으로 정적 이미지와 해당 설명 텍스트를 공동으로 표현하는 것을 고려합니다.
하나의 직관적인 접근 방식은 BERT에 최소한의 변경을 가하는 것입니다. 즉, 시각적 입력 공간을 클러스터링을 통해 이산화하고 이러한 시각적 '토큰'을 텍스트 입력과 완전히 동일하게 다루고 사전 훈련된 BERT 모델에서 시작하는 것입니다. 이 아키텍처는 몇 가지 단점이 있습니다. 첫째, 초기 클러스터링은 이산화 오류를 일으키고 중요한 시각적 세부 정보를 잃을 수 있습니다. 둘째, 두 가지 모드의 입력을 동일하게 처리하며, 이들은 입력 표현의 본질적인 복잡성이나 초기 입력 표현의 수준에 따라 서로 다른 처리 수준이 필요할 수 있음을 무시합니다. 예를 들어, 이미지 영역은 문장 내 단어보다 더 약한 관계를 가질 수 있으며, 시각적 특성은 종종 이미 매우 깊은 네트워크의 출력입니다. 마지막으로, 미리 훈련된 가중치를 추가적인 시각적 '토큰' 집합으로 적응시키려고 하면 학습된 BERT 언어 모델에 손상을 줄 수 있습니다. 대신, 각 모드를 개별적으로 모델링하고 그런 다음 소수의 기반 시각적 특징 간의 어텐션 기반 상호 작용을 통합하는 이중 스트림 아키텍처를 개발합니다. 이 접근 방식은 각 모드에 대해 가변 네트워크 깊이를 허용하며 서로 다른 깊이에서 교차 모달 연결을 가능하게 합니다.
우리가 ViLBERT라고 부르는 모델은 그림 1에 나와 있으며 이미지 영역과 텍스트 세그먼트 위에서 작동하는 두 개의 병렬 BERT 스타일 모델로 구성됩니다. 각 스트림은 변형기 블록 (TRM)과 새로운 공동 어텐션 변형기 레이어 (Co-TRM)의 연속으로 이루어져 있으며, 이를 통해 모드 간의 정보 교환을 가능하게 합니다. 이미지 I는 지역 특성 v1, . . . , vT로 표현되고 텍스트 입력은 w0, . . . , wT입니다. 우리 모델은 최종 표현 hv0, . . . , hvT 및 hw0, . . . , hwT를 출력합니다. 두 스트림 간의 교환은 특정 레이어 사이에서만 제한되며 텍스트 스트림은 시각적 특성과 상호 작용하기 전에 훨씬 더 많은 처리를 하게 되는데, 이는 선택한 시각적 특성이 이미 상당히 고수준이며 단어와 비교하여 제한된 컨텍스트 집계가 필요함을 반영하고 있습니다.

그림 3: ViLBERT를 Conceptual Captions [24] 데이터셋에서 시각적 기반을 학습하기 위해 두 가지 훈련 작업으로 훈련합니다. 마스크된 다중 모달 학습에서 모델은 관측된 입력을 기반으로 마스크된 입력에 대한 이미지 영역 범주 또는 단어를 재구성해야 합니다. 다중 모달 정렬 예측에서 모델은 캡션이 이미지 내용을 설명하는지 여부를 예측해야 합니다. 이러한 스트림 간의 교환은 특정 레이어에서만 제한되며 텍스트 스트림은 시각적 특성과 상호 작용하기 전에 훨씬 더 많은 처리를 하게 되는데, 이는 선택한 시각적 특성이 이미 상당히 고수준이며 단어와 비교하여 제한된 컨텍스트 집계가 필요함을 반영하고 있습니다.

Co-Attentional Transformer Layers. 그림 2b에 나와 있는 co-attentional transformer layer를 소개합니다. 중간 시각적 및 언어적 표현인 
H(i)V와 H(j)W가 주어지면 이 모듈은 일반적인 transformer 블록과 마찬가지로 쿼리, 키 및 값 행렬을 계산합니다. 그러나 각 모달리티의 키와 값은 다른 모달리티의 다중 헤드 어텐션 블록에 입력으로 전달됩니다. 따라서 어텐션 블록은 각 모달리티에 대해 다른 모달리티에 의존한 어텐션 풀링된 피처를 생성하게 되며, 실제로는 시각적 스트림에서 언어에 의존한 어텐션을 수행하고 언어 스트림에서 시각에 의존한 어텐션을 수행합니다. 후자는 시각과 언어 모델에서 발견되는 공통 어텐션 메커니즘을 모방합니다 [30]. 이후의 transformer 블록은 이전과 마찬가지로 진행되며 초기 표현과의 잔여 추가를 포함하여 다중 모달 피처를 생성합니다.
일반적으로 시각 및 언어에 대한 공동 어텐션은 새로운 개념이 아닙니다([31]에서 처음 제안되었습니다) 그리고 동시에 진행 중인 연구([32, 33])는 시각적 질문 응답 [3] 작업에서 유사한 공동 어텐션 transformer 구조의 효과를 보여주었습니다.

이미지 표현. 이미지 영역 피처를 생성하기 위해 사전 훈련된 객체 감지 네트워크에서 바운딩 박스와 시각적 특성을 추출합니다(3.1절 참조). 텍스트와 달리 이미지 영역은 자연스러운 순서가 없습니다. 대신 공간적 위치를 인코딩하여 영역 위치(정규화된 좌상단 및 우하단 좌표)와 이미지 영역에서 차지하는 이미지 영역의 비율로 구성된 5차원 벡터를 만듭니다. 이 벡터는 시각적 특성의 차원과 일치하도록 투영되고 더해집니다. 이미지 영역 시퀀스의 시작은 전체 이미지를 나타내는 IMG 토큰으로 표시됩니다(즉, 공간 인코딩과 일치하는 평균 풀링된 시각적 특성을 나타냄).

훈련 작업 및 목표. 이전 섹션에서 설명한 것과 유사하게 두 가지 사전 훈련 작업을 고려합니다: 마스크된 다중 모달 모델링 및 다중 모달 정렬 예측.
마스크된 다중 모달 모델링 작업(그림 3a에 표시)은 표준 BERT의 마스크 언어 모델링 작업에서 비롯됩니다. 입력 토큰의 약 15%가 마스크로 나누어지고 남은 입력을 기반으로 마스크된 토큰을 재구성하는 모델을 작업합니다. 마스크된 이미지 영역은 90%의 경우 이미지 특성을 제로로 만들고 10%의 경우 변경되지 않습니다. 마스크된 텍스트 입력은 BERT에서와 같이 처리됩니다. 마스크된 기능 값에 대한 직접적인 회귀 대신 모델은 해당 이미지 영역에 대한 의미적 클래스의 분포를 예측합니다. 이를 감독하기 위해 이미지 영역에 대한 출력 분포를 기능 추출에 사용된 동일한 사전 훈련된 감지 모델에서 가져옵니다. 이 두 분포 간의 KL 발산을 최소화하도록 모델을 훈련합니다. 이 선택은 언어가 시각적 콘텐츠의 고수준 의미만 식별하고 정확한 이미지 특성을 재구성하기 어려울 가능성이 높기 때문입니다. 또한 회귀 손실을 적용하면 이미지와 텍스트 입력에 의해 발생하는 손실을 균형 잡기 어려울 수 있습니다.
다중 모달 정렬 작업(그림 3b에 표시)에서 모델은 {IMG, v1, . . . , vT , CLS, w1, . . . , wT , SEP}와 같은 형식의 이미지-텍스트 쌍을 제공받고 이미지와 텍스트가 정렬되었는지 여부, 즉 텍스트가 이미지를 설명하는지 여부를 예측해야 합니다. 우리는 텍스트와 이미지의 종합적인 표현을 hIMG와 hCLS의 요소별 곱으로 계산하고 이미지와 텍스트가 정렬되었는지 여부를 이진 분류 예측을 수행하기 위해 선형 레이어


3 실험 설정
이 섹션에서는 모델을 어떻게 훈련하고 훈련된 모델을 전송하는 시각 및 언어 작업에 대한 개요를 제공합니다.
3.1 ViLBERT 훈련
전체 ViLBERT 모델을 훈련하기 위해 우리는 Conceptual Captions 데이터셋 [24]에 제시된 훈련 작업을 적용합니다. Conceptual Captions는 alt-text 활성화된 웹 이미지에서 자동으로 가져온 330만 개의 이미지 캡션 쌍입니다. 자동 수집 및 정리 과정에서 약간의 노이즈가 남아 있으며 '캡션'은 때로는 인간과 같지 않거나 세부 정보가 부족한 경우도 있습니다(예: "배우들이 축제에서 프리미어에 참석"). 그러나 이 데이터셋은 시각적 콘텐츠의 많은 다양성을 제공하며 우리의 목적에 탁월한 데이터셋으로 기능합니다. 일부 링크는 데이터 다운로드 시간에 깨져 버린 경우도 있으므로 모델은 약 310만 개의 이미지 캡션 쌍으로 훈련됩니다.
구현 세부 정보. ViLBERT 모델의 언어 스트림을 초기화하기 위해 BookCorpus [17]와 영어 위키피디아에서 사전 훈련된 BERT 언어 모델을 사용합니다. 구체적으로 우리는 12개의 transformer 블록 레이어와 각 블록이 762 크기의 숨겨진 상태와 12개의 어텐션 헤드를 갖는 BERTBASE 모델 [12]을 사용합니다. 훈련 시간에 대한 우려로 인해 BASE 모델을 선택했지만 더 강력한 BERTLARGE 모델이 성능을 더 향상시킬 수 있을 것으로 생각됩니다. 시각 스트림에서 Transformer 및 공동 어텐션 트랜스포머 블록은 1024 크기의 숨겨진 상태와 8 개의 어텐션 헤드를 갖습니다.
우리는 총 배치 크기가 512 인 8 대의 TitanX GPU에서 10 에포크 동안 훈련합니다. 초기 학습률이 1e-4 인 Adam 옵티마이저를 사용합니다. 모델을 훈련하기 위해 웜 업과 함께 선형 감쇠 학습률 일정을 사용합니다. 두 훈련 작업 손실은 동일하게 가중됩니다.

3 실험 설정
이 섹션에서는 모델을 어떻게 훈련하고 훈련된 모델을 전송하는 시각과 언어 작업에 대한 개요를 제공합니다.
3.1 ViLBERT 훈련
전체 ViLBERT 모델을 훈련하기 위해 Conceptual Captions 데이터셋 [24]에서 제시된 훈련 작업을 적용합니다. Conceptual Captions는 alt-text 활성화된 웹 이미지에서 자동으로 수집된 330만 개의 이미지 캡션 쌍으로 구성되어 있습니다. 자동 수집 및 정리 과정에서 약간의 노이즈가 발생하며 '캡션'은 때로는 인간과 같지 않거나 세부 내용이 부족한 경우도 있습니다(예: "배우들이 축제에서 프리미어에 참석"). 그러나 이 데이터셋은 시각적 콘텐츠의 다양성을 제공하며 우리의 목적에 매우 적합한 데이터셋입니다. 데이터 다운로드 시점에서 일부 링크가 손상되었기 때문에 모델은 약 310만 개의 이미지 캡션 쌍을 사용하여 훈련되었습니다.
구현 세부 사항. ViLBERT 모델의 언어 스트림은 BookCorpus [17]와 영어 위키피디아에서 사전 훈련된 BERT 언어 모델을 초기화합니다. 구체적으로는 12개의 transformer 블록 레이어와 각 블록이 762 크기의 숨겨진 상태와 12개의 어텐션 헤드를 가진 BERTBASE 모델 [12]을 사용합니다. 훈련 시간에 대한 우려로 인해 BASE 모델을 선택했지만 더 강력한 BERTLARGE 모델이 성능을 더 향상시킬 수 있을 것으로 판단됩니다. 시각 스트림에서 Transformer 및 공동 어텐션 트랜스포머 블록은 1024 크기의 숨겨진 상태와 8개의 어텐션 헤드를 가지고 있습니다.
우리는 총 8개의 TitanX GPU에서 총 배치 크기가 512인 상태에서 10 에포크 동안 훈련합니다. 초기 학습률은 1e-4이며, 모델을 훈련하기 위해 웜 업 및 선형 감쇠 학습률 스케줄을 사용합니다. 두 훈련 작업 손실은 동일하게 가중됩니다.


3.2 시각 및 언어 전이 작업

저희는 미리 훈련된 ViLBERT 모델을 4개의 확립된 시각 및 언어 작업과 하나의 진단 작업에 전송합니다(그림 4에서 예제 확인 가능). 우리는 미리 훈련된 기본 모델을 수정하여 새 작업을 수행하도록 변경한 다음 전체 모델을 끝까지 훈련시키는 세부전략을 따릅니다. 모든 경우에 수정 사항은 미미하며, 일반적으로는 분류 레이어를 학습하는 것입니다. 이는 각 작업에 대해 특수화된 모델을 개발하는 커뮤니티 내에서 수행된 중요한 노력과는 대조적입니다. 아래에서 각 작업에 대한 문제, 데이터셋, 모델 수정 및 훈련 목표를 설명합니다.
시각적 질문 응답 (VQA). VQA 작업은 이미지에 대한 자연어 질문에 대답해야 하는 작업입니다. 저희는 VQA 2.0 데이터셋 [3]에서 훈련 및 평가를 수행하며, 이 데이터셋은 COCO 이미지 [5]에 대한 110만 개의 질문과 각 질문에 대한 10개의 답변으로 구성되어 있습니다. ViLBERT를 VQA에 대한 미세조정을 수행하기 위해 이미지와 텍스트 표현인 hIMG와 hCLS의 원소별 곱셈 위에 두 개의 레이어 MLP를 학습하여 이 표현을 3,129가지 가능한 답변으로 매핑합니다. 우리는 VQA를 다중 레이블 분류 작업으로 취급하며, 각 답변에 대한 인간 답변 응답과의 관련성을 기반으로 소프트 타깃 점수를 할당합니다. 그런 다음 소프트 타깃 점수에 대한 이진 크로스 엔트로피 손실을 사용하여 최대 20 에포크 동안 배치 크기 256으로 훈련합니다. 초기 학습률은 4e-5로 설정합니다. 추론에서는 간단하게 소프트맥스를 취합니다.

시각적 공감 추론. Visual Commonsense Reasoning (VCR). 
VCR 작업은 이미지가 주어졌을 때 두 가지 문제를 제시합니다. 시각적 질문 응답 (Q→A)과 답변 정당화 (QA→R)로서 둘 다 다중 선택 문제로 제시됩니다. 통합 설정 (Q→AR)은 선택한 답변과 선택한 이유가 모두 정확해야 합니다. Visual Commonsense Reasoning (VCR) 데이터셋은 110,000개의 영화 장면에서 유도된 290,000개의 다중 선택형 QA 문제로 구성되어 있습니다. VQA 데이터셋과는 다르게 VCR은 언어에 객체 태그를 통합하여 직접적인 지속적 지도를 제공하며 명시적으로 지칭 표현을 배제합니다. 이 작업에 대한 미세 조정을 수행하기 위해 우리는 질문과 각 가능한 응답을 연결하여 네 가지 다른 텍스트 입력을 형성하고 이미지와 함께 각각을 ViLBERT를 통과시킵니다. 우리는 각 쌍에 대한 점수를 예측하기 위해 원소별 곱셈 표현 위에 학습된 선형 레이어를 학습합니다. 최종 예측은 이 네 가지 점수에 대한 소프트맥스이며 배치 크기가 64이고 초기 학습률이 2e-5인 20개 epoch 동안 교차 엔트로피 손실 아래에서 교육됩니다.

지시어 정지 표현. 
지시어 정지 표현 작업은 자연어 참조가 주어졌을 때 이미지 영역을 지역화하는 작업입니다. 우리는 RefCOCO+ 데이터셋에서 교육하고 평가합니다. 이 작업에 대한 일반적인 접근 방식은 지시어 참조를 고려한 이미지 영역 제안을 재랭크하는 것입니다. 따라서 우리는 [33]에서 제공한 경계 상자 제안을 직접 사용합니다. 이 경계 상자는 COCO 데이터셋에서 사전 훈련된 Mask R-CNN을 사용합니다. 미세 조정을 위해 각 이미지 영역 i에 대한 최종 표현 hvi를 학습된 선형 레이어로 전달하여 일치 점수를 예측합니다. 우리는 각 제안 상자를 지면 실제 상자와의 IoU를 계산하고 0.5에서 임계값 처리하여 레이블을 지정합니다. 우리는 최대 20 epoch 동안 이진 교차 엔트로피 손실로 교육하며 배치 크기는 256이며 초기 학습률은 4e-5입니다. 추론에서 우리는 가장 높은 점수의 영역을 예측으로 사용합니다.

캡션 기반 이미지 검색. 
캡션 기반 이미지 검색은 콘텐츠를 설명하는 캡션이 주어진 경우 풀에서 이미지를 식별하는 작업입니다. 우리는 Flickr30k 데이터셋에서 교육하고 평가합니다. 이 데이터셋은 각각 다섯 개의 캡션을 가진 31,000개의 Flickr 이미지로 구성되어 있습니다. [35]에서 제시된 분할을 따라 우리는 검증용으로 1,000개 이미지를 사용하고 각각 테스트하며 나머지에 대해 교육합니다. 이러한 캡션은 시각적 콘텐츠에 대한 것으로 잘 기술되어 있으며 Conceptual Captions과는 질적으로 다릅니다. 4가지 방식의 다중 선택 설정에서 교육하며 각 이미지-캡션 쌍에 대해 세 가지 이탈자를 무작위로 샘플링하여 무작위 캡션, 무작위 이미지 또는 대상 이미지의 100개 가장 가까운 이웃 중 하나로 대체합니다. 우리는 각각에 대한 정렬 점수를 계산하고 소프트맥스를 적용합니다. 이 모델을 교차 엔트로피 손실 하에 64의 배치 크기로 20 epoch 동안 훈련하여 참된 이미지-캡션 쌍을 선택합니다. 추론에서 우리는 테스트 세트의 각 캡션-이미지 쌍을 점수화하고 정렬합니다. 효율성을 위해 언어 스트림 표현을 첫 번째 Co-TRM 레이어 이전에 캐시하여 결국 언어 표현을 동결합니다.

'제로샷' 캡션 기반 이미지 검색. 
이전 작업은 데이터셋별 미세 조정을 포함하는 전송 작업입니다. 이 '제로샷' 작업에서는 Flickr30k [26]의 캡션 기반 이미지 검색에 사전 훈련된 다중 모달 정렬 예측 메커니즘을 미세 조정 없이 직접 적용합니다 (따라서 '제로샷'이라는 설명이 있음). 이 작업의 목표는 사전 훈련이 텍스트를 지속 가능하게 하는 능력을 개발하고 이를 작업 특정 미세 조정 없이 시각 및 언어적 변형에 일반화할 수 있는지를 시연하는 것입니다. 우리는 Sec. 3.1에서 설명한 Conceptual Captions 데이터셋으로 교육된 ViLBERT 모델을 직접 사용합니다. 우리는 정렬 예측 목표를 점수 함수로 사용하고 이전에 설명한 캡션 기반 이미지 검색 작업과 동일한 분할에서 테스트합니다.

4. 결과 및 분석
베이스라인. 사전 훈련된 ViLBERT 모델을 두 가지 제거 베이스라인과 비교합니다.

Single-Stream은 시각 및 언어 입력을 동일한 트랜스포머 블록 세트를 통해 처리하는 단일 BERT 아키텍처로 구성되며, 시각 및 언어 입력 모두에 대한 파라미터 및 처리 스택을 공유합니다. [29]와 같이 이 모델은 BERT 아키텍처를 변경하지 않으므로 우리 모델보다 훨씬 깊은 시각 처리와 모달 간 상호 작용이 더 빠르게 진행됩니다. 모델은 BERTBASE로 초기화되며 우리의 전체 모델과 동일하게 교육됩니다. 두 스트림이 전체 과정에서 상호 작용하기 때문에 효율성을 위해 어떤 표현도 캐시할 수 없습니다. 따라서 이미지 검색 및 제로샷 이미지 검색에 대한 이 베이스라인을 평가하지 않습니다.

ViLBERT†는 ViLBERT 아키텍처이지만 우리의 사전 교육 작업을 거치지 않은 모델입니다. 중요한 점은 언어 스트림을 위한 BERT 초기화가 여전히 있으며 이미지 지역을 전체 ViLBERT 모델과 동일한 Faster R-CNN 모델로 표현합니다. 우리의 아키텍처, 언어 초기화 또는 시각적 기능이 아닌 Conceptual Captions에 대한 사전 교육 과정에 기인할 수 있는 작업별 베이스라인 모델을 초월하는 이득을 분리하기 위해 이 베이스라인과 비교합니다.


표 1: 저희 ViLBERT 모델과 기존의 최첨단 모델 및 합리적인 구조 제거 모델 간의 전이 작업 결과입니다. †
는 Conceptual Captions에 대한 사전 훈련이 없는 모델을 나타냅니다. VCR 및 VQA에는 개별 테스트 세트가 있는데, 이 테스트 결과는 (괄호 안에) 우리의 전체 모델에 대해서만 보고합니다. 저희 전체 ViLBERT 모델은 모든 작업에서 과제별 최첨단 모델을 능가하고 있습니다.

작업별 기준선. 
결과를 맥락에 맞게 제시하기 위해 각 작업에 대한 문제별 방법 중 현재까지 우리의 지식으로는 최첨단인 발표된 결과를 제시합니다: VQA의 경우 DFAF [36], VCR의 경우 R2C [25], RefCOCO+의 경우 MAttNet [33], 캡션 기반 이미지 검색의 경우 SCAN [35]입니다.
결과. 표 1은 모든 전이 작업에 대한 결과를 보여주며 아래에서 주요 발견 사항을 강조합니다.

우리의 아키텍처는 단일 스트림 모델 대비 성능을 향상시킵니다. 우리는 사전 훈련된 (Single-Stream 대 ViLBERT) 및 사전 훈련되지 않은 (Single-Stream† 대 ViLBERT†) 모델 모두에 대해 ViLBERT가 단일 스트림 기준선 대비 개선 사항을 관찰합니다. VQA 및 RefCOCO+의 경우 가장 큰 이익을 얻었습니다.
사전 훈련 작업은 시각언어 표현을 개선시킵니다. 저희 모델은 우리의 프록시 작업 아래에서 사전 훈련된 ViLBERT 모델을 사용할 때 작업 전반에 걸쳐 개선됩니다 (ViLBERT 대 ViLBERT†). 또한 우리의 프록시 작업이 다른 모델 아키텍처에 일반화될 수 있다는 것을 확인하기 위해 Single-Stream에서도 개선 사항을 관찰합니다.
ViLBERT에서의 미세 조정은 시각 언어 작업에 대한 강력한 전략입니다. 하나의 기본 아키텍처로 전송 작업 성능이 모든 네 가지 확립된 작업에 대한 현재 최첨단 작업별 모델을 초과합니다. 우리는 VCR, RefCOCO+ 및 이미지 검색의 현재 최첨단을 큰 폭으로 개선합니다 (7-10 포인트 향상). 더 나아가 이러한 작업으로 확장하는 것은 단일 작업에 대한 단일 분류기를 추가하는 것만 필요했습니다.
이러한 결과는 저희 ViLBERT 모델이 하류 작업에서 활용할 수 있는 중요한 시각-언어 관계를 학습할 수 있다는 것을 보여줍니다.
시각 스트림 깊이의 영향. 표 2에서는 다양한 깊이의 ViLBERT 모델에서의 전이 결과를 비교합니다. 우리는 모델 내의 반복된 CO-TRM→TRM 블록의 수를 기준으로 깊이를 고려합니다 (Fig. 1에서 점선 상자로 표시됨). VQA 및 이미지 검색 작업에서는 더 큰 깊이에서 성능이 증가하는 것으로 나타났습니다. 마찬가지로, 제로샷 이미지 검색은 깊이가 증가함에 따라 계속해서 큰 이익을 얻고 있습니다. 반면에 VCR 및 RefCOCO+는 더 얕은 모델에서 이점을 얻는 것으로 보입니다.

대규모 훈련 데이터의 이점. 또한 사전 훈련 데이터 집합의 크기가 미치는 영향을 연구했습니다. 이 실험에서는 개념적 캡션 데이터 세트에서 25%와 50%의 임의 하위 집합을 추출하고 위와 동일한 설정을 사용하여 ViLBERT를 사전 훈련 및 미세 조정했습니다. 데이터 양이 증가함에 따라 정확도가 단조로이 증가하는 것을 볼 수 있으며, 이는 ViLBERT가 더 많은 사전 훈련 데이터에서 이익을 얻을 수 있을 것으로 보입니다.

표 2: 저희 모델의 깊이에 대한 제거 연구 결과로서, Co-TRM→TRM 블록 수 (Fig. 1의 점선 상자에 표시됨)와 관련하여 다른 작업들이 서로 다른 네트워크 깊이에서 더 잘 수행되는 것으로 나타났습니다. 이것은 각 작업이 더 많거나 더 적은 맥락 집계가 필요할 수 있음을 시사합니다.


표 3: 사전 훈련 중 개념적 캡션 데이터 세트의 백분율에 따른 ViLBERT의 전송 작업 결과. 사전 훈련 데이터 세트 크기가 커짐에 따라 단조롭게 성능이 향상되는 것을 볼 수 있습니다.


