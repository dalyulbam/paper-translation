희소 트랜스포머를 사용하여 긴 시퀀스 생성하기

레원 차일드1 스콧 그레이1 알렉 라드포드1 일리야 숫케버1

요약

크고 강력한 시퀀스 모델인 트랜스포머는
시간과 메모리를 제곱으로 증가시키는 것이 필요합니다.

시퀀스 길이에 따라 계산량이 증가하는 문제를 해결하기 위해, 본 논문에서는 어텐션 행렬의 희소 분해를 소개합니다.

O(n√n). 우리는 또한 a) 더 깊은 네트워크를 훈련시키기 위한 아키텍처와 초기화의 변형, b) 메모리를 절약하기 위한 주의 행렬의 재계산을 소개합니다.

c) 훈련을 위한 빠른 주의 커널입니다. 이러한 변경 사항이 있는 네트워크를 Sparse Transformers라고 부르며, 수백 개의 레이어를 사용하여 수만 개의 타임스텝으로 이루어진 시퀀스를 모델링할 수 있다는 것을 보여줍니다. 우리는 동일한 아키텍처를 사용하여 이미지, 오디오 및 원시 바이트로부터 텍스트를 모델링하며, 새로운 상태를 설정합니다.

Enwik8, CIFAR-10 및 ImageNet-64의 밀도 모델링 예술에 대한 결과입니다. 우리는 전역 일관성과 다양성을 보여주는 조건 없는 샘플을 생성하며, 자기 주의를 사용하여 원칙적으로 백만 개 이상의 시퀀스를 모델링할 수 있다는 것을 보여줍니다.

1. 소개

be a key step towards achieving artificial general intelligence.

비지도 표현 학습의 주요 구성 요소가 되다.

최근에, 신경 자기회귀 모델은 성과를 이루었습니다.

이 도메인에서 인상적인 결과를 달성하여 자연어 모델링에서 최첨단을 달성했습니다 (Jozefowicz et al., 2016) (Radford et al., 2018) (Dai et al., 2018), 원시 오디오 (VanDenOord et al., 2016) (Mehri et al., 2016) 및 이미지 (Oord et al., 2016) (Menick & Kalchbrenner, 2018) (Salimans et al., 2017) (Reed et al., 2017) (Chen et al., 2017).

이러한 방법들은 결합 확률 분포를 조건부 확률의 곱으로 분해합니다. 그러나 이러한 조건부 분포를 모델링하는 것은 매우 어려운 과제입니다.

그들은 많은 복잡하고 장거리 의존성을 포함하고 있으며, 
이를 학습하기 위해 적절한 표현력 있는 모델 구조가 필요합니다.

CNN을 기반으로 한 아키텍처들은 (Oord et al., 2016)에 의해 만들어졌다.

그림 1. 우리의 신경 자기 회귀 모델로부터의 무조건적인 샘플들

ImageNet 64와 클래식 음악 데이터셋을 기반으로한 모델을 사용했습니다.

오디오, 이미지, 텍스트에 대해 동일한 자기 주의 기반 아키텍처.

위의 샘플들은 소프트맥스 온도 1.0으로 생성되었습니다.

12,288와 65,536의 길이를 가지고 있습니다. 오디오 샘플은 다음 링크에서 들을 수 있습니다: https://openai.com/blog/sparse-transformer

이 방향으로 큰 진전이 있었지만, 상당한 깊이가 필요합니다.

수용 영역을 확장하기 위해. 이를 해결하기 위해 WaveNet (Van Den Oord et al., 2016)은 확장된 합성곱을 도입하여 네트워크가 로그 층 수로 장거리 의존성을 모델링할 수 있도록 했습니다.

별도로, Transformer (Vaswani et al., 2017)는 많은 자연어 작업에서 뛰어난 성과를 보여주었으며, 이는 임의의 종속성을 일정한 수의 레이어로 모델링할 수 있는 능력 때문일 수도 있습니다. 각 self-attention 레이어마다

전역 수용 영역을 가지고 있으므로, 네트워크는 입력 영역에 대한 표현 능력을 할당할 수 있습니다.

가장 유용합니다. 따라서 건축은 더 유연할 수 있습니다.

고정된 연결 패턴을 가진 네트워크보다 다양한 데이터 유형을 생성하는 능력이 있다.

그러나, 기억력과 계산 요구 사항은

이러한 네트워크는 시퀀스 길이와 제곱 관계로 성장하며,
이로 인해 긴 시퀀스에서 사용할 수 없다.

이 작업의 주요 기여는 성능을 희생하지 않으면서 시퀀스 길이에 따라 O(n p √n)로 스케일링되는 여러 희소 인수분해를 주목 행렬에 도입하는 것입니다. 이 작업은 전체 주목 연산을 여러 빠른 주목 연산으로 분리하여 결합할 수 있으며, 이를 통해 밀집 주목 연산을 근사할 수 있습니다.

이를 사용하여 전례 없는 길이의 시퀀스에 자기 주의를 적용합니다.

또한, 우리는 몇 가지 다른 변경 사항을 소개합니다.

트랜스포머, 포함:

• 매우 깊은 네트워크의 훈련을 개선하기 위한 재구성된 잔여 블록과 가중치 초기화

효율적으로 주의 행렬의 부분집합을 계산하는 희소 주의 커널 세트

• 메모리 사용량을 줄이기 위해 역전파 중 주의 가중치 재계산

이 방식으로 보강된 모델들을 경험적으로 검증합니다.

네가 상태-of-the-art 압축과 생성을 달성할 수 있습니다
자연어, 원시 오디오 및 자연 이미지. 
아키텍처의 간단함으로 인해 우리는 많은 관심 문제에 유용 할 수 있다고 믿습니다.

관련 연구

가장 관련된 작업은 자기회귀 생성 모델의 확장을 위한 다른 기술들을 포함한다. 이미지에 대해서는 (Reed et al., 2017)에서 조건부 독립성을 모델링한다.

픽셀을 병렬로 많은 위치를 생성하기 위해 사용하며, (Menick & Kalchbrenner, 2018)는 순서를 부과하고 고정도 샘플을 생성하기 위해 다중 스케일 업샘플링 절차를 적용합니다. (Parmar et al., 2018)는 이미지에 Transformer를 적용하기 위해 로컬 어텐션 블록을 사용합니다. 텍스트에 대해서는 (Dai et al., 2018)은 장기 기억을 모델링하기 위해 "메모리"라는 상태 재사용을 도입합니다.

의존성. 그리고 오디오에 대해서는 (Van Den Oord et al., 2016)에 추가로, (Mehri et al., 2016)은 계층 구조와 다양한 클럭 속도의 RNN을 사용하여 추론 중에 긴 문맥을 사용하였으며, 이는 (Koutnik et al., 2014)와 유사합니다. (Huang et al., 2018)은 효율적인 상대적인 어텐션을 사용하여 MIDI 생성에 Transformers를 적용합니다.

of the techniques mentioned above are complex and specialized for specific types of data.

위의 기술들은 우리의 것과 직교한다.
또한 우리의 것과 함께 사용될 수 있다.

생성 모델링 외에도, 여러 연구들이 있습니다.

주의력 향상을 위한 청크화 (Chiu & Raffel, 2017) 또는 고정 길이 표현 (Britz et al., 2017)을 사용하는 것과 관련이 있습니다. 다른 연구들은 (Sukhbaatar et al., 2015)와 (Gehring et al., 2017)과 같이 다중 "점프"를 사용하여 주의력을 조사했습니다.

Gated Pixel CNN (Oord et al., 2016)과 WaveNet (Van Den Oord et al., 2016)은 자가 주의와 관련된 네트워크에서 곱셈 상호작용을 사용한다는 점에 주목할 가치가 있다.

배경

우리는 자기회귀 시퀀스 생성 작업을 고려합니다. 여기서 시퀀스 x = {x1, x2, ..., xn}의 결합 확률은 조건부 확률 분포의 곱으로 모델링되며, 네트워크 θ에 의해 매개변수화됩니다.

p(x) = n ∏ i=1 p(xi|x1,...,xi−1;θ) (1)

우리는 이미지, 텍스트, 오디오를 일련의 이산 토큰으로 처리합니다. 일반적으로 원시 바이트 형태로 처리합니다. 네트워크 θ는 토큰의 시퀀스를 입력으로 받고, 소프트맥스 함수를 사용하여 다음 토큰의 범주 분포를 출력합니다. 여기서 v는 어휘 크기에 해당하는 가능한 값의 개수입니다. 훈련 목표는 데이터의 로그 확률을 θ에 대해 최대화하는 것입니다.

모델 θ에 대한 간단하고 강력한 선택은 Transformer입니다 (Vaswani et al., 2017). 이 모델은 디코더만을 사용하는 모드로 사용되며, 이는 (Radford et al., 2018)와 (Liu et al., 2018)에 의해 증명되었습니다. 이러한 모델은 입력 시퀀스를 전체 시퀀스에 걸쳐 다중 헤드 셀프 어텐션 블록을 사용하여 변환한 후, 각 시퀀스 요소에 대해 밀집 변환을 수행합니다. 그러나 네트워크의 셀프 어텐션 부분은 각 요소마다 n개의 가중치를 계산해야 하며, 시퀀스 길이가 길어질수록 계산이 어려워질 수 있습니다.

다음 섹션에서는 긴 시퀀스를 모델링하기에 더 적합하도록 Transformer 아키텍처를 수정한 내용을 설명합니다.

4. 인수분해된 셀프 어텐션

희소 트랜스포머는 전체 자기-주의 연산을 주의의 여러 단계로 분리하여 수행합니다. 이는 그림 3(b)와 3(c)에서 시각화되었습니다. 접근 방식을 동기부여하기 위해 먼저 수행합니다.

이미지 데이터셋에서 표준 Transformer가 학습한 주의 패턴의 질적 평가.
Sparse Transformers를 사용하여 긴 시퀀스 생성.

그림 2. CIFAR-10에서 전체 주의를 받은 128층 신경망으로부터 학습된 주의 패턴입니다. 흰색 하이라이트는 주의를 나타냅니다.

주어진 픽셀을 생성하는 동안 머리에 대한 가중치를 사용하며, 검은색은 자기회귀 마스크를 나타냅니다. 레이어는 다양한 것을 학습할 수 있습니다.

특화된 희소 구조는 다양한 도메인에 적응할 수 있는 능력을 설명할 수 있습니다. a) 네트워크의 초기 레이어들은 많은 것을 배웁니다.

지역적으로 연결된 패턴은 합성곱과 유사합니다. b) 19번째와 20번째 레이어에서, 네트워크는 주의를 분산시키는 것을 학습했습니다.

행 주의와 열 주의는 전역 주의 계산을 효과적으로 인수분해합니다. c) 여러 개의 주의 층은 전역적인 특징을 보였습니다.

데이터에 따라 액세스 패턴이 달라집니다. d) 64-128 레이어의 전형적인 레이어들은 높은 희소성을 보였으며, 위치들은 드물게 활성화되고 있으며 오직 ~

구체적인 입력 패턴.

(a) 트랜스포머
(b) 희소 트랜스포머 (스트라이드)
(c) 희소 트랜스포머 (고정)

그림 3. 표준 Transformer의 전체 어텐션과 비교하여 우리가 평가한 두 개의 2D 인자화 어텐션 방식 (a). 상단

행은 예를 들어 6x6 이미지의 경우, 주어진 출력을 계산할 때 입력으로 받는 두 개의 어텐션 헤드가 위치하는 것을 나타냅니다.

아래 행은 모든 출력 (행)과 입력 (열) 사이의 연결성 행렬을 나타냅니다. 연결성에서의 희소성.

행렬은 계산 속도를 상당히 빠르게 할 수 있습니다. (b)와 (c)에서는 두 개의 헤드가 연결된 요소들 사이의 완전한 연결성이 유지됩니다.

연속적으로 계산되었습니다. 우리는 이러한 인수분해가 Figure 2의 다양한 연결 패턴과 성능적으로 일치할 수 있는지 테스트했습니다.
희소 트랜스포머를 사용하여 긴 시퀀스 생성하기

4.1. 배운 주의력의 질적 평가

패턴

우리는 CIFAR-10에서 128층의 self-attention 네트워크가 학습한 주의 패턴을 시각화하고, Figure 2에서 몇 가지 예시를 제시했습니다. 시각적 검사 결과, 대부분의 층에서 대부분의 데이터 포인트에 희소한 주의 패턴이 있음을 보였습니다.

일부 희소성 형태를 도입할 수 있다는 제안을 하고 있습니다. 성능에 큰 영향을 주지 않고도 몇 개의 레이어 (그림 2c)는 분명히 전역적인 패턴을 보여주었으며, 다른 몇 개의 레이어는 데이터에 의존하는 희소성을 보여주었습니다 (그림 2d). 이러한 모든 어텐션 행렬에 미리 결정된 희소성 패턴을 도입하면 이러한 패턴들에 영향을 미칠 것입니다.

이 논문에서는 우리는 주의력의 여러 단계에 걸쳐 모든 위치 간의 연결성을 가진 희소한 주의 패턴 클래스에 대한 조사를 제한했습니다. 이러한 방법들은

주어진 위치에 대해 전역적인 맥락을 제공하면서도 완전한 주의에 비해 더 효율적입니다. 우리는 이러한 인자화된 패턴의 성능을 다양한 작업에서 경험적으로 검증하기 위해 목표를 설정했습니다. 그들은 Figure 2의 매핑과 정확히 동일한 매핑을 학습할 수 없기 때문에. 우리는 아래에 인자화된 주의의 공식을 제시합니다.

4.2. 인수분해된 셀프 어텐션

자기 주의 레이어는 입력 임베딩 행렬 X를 출력 행렬로 매핑하며 연결 패턴 S = {S1,...,Sn}에 의해 매개변수화됩니다. 여기서 Si는 i번째 출력 벡터가 참조하는 입력 벡터의 인덱스 집합을 나타냅니다. 출력 벡터는 입력 벡터의 변환들의 가중 합입니다.

참석(X,S) = ∑(a(xi,Si))
i∈{1,...,n}

a(xi,Si) = 소프트맥스((Wqxi)KT Si √ d)

VSi (3)

KSi = 한국 주식 시장
(cid:16) 안녕하세요
Wkxj (cid:17) 안녕히 계세요

j∈Si
VSi = ∑ Wvxj

j∈Si
(4)
j는 Si에 속한다.

여기서 Wq, Wk 및 Wv는 주어진 xi를 쿼리, 키 또는 값으로 변환하는 가중치 행렬을 나타내며, d는 쿼리와 키의 내부 차원입니다. 각 위치의 출력은 키와 쿼리의 스케일 조정된 점곱 유사도에 의해 가중치가 부여된 값들의 합입니다.

전체 자기 주의 모델에서 자기 주의는 Si = {j : j ≤ i}를 정의하며, 모든 요소가 이전 위치와 자신의 위치에 모두 주의를 기울일 수 있게 합니다.

인자화된 자기 주의는 p개의 별개의 주의 헤드를 가지며, m번째 헤드는 인덱스의 부분집합 A(m)i를 정의하고 Si = A(m)이라고 합니다.

우리는 주로 부분집합 A에 대한 효율적인 선택에 관심이 있습니다. 여기서 |A(m)i| ∝ p √ n입니다.

또한, 우리는 현재로서 A의 유효한 선택지를 고려합니다. 여기서 모든 입력 위치는 어텐션의 p 단계 동안 모든 미래 출력 위치에 연결되어 있습니다.

각 j ≤ i 쌍에 대해, i가 참석할 수 있는 위치 경로를 가진 A를 설정합니다.
특히, (j,a,b,c,...,i)가 인덱스 경로라면
j ∈ A(1) a , a ∈ A(2) b , b ∈ A(3) c 등이 됩니다.

이 두 가지 기준은 우리에게 변압기의 능력을 유지하면서 임의의 입력 위치에서 임의의 출력 위치로 신호를 전파하는 것을 일정한 단계 수로 가능하게 합니다. 동시에 전체 유효한 계산을 O(n p)로 줄입니다.

우리는 또한 유효성 기준을 완화하는 것(예: 지역적으로만 연결된 레이어의 시리즈)이 특정 도메인에 유용한 귀납적 편향일 수 있다는 것을 주목합니다.

이 작업에서는 p = 2에 대해 두 가지 인수분해를 탐구합니다. 이에 대해 다음 섹션에서 설명하지만, 동일한 기술이 더 높은 차원으로 쉽게 확장될 수 있다는 점을 언급합니다.

4.3. 이차원 요소 분해된 주의력

두 차원에서 인자화된 주의 패턴을 정의하는 자연스러운 방법은 한 헤드가 이전 위치에 주의를 기울이고, 다른 헤드가 매 l번째 위치에 주의를 기울이는 것입니다. 여기서 l은 스트라이드이며, 가까운 값으로 선택됩니다.

√
n, 방법
우리는 strided attention이라고 부릅니다.

공식적으로, A(1) = {t,t + 1,...,i} (단, t = max(0,i − l)) 이고 A(2) = {j : (i − j) mod l = 0} 이다. 이 패턴은 그림 3(b)에서 시각화될 수 있다.

이 공식은 데이터가 이미지나 일부 음악과 같이 걸음걸이와 일치하는 구조를 자연스럽게 가지고 있는 경우 편리합니다. 그러나 텍스트와 같이 주기적인 구조가 없는 데이터의 경우, 우리는 네트워크가 걸음걸이 패턴으로 정보를 제대로 전달하지 못할 수 있다는 것을 발견합니다.

원소의 좌표는 미래에 해당 원소가 가장 관련성이 높을 수 있는 위치와 반드시 일치하지 않습니다.

그러한 경우에는 대신 고정된 주의 패턴 (그림 3(c))을 사용합니다. 여기서 특정 셀은 이전 위치를 요약하고 그 정보를 모든 이후 셀로 전파합니다.

= {j : ((cid:98)j/l(cid:99) = (cid:98)i/l(cid:99))}.

i = {j : j mod l ∈ {t,t + 1,...,l}, where t = l − c and c is a hyperparameter.

i = {j : j mod l ∈ {t,t + 1,...,l}, 여기서 t = l − c이고 c는 하이퍼파라미터입니다.

구체적으로, 만약 스트라이드가 128이고 c = 8이면, 128보다 큰 모든 미래 위치는 120-128 위치에 참여할 수 있으며, 256보다 큰 모든 위치는 248-256에 참여할 수 있으며, 이와 같이 계속됩니다.

c = 1로 고정된 주의 패턴은 네트워크의 표현력을 제한하여 많은 표현을 제한합니다.
"Generating Long Sequences with Sparse Transformers"에서.

네트워크는 한 블록에만 사용됩니다. 작은

모든 블록에 의해 사용되는 위치의 수입니다. 대신, 우리는 일반적인 값인 l ∈ {128,256}에 대해 c ∈ {8,16,32}를 선택하여 잘 작동하는 것을 발견했습니다. 그러나 이는 strided attention에 비해 이 방법의 계산 비용을 c만큼 증가시킨다는 점에 유의해야 합니다.

또한, 우리는 여러 개의 헤드를 사용할 때,
길이가 c인 서로 다른 하위 블록에 주의를 기울이는 것이
크기가 l인 블록 내에서 동일한 하위 블록에 주의를 기울이는 것보다 선호된다는 것을 발견했습니다.

다음 섹션에서는 희소 트랜스포머 아키텍처에 인수화된 어텐션을 통합하는 방법에 대해 설명합니다.

참다.

희소 트랜스포머

여기에서는 Sparse Transformer 아키텍처에 대해 자세히 설명합니다. 이는 Transformer (Vaswani et al., 2017)의 수정 버전입니다.

5.1. 인수분해된 어텐션 헤드

표준 밀집 어텐션은 단순히 식 2에서 정의된 어텐드 함수의 선형 변환을 수행합니다.

주의(X) = Wp · 참석(X,S) (5)

Wp는 포스트 어텐션 가중치 행렬을 나타냅니다. 인수분해된 셀프 어텐션을 통합하는 가장 간단한 기술은 잔여 블록마다 하나의 어텐션 유형을 사용하고, 이를 순차적으로 또는 하이퍼파라미터로 결정된 비율로 교차로 사용하는 것입니다.

주의(X) = Wp · 참석(X,A(r mod p)) (6)

여기서 r은 현재 잔여 블록의 인덱스이고 p는 인수 분해된 어텐션 헤드의 수입니다.

두 번째 접근 방식은 단일 헤드가 인자화된 두 헤드가 관심을 가질 픽셀의 위치에 참석하는 것이다. 이를 병합된 헤드라고 한다.

주의(X) = Wp · 참석(X,

p
(cid:91) m=1A(m)) (7)

p
(cid:91) m=1A(m)) (7)

이것은 약간 더 계산적으로 집중적이지만, 상수 배로만 증가합니다. 세 번째 접근 방식은 다중 헤드 어텐션(Vaswani et al., 2017)을 사용하는 것입니다. 여기서 nh 개의 어텐션 결과가 병렬로 계산되고, 그런 다음 특성을 따라 연결됩니다.

차원

주의(X) =
Wp(cid:16) 참석(X,A)(i)(cid:17)

i∈{1,...,nh}
(8)
i∈{1,...,nh}
(8)

임베드

선형

소프트맥스
노름

표준

표준
중퇴

중퇴
주의

피드포워드

. . .

그림 4. 희박 변환기의 하나의 잔여 블록을 나타내는 다이어그램. 그림자가 들어간 배경은 체크포인트(Chen et al., 2016)되고 GPU 메모리에 저장된 텐서를 나타냅니다. 다른

텐서, 어텐션 가중치 및 피드포워드 네트워크를 포함한 것들

활성화는 그래디언트 계산 중에 재계산됩니다.

기억 공간 사용량을 상당히 줄이기.

여기에서 A는 분리된 주의 패턴, 병합된 패턴 또는 Eq. 2와 같이 교차된 패턴일 수 있습니다. 또한, 주의 함수 내부의 가중치 행렬의 차원은 1/nh의 비율로 감소되어 매개변수의 수가 nh의 값에 관계없이 불변합니다.

우리는 일반적으로 여러 명의 헤드가 잘 작동하는 것을 발견합니다.

주의가 계산 시간을 지배하는 극도로 긴 시퀀스의 경우, 한 번에 하나씩 순차적으로 수행하는 것이 더 가치있습니다.

5.2. 수백 개의 레이어로 확장하기

우리는 트랜스포머를 훈련하기 어려웠다는 것을 발견했습니다.

많은 층들, (Al-Rfou et al., 2018)에 의해 언급되었다. 보조 손실을 포함하는 대신, 우리는 다음을 채택했다. Sparse Transformers로 긴 시퀀스 생성하기.

건축적인 변화.

먼저, 우리는 (He et al., 2016)의 사전 활성화 잔차 블록을 사용하여 다음과 같이 N개의 레이어로 구성된 네트워크를 정의합니다.

H0 = X와 We를 임베드한다. (9)

Hk = Hk−1 + resblock(Hk−1) (10)
Hk = Hk−1 + resblock(Hk−1) (10)

y = 소프트맥스(정규화(HN)Wout) (11)

임베드(embed)는 다음 섹션에서 설명하는 함수입니다.
Wout은 가중치 행렬이며, resblock(h)는 어텐션 블록과 위치별 피드포워드 네트워크의 입력을 정규화합니다.

a(H) = 중도탈락(주의(정규화(H))) (12)

b(H) = 드롭아웃(ff(정규화(H + a(H)))) (13)

resblock(H) = a(H) + b(H) (14)
레스블록(H) = a(H) + b(H) (14)

정규화 함수는 레이어 정규화를 나타냅니다 (Ba et al., 2016), 그리고 ff(x) = W2 f(W1x + b1) + b2입니다. 우리의 f 선택은 가우시안 에러 선형 유닛 (Hendrycks & Gimpel, 2016)인 f(X) = X (cid:12) sigmoid(1.702 · X)입니다. 이는 (Radford et al., 2018)에서 사용되었습니다. W1의 출력 차원은 입력 차원의 4.0배입니다. 그렇지 않은 경우에는 특별히 언급하지 않습니다.

HN은 함수 a와 b의 N번의 적용의 합임을 관찰하십시오. 따라서 각 함수 블록은 출력 레이어로부터 직접 그래디언트를 받습니다. 우리는 방정식 5에서 W2와 Wp의 초기화를 1/√로 스케일링합니다.

2N
입력 비율을 유지하기 위해

임베딩 스케일을 잔여 블록에 임베딩하여 N의 값에 관계없이 스케일 불변성을 유지합니다.

5.3. 다양한 데이터 유형 모델링

입력 심볼의 임베딩에 추가로, 위치 임베딩은 일반적으로 트랜스포머와 다른 위치에 무관한 아키텍처에서 공간 관계를 인코딩하는 데 사용됩니다.

데이터의 배들 (Gehring et al., 2017), (Parmar et al., 2018).

우리는 데이터의 구조를 인코딩한 학습된 임베딩을 사용하여, 또는 인자화된 주의 패턴을 인코딩한 학습된 임베딩을 사용하여 찾았습니다.

우리 모델의 성능에 중요했습니다.

우리는 각 입력 위치에 nemb = ddata 또는 nemb = dattn 임베딩을 추가했습니다. 여기서 ddata는 데이터의 차원 수를 나타내고, dattn은 인자화된 어텐션의 차원 수를 나타냅니다. 만약 xi가 시퀀스에서 i번째 원-핫 인코딩된 요소라면, o(j)는...

만약 i가 xi의 j번째 차원에서의 one-hot 인코딩된 위치를 나타낸다면:

embed(X,We) = 임베드(X, We) = 

우리는 함께 성장합니다.

j=1
o(j) i
Wj


xi∈X
(15)
이미지에 대해서는 입력 바이트의 행, 열 및 채널 위치에 대한 데이터 임베딩을 사용했습니다. 데이터 차원은 ddata = 3입니다. 텍스트와 오디오에 대해서는 폭이 stride와 동일한 행렬의 각 위치의 행 및 열 인덱스에 해당하는 2차원 어텐션 임베딩을 사용했습니다. 데이터 차원은 dattn = 2입니다.

5.4. 주의 가중치를 재계산하여 메모리 절약하기

used (Kitaev et al., 2020).

처리된, 이 레이어들의 메모리 사용량이 계산 비용에 비해 높기 때문에.

tion models that are competitive with the state-of-the-art sparse attention models.

16,384 길이의 시퀀스에 수백 개의 레이어가 있는 신경망은 그렇지 않으면 현대 하드웨어에서 실행하기 어려울 것입니다.

우리의 실험에서는 역전파 동안에도 어텐션과 피드포워드 블록을 다시 계산합니다. 우리의 구현을 간소화하기 위해, 우리는 어텐션 블록 내에서 드롭아웃을 적용하지 않고 (Vaswani et al., 2017)에서처럼 각 잔차 추가의 끝에서만 적용합니다.

그림 4.

5.5. 효율적인 블록-희소 어텐션 커널

3(b)와 3(c)의 희소한 어텐션 마스크는 쿼리, 키, 밸류 행렬에서 서브 블록을 잘라내어 블록 단위로 곱을 계산함으로써 효율적으로 계산될 수 있습니다.

로컬 창 위의 어텐션은 그대로 계산될 수 있으며,
k 스트라이드로 어텐션을 계산하기 위해서는 행렬을 전치하고 로컬 창을 계산해야 합니다. 고정된 어텐션 위치는 블록으로 집계되고 계산될 수 있습니다.

테스트를 용이하게 하기 위해, 우리는 이러한 연산을 효율적으로 수행하는 GPU 커널 세트를 구현했습니다.
소프트맥스 연산은 단일 커널로 통합되며, 또한 레지스터를 사용하여 입력 데이터를 로드하는 것을 제거합니다.

한 번 이상 실행하여 간단한 비선형성과 동일한 속도로 실행할 수 있도록 허용합니다. 또한 어텐션 행렬의 상삼각형은 계산되지 않으며, (Vaswani et al., 2017)의 부정적 편향 용어를 제거하고 수행해야 할 작업 수를 절반으로 줄입니다.

5.6. 혼합 정밀도 훈련

우리는 네트워크 가중치를 단정밀도 부동 소수점으로 저장합니다.
하지만 네트워크 활성화와 그래디언트 계산은 반정밀도로 수행합니다. (Micikevicius et al., 2017)에서와 같이. 이는 V100 GPU에서 Tensor Core 연산을 사용하여 훈련을 가속화합니다. 그래디언트 계산 중에는 Sparse Transformers를 사용하여 긴 시퀀스를 생성합니다.

그림 5. ImageNet 64x64에서 생성된 조건 없는 샘플, 수정되지 않은 softmax 온도 1.0으로 생성되었습니다. 우리는 배울 수 있습니다.

다중 스케일 아키텍처를 사용하지 않고 픽셀에서 직접 장거리 종속성을 추출합니다.

수치 하한을 줄이기 위한 동적 손실 스케일링을 사용하며, 여러 개의 GPU를 통해 평균을 구할 때 반 정밀도 그래디언트를 사용하여 통신합니다. 샘플링할 때는 쿼리와 키를 단정밀도로 변환합니다. 쿼리-키 곱셈이 반 정밀도의 최대값을 초과할 수 있기 때문입니다.

6. 훈련

Adam 옵티마이저를 사용하며, 5000번의 선형 워마업과 1.0의 그래디언트 클리핑을 적용합니다.

중요한 모델 안정성을 위해 발견되었습니다. 우리는 가중치 감쇠 벌칙을 0.01로 사용합니다. 우리는 (Radford et al., 2018)에서와 같이 코사인 감쇠에 따라 학습률을 감소시켰습니다. 그 외에는 특별히 언급되지 않는 한 8개의 V100 GPU에서 훈련합니다.

모든 임베딩은 일정한 차원 d를 가지며, 일반적으로 {256, 512, 1024} 중 하나입니다. 기본적으로 모든 선형 변환은 동일한 차원으로 이루어지지만, 예외적으로 피드포워드 네트워크는 입력을 4d로 투영합니다. 단, "반 사이즈" 변환을 사용하는 경우에는 2d로 투영됩니다. 또한, 때로는 쿼리와 키 변환의 크기를 절반으로 줄일 수도 있습니다.

우리는 토큰 임베딩 We를 N(0, 0.125 √로 초기화합니다.

d) 그리고

위치 임베딩은 N(0, 0.125 √ dnemb)에서 나온다. 어텐션 및 피드포워드 구성 요소 내에서는 모든 편향이 초기화된다.

0으로 초기화되고 모든 가중치는 N(0, 0.125 √ din)에서 초기화됩니다. 여기서 din은 팬인 차원입니다. 출력 로짓을 위한 가중치 행렬은 0으로 초기화되었습니다.

7. 실험

우리는 자연 이미지, 텍스트 및 원시 오디오를 포함한 밀도 모델링 작업에서 우리의 아키텍처를 경험적으로 테스트합니다. 결과 요약은 표 1에서 확인할 수 있습니다. 우리는 다음을 발견했습니다.

그것은 full attention보다 훨씬 빠르게 실행되는 것 외에도, sparse patterns는 낮은 오류로 수렴되었습니다. 이는 우리가 도입한 희소 패턴으로 유용한 귀납적 편향을 가리킬 수도 있으며, full attention과 관련된 최적화 문제일 수도 있습니다. (Table 2에서 보여진 대로)

7.1. CIFAR-10

우리는 CIFAR-10 이미지를 3072바이트의 시퀀스로 표현하여 strided Sparse Transformers를 훈련시킵니다. 모델은 2개의 헤드, 128개의 레이어, d = 256, 반 사이즈 피드포워드 네트워크 및 쿼리-키 프로젝션을 가지고 있으며, 120 에포크 동안 훈련됩니다.

학습률이 0.00035이고 드롭아웃 비율이 0.25인 상태에서
검증 오류가 감소하지 않을 때까지.

testing.

우리 최고의 모델들의 성능을 평가하여 검증하기

테이블 1. 밀도 모델링 작업에 대한 결과 요약.

바이트당 비트로 결과가 보고됩니다. 이는 비트당 비트와 동일합니다.

이미지 작업에 대한 차원이 낮습니다. M은 백만 개의 매개변수를 의미합니다.

모델                  바이트 당 비트

CIFAR-10

PixelCNN (Oord et al., 2016) 3.03
PixelCNN++ (Salimans et al., 2017) 2.92
이미지 트랜스포머 (Parmar et al., 2018) 2.90
PixelSNAIL (Chen et al., 2017) 2.85
Sparse Transformer 59M (strided) 2.80

Enwik8
엔윅8

더 깊은 자기 주의 (Al-Rfou et al., 2018) 1.06
Transformer-XL 88M (Dai et al., 2018) 1.03
Transformer-XL 277M (Dai et al., 2018) 0.99
Sparse Transformer 95M (고정) 0.99

이미지넷 64x64

PixelCNN (Oord et al., 2016) 3.57
병렬 다중 스케일 (Reed et al., 2017) 3.7
Glow (Kingma & Dhariwal, 2018) 3.81
SPN 150M (Menick & Kalchbrenner, 2018) 3.52
Sparse Transformer 152M (strided) 3.44

고전 음악, 12 kHz에서 5초

스파스 트랜스포머 152M (스트라이드) 1.97

테스트 세트. 모델은 이전의 최고 성능인 2.85 비트 당 차원(Chen et al., 2017)에 비해 2.80 비트 당 차원(시드 1, 2, 3에서 2.798 ± 0.004)를 달성합니다. 또한 테이블 2에서 다른 어텐션 패턴의 성능을 비교합니다. 스트라이드 어텐션은 가장 짧은 시간 내에 가장 낮은 오류를 달성하여, 차원 당 2.82 비트의 밀도 어텐션의 오류를 능가합니다.

7.2. 텍스트

강한 2차원 구조가 없는 데이터셋에서 Sparse Transformers를 평가하기 위해, 우리는 EnWik8 데이터셋에서 모델을 훈련시켰습니다. 이 데이터셋은 위키피디아의 처음 108바이트를 나타내며 주기적인 구조에 많은 변동성을 포함하고 있습니다. 우리는 이전 접근법보다 더 긴 12,288의 컨텍스트 길이로 훈련을 진행했습니다.

우리는 처음 9000만 토큰을 훈련시켰고, 마지막 1000만 토큰은 검증과 테스트를 위해 보존했습니다. 우리는 30층 고정 Sparse Transformer를 사용했으며, 각각 8개의 헤드, d = 512, 그리고 dropout 비율 0.40을 사용했습니다. 검증 손실이 더 이상 감소하지 않을 때까지 80 에포크 동안 훈련했습니다. 우리는 128의 스트라이드와 c = 32를 사용하였으며, 인수분해된 어텐션 헤드를 병합했습니다.

우리 최고의 모델은 1, 2, 3번 시드를 통해 0.99 비트 당 차원 (0.992 ± 0.001)에 도달하여 1.03 최신 기술을 능가했습니다.

크기가 유사한 Transformer-XL (Dai et al., 2018)과
더블 이상의 모델로 훈련된 모델의 0.99와 일치시키기

표 2. 희박한 패턴은 속도가 증가하고 더 좋아졌습니다.

우리가 둘 다 비교할 수 있는 데이터셋에서의 손실, 이는

우리가 배운 패턴 중에서 유용한 귀납적 편향을 가리키세요.

주의를 기울여야 하는 기본 최적화 문제.

모델           바이트 당 비트 수   시간/반복

Enwik8 (12,288 문맥)

밀집 어텐션 1.00 1.31
희소 트랜스포머 (고정) 0.99 0.55
희소 트랜스포머 (간격) 1.13 0.35

CIFAR-10 (3,072 컨텍스트)

밀집 어텐션 2.82 0.54
희소 트랜스포머 (고정) 2.85 0.47
희소 트랜스포머 (간격) 2.80 0.38

테이블 3. 우리는 Enwik8의 압축이 길어짐에 따라 증가하는 것을 관찰합니다.

희소 트랜스포머가 효과적으로 통합될 수 있다는 것을 시사하는 맥락

장기 의존성을 형성하다.

평가 중 최소 컨텍스트 길이 바이트 당 비트

6,144 토큰            0.9952
9,216 토큰            0.9936
10,752 토큰           0.9932
11,904 토큰           0.9930
12,096 토큰           0.9922
12,160 토큰           0.9908

매개변수의 수. 스트라이드 어텐션은 이 데이터셋에서 잘 수행하지 못했으며, 고정된 패턴은 밀집 어텐션의 성능을 회복하고 뛰어넘을 수 있었습니다. 표 2에 명시된 대로.

게다가, 테스트 세트의 평가 중에는, 우리는 네트워크가 사용할 수 있는 최소 컨텍스트 길이를 수정했습니다. 병렬로 평가하는 토큰 수를 줄여서 성능이 단조롭게 증가하는 것을 관찰했습니다. 최대 12,160개의 토큰을 사용할 때까지.

훈련에 사용된 12,288개의 토큰 중 (표 3 참조), 이는 네트워크가 효과적으로 장기 의존성을 통합하고 있다는 것을 시사합니다.

7.3. 이미지넷 64x64

모델이 장거리 학습을 할 수 있는 능력을 테스트하기 위해

의존성과 대규모 데이터셋에 대한 규모 조정을 위해, 우리는 (Oord et al., 2016)가 공개한 다운샘플된 ImageNet 버전으로 훈련하고 검증 세트에서 평가합니다. 우리는 16개의 어텐션 헤드와 d = 512를 가진 48 레이어의 스트라이드 Sparse Transformer를 사용하여 총 1억 5200만 개의 매개변수를 사용했습니다. 우리는 128의 스트라이드, 0.01의 드롭아웃을 사용하고 70 에포크 동안 훈련했습니다.

64개의 V100 GPU에서 7일이 걸렸습니다.

우리의 모델은 이전의 3.52 (Menick & Kalchbrenner, 2018)에 비해 차원당 3.44 비트의 손실을 달성합니다. Sparse Transformers로 긴 시퀀스 생성하기

또한, 우리는 무조건적인 샘플을 생성합니다 (그림

5) 수정되지 않은 softmax 온도 1.0에서, 모델로부터와 레이어를 두 배로 훈련한 모델로부터 (총 300M 매개변수), 우리는 여기에 300M 매개변수 모델의 샘플을 포함시켰습니다. 시각적 평가에서 우리는 희소성 패턴으로 인한 이상 현상을 발견하지 않았으며 대부분의 이미지에서 장기적인 구조의 증거를 볼 수 있었습니다.

7.4. 원시 오디오에서 클래식 음악

Sparse Transformers가 매우 긴 맥락에 얼마나 확장 가능한지 테스트하기 위해, 우리는 (Dieleman et al., 2018)에 의해 공개된 클래식 음악 데이터셋으로 모델을 훈련시켰습니다. 데이터셋 처리에 대한 세부 정보가 없으므로 생략합니다.

다른 작업과의 직접적인 비교 대신에 어떤 크기의 Sparse Transformer를 훈련시킬 수 있는지 연구해보았습니다. 각 시퀀스 길이마다, 모델 병렬화 없이 16GB V100 가속기에 완전히 맞출 수 있는 가장 큰 모델을 훈련하려고 시도했습니다.

전반적으로, 우리는 시퀀스 길이를 4배로 증가시키면 모델 용량을 약 4√4 = 8 정도로 줄여야 한다는 것을 발견했습니다. 따라서 우리는 100만 개의 타임스텝을 가진 시퀀스에 대해 인자분해된 셀프 어텐션을 사용할 수 있다는 것을 발견했습니다.

매우 적은 매개변수(300만 개)를 가지고 있지만.

샘플은 길이가 65,536인 시퀀스에 대해 사용 가능합니다.

대략 12kHz에서 생성된 오디오로 약 5초에 해당합니다.
샘플은 샘플링된 기간 동안 전반적인 일관성을 명확하게 보여주며, 다양한 연주 스타일과 톤을 보여줍니다. 리듬적인 연주에서 강력한 연주로 전환합니다. 샘플을 들으려면 https://openai.com/blog/sparse-transformer를 방문하세요. 모델 용량이 감소하여 시퀀스 길이가 길어질수록 샘플 품질이 빠르게 저하됩니다.

테이블 4. 고전적인 방식에서의 희소한 스트라이드 트랜스포머의 성능.

오디오 데이터셋 (12 kHz에서 µ-law로 인코딩된)의 시퀀스에 대한 함수로서

길이와 모델 크기.

시퀀스 길이 매개변수 바이트 당 비트

65,536     152M    1.97
65,536     152M    1.97

262,144     25M    2.17
262,144     25M    2.17

1,048,576   3M     2.99
1,048,576   3M     2.99

결론

빈번하지 않은 트랜스포머를 소개하고, 표준 트랜스포머보다 적은 연산을 필요로 하면서도 긴 시퀀스의 밀도 모델링에서 동등하거나 더 나은 성능을 보여줍니다. 이 성능은 이미지와 텍스트에서 최고 수준이며, 원시 데이터에 쉽게 적용할 수 있습니다.

오디오. 이 모델은 장기적인 문맥의 사용법을 보여주며, 전체적으로 일관된 샘플을 생성합니다.

9. 감사의 말씀

아시쉬 바스와니에게 통찰력 있는 토론에 감사드립니다.

프로젝트의 시작 단계에서의 토론에 참여해 주신 분들께 감사드립니다. 또한 Joshua Meier와 Mark Chen에게 유익한 토론에 감사드리며, Johannes Otterbach, Prafulla Dhariwal, 그리고 David Luan에게 이 논문 초안에 대한 피드백에 감사드립니다.

참고문헌

알-르푸, R., 초에, D., 콘스탄트, N., 구오, M., 그리고 존스, L. 깊은 셀프 어텐션을 사용한 문자 수준 언어 모델링. arXiv 사전 인쇄 arXiv:1808.04444, 2018.

바, J.L., 키로스, J.R., 그리고 힌튼, G.E. 레이어 정규화.
arXiv 사전 인쇄 arXiv:1607.06450, 2016.

브리츠, D., 관, M. Y., 그리고 루옹, M.-T. 고정 크기의 메모리 표현을 사용한 효율적인 주의력. arXiv 사전 인쇄 arXiv:1707.00110, 2017.

첸, T., 쉬, B., 장, C., 그리고 게스트린, C. 서브리니어 메모리 비용으로 딥 네트워크를 훈련시키기. arXiv 사전인쇄 arXiv:1604.06174, 2016.

첸, X., 미슈라, N., 로하니네자드, M., 그리고 아벨, P.
픽셀스네일: 개선된 자기회귀 생성 모델.
arXiv 사전 인쇄 arXiv:1712.09763, 2017.

Chiu, C.-C.와 Raffel, C. Monotonic chunkwise attention.
arXiv 사전 인쇄 arXiv:1712.05382, 2017.

Dai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le,
Q. V., and Salakhutdinov, R. Transformer-xl: 언어

장기 의존성을 가진 모델링. 2018년.

Dieleman, S., van den Oord, A., and Simonyan, K. 도전

실제 음악 생성의 어려움: 원시 오디오 모델링의 규모 확장. "신경 정보 처리 시스템의 진보"에서, 2018년, 8000-8010쪽.

게링, J., 아울리, M., 그랑지에, D., 야랏츠, D., 그리고 도핀, Y. N. 합성곱 시퀀스 대 시퀀스 학습. arXiv 사전 인쇄 arXiv:1705.03122, 2017.

Gruslys, A., Munos, R., Danihelka, I., Lanctot, M., 그리고 Graves, A. 기억 효율적인 시간 역전파. 신경 정보 처리 시스템 발전, 4125-4133쪽, 2016년.

그는 K., Zhang, X., Ren, S., 그리고 Sun, J. (2016)의 논문 "깊은 잔여 네트워크에서의 Identity mappings"을 인용하였다. arXiv 사전 인쇄 arXiv:1603.05027.

헨드릭스, D.와 김펠, K. 비선형성과 연결하기

가우시안 에러 선형 유닛을 사용한 확률적 정규화.
arXiv 논문 arXiv:1606.08415, 2016.
희소 트랜스포머를 사용하여 긴 시퀀스 생성하기.

황, C.-Z. A., 바스와니, A., 우스코레이트, J., 샤지어, N.,

호손, C., 다이, A. M., 호프만, M. D., 그리고 에크, D. 음악 생성에 적용된 변압기를 위한 개선된 상대적인 자기주의 메커니즘. arXiv 사전 인쇄 arXiv:1809.04281, 2018.

조제포비치, R., 비니얼스, O., 슈스터, M., 샤지어, N., 그리고 우, Y. 언어 모델링의 한계 탐구. arXiv 사전 인쇄 arXiv:1602.02410, 2016.

Kingma, D. P. 및 Dhariwal, P. Glow: Invertible 1x1 컨볼루션을 사용한 생성 플로우. 신경 정보 처리 시스템에서의 진보, 2018년, 10236-10245쪽.

Koutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. A
시계 장치 RNN. arXiv 사전 인쇄 arXiv:1402.3511, 2014.

류, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., 그리고 Shazeer, N. (2018). 긴 시퀀스를 요약하여 위키피디아 생성하기. arXiv 사전 인쇄 arXiv:1801.10198.

메리, S., 쿠마르, K., 굴라자니, I., 쿠마르, R., 제인, S.

Sotelo, J., Courville, A., 그리고 Bengio, Y. Samplernn: 무조건적인 end-to-end 신경망 오디오 생성 모델. arXiv 사전 인쇄 arXiv:1612.07837, 2016.

Menick, J. 및 Kalchbrenner, N. 서브스케일 픽셀 네트워크와 다차원 업스케일링을 사용하여 고품질 이미지 생성. arXiv 사전 인쇄 arXiv:1812.01608, 2018.

미치케비치우스, P., 나랑, S., 알벤, J., 디아모스, G., 엘센, E., 가르시아, D., 긴즈버그, B., 휴스턴, M., 쿠체프, O., 벵카테시, G., 외. 혼합 정밀도 훈련. arXiv 사전 인쇄 arXiv:1710.03740, 2017.

Oord, A. v. d., Kalchbrenner, N., and Kavukcuoglu, K. 픽셀 재귀 신경망. arXiv 사전 인쇄 arXiv:1601.06759, 2016.

파마르, N., 바스와니, A., 우스코레이트, J., 카이저, Ł., 샤지어, N., 그리고 쿠, A. 이미지 트랜스포머. arXiv 사전 인쇄 arXiv:1802.05751, 2018.

Radford, A., Narasimhan, K., Salimans, T., and Sutskever,
래드포드, A., 나라심한, K., 살리만스, T., 그리고 서츠키버,

언어 이해력 향상을 위한 생성적 사전 훈련. URL https://s3-us-west-2.ama-
zonaws.com/openai-assets/research-covers/language-
unsupervised/language understanding paper.pdf, 2018.

리드, S., Oord, A. v. d., Kalchbrenner, N., Colmenarejo, S. G., Wang, Z., Belov, D., 그리고 de Freitas, N. 병렬 다중 스케일 자기회귀 밀도 추정. arXiv 사전 인쇄 arXiv:1703.03664, 2017.

Salimans, T., Karpathy, A., Chen, X., 그리고 Kingma, D. P.
Pixelcnn++: 이산화된 로지스틱 혼합 우도와 다른 수정 사항을 통해 Pixelcnn을 개선하는 것. arXiv 사전 인쇄 arXiv:1701.05517, 2017.

Sukhbaatar, S., Weston, J., Fergus, R., 등. 엔드 투 엔드 메모리 네트워크. 신경 정보 처리 시스템의 발전에서, 2440-2448쪽, 2015년.

Van Den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and
Kavukcuoglu, K. Wavenet: 원시 오디오를 위한 생성 모델. CoRR abs/1609.03499, 2016.

바스와니, A., 샤지어, N., 파마르, N., 우스코레이트, J., 존스, L., 고메즈, A. N., 카이저, Ł., 그리고 폴로수킨, I. 주의력은 당신이 필요한 모든 것이다. 신경 정보 처리 시스템 발전에서, 5998-6008쪽, 2017년.

