연결주의 시간 분류: 반복 신경망을 사용하여 세분화되지 않은 시퀀스 데이터에 레이블 지정하기

알렉스 그레이브스1                                             alex@idsia.ch
산티아고 페르난데스1                                  santiago@idsia.ch
파우스티노 고메즈1                                          tino@idsia.ch
유르겐 슈미트휴버1,2                                juergen@idsia.ch
1 인공지능 연구소 다레 몰레 이스티튜토 (IDSIA), Galleria 2, 6928 Manno-Lugano, Switzerland
2 뮌헨 공과대학교 (TUM), Boltzmannstr. 3, 85748 Garching, Munich, Germany

요약

많은 현실 세계의 시퀀스 학습 작업은 노이즈가 있는 세그먼트되지 않은 입력 데이터로부터 레이블 시퀀스를 예측하는 것을 요구합니다. 예를 들어 음성 인식에서는 음향 신호가 단어나 하위 단위로 전사됩니다. 순환 신경망(RNN)은 이러한 작업에 적합해 보이는 강력한 시퀀스 학습기입니다. 그러나 사전 세그먼트된 훈련 데이터와 출력을 레이블 시퀀스로 변환하기 위한 후처리가 필요하기 때문에 그 적용 가능성은 지금까지 제한되어 왔습니다. 본 논문은 RNN을 직접 세그먼트되지 않은 시퀀스에 레이블을 지정하기 위한 새로운 방법을 제시하여 이 두 가지 문제를 해결합니다. TIMIT 음성 말뭉치에 대한 실험은 기준선 HMM과 하이브리드 HMM-RNN보다 이 방법의 장점을 입증합니다.

1. 소개

비분할된 시퀀스 데이터에 레이블을 지정하는 것은 현실 세계의 시퀀스 학습에서 흔한 문제입니다. 이는 특히 지각 작업 (예 : 필기 인식, 음성 인식, 동작 인식)에서 흔하며, 노이즈가 있는 실수 값 입력 스트림에 문자나 단어와 같은 이산 레이블의 문자열이 주석으로 달립니다.

현재, 그래픽 모델인 숨겨진 마르코프 모델(HMM; Rabiner, 1989), 조건부 랜덤 필드(CRF; Lafferty et al., 2001) 및 그들의 변형들은 시퀀스 라벨링을 위한 주요한 프레임워크입니다.

2006년 피츠버그에서 개최된 제23회 국제 기계 학습 컨퍼런스 논문에 등재되었습니다. 저작권은 2006년에 저자/소유자에게 있습니다.

벨링. 이러한 접근 방식은 많은 문제에서 성공적으로 증명되었지만, 몇 가지 단점이 있습니다: (1) 일반적으로 상당한 양의 작업 특정 지식이 필요합니다. 예를 들어, HMM의 상태 모델을 설계하거나 CRF의 입력 특징을 선택하는 것과 같은 것들; (2) 추론을 가능하게 하기 위해 명시적인 (그리고 종종 의문이 있는) 종속성 가정이 필요합니다. 예를 들어, HMM의 경우 관측치가 독립적이라는 가정; (3) 표준 HMM의 경우 훈련은 생성적이지만, 시퀀스 라벨링은 판별적입니다.

반복 신경망(RNN)은 반면에 데이터에 대한 사전 지식을 필요로하지 않습니다. 입력과 출력 표현의 선택 이상의 것은 필요하지 않습니다. RNN은 구분적으로 훈련될 수 있으며, 내부 상태는 시계열 모델링에 강력하고 일반적인 메커니즘을 제공합니다. 게다가, RNN은 시간적 및 공간적 잡음에 대해 강건하다는 경향이 있습니다.

지금까지는 시퀀스 라벨링에 RNN을 직접 적용하는 것이 불가능했습니다. 문제는 표준 신경망 목적 함수가 훈련 시퀀스의 각 지점마다 별도로 정의되어 있다는 것입니다. 다시 말해, RNN은 독립적인 라벨 분류 시리즈를 만들기 위해 훈련될 수 있습니다. 이는 훈련 데이터가 사전 분할되어야 하며, 네트워크 출력은 최종 라벨 시퀀스를 얻기 위해 후처리되어야 한다는 것을 의미합니다.

현재 시점에서 시퀀스 레이블링에 대한 RNN의 가장 효과적인 사용은 HMM과 결합하는 하이브리드 접근법(Bourlard & Morgan, 1994; Bengio, 1999)을 사용하는 것입니다. 하이브리드 시스템은 HMM을 사용하여 데이터의 장거리 순차 구조를 모델링하고, 신경망을 사용하여 지역화된 분류를 제공합니다. HMM 구성 요소는 훈련 중에 시퀀스를 자동으로 세분화하고, 네트워크 분류를 레이블 시퀀스로 변환할 수 있습니다. 그러나 Connectionist Temporal Classification의 앞서 언급한 단점을 상속받는 것과 마찬가지로, HMM 구성 요소도 일부 단점을 가지고 있습니다.

HMMs, 하이브리드 시스템은 시퀀스 모델링을 위한 RNN의 전체 잠재력을 활용하지 못합니다.

이 논문은 RNN을 사용하여 시퀀스 데이터에 레이블을 지정하는 새로운 방법을 제시한다. 이 방법은 사전 분할된 훈련 데이터와 후처리된 출력물의 필요성을 제거하고, 시퀀스의 모든 측면을 하나의 네트워크 아키텍처 내에서 모델링한다. 기본 아이디어는 네트워크 출력을 주어진 입력 시퀀스에 대한 가능한 모든 레이블 시퀀스에 대한 확률 분포로 해석하는 것이다. 이 분포를 통해 올바른 레이블링의 확률을 직접 최대화하는 목적 함수를 유도할 수 있다. 목적 함수가 미분 가능하기 때문에, 네트워크는 표준 시간 역전파를 통해 훈련될 수 있다 (Werbos, 1990).

다음에서는 미분류된 데이터 시퀀스를 시간 분류(Kadous, 2002)로 레이블링하는 작업에 대해 언급하며, 이를 위해 RNN을 사용하는 것을 연결주의적 시간 분류(CTC)라고 합니다. 반면에 입력 시퀀스의 각 시간 단계 또는 프레임을 독립적으로 레이블링하는 것을 프레임별 분류라고 합니다.

다음 섹션에서는 시간 분류를 위한 수학적 형식과 이 논문에서 사용된 오차 측정 방법을 제공합니다. 섹션 3에서는 RNN을 시간 분류기로 사용할 수 있게 하는 출력 표현을 설명합니다. 섹션 4에서는 CTC 네트워크를 훈련하는 방법을 설명합니다. 섹션 5에서는 TIMIT 음성 말뭉치에서 CTC를 하이브리드 및 HMM 시스템과 비교합니다. 섹션 6에서는 CTC와 다른 시간 분류기 사이의 주요 차이점을 논의하고, 향후 연구 방향을 제시합니다. 논문은 섹션 7로 마무리됩니다.

2. 시간적 분류

S가 고정된 분포 DX×Z에서 추출된 훈련 예제의 집합이라고 하자. 입력 공간 X = (Rm)∗는 m 차원 실수 벡터의 모든 시퀀스의 집합이다. 목표 공간 Z = L∗는 (유한한) 알파벳 L의 모든 시퀀스의 집합이다. 일반적으로, L∗의 요소를 레이블 시퀀스 또는 레이블링이라고 한다. S의 각 예제는 (x,z)라는 시퀀스 쌍으로 구성된다. 목표 시퀀스 z = (z1,z2,...,zU)는 입력 시퀀스 x = (x1,x2,...,xT)보다 길이가 최대 U ≤ T이다. 입력과 목표 시퀀스가 일반적으로 같은 길이가 아니기 때문에, 이들을 사전에 정렬하는 방법은 없다.

목표는 S를 사용하여 시간 분류기를 훈련시키는 것입니다.
h : X 7→ Z는 이전에 본 적 없는 입력 시퀀스를 분류하는 데 일부 작업 특정 오류 측정을 최소화하는 방식입니다.

2.1. 라벨 오류율

이 논문에서는 다음과 같은 오류 측정에 관심이 있습니다: 테스트 세트 S0 ⊂ DX×Z (S와 겹치지 않는)가 주어졌을 때, 시간 분류기 h의 레이블 오류율 (LER)을 정의합니다. 이는 S0에서의 분류 결과와 대상 간의 정규화된 편집 거리입니다. 즉,

LER(h,S0) = 1
Z
X

(x,z)∈S0
ED(h(x))  (1)

(x,z)은 S0에 속한다.
ED(h(x))  (1)

Z는 S0에서 대상 레이블의 총 수입니다.
ED(p,q)는 두 시퀀스 p와 q 사이의 편집 거리를 나타냅니다. 즉, p를 q로 변경하기 위해 필요한 최소한의 삽입, 대체 및 삭제 횟수입니다.

이것은 음성 또는 필기 인식과 같은 작업에 대한 자연스러운 측정 방법입니다. 이 방법은 전사 오류의 비율을 최소화하는 것을 목표로 합니다.

3. 연결주의적 시간 분류

이 섹션은 CTC에 사용되는 순환 신경망의 출력 표현을 설명합니다. 중요한 단계는 네트워크 출력을 레이블 시퀀스에 대한 조건부 확률 분포로 변환하는 것입니다. 그런 다음 네트워크는 주어진 입력 시퀀스에 대해 가장 가능성이 높은 레이블을 선택하여 분류기로 사용할 수 있습니다.

3.1. 네트워크 출력에서 라벨링으로

CTC 네트워크는 L에 있는 레이블보다 하나 더 많은 유닛을 가진 소프트맥스 출력 레이어(Bridle, 1990)를 가지고 있습니다. 첫 번째 |L| 유닛의 활성화는 특정 시간에 해당 레이블을 관찰할 확률로 해석됩니다. 추가 유닛의 활성화는 '공백' 또는 레이블이 없을 확률입니다. 이러한 출력들은 모든 가능한 레이블 시퀀스를 입력 시퀀스와 정렬하는 모든 가능한 방법의 확률을 정의합니다. 어떤 한 레이블 시퀀스의 총 확률은 다양한 정렬의 확률을 합산하여 찾을 수 있습니다.

길이 T의 입력 시퀀스 x에 대해 더 형식적으로, m개의 입력, n개의 출력 및 가중치 벡터 w를 가진 순환 신경망을 연속적인 맵 Nw로 정의합니다: (Rm)T 7→ (Rn)T. 네트워크 출력의 시퀀스를 y = Nw(x)로 정의하고, yt로 표기합니다.

k
활성화
시간 t에서의 출력 단위 k. 그런 다음 yt

k는 시간 t에서 레이블 k를 관찰할 확률로 해석되며, 이는 알파벳 L ∪ {blank} 상의 길이 T 시퀀스 집합 L0T에 대한 분포를 정의합니다.

p(π|x) = T Y t=1yt πt, ∀π ∈ L0T. (2)
연결주의 시간 분류

0 l - 영 엘
a b - 에이 비
e l - 이 엘
p r - 피 알
o - 오
b - 비
a - 에이
b i l i t y - 비 얼 리 티

1. 안녕하세요.
2. 오늘은 날씨가 좋네요.
3. 저는 한국어를 배우고 있어요.
4. 저는 음악을 좋아해요.
5. 저는 친구들과 함께 시간을 보내는 것을 좋아해요.

0

1. 안녕하세요, 저는 한국어를 배우고 있습니다.

나는 너를 사랑해

프레임별

소리

파형

CTC

죄송합니다. 제가 이해할 수 없는 문장입니다.

그림 1. 프레임별 및 CTC 네트워크가 음성 신호를 분류하는 것. 그림에서 그림자가 드는 선은 특정 시간에 음소를 관찰할 확률에 해당하는 출력 활성화를 나타냅니다. CTC 네트워크는 일련의 스파이크로 예측되는 음소 시퀀스만 예측하며, '빈칸' 또는 널 예측으로 구분됩니다. 한편, 프레임별 네트워크는 수동 분할(수직선)과 일치시키려고 시도합니다. 프레임별 네트워크는 세그먼트 경계를 잘못 정렬하면 (예: 'dh'와 같은 올바른 음소를 예측하더라도) 오류를 받습니다. 한 음소가 항상 다른 음소 옆에 발생하는 경우 (예: 클로저 'dcl'과 스톱 'd'), CTC는 이들을 두 번의 스파이크로 함께 예측하는 경향이 있습니다. 레이블링 선택은 CTC 출력에서 직접 읽을 수 있으며 (스파이크를 따라가세요), 프레임별 네트워크의 예측은 사용하기 전에 후처리되어야 합니다.

이제부터, 우리는 L0T의 요소들을 경로로 참조하며, 그것들을 π로 표기합니다.

(2)에 내재된 가정은 네트워크의 다른 시간에서의 출력이 네트워크의 내부 상태에 조건부로 독립적이라는 것입니다. 이는 출력 계층에서 자체 또는 네트워크로부터의 피드백 연결이 존재하지 않도록 요구함으로써 보장됩니다.

다음 단계는 다대일 맵 B를 정의하는 것입니다:
L0T 7→ L≤T, 여기서 L≤T는 가능한 라벨링의 집합입니다
(즉, 원래 라벨 알파벳 L을 사용하여 길이가 T 이하인 시퀀스의 집합). 
우리는 이를 단순히 경로에서 모든 공백과 반복된 라벨을 제거하여 수행합니다
(예: B(a − ab−) = B(−aa − −abb) = aab).
직관적으로, 이는 네트워크가 라벨을 예측하지 않는 상태에서 라벨을 예측하는 상태로 전환하거나
하나의 라벨을 다른 라벨로 예측할 때 새로운 라벨을 출력하는 것과 일치합니다
(그림 1의 CTC 출력과 비교). 마지막으로, 우리는 B를 사용하여 주어진 라벨링 l ∈ L≤T의 조건부 확률을 정의합니다
이에 해당하는 모든 경로의 확률의 합으로:

p(l|x) = X
π∈B−1(l)p(π|x). (3)

3.2. 분류기 구축

위의 공식에 따라서, 분류기의 출력은 입력 시퀀스에 대한 가장 가능성 있는 라벨링이어야 합니다.

h(x) = arg max
l∈L≤T
p(l|x).

h(x) = arg max
l∈L≤T
p(l|x).

HMM의 용어를 사용하여, 우리는 이 라벨링을 찾는 작업을 디코딩이라고 합니다. 불행히도, 우리 시스템에 대해 일반적이고 처리 가능한 디코딩 알고리즘은 알지 못합니다. 그러나 다음 두 가지 근사적인 방법은 실제로 좋은 결과를 제공합니다.

첫 번째 방법 (최적 경로 디코딩)은 가장 가능성 있는 경로가 가장 가능성 있는 라벨링과 일치할 것이라는 가정에 기반을 두고 있습니다.

h(x) ≈ B(π∗)        (4)
π∗ = arg max
π∈Nt
p(π|x).

h(x) ≈ B(π∗)        (4)
π∗ = arg max
π∈Nt
p(π|x).

최적 경로 디코딩은 계산하기 쉽습니다. 왜냐하면 π∗는 각 시간 단계에서 가장 활발한 출력들을 연결한 것이기 때문입니다. 그러나 가장 확률이 높은 라벨링을 찾는 것은 보장되지 않습니다.

두 번째 방법 (접두사 검색 디코딩)은 섹션 4.1의 전진-후진 알고리즘을 수정함으로써 라벨링 접두사의 연속적인 확장의 확률을 효율적으로 계산할 수 있다는 사실에 의존한다 (그림 2).

충분한 시간이 주어진다면, 접두사 검색 디코딩은 항상 가장 가능성이 높은 라벨링을 찾을 수 있습니다. 그러나 입력 시퀀스의 길이에 따라 확장해야 하는 접두사의 최대 개수는 기하급수적으로 증가합니다. 출력 분포가 최빈값 주변에 충분히 집중되어 있다면, 합리적인 시간 내에 완료될 것입니다. 그러나 이 논문의 실험을 위해서는 추가적인 휴리스틱이 필요했습니다.

훈련된 CTC 네트워크의 출력을 관찰하면

그림 2. 알파벳 X, Y에 대한 접두사 검색 디코딩. 각 노드는 부모 노드에서 접두사를 종료('e')하거나 확장합니다. 확장 노드 위의 숫자는 해당 접두사로 시작하는 모든 라벨링의 총 확률입니다. 종료 노드 위의 숫자는 해당 부모에서 끝나는 단일 라벨링의 확률입니다. 각 반복에서 가장 확률이 높은 남은 접두사의 확장이 탐색됩니다. 검색은 단일 라벨링('XY' 여기서)이 남은 접두사보다 더 확률이 높을 때 종료됩니다.

일련의 강한 예측 공백으로 구분된 스파이크들을 형성하는 경향이 있습니다 (그림 1). 이를 위해 출력 시퀀스를 공백으로 시작하고 끝날 가능성이 매우 높은 섹션으로 나눕니다. 이를 위해 공백 레이블을 관찰할 확률이 특정 임계값 이상인 경계점을 선택합니다. 그런 다음 각 섹션에 대해 가장 가능성이 높은 레이블을 계산하고 이를 연결하여 최종 분류를 얻습니다.

실제로는, 이 휴리스틱과 함께 접두사 검색은 잘 작동하며, 일반적으로 최적 경로 디코딩보다 우수한 성능을 보입니다. 그러나 경우에 따라 실패할 수도 있습니다. 예를 들어, 동일한 레이블이 섹션 경계의 양쪽에서 약하게 예측된 경우입니다.

4. 네트워크 훈련하기

지금까지 우리는 CTC를 위해 RNN을 사용할 수 있게 해주는 출력 표현을 설명해왔습니다. 이제 우리는 경사 하강법을 사용하여 CTC 네트워크를 훈련하기 위한 목적 함수를 유도합니다.

사전 확률의 원리에서 목적 함수가 유도된다. 즉, 이를 최소화함으로써 대상 레이블의 로그 우도를 최대화한다. 주의할 점은 이것이 표준 신경망 목적 함수의 기본 원리와 동일하다는 것이다 (Bishop, 1995). 목적 함수와 네트워크 출력에 대한 도함수가 주어지면, 표준 시간 역전파를 통해 가중치 그래디언트를 계산할 수 있다. 그런 다음 네트워크는 현재 사용 가능한 기울기 기반 최적화 알고리즘으로 훈련될 수 있다.

신경망에 사용되는 (LeCun et al., 1998; Schraudolph, 2002).

우리는 최대 우도 함수에 필요한 알고리즘으로 시작합니다.

4.1. CTC 전진-후진 알고리즘

우리는 개별 라벨링의 조건부 확률 p(l|x)을 효율적으로 계산하는 방법이 필요합니다. 처음 보면 (3)은 이것이 문제가 될 것으로 보입니다: 합은 주어진 라벨링에 해당하는 모든 경로에 대해 이루어지며, 일반적으로 이러한 경로는 매우 많습니다.

다행히도 이 문제는 동적 프로그래밍 알고리즘을 사용하여 해결할 수 있습니다. 이 알고리즘은 HMMs (Rabiner, 1989)의 forward-backward 알고리즘과 유사합니다. 핵심 아이디어는 레이블링에 해당하는 경로의 합을 해당 레이블링의 접두사에 해당하는 경로의 반복적인 합으로 분해할 수 있다는 것입니다. 그런 다음 재귀적인 forward 및 backward 변수를 사용하여 이러한 반복을 효율적으로 계산할 수 있습니다.

길이가 r인 일련의 q에 대해, q1:p와 qr−p:r은 각각 첫 번째와 마지막 p개의 기호를 나타낸다. 그런 다음 라벨링 l에 대해, 시간 t에서 l1:s의 총 확률을 나타내는 전방 변수 αt(s)를 정의한다. 즉.

αt(s) def = X
π∈NT:
B(π1:t)=l1:s
t
Y t0=1yt0 πt0. (5)

αt(s) 정의 = X
π∈NT:
B(π1:t)=l1:s
t
Y t0=1yt0 πt0. (5)

우리가 볼 것처럼, αt(s)는 αt−1(s)와 αt−1(s − 1)로부터 재귀적으로 계산될 수 있습니다.

출력 경로에 공백을 허용하기 위해, 우리는 수정된 라벨 시퀀스 l0을 고려합니다. l0에는 시작과 끝에 공백이 추가되고, 각 라벨 쌍 사이에도 삽입됩니다. l0의 길이는 따라서 2|l| + 1입니다. l0의 접두사들의 확률을 계산할 때, 우리는 공백과 비공백 라벨 사이의 모든 전이, 그리고 서로 다른 비공백 라벨 쌍 사이의 전이를 허용합니다. 모든 접두사들은 공백(b)이나 l의 첫 번째 심볼(l1)로 시작할 수 있습니다.

이는 초기화에 대한 다음 규칙을 우리에게 제공합니다.

α1(1) = y1
b
α1(2) = y1
l1
α1(s) = 0, ∀s > 2

그리고 재귀

αt(s) = 
(
¯ αt(s)yt
l0s
만약 l0
s
= b 이거나 l0
s−2
= l0
s (cid:0) ¯ αt(s) + αt−1(s − 2)(cid:1) yt l0s 아니면

(6)
어디에
¯ αt(s) def = αt−1(s) + αt−1(s − 1). (7)
연결주의적 시간 분류

그림 3. 'CAT' 라벨에 적용된 전진-후진 알고리즘의 설명. 검은 원은 라벨을 나타내고, 흰 원은 공백을 나타냅니다. 화살표는 허용된 전이를 나타냅니다. 전진 변수는 화살표의 방향으로 업데이트되고, 후진 변수는 그에 반대로 업데이트됩니다.

주의하세요, αt(s) = 0 ∀s < |l0| − 2(T − t) − 1이라는 것은
이러한 변수들이 시퀀스를 완료하기에 충분한 시간 단계가 남아있지 않은 상태에 해당한다는 것을 의미합니다
(그림 3의 오른쪽 상단에 있는 연결되지 않은 원들).
또한 αt(s) = 0 ∀s < 1입니다.

l의 확률은 그 때의 l0의 총 확률의 합이다. 시간 T에서 마지막 공백이 있는 경우와 없는 경우의 l0의 확률을 더한 것이다.
p(l|x) = αT(|l0|) + αT(|l0| − 1). (8)

비슷하게, 역방향 변수 βt(s)는 시간 t에서 l s:|l|의 총 확률로 정의된다.

βt(s) 정의 = X
π∈NT:
B(πt:T) = ls:|l|
T Y t0 = tyt0
πt0
(9)

βT(|l0|) = yT
b

βT(|l0|) = yT
b

βT(|l0| − 1) = yT
l|l|
βT(s) = 0, ∀s < |l0| − 1

βt(s) = (¯ βt(s)yt l0s if l0 s = b or l0 s+2 = l0 s (cid:0)¯ βt(s) + βt+1(s + 2)(cid:1) yt

βt(s) = (¯ βt(s)yt l0s 만약 l0 s = b 또는 l0 s+2 = l0 s (cid:0)¯ βt(s) + βt+1(s + 2)(cid:1) yt

l0s
그렇지 않으면
(10)
어디에
¯ βt(s) def = βt+1(s) + βt+1(s + 1). (11)

주의하세요, βt(s) = 0 ∀s > 2t (그림 3의 왼쪽 아래에 있는 연결되지 않은 원들) 그리고 ∀s > |l0|.

실제로는, 위의 재귀는 곧 어떤 디지털 컴퓨터에서도 언더플로우를 초래할 것입니다. 피하는 한 가지 방법은-

이는 전방 및 후방 변수를 재조정하는 것입니다 (Rabiner, 1989). 만약 우리가 정의한다면

Ct def = X
s
αt(s),  ˆ αt(s) def = αt(s)

Ct

그리고 (6)와 (7)의 RHS에서 α를 ˆ α로 대체하면, forward 변수는 계산 범위 내에 유지됩니다. 마찬가지로, backward 변수에 대해서도 정의합니다.

Dt def = X
s
βt(s),   ˆ βt(s) def = βt(s)

Dt 정의 = X
s
βt(s),   ˆ βt(s) 정의 = βt(s)

Dt

그리고 (10)과 (11)의 RHS에는 ˆ β 대신에 β를 대입하십시오.

최대 우도 오차를 평가하기 위해서는 대상 레이블 확률의 자연로그가 필요합니다. 재조정된 변수로 이들은 특히 간단한 형태를 가지고 있습니다.

ln(p(l|x)) = ln(주어진 x에 대한 l의 확률의 자연로그)

t=1
ln(Ct)

4.2. 최대 우도 훈련

최대 우도 훈련의 목표는 훈련 세트에서 모든 올바른 분류의 로그 확률을 동시에 최대화하는 것입니다. 우리의 경우, 다음 목적 함수를 최소화하는 것을 의미합니다.

OML(S,Nw) = − X
(x,z)∈S
ln p(z|x) (12)

경사 하강법으로 네트워크를 훈련시키기 위해서는 네트워크 출력에 대해 (12)를 미분해야 합니다. 훈련 예제들이 독립적이기 때문에 우리는 각각을 개별적으로 고려할 수 있습니다.

∂OML({(x,z)},Nw) ∂yt k = −∂ln(p(z|x)) ∂yt k (13)

우리는 이제 4.1절의 알고리즘을 사용하여 (13)을 계산하는 방법을 보여줍니다.

주요 포인트는 라벨링 l에 대해, 주어진 s와 t에서 전방 변수와 후방 변수의 곱은 l에 해당하는 모든 경로들 중에서 시간 t에서 심볼 s를 통과하는 경로들의 확률입니다. 더 정확히는, (5)와 (9)에서는 다음과 같습니다.

αt(s)βt(s) = X
π∈B−1(l):
πt=l0
s
yt
l0s
T
Y t=1yt πt.

αt(s)βt(s) = X
π∈B−1(l):
πt=l0
s
yt
l0s
T
Y t=1yt πt.

(2)로부터 재배열하고 대체하면 다음과 같다.

αt(s)βt(s)
yt l0s
=
X

π∈B−1(l): π는 B의 역함수(l)에 속한다.
πt=l0: πt는 l0와 같다.
s: s
p(π|x): π가 주어진 x에 대해 확률 p를 가진다.
Connectionist Temporal Classification: 연결주의적 시계열 분류

(3)에서 우리는 l0를 통과하는 경로들로 인해 전체 확률 p(l|x)의 일부분임을 알 수 있습니다.

t 시간에 s. 모든 t에 대해, 우리는 따라서 모든 s를 합할 수 있습니다.

p(l|x) = 
|l0|
X

s=1
αt(s)βt(s)
yt
l0s
.     (14)

s=1
αt(s)βt(s)
yt
l0s
.     (14)

yt k 에 대해 이를 구별하기 위해서는
시간 t 에 라벨 k 를 지나는 경로만을 고려해야 합니다.
동일한 라벨 (또는 공백)이 단일 라벨링 l 에 대해 여러 번 반복될 수 있으므로,
라벨 k 가 발생하는 위치의 집합을 다음과 같이 정의합니다.
lab(l,k) = {s : l0

s = k}, 이는 비어 있을 수도 있습니다.
그런 다음 (14)를 미분하여 다음과 같이 얻습니다.

∂p(l|x) = ∂yt k = 1 yt k2 X s∈lab(l,k)αt(s)βt(s). (15)

관찰하다

∂ln(p(l|x)) = ∂ln(p(l|x))
∂yt = ∂yt
k = k
= 1
p(l|x) = 1
∂p(l|x) = ∂p(l|x)
∂yt = ∂yt
k = k

우리는 l = z로 설정하고 (8)과 (15)를 (13)에 대입하여 목적 함수를 미분할 수 있습니다.

마지막으로, 소프트맥스 레이어를 통해 그래디언트를 역전파하기 위해서는 정규화되지 않은 출력 ut k에 대한 목적 함수 도함수가 필요합니다.

4.1 섹션의 재조정을 사용하면 다음과 같습니다.

∂OML({(x,z)},Nw)을 한국어로 번역해주세요.
∂ut k
= yt
k
− 1
yt kZt
X를 한국어로 번역해주세요.

s∈lab(z,k)
ˆ αt(s)ˆ βt(s)

s는 lab(z,k)에 속한다.
ˆ αt(s)ˆ βt(s)

(16)
어디에
Zt def = |l0| X
s=1
ˆ αt(s)ˆ βt(s)
yt
l0s
.

Eqn (16)은 훈련 중에 네트워크가 받는 '오류 신호'입니다 (그림 4).

5. 실험

우리는 CTC의 성능을 HMM과 HMM-RNN 하이브리드와 비교했습니다. 이 비교는 실제 시간 분류 문제인 TIMIT 음성 말뭉치에서 음성 라벨링을 기준으로 이루어졌습니다. 더 정확히 말하면, TIMIT 테스트 세트의 발화를 가장 낮은 라벨 오류율을 가진 음소 시퀀스로 주석을 달아야 하는 작업이었습니다 (2.1절에서 정의된 대로).

비교를 공정하게 하기 위해 CTC와 하이브리드 네트워크는 동일한 RNN 아키텍처를 사용했습니다: 양방향 Long Short-Term Memory (BLSTM; Graves & Schmidhuber, 2005). BLSTM은 능력을 결합합니다.

(b)

오류 출력
(c)
(a)

그림 4. 훈련 중 CTC 오류 신호의 진화. 왼쪽 열은 훈련의 다양한 단계에서 동일한 시퀀스의 출력 활성화를 보여줍니다 (점선은 '공백' 유닛입니다). 오른쪽 열은 해당 오류 신호를 보여줍니다. 수평 축 위의 오류는 해당 출력 활성화를 증가시키고, 수평 축 아래의 오류는 해당 출력 활성화를 감소시킵니다. (a) 초기에는 네트워크가 작은 무작위 가중치를 가지며, 오류는 대상 시퀀스에 의해 결정됩니다. (b) 네트워크가 예측을 시작하면 오류가 그 주변에 집중됩니다. (c) 네트워크가 올바른 레이블을 강하게 예측하고 오류가 사실상 사라집니다.

장기 단기 기억(LSTM; Hochreiter & Schmidhuber, 1997)의 사용으로 긴 시간 지연을 극복하고 양방향 RNN(BRNNs; Schuster & Paliwal, 1997)의 과거와 미래 문맥에 접근합니다. 다른 아키텍처를 사용할 수도 있다는 점을 강조합니다. 우리는 같은 작업에서 표준 BRNN과 단방향 네트워크로 수행한 실험 결과가 더 나쁜 결과를 보였기 때문에 BLSTM을 선택했습니다.

5.1. 데이터

TIMIT는 영어 발화를 유도한 녹음을 포함하고 있으며, 수동으로 분할된 음성적인 전사를 동반합니다. 이는 61개의 구별되는 음소로 이루어진 어휘를 가지고 있으며, 훈련 세트와 테스트 세트로 나뉘어져 있으며 각각 4620개와 1680개의 발화를 포함합니다. 훈련 세트의 5% (184개)는 무작위로 선택되어 하이브리드 및 CTC 실험에서 조기 중지를 위한 검증 세트로 사용되었습니다. 오디오 데이터는 10ms 프레임으로 전처리되었으며, 5ms로 겹쳐지며, 26개의 필터 뱅크 채널에서 12개의 Mel-Frequency Cepstrum Coefficients (MFCCs)를 사용했습니다. 로그 에너지와 모든 계수의 첫 번째 미분도 포함되어 있으며, 총 1프레임당 26개의 계수 벡터를 제공합니다. 이러한 계수는 훈련 세트에서 평균이 0이고 표준 편차가 1이 되도록 개별적으로 정규화되었습니다.
연결주의적 시간 분류 (CTC)

5.2. 실험 설정

CTC 네트워크는 확장된 BLSTM 아키텍처를 사용했습니다. 이 아키텍처에는 피플홀과 포게이트가 있었고 (Gers et al., 2002), 각각의 순방향 및 역방향 은닉층에 100개의 블록이 있었습니다. 입력 및 출력 셀 활성화 함수로는 쌍곡탄젠트를 사용하였으며, 게이트에는 로지스틱 시그모이드 함수가 [0,1] 범위에서 사용되었습니다.

은닉층은 자신과 출력층과 완전히 연결되었으며, 입력층에서 완전히 연결되었습니다. 입력층의 크기는 26이었고, 소프트맥스 출력층의 크기는 62였습니다 (61개의 음운 범주와 빈 레이블 포함), 그리고 총 가중치의 수는 114,662였습니다.

훈련은 시간을 통한 역전파와 온라인 경사 하강법(각 훈련 예제 후 가중치 업데이트)을 사용하여 수행되었습니다. 학습률은 10^-4이고 모멘텀은 0.9로 설정되었습니다. 네트워크 활성화는 각 예제의 시작 시 0으로 재설정되었습니다. 접두사 검색 디코딩(3.2절)을 위해 빈 확률 임계값은 99.99%로 설정되었습니다. 가중치는 [-0.1, 0.1] 범위의 균일한 무작위 분포로 초기화되었습니다. 훈련 중에는 일반화를 향상시키기 위해 입력에 표준 편차가 0.6인 가우시안 노이즈가 추가되었습니다.

기준 HMM과 하이브리드 시스템은 (Graves et al., 2005)에서와 같이 구현되었습니다. 간단히 말해서, 기준 HMM은 문맥 독립 및 문맥 의존적인 세 상태의 왼쪽에서 오른쪽으로 모델을 가진 HMM으로 HTK Toolkit1을 사용하여 훈련 및 테스트되었습니다. 관측 확률은 가우시안 혼합으로 모델링되었습니다. 가우시안의 수와 삽입 페널티는 작업에서 최상의 성능을 얻기 위해 선택되었습니다. 언어 정보나 부분적인 전화 시퀀스의 확률은 시스템에 포함되지 않았습니다. 총 900,000개 이상의 매개변수가 있었습니다.

하이브리드 시스템은 HMM과 BLSTM 네트워크로 구성되었으며, Viterbi 기반의 강제 정렬(Robinson, 1994)을 사용하여 훈련되었습니다. 61개의 모델에 대한 전이 및 사전 확률의 초기 추정은 훈련 세트의 올바른 전사를 사용하여 수행되었습니다. 네트워크 출력 확률은 사전 확률로 나누어 HMM의 가능도를 얻었습니다. 삽입 페널티는 작업에서 최상의 성능을 얻기 위해 선택되었습니다.

BLSTM 아키텍처와 매개변수는 CTC에 사용된 것과 동일하며, 다음과 같은 예외가 있었습니다: (1) 하이브리드 네트워크의 학습률은 10^-5였습니다; (2) 주입된 잡음의 표준 편차는 0.5였습니다; (3) 출력 레이어는 62 대신 61개의 유닛을 가지고 있었습니다.

1. http://htk.eng.cam.ac.uk/

테이블 1. TIMIT에서의 레이블 오류율 (LER). CTC 및 하이브리드 결과는 5회 실행의 평균값이며, ± 표준 오차입니다. 가중 오류 BLSTM/HMM과 CTC (최적 경로) 사이를 제외하고 모든 차이는 유의미했습니다 (p < 0.01).

시스템             LER

문맥에 독립적인 HMM 38.85%
문맥에 의존적인 HMM 35.21%
BLSTM/HMM 33.84 ± 0.06%
가중 오류 BLSTM/HMM 31.57 ± 0.06%
CTC (최적 경로) 31.47 ± 0.21%
CTC (접두사 탐색) 30.51 ± 0.19%

(no blank label). 소음과 학습률은 두 시스템에 대해 독립적으로 설정되었으며, 매개변수 공간에서 대략적인 탐색을 거쳤다. 혼합 네트워크는 총 114,461개의 가중치를 가지고 있었으며, HMM은 추가로 183개의 매개변수를 추가했다. 가중 오차 실험에서 오차 신호는 긴 음소와 짧은 음소에 동일한 가중치를 부여하기 위해 스케일링되었다 (Robinson, 1991).

5.3. 실험 결과

표 1의 결과는 접두사 검색 디코딩을 사용할 때 CTC가 기준선 HMM 인식기와 동일한 RNN 구조를 가진 HMM-RNN 하이브리드보다 우수한 성능을 보였음을 보여줍니다. 또한, 최적 경로 디코딩보다 접두사 검색이 약간의 개선을 보여줍니다.

최상의 하이브리드 결과는 가중 오류 신호로 얻어졌습니다. 이러한 휴리스틱은 CTC에는 필요하지 않습니다. CTC의 목적 함수는 레이블의 순서에만 의존하며, 그들의 지속 시간이나 분할에는 의존하지 않습니다.

입력 잡음은 CTC에 대한 일반화에 하이브리드 시스템보다 더 큰 영향을 미쳤으며, CTC에는 더 높은 수준의 잡음이 최적임이 확인되었습니다.

6. 토론과 향후 작업

CTC와 다른 시간 분류기들과의 주요 차이점은 CTC가 입력 시퀀스를 명시적으로 분할하지 않는다는 것입니다. 이는 몇 가지 이점을 가지고 있습니다. 예를 들어, 음성이나 필기에서 본질적으로 모호한 레이블 경계를 찾을 필요가 없어지며, 여러 레이블이 함께 자주 발생하는 경우 레이블 예측을 함께 그룹화할 수 있습니다. 어쨌든, 분할을 결정하는 것은 레이블 시퀀스만 필요한 경우 모델링 노력의 낭비입니다.

분할이 필요한 작업(예: 단백질 이차 구조 예측)의 경우 CTC를 사용하는 것은 문제가 될 수 있습니다. 그러나 그림 1에서 볼 수 있듯이 CTC는 각 레이블 예측을 자연스럽게 정렬하는 경향이 있습니다. 연결주의 시간적 분류

해당 시퀀스 부분과 일치하는 위치에 대한 주석을 추가하십시오. 이렇게 하면 키워드 감지와 같은 작업에 적합해질 것입니다. 근사적인 분할이 충분한 경우입니다.

CTC의 또 다른 특징은 레이블 간의 의존성을 명시적으로 모델링하지 않는다는 것이다. 이는 그래픽 모델과 대조적인 점이다. 그래픽 모델에서는 일반적으로 레이블이 k차 마르코프 체인을 형성한다고 가정한다. 그러나 CTC는 암시적으로 레이블 간의 의존성을 모델링한다. 예를 들어, 자주 함께 발생하는 레이블을 이중 스파이크로 예측함으로써 의존성을 암시적으로 모델링한다 (그림 1 참조).

구조화된 데이터를 다루는 하나의 일반적인 방법은 시간적 분류기의 계층 구조입니다. 여기서 한 수준의 라벨링(예: 글자)은 다음 수준의 라벨링(예: 단어)의 입력이 됩니다. 계층적 CTC에 대한 예비 실험은 격려되었으며, 우리는 이 방향을 더 탐구할 의도입니다.

최대 우도 훈련에서는 항상 좋은 일반화가 어렵지만, CTC의 경우 특히 그렇습니다. 앞으로도 가중치 감쇠, 부스팅 및 여유 최대화와 같은 과적합을 줄이기 위한 방법을 계속해서 탐구할 것입니다.

결론

우리는 RNN을 사용한 새로운 일반적인 시간 분류 방법을 소개했습니다. 우리의 방법은 기존의 신경망 분류기 프레임워크에 자연스럽게 적용되며, 동일한 확률 원리에서 유도되었습니다. 이 방법은 사전 분할된 데이터의 필요성을 없애고, 네트워크를 직접 시퀀스 레이블링을 위해 훈련할 수 있게 합니다. 더욱이, 특정 작업 지식을 요구하지 않으면서도, 실제 시간 분류 문제에서 HMM과 HMM-RNN 하이브리드보다 우수한 성능을 보였습니다.

감사의 말씀

우리는 유용한 수학적 토론을 위해 Marcus Hutter에게 감사드립니다. 이 연구는 SNF 보조금 200021-111968/1 및 200020-107534/1로 지원되었습니다.

참고문헌

Bengio., Y. (1999). 순차 데이터에 대한 마르코프 모델. 신경 컴퓨팅 조사, 2, 129-162.

주교, C. (1995). 패턴 인식을 위한 신경망, 6장. 옥스포드 대학 출판사, 주식회사.

Bourlard, H., & Morgan, N. (1994). 연결주의 음성인식: 하이브리드 접근 방식. Kluwer 학술 출판사.

Bridle, J. (1990). 전방향 분류 네트워크 출력의 확률적 해석.

통계적 패턴 인식과의 관계에 대한 연구입니다. F. Soulie와 J. Herault (편집), Neurocomputing: 알고리즘, 아키텍처 및 응용, 227-236쪽. Springer-Verlag.

Gers, F., Schraudolph, N., & Schmidhuber, J. (2002).
LSTM 순환 신경망을 사용하여 정확한 타이밍을 학습하기. 기계 학습 연구 저널, 3, 115-143.

Graves, A., Fernández, S., & Schmidhuber, J.
(2005). 개선된 음소 분류 및 인식을 위한 양방향 LSTM 네트워크. 2005년 국제 인공 신경망 컨퍼런스 논문집. 폴란드, 바르샤바.

Graves, A., & Schmidhuber, J. (2005). 양방향 LSTM과 다른 신경망 구조를 사용한 프레임별 음소 분류. 신경망, 18, 602-610.

Hochreiter, S., & Schmidhuber, J. (1997). 장기 단기 기억. 신경 계산, 9, 1735–1780.

카도우스, M. W. (2002). 시간 분류: 다변량 시계열에 분류 패러다임을 확장하는 것. 박사학위 논문, 컴퓨터 과학 및 공학 학부, 뉴사우스웨일즈 대학교.

Lafferty, J., McCallum, A., & Pereira, F. (2001). 조건부 랜덤 필드: 시퀀스 데이터의 분할과 레이블링을 위한 확률 모델. 제18회 국제 기계 학습 컨퍼런스 논문집 (pp. 282-289). Morgan Kaufmann, 샌프란시스코, CA.

LeCun, Y., Bottou, L., Orr, G., & Muller, K. (1998).
효율적인 역전파. 신경망: 전문가의 노하우. 스프링거.

라비너, L. R. (1989). 음성 인식에서의 숨겨진 마르코프 모델과 선택된 응용에 대한 튜토리얼. IEEE 학회 논문집 (pp. 257-286). IEEE.

로빈슨, A. J. (1991). 재발 에러 전파 네트워크 전화 인식 시스템에 대한 여러 개선 사항 (기술 보고서 CUED/F-INFENG/TR82). 캠브리지 대학교.

로빈슨, A. J. (1994). 전화 확률 추정에 순환 신경망의 응용. IEEE 신경망 트랜잭션, 5, 298-305.

슈라우돌프, N. N. (2002). 두 번째 순서 경사 하강을 위한 빠른 곡률 행렬-벡터 곱셈. 신경 계산, 14, 1723-1738.

슈스터, M., & 파리왈, K. K. (1997). 양방향 순환 신경망. IEEE 신호처리 트랜잭션, 45, 2673-2681.

Werbos, P. (1990). 시간을 통한 역전파: 
그것이 무엇을 하는지와 어떻게 하는지. 
IEEE 학회 논문집, 78, 1550 - 1560.

