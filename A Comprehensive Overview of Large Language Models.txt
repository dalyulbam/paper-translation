사전 인쇄

대규모 언어 모델에 대한 포괄적인 개요

훔자 나비드1, 아사드 울라 칸1,∗, 시 큐2,∗, 무함마드 사킵3,4,∗,
사이드 안와르5,6, 무함마드 우스만5,6, 나비드 아크타르7, 닉 바네스8, 아즈말 미안9

1. 파키스탄 라호르 공과대학교 (UET)
2. 중국 홍콩 중문 대학교 (CUHK)
3. 호주 시드니 공과대학교 (UTS)
4. 호주 시드니 공동연구기구 (CSIRO)
5. 사우디 아라비아 다하란 왕파드 석유 및 광물 대학교 (KFUPM)
6. 다하란 왕파드 인공지능 공동연구센터 (JRCAI)

7멜버른 대학교 (UoM), 멜버른, 호주
8오스트레일리아 국립 대학교 (ANU), 캔버라, 호주
9서부 오스트레일리아 대학교 (UWA), 퍼스, 호주

요약—
최근에는 대형 언어 모델(Large Language Models, LLMs)이 자연어 처리 작업 및 그 이상의 능력을 보여주고 있다. LLMs의 이러한 성공은 이 분야에서의 다양한 주제에 대한 연구 기여의 큰 증가로 이어졌다. 이러한 연구들은 아키텍처 혁신, 훈련 전략 개선, 문맥 길이 개선, 세부 조정, 다중 모달 LLMs, 로봇 공학, 데이터셋, 벤치마킹, 효율성 등 다양한 주제를 포함하고 있다. LLM 연구 기술의 신속한 발전과 정기적인 돌파구로 인해 이 분야의 발전 상황을 파악하는 것은 상당히 어려워졌다. LLM에 관한 다양한 문헌이 급속하게 등장하고 있으므로, 연구 커뮤니티가 이 분야의 최근 발전에 대한 간결하면서도 포괄적인 개요를 활용할 수 있어야 한다. 본 논문은 LLM 관련 개념의 기존 문헌에 대한 개요를 제공한다. 우리의 자체 포괄적인 LLM 개요는 관련 배경 개념을 논의하면서 LLM 연구의 최신 주제를 다루고 있다. 이 문헌 검토 논문은 LLM 연구를 진전시키기 위해 기존 작업의 포괄적인 정보 요약에서 통찰력을 얻기 위한 체계적인 조사뿐만 아니라 빠른 포괄적인 참고 자료를 제공하기 위한 것이다.

색인 용어 - 대형 언어 모델, LLM, chatGPT, 증강된 LLM, 다중 모달 LLM, LLM 훈련, LLM 벤치마킹

I. 소개

언어는 인간들의 의사소통과 자기 표현을 원활하게 도와주며, 기계와의 상호작용에도 중요한 역할을 합니다. 일반화된 모델의 필요성은 다음과 같은 이유에서 비롯됩니다.

동등한 기여를 위한 것입니다.
연락 이메일: humza_naveed@yahoo.com
이메일: humza_naveed@yahoo.com, aukhanee@gmail.com,
shiqiu@cse.cuhk.edu.hk, muhammad.saqib@data61.csiro.au,
saeed.anwar@kfupm.edu.sa, muhammad.usman@kfupm.edu.sa,
naveed.akhtar1@unimelb.edu.au, nick.barnes@anu.edu.au,
ajmal.mian@uwa.edu.au
저장소: https://github.com/humza909/LLM_Survey.git

그림 1: 연도별 도입된 LLM 모델 수의 추이.

복잡한 언어 작업을 처리하는 기계에 대한 수요가 증가하고 있으며, 번역, 요약, 정보 검색, 대화 상호작용 등을 포함합니다. 최근에는 언어 모델에서 상당한 발전이 이루어졌으며, 이는 주로 트랜스포머 [1], 증가된 계산 능력 및 대규모 훈련 데이터의 가용성에 기인합니다. 이러한 발전은 다양한 작업에서 인간 수준의 성능을 근사화할 수 있는 LLMs (Large Language Models)의 생성을 가능하게 하여 혁명적인 변화를 가져왔습니다 [2], [3]. LLMs는 일관된 커뮤니케이션을 가진 텍스트를 처리하고 생성할 수 있는 최첨단 인공지능 시스템으로 등장하였으며, 여러 작업에 일반화할 수 있습니다 [4], [5], [6].
자연어 처리(NLP)에서의 역사적인 진보는 통계적 언어 모델링에서 신경망 언어 모델링으로 진화하였으며, 그 후에는 사전 훈련된 언어 모델(PLMs)에서 LLMs로 진화하였습니다. 전통적인 언어 모델링(LM)은 작업별로 훈련되지만, LLMs는 다양한 작업에 일반화할 수 있습니다.

2019년       2020년        2021년       2022년        2023년        2024년
10월 T5

5월 GPT-3
10월 mT5     4월 PanGu-α

준 CPM-2

줄 코덱스

어니 3.0

8월 쥬라기-1

세프 하이퍼클로바

옥트 유안 1.0
T0

디씨 고퍼

ERNIE 3.0 타이탄

GLaM

LaMDA

웹지피티
Jan MT-NLG

2월 알파코드
3월 코드젠

친칠라
4월 GPT-NeoX-20B

PaLM
Tk-Instruct
May UL2
OPT

팜
Tk-Instruct
메이 UL2
OPT

8월 AlexaTM

9월 스패로우
10월 GLM

U-PaLM

플랜유팜

11월 BLOOM
갤랙티카
mT0

챗GPT
12월 OPT-IML

2월 LLaMA

마르 판구-Σ

블룸버그 GPT

GPT-4
알파카
비쿠냐

클로드

바드
4월 화투오
위자드LM
코알라
5월 선원 2.0
스타코더
코드T5+
염소
MPT
6월 위자드코더
7월 람마 2
8월 코드 람마

잔

그림 2: LLM 출시의 연대적 표시: 연한 파란색 사각형은 '사전 훈련된' 모델을 나타내고, 진한 사각형은 '지시에 맞춰진' 모델에 해당합니다. 상단의 모델은 오픈 소스로 이용 가능하며, 하단의 모델은 폐쇄 소스입니다. 이 차트는 지시에 맞춰진 모델과 오픈 소스 모델로의 증가하는 추세를 보여주며, 자연어 처리 연구의 변화하는 풍경과 동향을 강조합니다.

지도 학습 환경에서 모델들은 대규모 텍스트 말뭉치 [7], [8], [9]에서 자기 지도 학습 환경에서 훈련되며, 다양한 NLP 작업에서 공유 가능한 범용 표현을 학습하는 것을 목표로 합니다. 하위 작업에 대한 세밀 조정 후, PLM은 전통적인 언어 모델링(LM)의 성능 향상을 능가합니다. 더 큰 PLM은 더 많은 성능 향상을 가져오며, 이로 인해 모델 파라미터를 크게 증가시켜 LLM으로 전환하는 경향이 있습니다(수십억 개에서 수백억 개) [10] 및 훈련 데이터셋(여러 GB 및 TB) [10], [11]. 이러한 발전에 따라 다양한 LLM이 문헌에서 제안되었습니다 [10], [11], [12], [6], [13], [14], [15]. LLM의 출시된 수와 몇 가지 중요한 LLM의 이름은 각각 그림 1과 그림 2에 나와 있습니다.
T5 [10] 및 mT5 [11]와 같은 LLM의 초기 작업은 GPT-3 [6]까지 전이 학습을 사용하여 LLM이 세밀 조정 없이 하위 작업으로 전이 가능함을 보여주었습니다. 작업 설명과 예시를 사용하여 작업 쿼리에 정확하게 응답합니다. 그러나 사전 훈련된 LLM은 사용자 의도를 따르지 못하며, 제로샷 설정에서는 훨씬 적은 성능을 보입니다. 작업 지침 데이터 [16], [17], [18], [19]와 사람의 선호도를 일치시키는 것 [20], [21]은 보이지 않는 작업에 대한 일반화를 향상시키고, 제로샷 성능을 크게 향상시키고, 정렬되지 않은 동작을 줄입니다.

일반화와 도메인 적응을 더 잘하기 위해,
LLM은 추론, 계획, 의사 결정, 문맥 학습, 제로샷 설정에서의 답변 등과 같은 신생 능력을 갖고 있는 것으로 보입니다. 이러한 능력들은 사전 훈련된 LLM이 특별히 이러한 속성을 갖도록 훈련되지 않았더라도 그들의 거대한 규모로 인해 습득된다고 알려져 있습니다 [22], [23], [24]. 이러한 능력들은 LLM이 다양한 환경에서 널리 채택되게 하였으며, 다중 모달, 로봇, 도구 조작, 질문에 대한 답변, 자율 에이전트 등의 분야에서 사용되고 있습니다. 이러한 분야에서도 과제별 훈련 [25], [26], [27], [28], [29], [30], [31] 또는 더 나은 프롬프팅 [32]을 통해 다양한 개선이 제안되었습니다.

LLM의 다양한 작업을 인간 수준의 성능으로 해결하는 능력은 훈련과 추론이 느리고, 많은 하드웨어 요구사항과 높은 운영 비용이 발생하는 대가를 지불해야 한다. 이러한 요구사항으로 인해 LLM의 채택이 제한되었으며, 더 나은 아키텍처 [15], [33], [34], [35] 및 훈련 전략 [36], [37], [21], [38], [39], [40], [41]을 개발할 수 있는 기회가 열렸다. 효율적인 LLM 활용을 위해 매개변수 효율적 조정 [38], [41], [40], 가지치기, 양자화, 지식 증류 및 문맥 길이 보간 [42], [43], [44], [45] 등의 방법이 널리 연구되고 있다.

LLM의 다양한 작업에서의 성공으로 인해, 최근 연구 문헌은 LLM과 관련된 기여물의 대량 유입을 경험하고 있다. 연구자들은 사전 인쇄물을 조직했다.

그림 3: LLMs의 보다 포괄적인 개요, LLMs를 다음 다섯 가지 분야로 나누어 설명합니다: 1. 훈련 2. 추론 3. 평가 4. 응용 5. 도전과제

LLMs 문학에 대한 조사 [46], [47], [48], [49] 및 주제별 조사 [50], [51], [52], [53], [54]. 이러한 조사와는 달리, 우리의 기여는 LLM 연구의 일반적인 방향에 대한 포괄적이면서 간결한 개요를 제공하는 데 초점을 맞추고 있습니다. 이 기사는 사전 훈련된 LLM의 구조 및 교육 세부 정보를 요약하고, 세부 개념인 fine-tuning, multi-modal LLMs, 로봇 공학, 증강 LLMs, 데이터셋, 평가 등에 대해 더 깊이 파고들어 자체 포괄적인 개요를 제공합니다. 우리의 주요 기여는 다음과 같이 요약됩니다.

ing procedures.
• We propose a novel approach that combines both su-
pervised and unsupervised learning techniques for im-
proved performance.
• Our experimental results demonstrate the effectiveness
of the proposed method on various benchmark datasets.
• We discuss the limitations of our approach and suggest
potential directions for future research.

상세한 내용을 설명하기 위해 LLMs의 연대순서에 특별한 주의를 기울이는 것 외에도, 우리는 인기 있는 기여의 주요 발견을 요약하고 LLMs의 주요 설계 및 개발 측면에 대한 자세한 논의를 제공하여 실무자들이 이 기술을 효과적으로 활용할 수 있도록 도와줍니다.

이 자체 포함된 글에서는 배경, 사전 훈련, 세부 조정, 로봇 공학, 다중 모달 LLMs, 증강 LLMs, 데이터셋, 평가 등을 포함하여 LLMs의 일반적인 방향을 포괄적으로 이해하기 위한 다양한 개념을 다룹니다.

우리는 기존 용어를 느슨하게 따르며 이 연구 방향의 표준화된 전망을 제공하기 위해 노력합니다. 예를 들어, [46]을 따라 우리의 조사는 사전 훈련된 것에 대해 논의합니다.

10B 개 이상의 매개변수를 가진 LLMs. 작은 사전 훈련된 모델에 관심이 있는 독자들은 [47], [48], [49]를 참조하시기 바랍니다.
이 논문의 구성은 다음과 같습니다. 제2절에서는 LLMs의 배경에 대해 논의합니다. 제3절에서는 LLMs의 개요, 구조, 훈련 파이프라인 및 전략, 그리고 다양한 측면에서의 활용에 초점을 맞춥니다. 제4절에서는 각 LLM에서 얻은 주요 결과를 제시합니다. 제5절에서는 이러한 모델의 작동에 중요한 역할을 하는 구성 및 매개변수를 강조합니다. 요약 및 토의는 제7절에서 제시됩니다. LLM 훈련 및 평가, 데이터셋 및 벤치마크는 제6절에서 논의되며, 도전과 미래 방향 및 결론은 각각 제8절과 제9절에서 다룹니다.

배경

이 섹션에서는 LLM에 관련된 기본 개념을 이해하기 위한 관련 배경을 제공합니다. 우리의 목표인 이 방향에 대한 포괄적인 개요를 제공하기 위해, 이 섹션은 기본 개념에 대한 포괄적이면서도 간결한 개요를 제공합니다. 우리는 직관적인 측면에 더 초점을 맞추고, 자세한 내용에 관심 있는 독자들에게는 원본 작품을 참고하도록 안내합니다.

토큰화

LLM은 텍스트를 예측하기 위해 텍스트에 대해 훈련되며, 다른 자연어 처리 시스템과 마찬가지로 토큰화 [55]를 필수적인 전처리 단계로 사용합니다. 이는 텍스트를 토큰이라고 불리는 비분해 단위로 구문 분석하는 것을 목표로 합니다. 토큰은 문자, 서브워드 [56], 심볼 [57] 또는 단어일 수 있으며, 모델의 크기와 유형에 따라 다릅니다. LLM에서 일반적으로 사용되는 토큰화 방법 중 일부를 간단히 설명합니다. 자세한 설문은 [58]을 참조하시기 바랍니다.
1. WordPiece [59]: 이는 일본어와 한국어 언어의 음성 검색 시스템을 위해 언어 모델을 개선하기 위해 [59]에서 도입된 새로운 텍스트 분할 기술입니다. WordPiece는 토큰을 선택하여 토큰으로 구성된 어휘로 훈련된 n-gram 기반 언어 모델의 가능성을 높입니다.
2. BPE [57]: Byte Pair Encoding (BPE)는 압축 알고리즘에서 비롯되었습니다. 이는 인접한 심볼 쌍을 새로운 심볼로 대체하고 입력 텍스트에서 가장 많이 발생하는 심볼의 발생 횟수를 병합하는 반복적인 과정으로 토큰을 생성합니다.
3. UnigramLM [56]: 이 토큰화에서는 초기 서브워드 단위 어휘를 사용하여 간단한 유니그램 언어 모델 (LM)을 훈련합니다. 어휘는 유니그램 LM에서 성능이 가장 나쁜 확률이 가장 낮은 항목을 반복적으로 제거하여 가지치기됩니다.

주의

주의, 특히 선택적 주의는 지각, 심리물리학 및 심리학에서 널리 연구되었습니다. 선택적 주의는 "어떤 자극이 처리되거나 인코딩될 것인지, 그리고 이것이 어떤 순서로 발생할지에 대한 O의 프로그래밍"으로 이해될 수 있습니다 [60]. 이 정의는 그 자체로 의미가 있습니다.

시각 인지에 뿌리를 둔 이론으로, 최근에 제시된 주의 [61], [62] (어떤 자극이 처리될 것인지)와 위치 인코딩 (어떤 순서로 처리될 것인지) [62]과 놀랍도록 유사한 특징을 가지고 있습니다. 우리는 각각 II-C와 II-D에서 이에 대해 논의합니다.

C. LLMs에 대한 주의

주의 메커니즘은 입력 시퀀스의 서로 다른 위치 (토큰)을 관련시켜 입력 시퀀스의 표현을 계산합니다. 주의를 계산하고 구현하는 다양한 방법이 있으며, 그 중 일부 유명한 유형은 아래에 제시되었습니다.
1. 자기 주의 [62]: 자기 주의는 인코더 또는 디코더의 동일한 블록에서 모든 쿼리, 키 및 값이 나오기 때문에 내부 주의라고도 알려져 있습니다. 자기 주의 계층은 입력의 장거리 종속성을 학습하기에 매우 유용한 O(1) 공간 복잡도로 모든 시퀀스 위치를 연결합니다.
2. 교차 주의: 인코더-디코더 아키텍처에서 인코더 블록의 출력은 디코더의 중간 표현을 쿼리로 사용하여 인코더에 의존하는 디코더의 표현을 계산하기 위한 키 및 값으로 작용합니다. 이 주의는 교차 주의라고 불립니다.
3. 전체 주의: 자기 주의를 계산하는 단순한 구현은 전체 주의라고 알려져 있습니다.
4. 희소 주의 [63]: 자기 주의의 시간 복잡도는 O(n2)이며, LLMs를 큰 문맥 창으로 확장할 때 이는 금지적입니다. [63]에서 자기 주의의 근사값이 제안되었으며, 이를 통해 GPT 시리즈 LLMs의 용량이 크게 향상되어 합리적인 시간 내에 더 많은 입력 토큰을 처리할 수 있게 되었습니다.
5. 플래시 주의 [64]: GPU를 사용하여 주의를 계산하는 병목 현상은 계산 속도보다는 메모리 액세스에 있습니다. 플래시 주의는 클래식한 입력 타일링 접근 방식을 사용하여 GPU의 온-칩 SRAM에서 입력 블록을 처리하고 High Bandwith Memory (HBM)에서 모든 토큰에 대해 IO를 수행하는 대신에 처리합니다. 이 접근 방식의 확장은 전체 주의 구현의 속도 향상을 따릅니다. 이 기교를 사용하면 희소 주의를 가진 LLMs와 비교하여 더 큰 문맥 길이 창을 사용할 수 있습니다.

D. 인코딩 위치

주의 모듈은 처리 순서를 고려하지 않습니다. Transformer [62]은 입력 시퀀스의 토큰 위치에 대한 정보를 전달하기 위해 "위치 인코딩"을 도입했습니다. 위치 인코딩의 여러 가지 변형이 제안되었습니다 [65], [66]. 흥미로운 점은 최신 연구 [67]에서 이 정보를 추가하는 것이 최첨단 디코더 전용 Transformer에는 중요하지 않을 수 있다는 것을 시사합니다.
1. 절대적: 이는 시퀀스 순서 정보를 추가하는 가장 직관적인 방법으로, 시퀀스의 각 위치에 고유한 식별자를 할당하여 주의 모듈에 전달하는 것입니다.

2. 상대적: 서열의 다른 위치에서 나타나는 서로 다른 토큰들의 상대적 종속성에 대한 정보를 전달하기 위해, 상대적 위치 인코딩이 어떤 학습을 통해 계산됩니다. 두 가지 유명한 상대적 인코딩 유형은 다음과 같습니다:
알리바이: [65] 이 접근 방식에서는 두 토큰의 위치 간 거리에 비례하여 증가하는 스칼라 편향이 어텐션 점수에서 빼집니다. 이 학습된 접근 방식은 최근 토큰을 어텐션에 더 많이 활용하는 효과적입니다.
RoPE: 키, 쿼리 및 값은 모두 LLMs의 벡터입니다. RoPE [66]는 입력 서열의 토큰들의 절대적 위치에 비례하여 쿼리와 키 표현을 회전시킵니다. 이 단계는 토큰 간 거리에 따라 감소하는 상대적 위치 인코딩 체계를 생성합니다.

활성화 함수

활성화 함수는 신경망의 곡선 맞춤 능력에 중요한 역할을 한다. 이는 [68]에서 증명되었다. LLMs에서 사용되는 현대적인 활성화 함수는 이전의 압축 함수와는 다르지만 LLMs의 성공에 중요하다. 이 섹션에서는 이러한 활성화 함수에 대해 논의한다.
1. ReLU [69]: ReLU(정류된 선형 유닛)는 다음과 같이 정의된다.

ReLU(x) = max(0,x)     (1)
ReLU(x) = max(0,x)     (1)

2. GeLU [70]: 가우시안 에러 선형 유닛 (GeLU)은 ReLU, 드롭아웃 [71] 및 존아웃 [72]의 조합입니다. 이것은 현대 LLM 문헌에서 가장 널리 사용되는 활성화 함수입니다.
3. GLU 변형 [73]: 게이트드 선형 유닛 [74]은 입력의 선형 변환과 시그모이드 변환 (σ)을 적용한 선형 투영의 요소별 곱셈 (⊗)입니다.

GLU(x,W,V,b,c) = (xW + b) ⊗ σ(xV + c), (2)
GLU(x,W,V,b,c) = (xW + b) ⊗ σ(xV + c), (2)

X는 레이어와 l의 입력이며, W, b, V, c는 학습된 매개변수입니다.
GLU는 [73]에서 수정되어 트랜스포머의 학습 및 테스트에서 다양한 변화의 효과를 평가하기 위해 사용되었으며, 이로 인해 더 나은 경험적 결과가 나타났습니다. [73]에서 소개된 다양한 GLU 변형은 다음과 같이 LLMs에서 사용되었습니다.

ReGLU(x,W,V,b,c) = max(0,xW + b)⊗,

ReGLU(x,W,V,b,c) = max(0,xW + b)⊗,

GEGLU(x,W,V,b,c) = GELU(xW + b) ⊗ (xV + c),

GEGLU(x,W,V,b,c) = GELU(xW + b) ⊗ (xV + c)입니다.

SwiGLU(x,W,V,b,c,β) = Swishβ(xW + b) ⊗ (xV + c). 

SwiGLU(x,W,V,b,c,β) = Swishβ(xW + b) ⊗ (xV + c).

F. 레이어 정규화

레이어 정규화는 빠른 수렴을 이끌어내며, 트랜스포머에서 널리 사용되는 구성 요소입니다. 이 섹션에서는 LLM 문헌에서 널리 사용되는 다양한 정규화 기법을 제공합니다.

1. 레이어 정규화: 레이어 정규화는 레이어 (l)의 모든 은닉 유닛에 대한 통계를 다음과 같이 계산합니다.

ul = 1
알
알 (cid:88)

나
알
나
σl =
무한대
무한대 무한대1
엔
엔 X

나
(알
나
− 울)2, (3)

l층과 al에서 뉴런의 수인 n이 있는 경우

i
l층의 i 뉴런의 합산 입력입니다. LayerNorm은 가중치의 재조정과 분포의 재중심화에 대한 불변성을 제공합니다.
2. RMSNorm: [75]은 LayerNorm의 불변성 특성이 가짜라고 주장하며, 우리는 속도와의 교환을 통해 재중심화 불변성과 동일한 성능 이점을 얻을 수 있는 계산적으로 효율적인 정규화 기술을 사용할 수 있다고 제안했습니다. LayerNorm은 l층에 정규화된 합산 입력을 다음과 같이 제공합니다.

알파이는 알파이에서 시그마 글리이를 뺀 값입니다.

(4) 

Please give me a glass of water.
I am going to the store to buy some groceries.
What time is the meeting?
Can you help me with this task?
I need to go to the bank to withdraw some money.

gl이 어디에 있나요?
나는 gain 매개변수입니다. RMSNorm [75]은 al을 수정합니다.

나
같이

알
나
= 알 i RMS(al)gl i, 여기서 RMS(al) =

안녕하세요
네
네 안녕하세요

나
나이 2. (5)

3. 사전 정규화와 사후 정규화: LLM은 몇 가지 변형과 함께 transformer [62] 아키텍처를 사용합니다. 원래 구현 [62]은 잔여 연결 이후에 레이어 정규화를 사용하여 일반적으로 사후-LN이라고 불리며 Multihead 어텐션 - 잔여 - LN의 순서에 관한 것입니다. 정규화의 다른 순서인 사전-LN이라고도 하는데, 이는 LN - Multihead 어텐션 - 잔여와 같이 자기 어텐션 레이어 이전에 정규화 단계가 위치하기 때문입니다. 사전-LN은 훈련에서 더 안정성을 제공하는 것으로 알려져 있습니다 [77].
4. DeepNorm: 사전-LN은 사후-LN 훈련에 비해 특정 이점을 가지지만, 사전-LN 훈련은 그래디언트에 불필요한 영향을 미칩니다 [77]. 이전 레이어의 그래디언트가 하단에 있는 레이어보다 큽니다. DeepNorm [78]은 이러한 그래디언트에 대한 부정적인 영향을 완화합니다. 다음과 같이 주어집니다.

xlf = LN(αxlp + Glp(xlp,θlp), (6) 

xlf = LN(αxlp + Glp(xlp,θlp), (6)

α는 상수이고, θlp는 lp층의 매개변수를 나타냅니다. 이러한 매개변수는 다른 상수 β에 의해 스케일링됩니다. 이 두 상수는 아키텍처에만 의존합니다.

G. 분산된 LLM 훈련

이 섹션에서는 분산 LLM 훈련 접근 방식에 대해 간략히 설명합니다. 자세한 내용은 [13], [37], [79], [80]에서 확인할 수 있습니다.
1. 데이터 병렬화: 데이터 병렬화는 배치의 데이터가 여러 장치에 분할되는 곳에서 모델을 복제합니다. 각 훈련 반복의 끝에서 가중치는 모든 장치 간에 동기화됩니다.
2. 텐서 병렬화: 텐서 병렬화는 텐서 계산을 장치에 분할합니다. 이는 가로 병렬화 또는 레이어 내 모델 병렬화로도 알려져 있습니다.

3. 파이프라인 병렬성: 파이프라인 병렬성은 모델 레이어를 다른 장치들에 분할하여 처리하는 것을 의미합니다. 이는 또한 수직 병렬성으로 알려져 있습니다.
4. 모델 병렬성: 텐서와 파이프라인 병렬성의 조합을 모델 병렬성이라고 합니다.
5. 3D 병렬성: 데이터, 텐서, 그리고 모델 병렬성의 조합을 3D 병렬성이라고 합니다.
6. 옵티마이저 병렬성: 옵티마이저 병렬성 또는 제로 중복 옵티마이저는 옵티마이저 상태 분할, 그래디언트 분할, 그리고 파라미터 분할을 장치들 간에 구현하여 메모리 소비를 줄이고 통신 비용을 최소화합니다.

H. 도서관

LLM 훈련에 일반적으로 사용되는 몇 가지 라이브러리는 다음과 같습니다: 1) Transformers [81], 2) DeepSpeed [36], 3) Megatron-LM [79], 4) JAX [82], 5) Colossal-AI [83], 6) BMTrain [80], 7) FastMoE [84], 그리고 프레임워크는 다음과 같습니다: 1) MindSpore [85], 2) PyTorch [86], 3) Tensorflow [87], 4) MXNet [88].

데이터 전처리

이 섹션은 LLMs 훈련에 사용되는 데이터 전처리 기술을 간략하게 요약합니다.
1. 품질 필터링: 더 나은 결과를 위해 훈련 데이터의 품질은 필수적입니다. 데이터 필터링에 대한 몇 가지 접근 방식은 다음과 같습니다: 1) 분류기 기반 및 2) 휴리스틱 기반. 분류기 기반 접근 방식은 고품질 데이터로 분류기를 훈련시키고 텍스트의 품질을 예측하여 필터링에 사용합니다. 반면 휴리스틱 기반은 언어, 메트릭, 통계 및 키워드와 같은 규칙을 사용하여 필터링을 수행합니다.
2. 데이터 중복 제거: 중복된 데이터는 모델 성능에 영향을 미칠 수 있으며 데이터 기억력을 증가시킬 수 있습니다. 따라서 LLMs를 훈련시키기 위해 데이터 중복 제거는 전처리 단계 중 하나입니다. 이는 문장, 문서 및 데이터셋과 같은 여러 수준에서 수행될 수 있습니다.
3. 개인 정보 축소: LLMs의 대부분의 훈련 데이터는 웹 소스를 통해 수집됩니다. 이 데이터에는 개인 정보가 포함되어 있으므로, 많은 LLMs는 이름, 주소 및 전화번호와 같은 정보를 배우지 않기 위해 휴리스틱 기반 방법을 사용하여 정보를 필터링합니다.

J. 아키텍처

여기에서는 어텐션의 적용과 트랜스포머 블록의 연결에 따라 발생하는 트랜스포머 아키텍처의 변형에 대해 더 높은 수준에서 논의합니다. 이러한 아키텍처의 어텐션 패턴은 그림 4에 나와 있습니다.
1. 인코더 디코더: 트랜스포머는 원래 시퀀스 변환 모델로 설계되었으며 기계 번역 시스템을 위한 다른 주류 모델 아키텍처를 따랐습니다. 인간의 언어 번역 작업을 훈련하기 위해 인코더-디코더 아키텍처를 선택했습니다. 이 아키텍처는 [10], [89]에서 채택되었습니다. 이 아키텍처 체계에서 인코더는 입력 시퀀스를 가변 길이의 컨텍스트 벡터로 인코딩하고, 이후 디코더에 전달하여 예측된 토큰 레이블과 실제 대상 토큰 레이블 간의 차이를 최소화하는 공동 목적을 최대화합니다.

그림 4: 언어 모델의 주의 패턴 예시, 이미지는 [91]에서 가져온 것입니다.

그림 5: 언어 모델 훈련 목표의 예시, [91]에서 가져온 이미지.

2. 인과 디코더: LLM의 근본적인 목표는 입력 시퀀스를 기반으로 다음 토큰을 예측하는 것입니다. 인코더로부터의 추가 정보는 예측을 문맥에 강하게 결합시키지만, 실제로는 인코더 없이도 LLM이 잘 수행될 수 있다는 것을 발견했습니다 [90]. 이 디코더는 원래의 인코더-디코더 아키텍처의 디코더 블록과 유사하게 정보의 흐름을 제한합니다. 즉, 예측된 토큰 tk는 tk-1까지의 토큰에만 의존합니다. 이것은 최신 LLM에서 가장 널리 사용되는 변형입니다.

3. 접두사 디코더: 인코더-디코더 아키텍처에서 인코더가 자기 어텐션을 사용하여 문장의 모든 토큰에 대해 모든 위치에서 참조할 수 있는 경우, 인과 마스크 어텐션은 합리적입니다. 이는 인코더가 tk를 위한 표현을 계산하는 동안 t1부터 tk-1까지의 토큰뿐만 아니라 tk+1부터 tn까지의 토큰에도 참조할 수 있다는 것을 의미합니다. 그러나 인코더를 제거하고 디코더만 유지하는 경우, 이러한 어텐션의 유연성도 잃게 됩니다. 디코더 전용 아키텍처에서는 입력 시퀀스의 일부에 엄격하게 인과적인 마스크 대신 완전히 가시적인 마스크를 적용하여 변형시킬 수 있습니다. 이를 그림 4에서 볼 수 있습니다. 접두사 디코더는 비인과적 디코더 아키텍처로도 알려져 있습니다.

K. 사전 훈련 목표

이 섹션은 LLM의 사전 훈련 목표를 설명합니다. 자세한 내용은 논문 [91]을 참조하십시오.
1. 전체 언어 모델링: 자기 회귀 언어 모델링 목표로, 모델은 이전 토큰을 기반으로 미래 토큰을 예측하도록 요청됩니다. 예시는 그림 5에 나와 있습니다.
2. 접두사 언어 모델링: 인과적이지 않은 훈련 목표로, 접두사가 무작위로 선택되고 남은 대상 토큰만을 사용하여 손실을 계산합니다. 예시는 그림 5에 나와 있습니다.

그림 6: 사전 훈련부터 프롬프팅/활용까지 다양한 LLMs의 단계를 보여주는 기본 플로우 다이어그램. 프롬프팅 LLMs는 사전 훈련, 지시 튜닝 또는 정렬 튜닝과 같은 다양한 훈련 단계에서 응답을 생성하는 것이 가능합니다.

3. 마스크된 언어 모델링: 이 훈련 목표에서는 토큰이나 구간(토큰의 연속)이 무작위로 마스크되며, 모델은 과거와 미래 문맥을 고려하여 마스크된 토큰을 예측하도록 요구됩니다. 예시는 그림 5에 나와 있습니다.
4. 통합 언어 모델링: 통합 언어 모델링 [92]은 인과적, 비인과적, 그리고 마스크된 언어 훈련 목표의 조합입니다. 여기서 마스크된 언어 모델링에서는 양방향이 아닌 단방향으로 주의가 집중되며, 좌에서 우로 또는 우에서 좌로 문맥에 주의를 기울입니다.

L. 모델 적응

프리트레이닝: 맨 처음 단계에서 모델은 대규모 말뭉치에서 자기지도 학습 방식으로 훈련되어 입력에 대한 다음 토큰을 예측합니다. LLM의 설계 선택사항은 인코더-디코더부터 디코더 전용 아키텍처까지 다양합니다.

II-F, II-E, II-K 섹션에서 다른 건물 블록과 손실 함수를 사용합니다.

2. 세부 조정: LLM을 세부 조정하는 다양한 스타일이 있습니다. 이 섹션에서는 세부 조정 접근 방식에 대해 간략히 논의합니다.
전이 학습: 사전 훈련된 LLM은 다양한 작업에 대해 잘 수행됩니다 [6], [15]. 그러나 하위 작업의 성능을 향상시키기 위해 사전 훈련된 모델은 작업별 데이터로 세부 조정됩니다 [10], [11], 전이 학습이라고도 합니다.
지시어 조정: 사용자 쿼리에 효과적으로 응답하기 위해 사전 훈련된 모델은 지시어 형식의 데이터, 즉 지시어와 입력-출력 쌍에 대해 세부 조정됩니다. 지시어는 일반적으로 일반 자연어로 된 다중 작업 데이터로, 모델이 프롬프트와 입력에 따라 응답하도록 안내합니다. 이러한 유형의 세부 조정은 제로샷 일반화 및 하위 작업 성능을 향상시킵니다. 지시어 데이터의 형식 및 다양한 스타일에 대한 자세한 내용은 [16], [46], [93]에서 확인할 수 있습니다.
조정 조율: LLM은 거짓, 편향 및 유해한 텍스트를 생성하기 쉽습니다. 이를 도움이 되는, 정직한 및 무해한 모델로 만들기 위해 인간의 피드백을 사용하여 모델을 조정합니다. 조정은 LLM에 예상치 못한 응답을 생성하도록 요청한 다음 이러한 응답을 피하기 위해 매개변수를 업데이트하는 과정을 포함합니다 [20], [21], [94].
PREPRINT 8

LLM은 인간의 의도와 가치에 따라 작동되도록 보장합니다. 모델이 "HHH" 또는 "도움이 되는, 정직한, 무해한" 세 가지 기준을 충족하는 경우 "일치된" 모델로 정의됩니다.
연구자들은 모델 정렬을 위해 인간 피드백과 강화학습 (RLHF) [96]를 사용합니다. RLHF에서는 데모를 기반으로 세밀하게 조정된 모델이 보상 모델링 (RM)과 강화학습 (RL)으로 추가적으로 훈련되며, 그림 6에 나와 있습니다. 아래에서는 RLHF에서 RM과 RL 파이프라인에 대해 간략히 설명합니다.
보상 모델링: 분류 목적을 사용하여 인간의 선호에 따라 생성된 응답을 순위 매기는 모델을 훈련합니다. 분류기를 훈련하기 위해 인간은 HHH 기준에 따라 LLM이 생성한 응답에 주석을 달게 됩니다.
강화학습: 보상 모델과 결합하여 다음 단계에서 정렬에 사용됩니다. 이전에 훈련된 보상 모델은 LLM이 생성한 응답을 선호하는지 여부로 순위를 매기며, 이를 사용하여 모델을 근사 정책 최적화 (PPO)로 정렬합니다. 이 과정은 수렴할 때까지 반복됩니다.
매개변수 효율적 조정: LLM을 훈련하기 위해 더 큰 메모리와 컴퓨팅이 필요합니다. 이를 적은 리소스를 사용하여 훈련하기 위해, 연구자들은 모델에 새로운 매개변수를 추가하거나 기존 매개변수를 업데이트하는 다양한 매개변수 효율적 조정 기술을 제안했습니다. 일반적으로 사용되는 몇 가지 방법에 대해 아래에서 설명하겠습니다.
프롬프트 튜닝: [40], [97]은 입력 토큰 임베딩에 대해 학습 가능한 프롬프트 토큰 임베딩을 접두사나 자유 형식으로 추가합니다. 세밀 조정 중에는 이러한 임베딩 매개변수만 후속 작업을 위해 훈련되며, 나머지 가중치는 고정됩니다.
접두사 튜닝: [41]은 트랜스포머 레이어에 작업별로 학습 가능한 접두사 벡터를 추가합니다. 여기서는 접두사 매개변수만 세밀 조정되며, 나머지 모델은 고정됩니다. 입력 시퀀스 토큰은 가상 토큰으로 작용하는 접두사에 참여할 수 있습니다.
어댑터 튜닝: 모듈은 인코더-디코더 아키텍처로, 어텐션 및 피드포워드 레이어와 순차적 또는 병렬로 배치됩니다. 이러한 레이어만 세밀 조정되며, 나머지 모델은 고정됩니다.
3. 프롬프팅/활용: 프롬프팅은 그림 6에 나와 있는 것처럼 훈련된 LLM에게 응답 생성을 요청하는 방법입니다. LLM은 다양한 프롬프트 설정에서 프롬프트를 받을 수 있으며, 이를 통해 세밀 조정 없이 지시사항에 적응하거나 다른 프롬프트 스타일이 포함된 데이터로 세밀 조정할 수 있습니다. 프롬프트 엔지니어링에 대한 좋은 안내는 [32]에서 제공됩니다. 아래에서는 널리 사용되는 다양한 프롬프트 설정에 대해 논의하겠습니다.
제로샷 프롬프팅: LLM은 제로샷 학습자로, 이전에 본 적이 없는 쿼리에 대한 답변을 할 수 있습니다. 이 스타일의 프롬프팅은 LLM이 프롬프트에서 어떤 예시도 보지 않고 사용자 질문에 답변하는 것을 요구합니다.
컨텍스트 학습: 몇 개의 입력-출력 데모 쌍을 모델에 보여주어 원하는 응답을 생성하도록 합니다. 이 적응 스타일은 몇 개의 샷 학습이라고도 불립니다. 컨텍스트 학습 (ICL) 템플릿에 대한 형식 지정에 대한 논의는 [50], [46], [18], [16]에서 찾을 수 있습니다.

LLM의 추론: LLM은 제로샷 추론자로서 논리 문제, 과제 계획, 비판적 사고 등에 대한 답변을 생성할 수 있습니다. 이러한 이유를 생성하기 위해서는 다양한 프롬프 스타일을 사용해야 하며, LLM의 추론 작업을 더 개선하기 위해 [16], [93] 등의 다양한 방법을 사용하여 추론 데이터셋에서 학습시킵니다. 아래에서는 추론을 위한 다양한 프롬프 기술에 대해 논의합니다.
Chain-of-Thought (CoT): 모델이 단계별 추론을 수행하여 결과를 생성할 수 있도록 입력과 출력과 함께 추론 정보가 포함된 데모를 사용하는 프롬프의 특수한 경우입니다. CoT 프롬프에 대한 자세한 내용은 [51], [101], [99]에서 확인할 수 있습니다.
Self-Consistency: CoT 성능을 향상시키기 위해 여러 응답을 생성하고 가장 빈도가 높은 답변을 선택합니다 [102].
Tree-of-Thought (ToT): 문제 해결을 위해 앞으로 전망하고 되돌아가며 여러 추론 경로를 탐색합니다 [103].
Single-Turn Instructions: 이 프롬프 설정에서는 LLM이 프롬프에 모든 관련 정보를 한 번에 쿼리합니다. LLM은 제로샷 또는 퓨샷 설정에서 문맥을 이해하여 응답을 생성합니다.
Multi-Turn Instructions: 복잡한 작업을 해결하기 위해서는 LLM과의 여러 상호작용이 필요하며, 다음 라운드를 위해 다른 도구로부터의 피드백과 응답을 LLM에게 입력으로 제공합니다. 이러한 방식으로 LLM을 루프에서 사용하는 것은 자율 에이전트에서 흔히 사용됩니다.

III. 대형 언어 모델

이 섹션은 LLM에 대해 검토하며, 그들의 구조, 훈련 목표, 파이프라인, 데이터셋 및 세부 조정에 대해 간단히 설명합니다.

사전 훈련된 LLMs

여기에서는 다양한 잘 알려진 사전 훈련된 LLM의 요약을 제공합니다. 이들 LLM은 NLP의 연구 및 개발 방향을 바꾸는 중요한 발견을 가지고 있습니다. 이러한 LLM은 NLU 및 NLG 도메인에서 성능을 크게 향상시키고, 다양한 하향 작업에 대해 널리 세밀하게 조정됩니다.
1. 일반적인 목적:
1.1 T5 [10]: 모든 NLP 문제에 대해 통합 텍스트 대 텍스트 훈련을 사용하는 인코더-디코더 모델로, 그림 7에 나와 있습니다. T5는 전통적인 트랜스포머 모델 [62]에서 잔차 경로 외부에 레이어 정규화를 배치합니다. T5는 마스크된 언어 모델링을 사용하여 사전 훈련 목적으로, 연속된 토큰 대신 단일 마스크로 구성된 범위(연속된 토큰)를 대체합니다. 이러한 유형의 마스킹은 더 짧은 시퀀스를 생성하여 훈련 속도를 높입니다. 사전 훈련 후에는 하향 작업에 대해 어댑터 레이어 [98]를 사용하여 모델을 세밀하게 조정합니다.
1.2 GPT-3 [6]: GPT-3 아키텍처는 GPT-2 [5]와 동일하지만 Sparse Transformer [63]와 유사한 희소 및 밀집 어텐션을 사용하는 트랜스포머 레이어가 있습니다. GPT-3는 대규모 모델이 더 큰 배치 크기와 낮은 학습률로 훈련될 수 있다는 것을 보여줍니다. 훈련 중 배치 크기를 결정하기 위해 GPT-3는 [104]와 같이 그래디언트 노이즈 스케일을 사용합니다. 전반적으로, GPT-3은 모델 파라미터를 175B로 증가시켜 보여줍니다.

그림 7: 통합 텍스트 대 텍스트 훈련 예시, 출처 이미지 [10]에서.

그림 8: 이미지는 [105]의 기사로, PanGu-α 아키텍처의 예시를 보여줍니다.

대형 언어 모델의 성능은 규모가 커짐에 따라 향상되며, 세밀하게 조정된 모델과 경쟁력을 갖습니다.
1.3 mT5 [11]: 101개 언어로 구성된 mC4 데이터셋을 사용하여 훈련된 다국어 T5 모델 [10]. 이 데이터셋은 공개된 공통 크롤 스크래핑에서 추출되었습니다. 이 모델은 여러 언어를 커버하기 위해 250,000개의 더 큰 어휘 크기를 사용합니다.
언어별로 과적합 또는 과소적합을 피하기 위해 mT5은 모든 언어에서 샘플을 선택하기 위한 데이터 샘플링 절차를 사용합니다. 논문은 영어 언어 데이터를 사용하여 작업을 세밀하게 조정할 때 모든 언어를 포함한 소량의 사전 훈련 데이터셋을 사용하는 것을 제안합니다. 이를 통해 모델은 올바른 비영어 출력을 생성할 수 있습니다.
1.4 PanGu-α [105]: 표준 트랜스포머 레이어의 끝에 쿼리 레이어를 가진 자기회귀 모델입니다. 예시는 그림 8에 나와 있으며, 다음 토큰을 예측하기 위한 목적으로 사용됩니다. 이 구조는 트랜스포머 레이어와 유사하지만, 주의 메커니즘에서 다음 위치를 위한 추가적인 임베딩이 있습니다. 이는 식 7에서 제시됩니다.

a = pnWq hWk hTHT
L
(7)

a = pnWq hWk hTHT
L
(7)

1.5 CPM-2 [12]: 비용 효율적인 사전 훈련 언어 모델(CPM-2)은 영어와 중국어로 이루어진 11B 및 198B의 혼합 전문가(MoE) 모델을 Wu-DaoCorpus [106] 데이터셋에서 사전 훈련합니다. 토큰화 과정에서는 문장 조각 토크나이저에서 "_" 공백 토큰을 제거합니다. 이 모델은 지식 상속을 통해 훈련되며, 첫 번째 단계에서는 중국어만 사용하고, 그 다음에는 영어와 중국어 데이터를 추가합니다. 이 훈련된 모델은 198B MoE 모델을 초기화하기 위해 여러 번 복제됩니다. 또한, 하위 작업에 모델을 사용하기 위해 CPM-2는 실험을 진행했습니다.

양쪽의 완전한 미세 조정과 프롬프트 미세 조정을 사용하여 [107]에서와 같이 프롬프트 관련 매개변수만 업데이트하여 프롬프트를 앞, 중간 및 뒤에 다양한 위치에 삽입함으로써 모델을 훈련시킵니다. CPM-2는 또한 메모리 효율적인 프레임워크인 INFMOE를 제안하며, 이는 100B 규모의 추론을 위해 매개변수를 동적으로 CPU로 오프로드하는 전략을 사용합니다. 이는 추론 계산과 데이터 이동을 겹쳐서 수행하여 추론 시간을 단축시킵니다.
1.6 ERNIE 3.0 [108]: ERNIE 3.0은 Transformer-XL [109]을 기반으로 하는 모듈식 아키텍처를 구축하기 위해 다중 작업 학습에서 영감을 받습니다. 모든 작업에서 공유되는 범용 표현 모듈은 작업별 표현 모듈의 기본 블록으로 작동하며, 자연어 이해, 자연어 생성 및 지식 추출을 위해 모두 함께 학습됩니다. 이 LLM은 주로 중국어에 초점을 맞추어 있으며, LLM 훈련을 위해 가장 큰 중국어 텍스트 말뭉치에서 훈련되었으며, 54가지 중국어 NLP 작업에서 최첨단 성능을 달성했습니다.
1.7 Jurassic-1 [110]: 7B 매개변수 J1-Large 모델과 178B 매개변수 J1-Jumbo 모델을 포함하는 자기 회귀 언어 모델의 한 쌍입니다. Jurassic-1의 훈련 어휘는 단어 조각, 완전한 단어 및 단어 경계가 없는 다중 단어 표현으로 구성되며, 가능한 경우 어휘 외의 인스턴스는 유니코드 바이트로 해석됩니다. Jurassic-1 모델은 GPT-3와 비교하여 더 균형 잡힌 깊이-너비 자기 주의 아키텍처 [111]와 더 빠른 예측을 위한 개선된 토크나이저를 적용하여 제로샷 학습 작업에서 비슷한 성능을 달성하고, 더 많은 예시를 프롬프트로 사용할 수 있는 능력으로 인해 퓨샷 학습 작업에서 우수한 성능을 달성합니다.
1.8 HyperCLOVA [112]: GPT-3 아키텍처를 사용한 한국어 언어 모델입니다.
1.9 Yuan 1.0 [113]: 인터넷에서 수집한 고품질 텍스트로 구성된 5TB의 중국어 말뭉치로 훈련되었습니다. Spark에서 구축된 대용량 데이터 필터링 시스템(MDFS)은 원시 데이터를 거친 필터링 기술을 통해 처리합니다. 에너지 비용과 탄소 배출을 절약하기 위해 Yuan 1.0의 훈련을 가속화하기 위해 아키텍처와 훈련에 분산 훈련의 성능을 향상시키는 다양한 요소가 통합되었습니다. 숨겨진 크기의 수를 늘리면 파이프라인 및 텐서 병렬성 성능이 향상되고, 더 큰 마이크로 배치는 파이프라인 병렬성 성능을 향상시키며, 더 높은 전역 배치 크기는 데이터 병렬성 성능을 향상시킵니다. 실제로 Yuan 1.0 모델은 텍스트 분류, 윈로그라드 스키마, 자연어 추론 및 독해 작업에서 우수한 성능을 발휘합니다.
1.10 Gopher [114]: Gopher 모델은 LLM의 성능에 대한 규모의 영향을 연구하기 위해 44M에서 280B 매개변수의 크기 범위를 가지고 있습니다. 280B 모델은 평가된 작업의 81%에서 GPT-3 [6], Jurassic-1 [110], MT-NLG [115] 등을 능가합니다.
1.11 ERNIE 3.0 TITAN [35]: ERNIE 3.0 Titan은 후자의 26배 매개변수 수를 가진 더 큰 모델을 훈련하여 ERNIE 3.0을 확장합니다. 이 더 큰 모델은 68개의 NLP 작업에서 다른 최첨단 모델보다 우수한 성능을 발휘합니다. LLM은 잘못된 사실을 포함한 텍스트를 생성합니다. 사실 일관성을 가진 생성된 텍스트를 제어하기 위해 ERNIE 3.0 Titan은 추가적인 기능을 추가합니다.

작업, 신뢰할 수 있고 제어 가능한 세대를 다중 작업 학습 설정에 추가합니다. 이는 사전 훈련 단계에 추가적인 자기 지도적 적대적 및 제어 가능한 언어 모델링 손실을 도입하여 ERNIE 3.0 Titan이 수동으로 선택된 사실 기반 QA 작업 세트 평가에서 다른 LLM을 이기도록합니다.
1.12 GPT-NeoX-20B [116]: GPT-3를 크게 따르지만 몇 가지 아키텍처 디자인에서 차이가 있는 자기 회귀 모델입니다. 데이터 중복을 제거하지 않고 Pile 데이터 세트에서 훈련되었습니다. GPT-NeoX는 변압기 블록에서 병렬 어텐션 및 피드포워드 레이어를 가지며, Eq. 8에서 제공됩니다. 이를 통해 처리량이 15% 증가합니다. [66]에서 제공하는 회전 위치 임베딩을 사용하며, 임베딩 벡터 차원의 25%에만 적용합니다. 이는 성능 저하 없이 계산을 줄입니다. GPT-3에서는 밀집 및 희소 레이어를 사용하는 반면, GPT-NeoX-20B는 밀집 레이어만 사용합니다. 이 규모에서의 하이퍼파라미터 튜닝은 어렵기 때문에, 모델은 20B 모델을 위해 13B 및 175B 모델 사이의 값을 [6] 방법에서 선택하고 보간합니다. 모델 훈련은 텐서 및 파이프라인 병렬 처리를 사용하여 GPU 간에 분산됩니다.

x + Attn(LN1(x)) + FF(LN2(x)) (8) 

x + 주의(LN1(x)) + 전방(LN2(x)) (8)

1.13 OPT [14]: GPT-3와 동일한 성능을 재현하는 오픈소스 모델을 개발하기 위해 개발된 OPT는 동적 손실 스케일링 [118]을 사용하며, 손실 발산이 관측될 때마다 이전 체크포인트에서 낮은 학습률로 재시작합니다. 전반적으로, OPT-175B 모델의 성능은 GPT3-175B 모델과 비교할 수 있습니다.
1.14 BLOOM [13]: ROOTS 말뭉치에서 훈련된 인과 디코더 모델인 BLOOM은 LLM을 오픈소스화하기 위해 개발되었습니다. BLOOM의 아키텍처는 Figure 9에 나와 있으며, ALiBi 위치 임베딩, 임베딩 레이어 이후의 추가 정규화 레이어 등의 차이점이 있습니다. 이러한 변경 사항은 훈련을 안정화시키고 하류 성능을 향상시킵니다.
1.15 GLaM [119]: 일반적인 언어 모델인 GLaM은 희소하게 활성화된 디코더 전용 전문가 혼합(MoE) 구조 [120], [121]를 사용합니다. 계산량을 줄이면서 모델 용량을 더욱 늘리기 위해 전문가들은 토큰마다 최상의 두 전문가만 사용하여 활성화됩니다. 가장 큰 GLaM 모델인 GLaM (64B/64E)은 GPT-3 [6]보다 약 7배 더 크지만 입력 토큰 당 일부 매개변수만 활성화됩니다. 가장 큰 GLaM (64B/64E) 모델은 GPT-3와 비교하여 전반적으로 더 나은 결과를 달성하면서 GPT-3의 1/3만큼의 훈련 에너지를 소비합니다.
1.16 MT-NLG [115]: GPT-2 아키텍처를 기반으로 한 530B 인과 디코더인 MT-NLG는 대략적으로 GPT-3 모델 파라미터의 3배입니다. MT-NLG는 다양한 공개 데이터셋에서 수집된 정제된 고품질 데이터로 훈련되며, 단일 배치에서 다양한 유형의 데이터셋을 혼합합니다. 이로써 MT-NLG는 여러 평가에서 GPT-3를 능가합니다.

1. https://github.com/TimDettmers/bitsandbytes
1. https://github.com/TimDettmers/bitsandbytes

그림 9: [13]에서 인용된 BLOOM 아키텍처 예시.

1.17 친칠라 [122]: 고퍼 [114]와 동일한 데이터셋으로 훈련된 인과 디코더이지만 약간 다른 데이터 샘플링 분포를 가지고 있습니다 (MassiveText에서 샘플링됨). 모델 아키텍처는 고퍼에 사용된 것과 유사하지만 Adam 대신 AdamW 옵티마이저를 사용합니다. 친칠라는 모델 크기가 훈련 토큰의 배로 두 배씩 증가해야 함을 식별합니다. 70백만에서 160억 개 이상의 매개변수를 가진 400개 이상의 언어 모델이 50억에서 5000억 개의 토큰을 사용하여 계산 최적의 훈련을 위한 추정치를 얻기 위해 훈련됩니다. 저자들은 Gopher (280B)와 동일한 계산 예산으로 70B 모델을 훈련하지만 4배 더 많은 데이터를 사용합니다. 세부 작업에서 Gopher [114], GPT-3 [6] 및 기타 모델보다 우수한 성능을 발휘합니다.

1.18 AlexaTM [123]: 인코더-디코더 모델로, 인코더 가중치와 디코더 임베딩은 사전 훈련된 인코더로 초기 훈련 속도를 높이기 위해 초기화됩니다. 인코더는 처음 100,000 단계 동안 고정되어 있으며, 이후에는 끝-끝 훈련을 위해 해제됩니다. 이 모델은 노이즈 제거 및 인과 언어 모델링 (CLM) 목적의 조합으로 훈련되며, 모드 전환을 위해 [CLM] 토큰을 시작 부분에 연결합니다. 훈련 중에는 CLM 작업이 시간의 20%에 적용되어 문맥 학습 성능을 향상시킵니다.

1.19 PaLM [15]: Eq. 8과 유사한 병렬 어텐션 및 피드포워드 레이어를 가진 인과 디코더로, 훈련 속도를 15배 빠르게 합니다. 전통적인 트랜스포머 모델에는 SwiGLU 활성화, RoPE 임베딩, 디코딩 중 계산 비용을 절약하는 멀티-쿼리 어텐션 및 공유 입력-출력 임베딩과 같은 추가 변경 사항이 있습니다. 훈련 중에는 손실 스파이킹이 관찰되었으며, 이를 해결하기 위해 모델 훈련은 스파이크 주변의 200-500 배치를 건너뛰어 100 단계 이전 체크포인트에서 다시 시작되었습니다. 또한, 540B 모델 규모에서 모델은 훈련 데이터의 약 2.4%를 기억하는 것으로 발견되었으며, 이 숫자는 더 작은 모델에 대해서는 낮았습니다.

PaLM-2 [124]: PaLM의 더 작은 다국어 변형으로, 더 좋은 품질의 데이터셋에서 더 큰 반복 횟수로 훈련되었습니다. PaLM-2는 PaLM보다 상당한 개선을 보여주며, 크기가 작아서 훈련 및 추론 비용을 줄일 수 있습니다. 유해한 응답 생성을 줄이기 위해 사전 훈련 데이터의 일부에 특수 토큰을 추가합니다.

1.20 U-PaLM [125]: 이 방법은 PaLM을 0.1% 추가 계산으로 훈련합니다. UL2 (또는 UL2Restore로도 알려짐)

목표 [89]는 동일한 데이터셋을 사용하고 기준선을 크게 능가하여 다양한 NLP 작업에서 우수한 성능을 보입니다. 이에는 UL2R로 훈련하는 것이 포함되며, 인과적 디코더 PaLM을 비인과적 디코더 PaLM으로 변환하고 50%의 순차적 노이즈 제거, 25%의 정규 노이즈 제거, 25%의 극단적 노이즈 제거 손실 함수를 사용합니다.
1.21 UL2 [89]: MoD (Mixture of Denoisers) 목적을 사용하여 훈련된 인코더-디코더 아키텍처입니다. Denoisers에는 1) R-Denoiser: 정규 스팬 마스킹, 2) S-Denoiser: 연속된 토큰을 손상시키는 것, 3) X-Denoiser: 무작위로 많은 수의 토큰을 손상시키는 것이 포함됩니다. 사전 훈련 중에 UL2는 R, S, X의 노이즈 제거 설정을 나타내는 노이즈 토큰을 포함합니다. 이는 상류 훈련 모드 중 하나에 작업을 바인딩하는 하류 작업의 성능을 향상시키는 데 도움이 됩니다. 이러한 MoD 스타일의 훈련은 많은 벤치마크에서 T5 모델을 능가합니다.
1.22 GLM-130B [33]: GLM-130B는 GLM [126]과 유사한 자기회귀 마스크 채우기 사전 훈련 목적을 사용하여 영어와 중국어로 이루어진 양방향 모델입니다. 이 훈련 스타일은 단방향인 GPT-3와 비교하여 모델을 양방향으로 만듭니다. GLM과는 달리, GLM-130B의 훈련에는 자기지도적인 마스크 채우기와 함께 일부 멀티태스크 지시 사전 훈련 데이터 (전체 데이터의 5%)가 포함됩니다. 훈련을 안정화하기 위해 임베딩 레이어 그래디언트 축소를 적용합니다.
1.23 LLaMA [127], [21]: 7B에서 70B까지 다양한 파라미터를 가진 디코더 전용 언어 모델의 집합입니다. LLaMA 모델 시리즈는 파라미터 효율성과 지시 조정을 위해 커뮤니티에서 가장 유명합니다.
LLaMA-1 [127]: 마스크된 어텐션 가중치와 키/쿼리 점수를 저장하고 계산하지 않음으로써 효율적인 인과적 어텐션 [128]을 구현합니다. 다른 최적화로는 역전파에서 다시 계산되는 활성화 수를 줄이는 것이 있습니다.
LLaMA-2 [21]: 이 작업은 대화 생성을 위한 더 안전하고 더 나은 LLaMA-2-Chat 모델의 세부 튜닝에 더 초점을 맞추고 있습니다. 사전 훈련된 모델은 더 큰 컨텍스트 길이와 그룹화된 쿼리 어텐션을 가진 40% 더 많은 훈련 데이터를 가지고 있습니다.
1.24 PanGu-Σ [130]: PanGu-α에서 파라미터를 복사하고 Random Routed Experts (RRE)를 사용하여 1조 규모로 확장된 자기회귀 모델입니다. RRE는 MoE 아키텍처와 유사하지만, 토큰이 학습 가능한 게이팅 방법 대신 도메인 내 전문가에게 무작위로 라우팅됩니다. 이 모델은 하위 레이어가 밀집하게 활성화되고 모든 도메인에서 공유되는 반면, 상위 레이어는 도메인에 따라 희소하게 활성화됩니다. 이러한 훈련 스타일은 작업별 모델을 추출하고 지속적인 학습의 치명적인 잊혀짐 효과를 줄이는 데 도움이 됩니다.
2. 코딩:
2.1 CodeGen [131]: CodeGen은 PaLM [15]과 유사한 아키텍처를 가지고 있으며, 병렬 어텐션, MLP 레이어 및 RoPE 임베딩을 사용합니다. 이 모델은 자연어와 프로그래밍 언어 데이터를 순차적으로 훈련합니다 (첫 번째 데이터셋으로 훈련한 다음 두 번째, 그리고 이어서). 사용된 데이터셋은 다음과 같습니다. 1) PILE, 2) BIGQUERY, 3) BIGPYTHON. CodeGen은 코드 합성을 위한 다단계 접근법을 제안합니다. 이는 이전 프롬프트와 생성된 코드를 입력으로 제공하여 긴 시퀀스의 생성을 단순화하는 것을 목적으로 합니다.

다음 프롬프트는 다음 코드 시퀀스를 생성하는 것입니다. CodeGen
오픈 소스인 Multi-Turn Programming Benchmark (MTPB)를 사용하여 다단계 프로그램 합성을 평가합니다.
2.2 Codex [132]: 이 LLM은 공개 Python Github 저장소의 일부를 사용하여 docstrings에서 코드를 생성합니다. 컴퓨터 프로그래밍은 종종 요구 사항을 충족하기 전에 프로그램을 디버깅하고 업데이트하는 반복적인 과정입니다. 이와 마찬가지로 Codex는 주어진 설명에 대해 반복적인 샘플링을 통해 프로그램의 100개 버전을 생성하여 문제의 77.5%에 대한 작동하는 솔루션을 생성합니다. 이 강력한 버전은 Github Copilot2를 구동합니다.
2.3 AlphaCode [133]: 300M에서 41B 매개변수로 이루어진 대규모 언어 모델 세트로, 경쟁 수준의 코드 생성 작업에 사용됩니다. 이 모델은 다중 쿼리 어텐션을 사용하여 메모리 및 캐시 비용을 줄입니다. 경쟁 프로그래밍 문제는 깊은 추론과 복잡한 자연어 알고리즘의 이해를 많이 요구하기 때문에 AlphaCode 모델은 인기있는 언어로 필터링된 GitHub 코드로 사전 훈련되고, 그 후에 CodeContests라는 새로운 경쟁 프로그래밍 데이터셋에서 세밀하게 조정됩니다. CodeContests 데이터셋은 주로 Codeforces 플랫폼에서 수집된 문제, 솔루션 및 테스트 케이스를 포함합니다. 사전 훈련은 표준 언어 모델링 목적을 사용하고, 세밀 조정은 CodeContests 데이터에 대한 훈련 목적으로 GOLD [135]와 tempering [136]을 사용합니다. AlphaCode의 성능을 평가하기 위해 Codeforces 플랫폼에서 시뮬레이션 프로그래밍 대회가 개최됩니다. 전체적으로 AlphaCode는 5000명 이상의 참가자 중 상위 54.3%에 위치하며, Codeforces 등급은 최근 참가한 사용자 중 상위 28%에 해당합니다.
2.4 CodeT5+ [34]: CodeT5+는 CodeT5 [137]를 기반으로 하며, 얕은 인코더와 깊은 디코더로 구성되어 있으며, 초기에는 단일 모달 데이터(코드)를, 나중에는 양모달 데이터(텍스트-코드 쌍)를 훈련합니다. 각 훈련 단계는 다른 훈련 목적을 가지며, 작업에 따라 인코더, 디코더 또는 둘 다를 활성화합니다. 단일 모달 사전 훈련에는 span denoising과 CLM 목적이 포함되어 있으며, 양모달 사전 훈련 목적에는 대조 학습, 매칭 및 텍스트-코드 쌍의 CLM이 포함됩니다. CodeT5+는 텍스트와 함께 특수 토큰을 추가하여 작업 모드를 활성화합니다. 예를 들어, 대조 손실을 위한 [CLS], 텍스트-코드 매칭을 위한 [Match] 등이 있습니다.
2.5 StarCoder [138]: SantaCoder 아키텍처를 사용한 디코더 전용 모델로, Flash 어텐션을 사용하여 문맥 길이를 8k까지 확장합니다. StarCoder는 훈련 데이터에서 이름, 이메일 및 기타 개인 데이터를 필터링하기 위해 인코더를 훈련합니다. 세밀하게 조정된 변형은 HumanEval 및 MBPP 벤치마크에서 PaLM, LLaMA 및 LAMDA를 능가합니다.
3. 과학적 지식:
3.1 Galactica [139]: 4800만 개의 논문, 교과서, 강의 노트, 수백만 개의 화합물과 단백질, 과학 웹사이트, 백과사전 등으로 구성된 대규모 인간 과학 지식 데이터셋입니다. 이 데이터셋은 metaseq 라이브러리3를 사용하여 훈련되었으며, PyTorch와 fairscale [140] 위에 구축되었습니다. 모델은 <work> 토큰으로 추론 데이터셋을 래핑하여 제공합니다.

2https://github.com/features/copilot
3https://codeforces.com/
PREPRINT                                                              12

2https://github.com/features/copilot
3https://codeforces.com/
PREPRINT                                                              12

그림 10: 이 예시는 [130]에서 가져온 이미지에 나타난 PanGu-(cid:80) 아키텍처를 보여줍니다.

단계별 추론 컨텍스트를 모델에 적용하여 추론 작업의 성능을 향상시킨 것으로 입증되었습니다.
4. 대화:
4.1 LaMDA [141]: 공개 대화 데이터, 공개 대화 발언 및 공개 웹 문서로 사전 훈련된 디코더 전용 모델입니다. 사전 훈련 데이터의 90% 이상이 영어로 구성되어 있습니다. LaMDA는 고품질, 안전성 및 기반을 갖춘 응답을 생성하는 것을 목표로 훈련되었습니다. 이를 위해 구별적 및 생성적 세부 튜닝 기술이 모델의 안전성과 품질 측면을 향상시키기 위해 통합되었습니다. 결과적으로, LaMDA 모델은 다양한 작업을 수행하는 일반 언어 모델로 활용될 수 있습니다.
5. 금융:
5.1 BloombergGPT [142]: 금융 ("Bloomberg 아카이브"의 "FINPILE" 데이터셋) 및 일반 목적 데이터셋을 모두 사용하여 훈련된 인과성이 없는 디코더 모델입니다. 이 모델의 아키텍처는 BLOOM [13] 및 OPT [14]와 유사합니다. 접근 방식 [143]을 사용하여 모델의 다양한 블록에 500억 개의 매개변수를 할당합니다. 효과적인 훈련을 위해, BloombergGPT는 문서를 < |endoftext| >와 함께 묶어 최대 시퀀스 길이를 사용하고, 1024에서 2048까지 워마업 배치 크기를 사용하며, 훈련 중에 학습률을 여러 번 수동으로 감소시킵니다.
5.2 Xuan Yuan 2.0 [144]: BLOOM [13]의 아키텍처를 사용하여 일반 목적, 금융, 일반 목적 지침 및 금융 기관 데이터셋의 조합으로 훈련된 중국 금융 챗봇 모델입니다. Xuan Yuan 2.0은 재앙적인 잊혀짐을 피하기 위해 사전 훈련 및 세부 튜닝 단계를 결합했습니다.

B. 세밀하게 조정된 LLMs

사전 훈련된 LLM은 보이지 않은 작업에 대해 탁월한 일반화 능력을 갖고 있습니다. 그러나 일반적으로 다음 토큰 예측을 목표로 훈련되기 때문에 LLM은 사용자 의도를 따르는 능력이 제한되며 비윤리적이거나 유해하거나 부정확한 응답을 생성할 가능성이 있습니다 [20]. 효과적인 활용을 위해 LLM은 지침을 따르도록 세밀하게 조정되고 안전한 응답을 생성하도록 조정됩니다 [16], [17], [93], 이로 인해 결과적으로

그림 11: [16]에서 가져온 Flan 훈련 패러다임의 한 예시 이미지를 보여줍니다.

제로샷, 퓨샷 및 크로스태스크 일반화에서 0.2%의 증가로 PaLM 540B의 총 사전 훈련을 위한 최소한의 계산을 사용하여 [93], [16], [18]을 증가시킵니다.
이 섹션에서는 효과적인 세부 튜닝을 위한 다양한 세부 튜닝 LLM과 전략을 검토합니다.
1. 수동으로 생성된 데이터셋을 사용한 지시어 튜닝:
다양한 디자인 선택지를 가진 수동으로 생성된 지시어 튜닝 데이터셋이 문헌에서 제안되어 LLM을 지시어 튜닝합니다. 세부 튜닝된 LLM의 성능은 데이터셋, 지시어 다양성, 프롬프트 템플릿, 모델 크기 및 훈련 목표와 같은 여러 요소에 따라 달라집니다. 이를 고려하여 수동으로 생성된 데이터셋을 사용하여 다양한 세부 튜닝된 모델이 문헌에서 등장했습니다.
모델 T0 [17] 및 mT0 (다국어) [146]은 기존 데이터셋을 프롬프트 데이터셋으로 변환하기 위해 템플릿을 사용합니다. 이들은 제로샷 및 보류된 작업의 일반화에 향상을 보였습니다. Tk-Instruct [18]는 테스트 시에 인콘텍스트 지시어를 제공할 때 보이지 않는 작업에 대한 일반화를 연구하기 위해 T5 모델을 세부 튜닝했습니다. 이 모델은 Instruct-GPT보다 성능이 우수했으며, GPT-3의 175B에 비해 11B의 파라미터 크기를 가지고 있습니다.
작업 및 프롬프트 설정의 증가: 작업 수집 및 프롬프트 스타일을 확장함으로써 제로샷 및 퓨샷 성능이 크게 향상됩니다. OPT-IML [93] 및 Flan [16]은 각각 더 큰 2k 및 1.8k 작업 데이터셋을 선별했습니다. 작업 크기만 증가하는 것만으로는 충분하지 않지만, OPT-IML 및 Flan은 데이터셋에 제로샷, 퓨샷 및 CoT의 더 많은 프롬프트 설정을 추가합니다. 이어서 CoT Collection [99]은 Flan-T5를 1.88M CoT 샘플에 대해 추가로 세부 튜닝합니다. 또 다른 방법 [100]은 T0, Flan 등의 작업을 사용하는 상징적 작업을 사용합니다.

LLM 생성 데이터셋을 사용한 지시어 튜닝:
지시어 튜닝 데이터셋을 생성하기 위해서는 지시어와 입출력 쌍을 주의 깊게 작성해야 합니다. 이는 종종 사람들에 의해 작성되며 크기가 작고 다양하지 않습니다. 이를 극복하기 위해, self-instruct [19]는 LLM을 활용하여 지시어 튜닝 데이터셋을 생성하는 방법을 제안했습니다. self-instruct는 수동으로 생성된 SUPER-NATURALINSTRUCTIONS 데이터셋 (1600개 이상의 작업을 포함한 데이터셋) [18]보다 33% 더 우수한 성능을 보였습니다. 이는 175개의 작업, 1개의 지시어 및 1개의 샘플로 시작하여 GPT-3 [6]을 사용하여 새로운 지시어 (52,000개) 및 인스턴스 (82,000개의 입출력 쌍)를 반복적으로 생성합니다. 이와는 달리, Dynosaur [147]는 PREPRINT에서 사용합니다.

표 I: 사전 훈련된 대형 언어 모델에서 주목할 만한 발견과 통찰력.

모델들                             발견 및 통찰

T5

• 매개변수를 공유하지 않을 때, 공유 매개변수를 가진 인코더와 디코더는 동등한 성능을 발휘합니다.
• 어댑터 레이어를 세밀 조정하는 모델 레이어는 단순히 분류 레이어만 학습하는 전통적인 방법보다 더 좋은 결과를 보여줍니다.

GPT-3 • LLM의 퓨-샷 성능은 제로-샷보다 우수하며, 이는 LLM이 메타-러너라는 것을 시사한다.

mT5

대형 다국어 모델은 하위 작업에서 단일 언어 모델과 동등한 성능을 발휘합니다. 그러나 작은 다국어 모델은 성능이 떨어집니다.

판구-α • LLM은 몇 번의 샷 능력에 능하다.

CPM-2

• 프롬프트 파인튜닝은 전체 모델 파인튜닝과 비교할만한 성능을 달성하면서 매우 적은 매개변수를 업데이트하는 것을 요구합니다.
• 프롬프트 파인튜닝은 전체 모델 파인튜닝에 비해 수렴하는 데 더 많은 시간이 소요됩니다.
• 문장 사이에 프롬프트 토큰을 삽입하면 모델이 문장과 긴 시퀀스 간의 관계를 이해할 수 있게 됩니다.
• CPM-2의 분석에서 프롬프트는 모델에 대한 추가적인 맥락(컨텍스트)과 입력 텍스트와의 정보를 집계하는 공급자 역할을 한다는 것을 발견했습니다.

코덱스

• 이 LLM은 코드 평가에 초점을 맞추고 최상의 코드 샘플을 선택하는 새로운 방법을 소개합니다.
• 결과는 각 샘플의 상세한 평가 대신 휴리스틱 순위를 사용하여 코드 샘플을 정확하게 선택할 수 있다는 것을 나타냅니다. 이는 일부 상황에서 실행 가능하지 않을 수도 있습니다.

어니 3.0

• ERNIE 3.0는 범용 표현 모듈과 과제별 표현 모듈을 갖춘 모듈식 LLM 아키텍처가 세밀 조정 단계에서 도움이 된다는 것을 보여줍니다.
• 세밀 조정 단계에서 과제별 표현 네트워크의 매개변수를 최적화하는 것은 강력한 사전 훈련 모델의 장점을 효과적으로 활용하는 방법입니다.

쥬라기-1

• LLM의 성능은 네트워크 크기와 밀접하게 관련되어 있습니다.
• 실행 시간 성능을 향상시키기 위해, 연속적으로(깊이) 수행하는 대신 병렬로(너비) 더 많은 작업을 수행할 수 있습니다.
• 동일한 문맥 길이에 더 많은 텍스트를 효율적으로 표현하고 적합시키기 위해, 모델은 단어 경계에 제한을 두지 않고 더 큰 어휘를 사용하여 SentencePiece 토크나이저를 훈련시킵니다. 이 토크나이저 개선은 퓨-샷 학습 작업에 더 많은 이점을 제공할 수 있습니다.

하이퍼클로바

프롬프트 기반 튜닝을 사용하여, 입력의 역기울기에 접근할 수 있는 경우, 모델의 성능을 향상시킬 수 있으며, 종종 최첨단 모델을 능가합니다.

위안 1.0

사전 훈련 및 세밀 조정 사례에서 뛰어난 모델 아키텍처는 제로샷 및 퓨샷 학습에서는 상반된 행동을 보일 수 있습니다.

고퍼 • 상대 인코딩을 통해 모델은 훈련된 시퀀스보다 더 긴 시퀀스에 대해 평가될 수 있습니다.

ERNIE 3.0 타이탄

이 LLM은 ERNIE 3.0을 기반으로 하며, 자기 지도적 적대적 손실을 추가하여 텍스트가 생성된 것인지 원본인지를 구별합니다.
실제 텍스트와 생성된 텍스트 간의 구별 능력은 ERNIE 3.0과 비교하여 LLM의 성능을 향상시킵니다.

GPT-NeoX-20B

• 병렬 어텐션 + FF 레이어는 성능은 동일하면서 훈련 속도를 15% 높입니다.
• [145]에서 제시된 방법을 사용하여 잔차 이전에 피드포워드 출력 레이어를 초기화하면 활성화가 깊이와 너비가 증가함에 따라 증가하는 것을 방지할 수 있습니다.
• Pile에서의 훈련은 GPT-3보다 5회 시도에서 우수한 성능을 보입니다.

OPT

• 손실이 발산하면 이전 체크포인트에서 학습률을 낮추고 훈련을 다시 시작하세요.
• 모델은 반복적인 텍스트를 생성하고 루프에 갇힐 가능성이 있습니다.

개화하다

갤랙티카

• 갤랙티카의 성능은 검증 세트, 도메인 내, 도메인 외 벤치마크에서 계속해서 향상되었으며, 말뭉치를 여러 번 반복해도 기존 LLMs 연구에 비해 우수합니다.
• 작업 메모리 토큰 접근 방식은 수학 MMLU 및 MATH 벤치마크에서 기존 방법보다 강력한 성능을 달성할 수 있습니다. PubMedQA (77.6%) 및 MedMCQA dev (52.9%)와 같은 여러 하위 작업에서 새로운 최고 성능을 세우고 있습니다.

GLaM

• 각 Transformer 레이어의 feed-forward 구성 요소는 독립적인 feed-forward 네트워크 집합(즉, '전문가'들)으로 구성된 혼합 전문가(MoE) 모듈로 대체될 수 있습니다. 이러한 전문가들을 희소하게 활성화시킴으로써 모델 용량을 유지하면서 많은 계산을 절약할 수 있습니다.
• 희소성을 활용함으로써, 에너지 소비를 동시에 줄이면서 고품질 NLP 모델 개발을 위한 중요한 발전을 이룰 수 있습니다. 결과적으로, MoE는 미래의 확장 노력에 대한 견고한 후보로 부각됩니다.
• 필터링된 데이터로 훈련된 모델은 NLG 및 NLU 작업 모두에서 일관되게 더 좋은 성능을 보여줍니다. 필터링의 효과는 특히 전자 작업에 더 큰 영향을 미칩니다.
• 필터링된 사전 훈련 말뭉치는 LLM의 생성 능력에 중요한 역할을 합니다. 특히 하위 작업에 대해서는 더욱 그렇습니다.
• GLaM MoE 모델의 확장은 MoE 레이어의 크기나 전문가 수를 증가시킴으로써 이루어질 수 있습니다. 고정된 계산 예산이 주어진 경우, 더 많은 전문가가 더 나은 예측에 기여합니다.

LaMDA • 이 모델은 다양한 외부 정보 자원과 도구를 호출하는 방법을 학습하기 위해 세밀하게 조정될 수 있습니다.
MT-NLG • 없음.

알파코드

• 더 높은 효율과 효과를 위해, 트랜스포머 모델은 얕은 인코더와 깊은 디코더로 비대칭적으로 구성될 수 있습니다.
• 더 나은 성능을 얻기 위해서는 대량의 샘플링을 대규모로 확장한 후, 샘플을 필터링하고 클러스터링하여 압축된 집합으로 만드는 전략을 채택하는 것이 필요합니다.
• 대규모 샘플링을 용이하게 하는 새로운 샘플링 효율적인 트랜스포머 아키텍처의 활용은 중요합니다.
• 문제 설명을 단순화하는 것은 모델의 성능을 효과적으로 향상시킬 수 있습니다.

다음 페이지에서 테이블이 계속됩니다.
사전 인쇄물                                                              14

모델들                            발견 및 통찰

친칠라
• 친칠라 개발을 위해 수행된 실험들은 훈련 중 최적의 계산을 위해 모델 크기와 훈련 토큰의 수가 비례적으로 조정되어야 함을 확인했다: 모델 크기가 두 배로 증가할 때마다 훈련 토큰의 수도 두 배로 증가되어야 한다.

PaLM
• 영어 중심 모델은 영어로 번역할 때 비-영어 모델에 비해 더 좋은 번역을 생성합니다.
• 일반화된 모델은 특화된 작은 모델과 동등한 성능을 가질 수 있습니다.
• 더 큰 모델은 더 많은 훈련 데이터를 기억합니다.
• 성능은 아직 540B 규모에서도 포화되지 않았으며, 따라서 더 큰 모델이 더 나은 성능을 발휘할 가능성이 있습니다.

알렉사TM
• 일반적으로 사용되는 디코더 전용 트랜스포머 모델과 비교하여 seq2seq 아키텍처는 문맥에 대한 강력한 양방향 어텐션을 통해 생성형 LLM을 훈련하기에 더 적합합니다.
• 추가적인 인과 언어 모델링 (CLM) 작업을 추가함으로써 모델을 더 효율적인 문맥 학습으로 이점을 얻을 수 있으며, 특히 퓨-샷 학습 작업에 유리합니다.
• 강력한 seq2seq 기반 LLM을 훈련시키는 핵심은 추가적인 멀티태스크 훈련이 아닌 혼합 사전 훈련에 있습니다.
• 각 트랜스포머 레이어의 시작 부분에 레이어 정규화를 배치함으로써 대형 모델의 훈련 안정성을 향상시킬 수 있습니다.

U-PaLM
• 몇 가지 더 많은 FLOP에 대해 추가로 훈련된 경우, 노이즈 제거기의 혼합으로 훈련하는 것이 PaLM보다 우수한 성능을 보입니다.
• 노이즈 제거기의 혼합으로 훈련하는 것은 채움 능력과 무한한 텍스트 생성 다양성을 향상시킵니다.

UL2
• 모드 전환 훈련은 하류 작업에서 더 나은 성능을 발휘할 수 있게 합니다.
• CoT 프롬프팅은 UL2에서 표준 프롬프팅보다 우수한 성과를 보입니다.

GLM-130B • 작은 비율의 다중 작업 지시 데이터로 사전 훈련 데이터를 수행하면 전체 모델 성능이 향상됩니다.
CodeGen • 코드 합성을 위한 다단계 프롬프팅은 사용자 의도 이해와 코드 생성을 더욱 개선시킵니다.

LLaMA
• LLaMA는 오픈 소스이며 새로운 모델 또는 명령 기반 도구를 개발하기 위해 세밀하게 조정하거나 지속적으로 사전 훈련될 수 있습니다.
• LLaMA의 훈련 효율성을 향상시키기 위해 몇 가지 최적화 방법이 제안되었습니다. 이에는 다중 헤드 자기 주의의 효율적인 구현과 역전파 중 활성화 수량의 감소가 포함됩니다.
• 공개 데이터만을 사용하여 훈련하는 것으로도 최첨단 성능을 달성할 수 있습니다.
• 모델을 확장할 때 일정한 성능 향상이 이루어집니다.
• 더 작은 모델도 더 많은 훈련 데이터와 시간을 사용하여 좋은 성능을 얻을 수 있습니다.

팬구-Σ
• 희소 모델은 큰 모델의 이점을 더 낮은 계산 비용으로 제공합니다.
• 무작위 경로 전문가는 잊혀지기 쉬운 효과를 줄여 계속적인 학습에 필수적입니다.
• 무작위 경로 전문가는 배치 시 도메인 특정 하위 모델을 추출할 수 있어 원래 모델과 유사한 성능을 유지하면서 비용 효율적입니다.

블룸버그GPT • 일반적인 용도와 과제별 데이터로 사전 훈련을 진행하면 다른 모델 기능에 영향을 주지 않으면서 과제 성능을 향상시킬 수 있습니다.
XuanYuan 2.0 • 사전 훈련과 세부 조정 단계를 단일 훈련에서 결합함으로써 장기적인 기억 손실을 방지할 수 있습니다.

코드T5+
• 인코더-디코더 아키텍처에서 인과적 언어 모델은 모델의 생성 능력에 있어 중요합니다.
• 스팬 손상, 인과적 언어 모델, 일치 등과 같은 다양한 훈련 목표는 서로 보완하여 더 나은 성능을 발휘합니다.

스타코더 • 안트로픽의 HHH 프롬프트는 세부 조정 없이 모델이 지시에 따라 움직일 수 있게 합니다.

LLaMA-2
• 필터링되지 않은 데이터로 훈련된 모델은 더 독성이 있지만 fine-tuning 후에 하위 작업에서 더 좋은 성능을 발휘할 수 있습니다.
• 필터링되지 않은 데이터로 훈련된 모델은 안전 정렬을 위해 더 적은 샘플이 필요합니다.

PaLM-2
• 데이터 품질은 더 나은 모델을 훈련시키기 위해 중요합니다.
• 모델과 데이터 크기는 1:1 비율로 조정되어야 합니다.
• 더 큰 반복 횟수로 훈련된 작은 모델이 더 큰 모델보다 우수한 성능을 발휘합니다.

허깅페이스의 데이터셋 메타데이터는 LLMs에게 여러 작업 지시 튜닝 데이터셋을 생성하도록 유도합니다.
LLaMA 튜닝: 문헌에서 다양한 모델들은 GPT-3 [6] 또는 GPT-4 [149]로 LLaMA [148]를 튜닝하기 위해 생성된 데이터셋을 사용합니다. 이 중에서 Alpaca [150], Vicuna [151], 그리고 LLaMA-GPT-4 [152]는 몇 가지 일반 목적의 세부 튜닝 모델입니다. Alpaca는 text-davinci-003에서 52,000개의 샘플로 훈련되었으며, Vicuna는 ShareGPT.com에서 70,000개의 샘플로 훈련되었으며, LLaMA-GPT-4는 GPT-4에서 Alpaca 지시사항을 재생성하여 훈련되었습니다. Goat [153]은 ChatGPT에서 데이터를 생성하여 산술 작업을 위해 LLaMA를 세부 튜닝합니다 (100만 개의 샘플). 이 모델은 GPT-4, PaLM, BLOOM, OPT 등을 능가하며, LLaMA의 일관된 숫자 토큰화를 성공의 원인으로 귀착시킵니다. HuaTuo [154]는 의료 지식 모델로, 8,000개의 지시사항으로 생성된 QA 데이터셋으로 세부 튜닝되었습니다.
복잡한 지시사항: Evol-Instruct [155], [156]는 LLMs에게 주어진 지시사항을 더 복잡한 형태로 변환하도록 유도합니다. 지시사항은 복잡한 용어로 다시 작성하고 복잡한 문장으로 변형하여 반복적으로 진화시킵니다.

새로운 지침. 이 자동 지침 생성 스타일로 WizardLM [155] (250k 지침에 대한 fine-tuned LLaMA)은 Vicuna와 Alpaca를 능가하며, WizardCoder [156] (fine-tuned StarCoder)는 Claude-Plus, Bard 등을 이긴다.

3. 인간의 선호도에 맞추기: LLM에 인간의 선호도를 통합하는 것은 원치 않는 행동을 완화하고 정확한 결과를 보장하는 데 중요한 장점을 제공합니다. InstructGPT [20]와 같은 정렬에 대한 초기 작업은 3단계 접근법, 지시어 조정, 보상 모델링 및 강화 학습을 통해 GPT-3를 정렬합니다. 지도된 세밀 조정 GPT-3은 데모를 기반으로 쿼리되어 응답을 생성하며, 인간 레이블러는 인간의 가치에 따라 순위를 매깁니다. 그리고 순위가 매겨진 데이터를 기반으로 보상 모델이 훈련됩니다. 마지막으로, 보상 모델에서 생성된 데이터에 대한 보상을 사용하여 GPT-3가 근접 정책 최적화 (PPO)를 사용하여 훈련됩니다.

표 II: 교육에 맞춰진 대형 언어 모델 연구에서의 주요 인사이트 및 발견 사항.

모델들                             발견 및 통찰

T0

• 멀티태스크 프롬프팅은 제로샷 일반화를 가능하게 하며, 기준 모델보다 성능이 우수합니다.
• 데이터셋 작업당 단 하나의 프롬프트조차도 성능 향상에 충분합니다.

웹GPT

• LLM의 답변 품질은 인간의 피드백으로 더욱 개선될 수 있습니다.
• 모델이 관련 정보를 효과적으로 걸러내고 활용할 수 있도록 돕기 위해, 인간 라벨러는 검색된 문서의 유용성에 관한 질문에 중요한 역할을 합니다.
• 세밀하게 조정된 언어 모델을 텍스트 기반 웹 브라우징 환경과 상호작용시킴으로써, 모방 학습과 강화 학습을 통해 종단 간 검색과 종합을 개선할 수 있습니다.
• 참고 자료를 활용하여 답변을 생성하면 라벨러가 답변의 사실적 정확성을 쉽게 판단할 수 있습니다.

Tk-INSTRUCT

• 지시 튜닝은 보이지 않는 작업에 대한 강력한 일반화를 이끕니다.
• 더 많은 작업은 일반화를 향상시키지만, 작업 인스턴스만 늘리는 것은 도움이 되지 않습니다.
• 지도 학습된 모델이 일반화된 모델보다 더 좋습니다.
• 지시와 예제로 사전 훈련된 모델은 다양한 유형의 입력에 대해 잘 수행됩니다.

mT0와 BLOOMZ

• 지시 튜닝은 이전에 본 적 없는 작업에 대한 제로샷 일반화를 가능하게 합니다.
• 다국어 훈련은 영어와 비영어 모두에 대한 제로샷 일반화를 더욱 향상시킵니다.
• 기계 번역된 프롬프트로 훈련하는 것은 비영어 프롬프트를 가진 보류 작업의 성능을 향상시킵니다.
• 영어만을 대상으로 한 다국어 사전 훈련 언어 모델의 세밀 조정은 다른 사전 훈련 언어 작업에 대한 일반화에 충분합니다.

OPT-IML

• 대부분의 작업 예제를 포함하는 배치를 생성하기 위한 작업 크기 샘플링은 성능을 향상시키기 위해 중요합니다.
• 단순한 예제 비례 샘플링만으로는 충분하지 않으며, 훈련 데이터셋/벤치마크도 비례해야 일반화/성능이 향상됩니다.
• 완전히 보류된 작업과 부분적으로 지도된 작업의 성능은 작업이나 범주를 확장함으로써 향상되지만, 완전히 지도된 작업에는 영향을 미치지 않습니다.
• 미세 조정 중 5% 정도의 사전 훈련 데이터를 포함하는 것이 효과적입니다.
• 추론 데이터를 1%만 추가하면 성능이 향상되지만, 더 많이 추가하면 성능이 저하됩니다.
• 대화 데이터를 추가하면 성능이 더 나빠집니다.

플랑

• CoT와 함께한 파인튜닝은 보유한 작업의 성능을 향상시킵니다.
• CoT 데이터와 함께한 파인튜닝은 추론 능력을 향상시킵니다.
• CoT 튜닝은 제로샷 추론을 향상시킵니다.
• 작업이 더 많아질수록 성능이 향상됩니다.
• 사전 훈련된 모델에는 도전적인 사용성을 개선하기 위해 지시어 파인튜닝이 필요합니다.
• 지시어 튜닝을 통해 모델의 성능을 향상시키는 것은 계산 효율적입니다.
• 멀티태스크 프롬프팅은 LLM에서 제로샷 일반화 능력을 가능하게 합니다.

참새

• 라벨러의 판단과 정의된 규칙과의 일치는 모델이 더 나은 응답을 생성하는 데 도움이 될 수 있습니다.
• 좋은 대화 목표는 에이전트와 평가자를 위한 자연어 규칙으로 세분화될 수 있습니다.
• 강화 학습(RL)과 재순위화의 조합은 선호도 승률과 적대적 탐사에 대한 탄력성 면에서 최적의 성능을 제공합니다.

위자드코더 • 복잡한 세트로 재작성된 지시어 튜닝 데이터로 세부 조정을 하면 성능이 크게 향상됩니다.

LLaMA-2-Chat

• 모델은 안전한 시나리오에서의 데모를 통해 안전한 응답을 작성하는 방법을 학습하며, 추가적인 RLHF 단계는 모델의 안전성을 더욱 향상시키고 탈옥 공격에 덜 취약하게 만듭니다.

리마 • 고품질 데이터보다는 적은 양의 데이터로도 세밀하게 조정된 모델이 일반화에 충분하다.

LLaMA 2-Chat [21]은 보상 모델링을 도움과 안전 보상으로 나누고, PPO에 추가로 거부 샘플링을 사용하여 정렬을 개선합니다. LLaMA 2-Chat의 초기 4개 버전은 거부 샘플링과 PPO를 사용하여 세밀하게 조정됩니다. 

지원되는 증거와의 정렬: 이 정렬 스타일은 모델이 증거와 사실을 포함한 응답을 생성할 수 있도록 하며, 환각을 줄이고 인간을 보다 효과적으로 지원하여 모델의 출력에 대한 신뢰를 높입니다. RLHF 훈련 스타일과 유사하게, 웹 인용을 포함한 생성된 응답을 순위 매기기 위해 보상 모델이 훈련되며, 이후 모델을 훈련하는 데 사용됩니다. 이와 같은 방식은 GopherCite [157], WebGPT [158], Sparrow [159]에서 사용됩니다. Sparrow [159]의 순위 모델은 선호 보상과 규칙 보상으로 나뉘며, 인간 주석자들은 모델을 공격하여 규칙을 깨뜨립니다. 이 두 가지 보상은 RL로 훈련할 응답을 순위 매기는 데 사용됩니다.

SFT와 직접 정렬: RLHF 파이프라인의 PPO는 복잡하고 메모리 집약적이며 불안정하여 안정적인 훈련을 위해 추가적인 작업이 필요합니다.

다중 모델, 보상, 가치, 정책 및 참조 모델.
이 복잡한 정렬 파이프라인을 피하는 것은 가능합니다.
[160], [161], [162]와 같이 지도된 세밀한 조정 (SFT) 파이프라인에 최소한의 변경을 통합함으로써 PPO와 비교 가능한 또는 더 나은 성능을 얻을 수 있습니다.
직접 선호도 최적화 (DPO) [160]는 선호되는 응답의 가능성을 최대화하기 위해 모델을 직접 인간의 선호 대 선호하지 않는 응답에 대한 우도를 최대화하도록 훈련시킵니다. 샘플당 중요도 가중치를 사용합니다.
보상 순위 세밀 조정 RAFT [161]는 보상 모델에 의해 순위가 매겨진 응답에 대해 모델을 세밀 조정합니다. 선호도 순위 최적화 (PRO) [163] 및 RRHF [162]는 모델을 인간의 선호도에 따라 응답을 순위 매기도록 하고 지도 손실을 가중치로 합니다. 반면에, chain-of-hindsight (CoH) [164]는 좋은 응답 대 나쁜 응답을 학습하기 위해 보상이 아닌 언어로 모델에 피드백을 제공합니다.
합성 피드백과의 정렬: 인간의 피드백과 LLMs를 정렬하는 것은 느리고 비용이 많이 듭니다. 문헌은 LLMs를 정렬하기 위해 LLMs에 프롬프트를 제공하여 반자동 프로세스를 제안합니다.

쿼리에 대한 도움이 되는, 정직하고 윤리적인 응답을 생성하고,
새로 생성된 데이터셋을 사용하여 세부 조정합니다. 헌법
AI [165]는 인공지능과 함께 RLHF에서 인간의 피드백을 대체하여
RLAIF라고 부릅니다. AlpacaFarm [166]은
LLMs API를 사용하여 인간의 피드백을 모방하기 위한 프롬프트를 설계합니다.
헌법 AI와는 반대로, AlpacaFarm은 피드백에 노이즈를 주입하여
인간의 실수를 복제합니다. Self-Align [94]는 LLM에 ICL 예제를 제시하여
LLM에게 응답이 유용하고 윤리적으로 간주되기 위해 포함되어야 하는 내용을 지시합니다.
동일한 LLM은 나중에 새로운 데이터셋으로 세부 조정됩니다.
프롬프트와 일치시키기: LLM은 훈련 없이도 프롬프트로 조종될 수 있습니다.
[167], [168]에서 자기 수정 프롬프팅은
지시와 CoT를 질문과 연결하여 모델이
실제 답변 전에 지시에 따라 동작하도록 안전성을 보장합니다. 이 전략은
생성된 응답의 피해를 크게 줄이는 것으로 입증되었습니다.
레드팀/제재/적대적 공격: LLM은
적대적 탐사를 통해 유해한 행동, 환각, 개인 정보 유출
및 기타 결함을 나타냅니다. 모델은 안전을 위해 조정되었음에도 불구하고
[169], [170]에서도 유해한 응답을 생성할 수 있습니다. 레드팀은
불법적인 출력을 해결하기 위한 일반적인 접근 방식으로,
LLM은 유해한 출력을 생성하도록 유도됩니다. [170], [171]을 통해
레드팀을 통해 수집된 데이터셋은 모델의 안전성을 위해 세부 조정하는 데 사용됩니다.
레드팀은 주로 인간 주석가에 의존하지만, 다른 작업 [172]은 다른 LLM의 유해한 출력으로 이어지는 프롬프트를 찾기 위해 LLM을 레드팀으로 테스트합니다.

4. 계속해서 사전 훈련을 진행합니다: 세밀 조정은 모델의 성능을 향상시키지만, 이전에 학습한 정보를 잊어버리는 치명적인 문제를 야기합니다. 매 반복마다 세밀 조정 데이터와 몇 개의 임의로 선택된 사전 훈련 샘플을 연결하여 네트워크의 기억력을 유지합니다 [173], [144]. 이는 세밀 조정 데이터가 적고 원래 용량을 유지해야 하는 경우 LLMs를 적응시키는 데에도 효과적입니다. 프롬프트 기반 지속적인 사전 훈련 (PCP) [174]은 과제와 관련된 텍스트와 지시사항으로 모델을 훈련시킨 다음 최종적으로 하위 작업에 맞게 모델을 조정합니다.

5. 샘플 효율성: 파인튜닝 데이터는 일반적으로 사전 훈련 데이터보다 훨씬 작지만, 성능을 위해서는 충분히 크여야 하며, 비례하는 컴퓨팅 자원이 필요합니다. 기존 문헌 [175], [176]에서는 데이터가 적은 모델이 더 많은 데이터로 훈련된 모델보다 성능이 우수할 수 있다는 것을 발견했습니다. [175]에서는 총 하류 데이터의 25%가 최첨단 성능을 위해 충분하다고 합니다. [176]에서는 코어셋 기반의 0.5%의 총 지시어 튜닝 데이터를 선택함으로써, 완전한 데이터 튜닝과 비교하여 모델 성능이 2% 향상되었습니다. 정렬을 위한 더 적은 것 (LIMA) [177]은 정교하게 생성된 1000개의 데모만을 사용하여 모델을 파인튜닝하였으며, GPT-4와 비교하여 비슷한 성능을 달성하였습니다.

C. 문맥 창 크기 증가

LLM은 비싼 어텐션과 높은 메모리 요구로 인해 제한된 문맥 창에서 훈련됩니다. 제한된 시퀀스 길이로 훈련된 모델은 추론 시에 보이지 않는 길이에 대해 일반화할 수 없습니다. 대안으로, ALiBi [65] 위치 인코딩을 사용하는 LLM은 제로샷 길이 추정을 수행할 수 있습니다. 그러나 ALiBi는 표현력이 더 낮으며 [66], 여러 벤치마크에서 성능이 떨어집니다 [42], 그리고 많은 LLM은 제로샷 추정을 수행할 수 없는 RoPE 위치 임베딩을 사용합니다. 더 긴 문맥 길이는 더 긴 문서의 더 나은 이해, 인컨텍스트 학습에서 더 많은 샘플, 더 큰 추론 프로세스 실행 등의 이점이 있습니다. 세밀 조정 중에 문맥 길이를 확장하는 것은 느리고 비효율적이며 계산 비용이 많이 듭니다 [45]. 따라서 연구자들은 아래에서 논의되는 다양한 문맥 창 추정 기술을 사용합니다.
위치 보간: [45]는 외삽보다 사전 훈련된 문맥 창 내에서 위치 인코딩을 보간하는 것이 더 효과적임을 보여줍니다. 이 연구는 원래 문맥 크기와 비교하여 더 큰 창에서 성능 손실 없이 더 나은 결과를 얻기 위해 단지 1000 단계의 세밀 조정이 충분하다는 것을 보여줍니다. Giraffe [42]는 RoPE에서 전력 스케일링을 사용하고, YaRN [43]은 NTK-aware 보간을 제안합니다.
효율적인 어텐션 메커니즘: 밀집 전역 어텐션은 더 큰 문맥 창 LLMs의 주요 제약 중 하나입니다. 로컬, 희소 및 확장된 어텐션과 같은 효율적인 어텐션 변형을 사용하면 계산 비용을 크게 줄일 수 있습니다. LongT5 [44]는 일시적인 전역 어텐션 (TGlobal)을 제안하여 로컬 및 전역 토큰에 어텐션을 적용합니다 (윈도잉 토큰 평균화). 이 모델은 T5 [10]의 어텐션을 TGlobal 어텐션으로 대체하고, 4098 시퀀스 길이로 모델을 사전 훈련시키고, 16k와 같은 큰 창 크기로 세밀 조정을 수행하여 더 긴 입력으로 작업 성능을 향상시킵니다. 이는 세밀 조정만으로도 TGlobal 어텐션의 추정 능력을 보여줍니다. COLT5 [179]는 경량 및 중량 어텐션 및 피드포워드 레이어를 사용하는 두 개의 브랜치를 사용합니다. 모든 토큰은 경량 브랜치에서 처리되고, 중요한 토큰만 중량 브랜치로 라우팅됩니다. LongNet [180]은 표준 어텐션을 확장된 어텐션으로 대체하여 시퀀스 길이를 10억 토큰까지 확장합니다. LongLoRA [181]는 세밀 조정 중에 밀집 어텐션 비용을 줄이기 위해 시프트-숏 어텐션을 제안하며, 추론 중에는 밀집 어텐션을 사용하여 전체 어텐션 세밀 조정과 유사한 성능을 달성할 수 있습니다.
훈련 없는 외삽: LM-Infinite [178]와 병렬 문맥 창 (PCW) [182]는 사전 훈련된 LLM을 사용하여 길이 외삽이 가능함을 보여줍니다. LM-Infinite는 원래 문맥 창 제한 내에서 Λ-모양 어텐션을 제안합니다. 마찬가지로, PCW는 더 큰 입력을 사전 훈련된 문맥 길이로 나누고 각 청크에 동일한 위치 인코딩을 적용합니다.

로봇공학

LLM은 다목적 능력으로 인해 과학 커뮤니티에서 다양한 분야에서 빠르게 채택되었습니다 [46]. 로봇 공학 연구에서도 LLM은 인간-로봇 상호작용을 향상시키는 등 매우 유망한 응용 분야가 있습니다.

액션 [28], [183], [184], [185], 작업 계획 [186], [187],
[188], 탐색 [189], [190], 그리고 학습 [191], [192].
이들은 로봇이 자연어를 이해하고 생성할 수 있게 해주어
지시 따르기, 데이터 주석 작성 및
공동 문제 해결에 도움이 될 수 있습니다. 이들은 로봇이 다양한 소스에서 정보에 접근하고 통합하여 지속적인 학습을 가능하게 할 수 있습니다. 이는 로봇이 새로운 기술을 습득하고 변화에 적응하며 실시간 데이터를 기반으로 성능을 개선하는 데 도움이 될 수 있습니다.
LLM은 편향 완화 및 통합 복잡성과 같은 도전과제에도 불구하고 로봇 공학에서 시뮬레이션 환경을 시험하고 혁신적인 연구의 잠재력을 제공하기 시작했습니다. [193]의 연구는 로봇 가정 청소 작업을 개인화하는 데 초점을 맞추고 있습니다. LLM을 언어 기반 계획 및 인식과 결합하여 사용자가 객체 배치 예시를 제공하도록 하여 LLM이 요약하여 일반화된 선호도를 생성하도록 하는 것을 보여줍니다. 이를 통해 로봇은 몇 가지 예시에서 사용자 선호도를 일반화할 수 있습니다. [26]에서는 센서 입력이 언어 토큰과 함께 임베딩되어 실제 상황에서 의사 결정을 강화하는 Transformer 기반 언어 모델을 사용하는 신체화된 LLM이 소개됩니다. 이 모델은 다양한 신체화된 작업을 위해 끝-끝으로 훈련되며, 언어 및 비전 도메인에서의 다양한 훈련을 통해 긍정적인 전이를 달성합니다. LLM은 인간-로봇 상호작용을 향상시키기 위한 제로샷 인간 모델로도 탐구되었습니다.
[28]의 연구는 방대한 텍스트 데이터로 훈련된 LLM이 특정 HRI 작업에 효과적인 인간 모델로서 작동할 수 있음을 보여줍니다. 이는 전문화된 기계 학습 모델과 비슷한 예측 성능을 달성합니다. 그러나 프롬프트에 대한 민감성과 공간/숫자 추론에 대한 어려움과 같은 제한 사항이 확인되었습니다. 다른 연구 [194]에서는 LLM을 자연어 피드백의 소스를 추론하기 위해 사용하여 로봇 제어 시나리오에서 행동을 처리하고 계획하는 능력을 향상시키는 "내부의 내적 대화"를 형성합니다. 그들은 LLM을 다양한 형태의 텍스트 피드백과 결합하여 LLM이 결론을 결정에 통합하도록 허용하여 다양한 도메인에서 사용자 지시의 실행을 개선하는 프로세스에 포함시킵니다. 이는 테이블 재배치 및 이동 조작을 포함한 시뮬레이션 및 실제 세계 로봇 작업을 포함한 다양한 도메인에서 사용자 지시의 실행을 개선하기 위해 LLM을 핵심 메커니즘으로 사용합니다.
계획: LLM은 로봇 공학에서 전략적 계획 [186], [195], [196]을 위해 점점 더 중요해지고 있습니다. 자연어를 처리하고 생성하는 능력은 인간-로봇 상호작용을 향상시키고 언어 지시에 기반한 복잡한 작업을 이해하고 실행하는 로봇을 가능하게 하는 데 중요합니다. LLM은 또한 과제 계획에서도 핵심적인 역할을 담당하며, 특정 목표를 달성하기 위해 순차적인 동작을 결정하는 더 높은 수준의 인지적 과정을 포함합니다. 이 능력은 자율적인 제조 공정부터 가사일에 이르기까지 다양한 응용 분야에서 중요한 의미를 가지는데, 다단계 지시를 이해하고 실행하는 능력이 매우 중요합니다.
조작: 조작 영역에서 [197], [198], [199], [200] LLM은 로봇의 능숙성과 적응성을 향상시키며, 물체 인식, 잡기 및 협력 작업과 같은 작업에서 뛰어납니다.

노동. 그들은 시각적 및 공간적 정보를 분석하여 물체와 상호작용하는 가장 효과적인 방법을 결정하며, 정밀성과 유연성이 필요한 수술 절차나 조립 라인 작업과 같은 작업에서 귀중한 역할을 한다. 또한, 센서 입력과 언어적 단서를 몸체 기반 프레임워크에 통합하여 실제 상황에서의 의사 결정을 향상시킨다. 언어와 시각 영역을 아우르는 다양한 훈련 데이터에서 통찰력을 얻고 일반화할 수 있도록 함으로써 다양한 몸체 작업에서 모델의 성능을 향상시킨다.

내비게이션: LLM은 로봇 공학에서 내비게이션을 혁신적으로 바꿨으며, 복잡한 환경에서 로봇의 정확성과 적응성을 향상시키는 큰 잠재력을 제공한다. 특히, 모션 플래닝은 LLM이 뛰어난 성과를 보여준 중요한 영역으로, 로봇을 위한 실행 가능한 경로와 궤적을 생성하는 데 능숙하게 사용되며, 복잡한 환경 세부 사항을 고려한다. 이 능력은 창고, 교통 및 의료 시설, 스마트 주택과 같은 환경에서 정확하고 동적으로 적응 가능한 내비게이션을 요구하는 상황에서 특히 가치가 있다. LLM은 또한 로봇 내비게이션의 기초 구성 요소인 위치 파악과 매핑에도 중요한 역할을 한다. 이를 통해 로봇은 환경 내에서 정확한 위치를 판단하고 동시에 주변 공간의 표현을 구축하거나 업데이트할 수 있다. 이 능력은 자율 탐사, 수색 및 구조 작전, 이동 로봇의 작업을 포함한 공간 인식을 필요로 하는 작업에 중요하다. 또한, 장애물과 동적 변화를 고려하면서 환경 내에서 충돌 없는 내비게이션의 능력을 크게 향상시켰으며, 자동 유도 차량(AGV) 및 배송 로봇(SADR - 배송인 없이 고객에게 물건을 전달하는 보행자 크기의 로봇)의 작업에서 정확성과 신뢰성을 요구하는 미리 정의된 경로를 횡단하는 로봇의 작업에 중요한 역할을 한다.

E. 다중 모달 LLMs

자연어 처리 응용 프로그램에서 LLM의 성공에 영감을 받아, 점점 더 많은 연구 작업들이 이미지 [205], [206], [207], 비디오 [208], [209], [210], 오디오 [211], [210], [212] 등과 같은 다양한 정보의 다양한 모드를 인식할 수 있도록 LLM을 지원하고 있습니다. 다중 모달 LLM(MLLM)은 텍스트만 처리하는 표준 LLM에 비해 상당한 이점을 제공합니다. 다양한 모드에서 정보를 통합함으로써, MLLM은 더 깊은 문맥 이해를 달성하여 다양한 표현이 담긴 더 지능적인 응답을 할 수 있습니다. 중요한 점은, MLLM은 우리의 다중 감각적 입력의 상호작용적인 특성을 활용하여 세계의 포괄적인 이해를 형성하는 데 기여합니다 [212], [26]. 사용자 친화적 인터페이스와 결합된 MLLM은 직관적이고 유연하며 적응 가능한 상호작용을 제공하여 사용자가 다양한 입력 방법을 통해 지능형 어시스턴트와 상호작용할 수 있도록 합니다. 현재의 MLLM은 모델 구축 방식에 따라 일반적으로 세 가지로 나뉠 수 있습니다.

사전 훈련, 세부 조정 및 프롬프팅. 이 섹션에서는 이러한 주요 흐름에 대한 자세한 내용과 시각적 추론에서 MLLM의 중요한 응용에 대해 논의할 것입니다.
사전 훈련: MLLM의 이 흐름은 통합된 end-to-end 모델을 사용하여 다양한 모드를 지원하기 위한 것입니다. 예를 들어, Flamingo [205]는 사전 훈련 및 고정된 시각 인코더와 LLM에서 수집된 시각 및 언어 모드를 융합하기 위해 게이트 크로스 어텐션을 적용합니다. 또한, BLIP-2 [206]는 시각과 언어 모드 간의 정렬을 위해 Querying Transformer (Q-Former)를 사전 훈련하기 위한 두 단계 전략을 제안합니다. 첫 번째 단계에서는 고정된 시각 인코더에서 시각-언어 표현 학습을 시작하고, 두 번째 단계에서는 고정된 LLM에서 시각-언어 생성 학습을 위해 제로샷 이미지-텍스트 생성을 수행합니다. 비슷하게, MiniGPT-4 [213]도 사전 훈련 및 고정된 ViT [214], Q-Former 및 Vicuna LLM [151]을 사용하며, 시각 및 언어 모드 정렬을 위해 선형 투영 계층만 훈련됩니다.
세부 조정: NLP 작업 [20], [16], [93]을 위한 지시어 조정 [16]에서 파생된 이 방법을 사용하여 사전 훈련된 LLM을 다중 모달 지시어로 세부 조정합니다. 이 방법을 따르면, LLM은 다중 모달 챗봇 [213], [207], [29] 및 다중 모달 작업 해결자 [215], [30], [216]로 쉽고 효과적으로 확장될 수 있습니다. 이 흐름의 MLLM의 핵심 문제는 세부 조정을 위한 다중 모달 지시어 수집입니다. 이 문제를 해결하기 위해 벤치마크 적응 [215], [218], [219], 자기 지도 학습 [19], [31], [220] 및 혼합 구성 [221], [216]의 해결책이 각각 사용됩니다. 원래 언어 모드와 추가 모드 간의 격차를 완화하기 위해 학습 가능한 인터페이스가 도입되어 고정된 사전 훈련 모델에서 다른 모드를 연결합니다. 특히, 학습 가능한 인터페이스는 매개변수 효율적인 조정 방식으로 작동할 것으로 기대됩니다. 예를 들어, LLaMA-Adapter [222]는 효율적인 transformer 기반 어댑터 모듈을 적용하고, LaVIN [221]은 혼합 모달 어댑터를 사용하여 다중 모달 특징 가중치를 동적으로 학습합니다. 학습 가능한 인터페이스와 달리 전문가 모델은 다중 모달을 언어로 직접 변환할 수 있습니다. 예를 들어, VideoChat-Text [208]는 음성 인식 전문가 모델인 Whisper [223]를 통합하여 주어진 비디오의 캡션을 생성하여 LLM의 이해를 위해 사용합니다.
프롬프팅: 과제별 데이터셋을 고려하여 모델 매개변수를 직접 업데이트하는 세부 조정 기술과는 달리, 프롬프팅 기술은 모델 매개변수를 변경하지 않고도 모델에 특정 컨텍스트, 예제 또는 지시사항을 제공하여 특화된 작업을 수행합니다. 프롬프팅은 대규모 다중 모달 데이터의 필요성을 크게 줄일 수 있기 때문에 MLLM을 구축하는 데 널리 사용되는 기술입니다. 특히, 다중 모달 Chain of Thought (CoT) 문제 [101]를 해결하기 위해 LLM은 다중 모달 입력을 고려하여 추론 과정과 답변을 생성하도록 프롬프팅됩니다. 이를 위해 다양한 학습 패러다임이 실제로 활용됩니다. 예를 들어, Multimodal-CoT [224]는 근거 생성과 답변 추론의 두 단계로 구성되며, 두 번째 단계의 입력은 원래 입력과 첫 번째 단계의 출력의 조합입니다. CoT-PT [225]는 프롬프팅 튜닝과 특정 시각적 편향을 모두 적용하여 추론 체인을 암묵적으로 생성합니다. CoT 문제 외에도, LLM

멀티모달 설명과 도구를 사용하여 유도될 수도 있으며,
복잡한 작업을 효과적으로 하위 작업으로 분할합니다 [226], [227].
시각적 추론 응용 프로그램: 최근 시각적 추론 시스템 [228], [229], [230], [231]은 더 나은 시각 정보 분석과 시각-언어 통합을 위해 LLM을 적용하는 경향이 있습니다.
이전 작업 [232], [233]과는 달리, 제한된 VQA 데이터셋과 소규모 신경망에 의존하는 현재의 LLM 지원 방법은 강력한 일반화 능력, 신흥 능력 및 상호작용 능력의 이점을 제공합니다 [217].
LLM의 도움으로 시각적 추론을 실현하기 위해 유도 및 세밀 조정 기술도 활용될 수 있습니다. 예를 들어, PointClip V2 [229]는 LLM을 사용하여 3D 특정 프롬프트를 생성하며, 이는 텍스트 기능으로 인코딩되고 시각적 기능과 결합되어 3D 인식에 사용됩니다. 또한 GPT4Tools [31]는 도구 관련 지침을 따라 LLM을 세밀 조정하기 위해 LoRA [234]를 사용합니다.
컨트롤러 [231], 의사 결정자 [235] 또는 의미 정제자 [228], [236]로서, LLM은 시각적 추론 연구의 진전을 크게 도움니다.

F. 증강된 LLMs

LLM은 입력과 연결된 예제로부터 학습할 수 있는 능력을 갖추고 있으며, 이를 문맥 증강, 문맥 학습 또는 페우샷 프롬프팅이라고 합니다. LLM은 페우샷 프롬프팅을 통해 적은 양의 예시로부터 보이지 않은 작업에 대해 우수한 일반화 능력을 보여주며, 훈련 중에 습득한 능력 이상의 질문에 답변할 수 있게 합니다. 이러한 신생 능력은 고가의 파인튜닝 없이 모델을 적응시킬 수 있도록 해줍니다. 이 외에도 LLM에서는 허구, 부정확하거나 사실적으로 부정확한 응답을 생성하는 환각이 흔하며, 이는 문맥 데이터를 증강함으로써 피할 수 있습니다. 사용자는 쿼리에 문맥 샘플을 제공할 수 있지만, 여기서는 외부 저장소에 프로그래밍 방식으로 액세스하는 방법을 통해 증강된 LLM을 지칭합니다. 문헌에는 LLM을 증강하기 위한 다양한 외부 메모리 디자인이 제안되었으며, 장기 [237], [238], [239], [240], 단기 [241], 상징적 [242], 비상징적 [243], [244]으로 분류됩니다. 메모리는 문서, 벡터 또는 데이터베이스와 같은 다른 형식으로 유지될 수 있습니다. 일부 시스템은 중간 메모리 표현을 유지하여 여러 반복에서 정보를 보존합니다 [240], [238], 반면 다른 시스템은 데이터셋에서 중요한 정보를 추출하여 메모리에 저장하여 추후에 검색할 수 있도록 합니다 [245]. 메모리의 읽기 및 쓰기 작업은 LLM의 협력 여부에 따라 수행될 수 있으며, [238], [246], [240], [247]에서 피드백 신호로 작용합니다. 아래에서 다양한 유형의 증강된 LLM에 대해 논의합니다.
1. 검색 증강 LLM: LLM은 제한된 메모리와 오래된 정보를 가지고 있어 부정확한 응답을 낼 수 있습니다. 외부 최신 저장소에서 관련 정보를 검색하여 LLM은 정확한 답변을 할 수 있고 더 많은 정보를 활용할 수 있습니다. 검색 증강을 통해 작은 모델도 큰 모델과 비슷한 성능을 발휘할 수 있습니다. 예를 들어, 11B 모델은 [25]에서 540B PaLM과 경쟁할 수 있으며, 7.5B 모델은 [239]에서 280B Gopher와 경쟁할 수 있습니다. 검색 증강 언어 모델링(RALM)은 Figure 12에 나와 있는 두 가지 주요 구성 요소를 갖고 있습니다. 즉, 1) 검색기와 2) 언어 모델입니다. RALM에서 검색기는 LLM의 응답을 주도하는 중요한 역할을 합니다.

그림 12: 검색 보강 LLM의 플로우 다이어그램. 검색기는 입력과 유사한 맥락을 추출하여 LLM에게 전달합니다. 이는 간단한 언어로 또는 Fusion-in-Decoder (FiD)를 통해 인코딩된 상태로 전달될 수 있습니다. 작업에 따라 검색과 생성이 여러 번 반복될 수 있습니다.

잘못된 정보는 LLM을 잘못된 행동으로 이끌 수 있습니다. 이로 인해 정확한 정보를 검색하고 쿼리와 퓨즈하기 위해 다양한 방법이 개발되었습니다. 
제로샷 검색 보강: 이러한 보강은 원래 LLM 아키텍처와 가중치를 유지하며 BM25 [248], 최근접 이웃 또는 Bert [7]와 같은 미리 훈련된 모델을 검색기로 사용합니다. 검색된 정보는 응답 생성을 위해 모델에 입력으로 제공되며, 검색 없이 LLM보다 성능을 향상시키는 것으로 나타냅니다. 일부 시나리오에서는 작업을 완료하기 위해 여러 번의 검색 반복이 필요합니다. 첫 번째 반복에서 생성된 출력은 유사한 문서를 가져오기 위해 검색기로 전달됩니다. 전방적인 액티브 검색 (FLARE) [243]은 초기에 응답을 생성하고 응답에 낮은 신뢰도 토큰이 포함되어 있다면 관련 문서를 검색하여 출력을 수정합니다. 마찬가지로 RepoCoder [250]는 코드 완성을 위해 재귀적으로 코드 스니펫을 가져옵니다. 
검색 보강으로 훈련: 검색 보강 생성 (RAG)에서 실패를 줄이기 위해 연구자들은 검색 보강 파이프라인으로 검색기와 LLM을 훈련하거나 세밀하게 조정합니다. 파이프라인의 각 훈련 과정에 중점을 둔 문헌을 아래에서 논의합니다. 
LLM 훈련: 검색 보강 파이프라인을 사용하여 작은 LLM을 사전 훈련하는 RETRO [239]는 RAG 없이 훈련된 GPT-3와 같은 큰 LLM보다 우수한 성능을 보입니다. RETRO는 MassiveText의 2조 토큰 하위 집합을 데이터베이스로 사용합니다. 검색 파이프라인은 입력 쿼리를 하위 집합으로 나누고 각 하위 집합에 대해 데이터베이스에서 관련 청크를 검색하여 토큰 생성을 위해 입력 중간 표현과 함께 인코딩합니다. 이는 이전 청크에 자기 회귀적으로 주의를 기울이기 위해 교차 청크 어텐션을 사용합니다. RETRO에 대한 연구 [251]는 RAG 없이 사전 훈련된 모델이 RAG를 사용하여 세밀하게 조정된 경우 얻은 성능 향상을 얻지 못한다는 것을 보여줍니다. 
검색기 훈련: LLM이 생성하는 응답의 품질은 문맥 예제에 크게 의존합니다.

따라서, [252], [253], [254], [255]은 LLM을 generation을 위해 동결시키면서 정확한 few-shot 샘플을 검색하기 위해 retriever를 훈련시킵니다. 검색된 샘플은 [252], [254]에서 대조적 학습을 통해 ground-truth 데이터를 구축하기 위해 순위를 매깁니다. RoBERTa는 ICL 샘플 검색을 위해 [253]에서 다운스트림 작업을 위해 훈련됩니다. REPLUG [255]는 동결된 LLM-generated 출력에서 지도 신호를 사용하여 retriever를 훈련시킵니다.
Retriever와 LLM 훈련: [25], [256], [257]에서 retriever와 모델을 함께 훈련함으로써 추가적인 이점을 얻을 수 있습니다. 이 경우, 오류가 retriever로 전파되어 언어 모델과 retriever 모두를 업데이트합니다. 마스크된 언어 모델링(MLM)은 일반적인 사전 훈련 목적입니다 [25], [257], 검색 사전 훈련 변환기(RPT) [256]는 긴 텍스트 모델링을 위해 문서 청크 예측을 사전 훈련 목적으로 사용했습니다.
인코딩된 컨텍스트 증강: 검색된 문서를 쿼리와 연결하는 것은 시퀀스 길이와 샘플 크기가 커짐에 따라 불가능해집니다. 컨텍스트를 인코딩하고 디코더와 융합시켜 (Fusion-in-Decoder) 교차 어텐션을 사용하여 더 많은 샘플을 증강시킬 수 있습니다. 이는 계산 비용을 크게 증가시키지 않고 가능하게 합니다 [258], [239], [256], [25].
웹 증강: LLM과는 별개로 로컬에 저장된 메모리는 제한된 정보만을 가지고 있습니다. 그러나 인터넷에는 많은 양의 정보가 정기적으로 업데이트되는데 이를 활용할 수 있습니다. 로컬에 정보를 저장하는 대신 다양한 방법을 사용하여 웹 검색을 통해 쿼리와 관련된 컨텍스트를 검색하고 LLM에 전달합니다 [259], [260], [158].

2. 도구 보조 LLMs: RAG는 질의에 대답하기 위해 리트리버가 LLM에 문맥을 제공하는 것에 의존하지만, 도구 보조 LLMs는 LLM의 추론 능력을 활용하여 작업을 하위 작업으로 분할하고 필요한 도구를 선택하며 작업을 완료하기 위해 조치를 취하는 것에 중점을 둡니다 [261], [262], [263], [27]. 도구 보조 LLMs의 일반적인 파이프라인은 그림 13에 나와 있으며, 그림 13의 다른 모듈은 작업 완료까지 반복적으로 선택됩니다.
제로샷 도구 보강: LLM의 문맥 학습 및 추론 능력은 훈련 없이 도구와 상호 작용할 수 있도록 합니다. 자동 추론 및 도구 사용 (ART) [263]는 추론 단계와 외부 도구 호출의 데모를 사용하여 작업 라이브러리를 구축합니다. 유사한 작업 예제를 검색하고 LLM에게 추론을 위한 문맥을 제공합니다. 이 외에도 [264]에서는 도구 문서만으로도 LLM에게 도구 사용을 가르칠 수 있다고 보여줍니다. RestGPT [265]는 LLM을 RESTful API와 통합하여 작업을 계획 및 API 선택 단계로 분해합니다. API 선택기는 작업에 적합한 API를 선택하고 실행 계획을 세우기 위해 API 문서를 이해합니다. ToolkenGPT [266]는 도구를 토큰으로 사용하여 도구 임베딩을 다른 토큰 임베딩과 연결합니다. 추론 중에 LLM은 도구 호출을 나타내는 도구 토큰을 생성하고 텍스트 생성을 중지하며 도구 실행 출력을 사용하여 다시 시작합니다.
도구 보강 훈련: LLMs는 다양한 도구와 상호 작용하도록 훈련되며, 제로샷 도구 보강의 한계를 극복하기 위해 계획 능력을 향상시킵니다 [267], [27], [268], [269]. Gorilla [267]는 API 문서에서의 정보 검색을 통해 LLaMA를 지시 튜닝합니다. 자체적으로 PREPRINT 20

그림 13: 도구 보조 LLM의 기본 플로우 다이어그램. 입력과 사용 가능한 도구 세트가 주어지면, 모델은 작업을 완료하기 위한 계획을 생성합니다. 도구 보조 LLM은 작업에 따라 검색기, 도구 실행, 메모리에 읽기-쓰기, 피드백 등과 같은 다양한 모듈을 반복적으로 활용합니다.

GPT-4를 사용하여 API 문서에서 검색한 문맥 예시를 제공하여 데이터 생성 파이프라인을 지시합니다. 도구 증강 언어 모델(TALM) [27]은 T5 [10]를 도구 사용을 위해 자가 대결 방식으로 세밀하게 조정합니다. 여기서 도구 조작 작업을 반복적으로 완료하고 훈련 세트에 포함시킴으로써 자가 대결 접근 방식을 사용합니다. ToolLLM [269]은 Rapi-dAPI에서 16k개의 API를 수집합니다. 목록에서 API를 샘플링하여 단일 도구 및 다중 도구 시나리오에서 ChatGPT를 사용하여 지시 조정 데이터셋을 생성합니다. 고품질 데이터셋을 위해 ToolLLM은 다양한 추론과 계획을 가진 그라운드 트루스를 생성하기 위해 깊이 우선 탐색 기반의 의사 결정 트리(DFSDT) 방법을 제안합니다. 다중 모달 도구 증강: LLM의 구성적 추론 능력은 LLM이 다중 모달 설정에서 도구를 조작할 수 있도록 합니다 [261], [262], [270]. Figure 13에 표시된 파이프라인을 따라 LLM은 일반적으로 계획 → 도구 선택 → 실행 → 검사 → 생성의 순서로 실행되는 계획을 개요로 제시하여 사용자 쿼리에 응답합니다. 여기서 도구 데이터베이스는 텍스트, 이미지 등과 같은 다양한 모달리티로 풍부합니다. 많은 다중 모달 도구 증강 시스템은 다중 모달 LLM [271], [272], [270], [262]을 사용하며, 다른 모달리티 도구를 사용하여 다중 모달 쿼리를 해결하기 위한 계획을 생성하는 단일 모달 LLM을 사용하는 시스템도 있습니다 [273].

IV. 결과 및 통찰력

기존의 작은 모델과 비교하여 10억 규모의 모델을 훈련시키는 것은 어렵습니다. LLM은 하드웨어 장애와 불안정성과 같은 다양한 불안정성을 겪을 수 있습니다. 이 외에도 LLM은 신생 능력, 향상된 제로샷, 퓨샷 및 추론 능력과 같은 다른 행동을 보입니다. 연구자들은 이러한 중요한 세부 사항들을 논문에 보고합니다.

결과 재현 및 현장 진행. 우리는 아키텍처, 훈련 전략 및 파이프라인과 같은 중요한 정보를 테이블 I와 II에서 식별합니다. 이러한 변경으로 인해 LLM의 성능이 향상되거나 기타 능력이 향상되는 사항을 섹션 III에서 언급합니다.

V. 모델 구성

이 섹션에서는 사전 훈련 및 지시어 조정 모델의 다양한 통계를 제공합니다. 이에는 테이블 III와 테이블 IV에 게재된 출판물 장소, 라이선스 유형, 모델 생성자, 훈련된 단계, 병렬성 등과 같은 정보가 포함됩니다. 사전 훈련된 LLM의 아키텍처 세부 정보는 테이블 V에서 확인할 수 있습니다. 지시어 조정 모델의 경우 이러한 세부 정보를 제공하는 것은 불필요합니다. 왜냐하면 지시어 데이터셋에 대해 사전 훈련된 모델을 세밀하게 조정하기 때문에 아키텍처 세부 정보는 기준선과 동일합니다. 또한 다양한 LLM의 최적화 설정은 테이블 VI와 테이블 VII에서 확인할 수 있습니다. 테이블 VII에는 정확도, 워머업 및 가중치 감소에 대한 세부 정보가 포함되어 있지 않습니다. 이러한 세부 정보는 지시어 조정 모델에 대해 언급할 필요가 없으며 논문에서 제공되지 않습니다.

VI. 데이터셋과 평가

훈련 및 평가 데이터셋 생성은 LLMs의 대규모 데이터 수요로 인해 비용이 많이 듭니다. 따라서 이러한 모델의 훈련 및 벤치마킹을 위한 데이터셋은 중요한 주제입니다. 그림 14에서는 다양한 NLP 작업을 위한 기존 데이터셋의 분포를 보여줍니다. 우리는 적어도 20개의 데이터셋을 포함하는 주요 작업만을 고려하여 분포를 제한합니다. LLMs는 이러한 데이터셋을 직접적으로 훈련 및 평가에 활용할 수 있습니다. LLMs에서 일반적으로 사용되는 훈련 및 평가 데이터셋에 대한 요약을 다음에 제공합니다.

