"대규모 약한 지도를 통한 견고한 음성 인식"

Alec Radford * 1 Jong Wook Kim * 1 Tao Xu 1 Greg Brockman 1 Christine McLeavey 1
Ilya Sutskever 1

요약

우리는 인터넷의 오디오 트랜스크립트를 예측하기 위해 훈련된 음성 처리 시스템의 능력을 연구했습니다. 이러한 모델은 680,000 시간의 다국어 및 다작업 감독 지도로 확장되었을 때, 표준 벤치마크에 대해 잘 일반화되며 종종 완전히 감독된 결과와 경쟁력이 있습니다. 또한 이러한 모델은 어떠한 파인튜닝도 필요하지 않는 제로샷 전송 설정에서 사용됩니다. 인간과 비교할 때, 이 모델들은 그들의 정확도와 견고성에 근접합니다. 우리는 이러한 모델과 추론 코드를 공개하여 견고한 음성 처리에 대한 추가 연구의 기반으로 제공합니다.

1. 서론

음성 인식 분야의 발전은 Wav2Vec 2.0 (Baevski et al., 2020)와 같은 비지도 사전 훈련 기술의 개발에 활기를 띠고 있습니다. 이러한 방법은 인간 레이블이 필요하지 않고 원시 오디오에서 직접 학습하기 때문에 레이블이 없는 대규모 음성 데이터셋을 생산적으로 활용할 수 있으며 빠르게 1,000,000 시간의 훈련 데이터로 확장되었습니다 (Zhang et al., 2021). 이는 학술적 지도 데이터셋에서 흔한 1,000 시간 정도보다 훨씬 많습니다. 이 접근 방식은 표준 벤치마크에서 파인튜닝될 때, 특히 데이터가 적은 환경에서 현재 기술 수준을 향상시켰습니다.

머신 러닝 모델이 다루는 오디오 데이터의 사전 훈련된 인코더는 높은 품질의 음성 표현을 학습하지만, 그들은 완전한 비지도 학습 방식이므로 이러한 표현을 유용한 출력으로 매핑하는 동등한 성능을 가진 디코더가 없습니다. 따라서 음성 인식과 같은 작업을 실제로 수행하려면 파인튜닝 단계가 필요합니다. 이는 불행히도 그들의 유용성과 영향을 제한하며, 파인튜닝은 여전히 숙련된 전문가가 필요한 복잡한 과정일 수 있습니다. 파인튜닝이 필요한 경우 추가적인 리스크가 발생합니다.

이러한 방법들은 학습 데이터셋 내에서 패턴을 발견하는 데 뛰어나며 동일한 데이터셋에서 유지되는 데이터의 성능을 향상시킵니다. 그러나 이러한 패턴 중 일부는 취약하고 잘못된 것으로, 다른 데이터셋과 분포에는 일반화되지 않습니다. 특히, Radford 등 (2021)은 컴퓨터 비전 모델을 ImageNet 데이터셋 (Russakovsky 등, 2015)에서 파인튜닝할 때 객체 분류 정확도가 9.2% 증가하는 것을 문서화했지만 동일한 객체를 일곱 가지 다른 자연 이미지 데이터셋에서 분류할 때 평균 정확도가 향상되지 않았습니다. 데이터셋을 기반으로 하는 특이한 점을 활용하고 있기 때문에 모델은 하나의 데이터셋에서 학습할 때 "초인적인" 성능을 달성할 수 있지만, 다른 데이터셋에서 평가될 때 여전히 많은 기본적인 오류를 낼 수 있습니다. 아마도 이는 사람들이 무시하는 데이터셋별 특이성을 이용하고 있기 때문일 것입니다 (Geirhos 등, 2020).

이러한 사실은 비지도 사전 훈련이 오디오 인코더의 품질을 현저하게 향상시켰지만, 동등한 고품질 사전 훈련된 디코더의 부재와 데이터셋별 파인튜닝 권장 프로토콜의 결합은 그들의 유용성과 견고성을 제한하는 중요한 약점입니다. 음성 인식 시스템의 목표는 각각의 배포 분포마다 디코더의 감독된 파인튜닝이 필요하지 않고 넓은 범위의 환경에서 신뢰성 있게 작동해야 합니다. Narayanan 등 (2018), Likhomanenko 등 (2020) 및 Chan 등 (2021)에 의한 실험 결과로 보여주듯, 다양한 데이터셋/도메인에서 감독된 방식으로 사전 훈련된 음성 인식 시스템은 단일 출처에서 훈련된 모델보다 더 높은 견고성을 보이며 더 효과적으로 보유 데이터셋에 일반화됩니다. 이러한 작업들은 가능한 많은 고품질 음성 인식 데이터셋을 결합함으로써 이를 달성합니다. 그러나 이러한 데이터 중에는 아직 적은 양만이 쉽게 이용 가능합니다. SpeechStew (Chan 등, 2021)은 5,140 시간의 감독 지도를 제공하는 7개의 기존 데이터셋을 결합합니다. 이는 무시할 만한 양은 아니지만, Zhang 등 (2021)에서 활용된 1,000,000 시간의 레이블되지 않은 음성 데이터와 비교하면 아직도 상대적으로 적은 양입니다.

고품질 감독 지도 데이터셋의 크기 제한을 인식하면서, 최근의 노력들로 음성 인식을 위한 더 큰 데이터셋이 만들어졌습니다. Chen 등 (2021) 및 Galvez 등 (2021)은 골드 표준 인간 검증된 트랜스크립트의 요구 사항을 완화함으로써 정교한 자동화 파이프라인을 활용하여 약하게 감독된 음성 인식을 10,000 시간 및 30,000 시간의 보다 노이지한 훈련 데이터로 확장했습니다. 이러한 품질과 양 사이의 균형은 종종 옳은 판단입니다. 음성 인식에 대한 연구가 아직 부족하긴 하지만, 최근 컴퓨터 비전 분야의 연구에서는 ImageNet (Russakovsky 등, 2015)과 같은 골드 표준 크라우드소싱 데이터셋을 넘어서 더 크고 약하게 감독된 데이터셋으로 이동함으로써 모델의 견고성과 일반화 성능이 크게 향상되는 것을 보여주었습니다 (Mahajan 등, 2018; Kolesnikov 등, 2020).

그럼에도 불구하고 이러한 새로운 데이터셋은 기존의 고품질 데이터셋의 합보다 몇 배 더 크지만, 이전 비지도 학습 작업보다 훨씬 작습니다. 본 연구에서는 이 간격을 메우고 약하게 감독된 음성 인식을 다음 수준으로 확장하여 레이블이 지정된 오디오 데이터 680,000 시간으로 확장합니다. 우리는 이 접근 방식을 Whisper2라고 부릅니다. 이 크기에서 훈련된 모델이 기존 데이터셋에 대해 제로샷 전송이 잘 되는 것을 보여줌으로써 데이터셋별 파인튜닝이 필요하지 않고도 고품질 결과를 달성할 수 있습니다.

또한, 우리의 연구는 크기뿐만 아니라 영어 음성 인식을 넘어 다중 언어 및 다중 작업으로 약하게 감독된 사전 훈련의 범위를 확장하는 데 중점을 둡니다. 이 680,000 시간의 오디오 중 117,000 시간은 다른 96개 언어를 커버합니다. 또한 데이터셋에는 X→en 번역 데이터 125,000 시간도 포함되어 있습니다. 충분히 큰 모델의 경우 공동 다국어 및 다작업 훈련에는 단점이 없으며 이점이 있는 것으로 확인됩니다.

우리의 연구는 음성 인식을 위한 약하게 감독된 사전 훈련의 간단한 스케일링이 음성 인식 분야에서 현재까지 충분히 인정받지 못했다는 것을 시사합니다. 우리는 최근 대규모 음성 인식 작업의 주요 요소였던 자기 감독 또는 자기 훈련 기술이 필요하지 않고 이러한 결과를 달성합니다. 견고한 음성 인식에 대한 추가 연구의 기반으로 제공하기 위해 다음 URL에서 추론 코드 및 모델을 공개합니다: https://github.com/openai/whisper


2. 접근 방식

2.1. 데이터 처리

최근 웹 규모의 텍스트를 활용하여 기계 학습 시스템을 훈련시키는 추세를 따라, 데이터 전처리에 대한 미니멀리스트 접근 방식을 채택합니다. 음성 인식에 대한 많은 연구와 대조적으로 Whisper 모델을 훈련시키는 데 있어 표준화 작업 없이 원시 텍스트를 예측하도록 합니다. 이는 시퀀스 투 시퀀스 모델의 표현 능력을 활용하여 발화와 그들의 전사 형태 간의 매핑을 학습하기 위해 사용됩니다. 이로써 음성 인식 파이프라인이 간소화되며 자연스러운 전사를 생성하기 위한 별도의 역 텍스트 정규화 단계가 필요하지 않습니다.
우리는 오디오와 웹의 텍스트와 짝을 이룬 데이터셋을 구성합니다. 이로써 많은 다양한 환경, 녹음 설정, 화자 및 언어에서의 많은 다양한 오디오 분포를 커버하는 매우 다양한 데이터셋이 생성됩니다. 오디오 품질의 다양성은 모델을 견고하게 훈련시키는 데 도움이 될 수 있지만, 전사 품질의 다양성은 비슷한 이점을 가지지 않습니다. 초기 조사에서 원시 데이터셋에서 많은 수준 이하의 전사가 나타났습니다. 이를 해결하기 위해 전사 품질을 개선하기 위한 여러 자동 필터링 방법을 개발했습니다.


"인터넷 상의 많은 전사는 실제로 인간이 생성한 것이 아니라 기존 ASR(자동 음성 인식) 시스템의 출력물입니다. 최근 연구에 따르면 인간과 기계 생성 데이터가 혼합된 데이터셋에서 훈련시키면 번역 시스템의 성능이 크게 저하될 수 있음이 밝혀졌습니다 (Ghorbani 등, 2021). '전사 언어'를 배우는 것을 피하기 위해 우리는 훈련 데이터셋에서 기계 생성 전사를 감지하고 제거하기 위한 여러 휴리스틱을 개발했습니다. 많은 기존 ASR 시스템은 쓰기 언어의 제한된 하위 집합만 출력하며, 이는 느낌표, 쉼표 및 물음표와 같은 복잡한 문장 부호, 문단과 같은 서식화 공백 또는 대문자와 같은 스타일적 측면과 같이 오디오 신호만으로 예측하기 어려운 측면을 제거하거나 정규화합니다. 대문자로만 이루어진 전사나 소문자로만 이루어진 전사는 매우 인간이 생성하지 않았을 가능성이 높습니다. 많은 ASR 시스템은 어느 정도의 역 텍스트 정규화를 포함하지만, 이는 종종 단순하거나 규칙 기반이며 쉼표를 포함하지 않는 것과 같은 다른 처리되지 않은 측면에서 여전히 감지할 수 있습니다.

또한 우리는 음성 언어 감지기를 사용합니다. 이 감지기는 VoxLingua107 (Valk & Alumae¨, 2021)의 데이터셋의 프로토타입 버전에서 훈련된 프로토타입 모델을 파인튜닝하여 만들어졌으며, 이를 통해 발화 언어가 CLD2에 따른 전사 언어와 일치하는지 확인합니다. 두 언어가 일치하지 않으면 (오디오, 전사) 쌍을 음성 인식 훈련 데이터로 포함시키지 않습니다. 다만, 전사 언어가 영어인 경우 예외를 만들어 이러한 쌍을 데이터셋에 X→en 음성 번역 훈련 예제로 추가합니다. 전사 텍스트의 흐린 중복 제거를 사용하여 훈련 데이터셋 내의 중복 및 자동 생성 콘텐츠 양을 줄입니다.
우리는 오디오 파일을 30초 길이의 세그먼트로 나누고 해당 시간 세그먼트 내에 발생하는 전사의 하위 집합과 짝을 이룹니다. 우리는 음성이 없는 세그먼트를 포함하여 모든 오디오에서 훈련을 진행하며 (일부 확률로 하위 샘플링됨), 이러한 세그먼트를 음성 활동 감지의 훈련 데이터로 사용합니다.


추가 필터링 단계로, 초기 모델 훈련 후에 초기 모델의 훈련 데이터 소스에 대한 오류율 정보를 집계하고 이러한 데이터 소스를 고오류율 및 데이터 소스 크기의 조합으로 정렬하여 저품질 데이터를 효율적으로 식별하고 제거하기 위한 수동 검사를 수행했습니다. 이 검사에서 부분적으로 전사된 데이터나 정렬/미정렬된 전사뿐만 아니라 품질이 낮은 기계 생성 캡션들이 여전히 남아 있음을 보여주었으며, 필터링 휴리스틱이 이를 감지하지 못한 것으로 나타났습니다.
오염을 피하기 위해, 우리는 높은 중첩 가능성이 있는 평가 데이터셋과 훈련 데이터셋 사이에서 전사 수준의 중복을 수행합니다. 주로 TED-LIUM 3 (Hernandez 등, 2018)과 같은 데이터셋을 대상으로 합니다.

2.2. Model

우리 연구의 초점은 음성 인식을 위한 대규모 감독 지도 사전 훈련의 능력을 연구하는 것이므로 모델 개선과 혼동을 피하기 위해 오프 더 셀프 아키텍처를 사용했습니다. 우리는 에코더-디코더 Transformer (Vaswani 등, 2017)를 선택했는데, 이 아키텍처는 안정적으로 스케일링되었음이 잘 검증되었기 때문입니다. 모든 오디오는 16,000 Hz로 재샘플링되며, 25 밀리초 창과 10 밀리초 간격으로 80채널 로그 크기의 Mel 스펙트로그램 표현이 계산됩니다. 특성 정규화를 위해 입력을 전체적으로 -1과 1 사이로 스케일링하며 사전 훈련 데이터셋 전체를 기준으로 거의 제로 평균을 갖게 합니다. 에코더는 두 개의 컨볼루션 레이어로 구성된 작은 스템으로 이 입력 표현을 처리하며 필터 폭은 3이고 GELU 활성화 함수 (Hendrycks & Gimpel, 2016)를 사용하며 두 번째 컨볼루션 레이어는 스트라이드가 2입니다. 그 후에는 사인 함수 형태의 위치 임베딩이 스템의 출력에 추가되고 그 이후에 에코더 Transformer 블록이 적용됩니다. Transformer는 사전 활성화 잔여 블록 (Child 등, 2019)을 사용하며 에코더 출력에 최종 레이어 정규화가 적용됩니다. 디코더는 학습된 위치 임베딩과 입력-출력 토큰 표현을 묶어 사용합니다 (Press & Wolf, 2017). 에코더와 디코더는 동일한 너비와 Transformer 블록 수를 가지고 있습니다. 그림 1은 모델 아키텍처를 요약한 것입니다.
영어 모델에서는 GPT-2 (Sennrich 등, 2015; Radford 등, 2019)에서 사용된 바이트 수준 BPE 텍스트 토크나이저를 동일하게 사용하고 다국어 모델에서는 GPT-2 BPE 어휘가 영어만 해당하므로 (어휘 크기는 동일하게 유지) 다른 언어에서 과도한 조각화를 피하기 위해 어휘를 재적합합니다.



2.3. Multitask Format

주어진 오디오 스니펫에서 어떤 단어들이 발화되었는지 예측하는 것은 전체 음성 인식 문제의 핵심 부분이며 연구에서 광범위하게 연구되었지만, 이것만이 아닙니다. 완전한 기능을 갖춘 음성 인식 시스템은 음성 활동 감지, 화자 구별 및 역 텍스트 정규화와 같은 많은 추가 구성 요소를 포함할 수 있습니다. 이러한 구성 요소들은 종종 별도로 처리되어 핵심 음성 인식 모델 주변에 비교적 복잡한 시스템을 만들어냅니다. 이 복잡성을 줄이기 위해 우리는 단일 모델이 핵심 인식 부분뿐만 아니라 전체 음성 처리 파이프라인 전체를 수행하는 것을 원합니다. 여기서 중요한 고려 사항은 모델의 인터페이스입니다. 동일한 입력 오디오 신호에 수행할 수 있는 다양한 작업이 있으며, 전사, 번역, 음성 활동 감지, 정렬 및 언어 식별이 몇 가지 예입니다.

단일 모델로 이러한 일대다 매핑을 작동시키려면 어떤 형태의 작업 명세가 필요합니다. 모든 작업 및 조건 정보를 디코더의 입력 토큰 순서로 지정하기 위해 간단한 형식을 사용합니다. 우리의 디코더는 오디오 조건 언어 모델이므로 현재 전사의 텍스트 기록을 조건으로 사용하도록 훈련되어 모호한 오디오를 해결하기 위해 더 긴 범위의 텍스트 컨텍스트를 사용할 것으로 기대됩니다. 구체적으로, 일부 확률로 현재 오디오 세그먼트 이전의 전사 텍스트를 디코더의 컨텍스트에 추가합니다. 예측의 시작을 나타내기 위해 토큰으로 표시합니다.
먼저 어떤 언어가 말되고 있는지 예측합니다. 각 언어에 대해 훈련 세트에서 고유한 토큰으로 표현됩니다 (총 99개). 오디오 세그먼트에 음성이 없는 경우 모델은 이를 나타내는 토큰을 예측하도록 훈련됩니다. 다음 토큰은 작업을 지정하며 이는 또는 토큰으로 지정됩니다. 이후 타임스탬프를 예측할 것인지 여부를 지정하기 위해 해당 경우에는 토큰을 포함합니다. 이 시점에서 작업과 원하는 형식이 완전히 지정되며 출력이 시작됩니다. 타임스탬프 예측의 경우 현재 오디오 세그먼트와 관련된 시간을 예측하며 모든 시간을 Whisper 모델의 기본 시간 해상도와 일치하는 20밀리초 단위로 양자화하고 각각에 대한 추가 토큰을 어휘에 추가합니다. 이러한 예측을 캡션 토큰과 교차로 진행합니다. 시작 시간 토큰은 각 캡션 텍스트 이전에 예측되고 종료 시간 토큰은 그 이후에 예측됩니다. 마지막 전사 세그먼트가 현재 30초 오디오 청크에 부분적으로만 포함될 때 타임스탬프 모드에서 해당 세그먼트의 시작 시간 토큰만 예측하여 이후 디코딩이 해당 시간과 일치하는 오디오 창에서 수행되어야 함을 나타냅니다. 그렇지 않으면 오디오를 해당 세그먼트를 포함하지 않도록 자릅니다. 마지막으로 토큰을 추가합니다. 모델은 이전 컨텍스트 텍스트에 대한 훈련 손실만 마스킹하고 다른 모든 토큰을 예측하도록 훈련합니다. 형식과 훈련 설정에 대한 개요는 Figure 1을 참조하십시오.

Figure 1. 저희 접근 방법 개요. 시퀀스 투 시퀀스 트랜스포머 모델은 다양한 음성 처리 작업, 다중 언어 음성 인식, 음성 번역, 말하기 언어 식별 및 음성 활동 감지를 포함하여 훈련됩니다. 이러한 모든 작업은 디코더에 의해 예측되어야 하는 토큰 시퀀스로 공동으로 표시되며, 전통적인 음성 처리 파이프라인의 여러 다른 단계를 대체할 수 있는 단일 모델을 가능하게 합니다. 다중 작업 훈련 형식은 작업 지정자 또는 분류 대상으로 작동하는 일련의 특수 토큰을 사용하며 이에 대한 자세한 설명은 섹션 2.3에서 더 설명되어 있습니다.

2.4. Training Details

저희는 Whisper의 스케일링 특성을 연구하기 위해 다양한 크기의 모델 스위트를 훈련합니다. 개요는 Table 1을 참조하십시오. FP16 및 동적 손실 스케일링 및 활성화 체크포인팅 (Griewank & Walther, 2000; Chen et al., 2016)을 사용하여 가속기 간 데이터 병렬 처리로 훈련합니다. 모델은 AdamW (Loshchilov & Hutter, 2017) 및 그래디언트 정규화 클리핑 (Pascanu et al., 2013)을 사용하여 훈련되며, 처음 2048개 업데이트 동안 워마핑 후에 선형 학습률 감소로 학습률을 0으로 줄입니다. 256 개 세그먼트의 배치 크기를 사용하고 모델은 데이터 세트를 두 번에서 세 번 통과하는 220 개의 업데이트 동안 훈련됩니다. 몇 번의 에포크만 훈련하기 때문에 과적합이 큰 걱정이 되지 않으며 데이터 증강이나 정규화를 사용하지 않고 대신 이러한 대규모 데이터 세트에 포함된 다양성을 활용하여 일반화와 견고성을 장려합니다. 전체 훈련 하이퍼 파라미터에 대한 자세한 내용은 Appendix F를 참조하십시오. 초기 개발 및 평가 중에 Whisper 모델은 스피커 이름에 대한 합리적인 추측을 하지만 거의 항상 부정확한 추측을 하는 경향이 있다는 것을 관찰했습니다. 이는 사전 훈련 데이터 세트에 많은 전사가 발화자의 이름을 포함하기 때문에 모델이 이를 예측하려고 시도하도록 유도되지만 이 정보는 오디오 컨텍스트의 가장 최근 30초만으로는 거의 추론할 수 없기 때문입니다. 이를 피하기 위해 스피커 주석을 포함하지 않는 전사의 하위 집합에서 Whisper 모델을 짧게 파인튜닝하여이 동작을 제거합니다.

3. Experiments
3.1. Zero-shot Evaluation

Whisper의 목표는 데이터 집합별 세부 조정이 필요하지 않고 특정 분포에서 고품질 결과를 달성하기 위해 데이터 집합 특정 fine-tuning이 필요하지 않는 단일 견고한 음성 처리 시스템을 개발하는 것입니다. 이 능력을 연구하기 위해 Whisper는 도메인, 작업 및 언어 간에 잘 일반화되는지 확인하기 위해 다양한 기존 음성 처리 데이터 집합을 재사용합니다. 이러한 데이터 집합에 대한 기존 평가 프로토콜 대신에 각 데이터 집합의 훈련 데이터를 사용하지 않고 제로샷 환경에서 Whisper를 평가하여 광범위한 일반화를 측정합니다.

3.2. 평가 지표
음성 인식 연구는 일반적으로 단어 오류율 (WER) 지표를 기반으로 시스템을 평가하고 비교합니다. 그러나 WER은 문자열 편집 거리에 기반하며 모델 출력과 참조 전사 간의 모든 차이를 처벌합니다. 이는 전사 스타일의 사소한 차이를 포함하여 모델 출력이 인간에 의해 올바르다고 판단 될 수있는 시스템도 작은 형식 차이로 인해 큰 WER을 가질 수 있음을 의미합니다. 이는 모든 전사자에게 문제가되며, 특히 Whisper와 같은 제로샷 모델에게는 특히 심각합니다. 이 모델은 특정 데이터 집합 전사 형식의 예를 관찰하지 않습니다.

이것은 새로운 관찰이 아닙니다. 인간 판단과 더 잘 상관되는 평가 지표 개발은 연구의 활성 영역이며, 일부 유망한 방법이 있지만 음성 인식에 대한 널리 퍼진 채택은 아직 없습니다. 우리는 이 문제를 WER 계산 전에 텍스트를 표준화하여 의미없는 차이의 처벌을 최소화 함으로써 해결하기로 결정했습니다. 우리의 텍스트 정규화 프로그램은 순진한 WER이 무해한 차이를 처벌하는 경우를 식별하기 위한 반복적 인 수동 검사를 통해 개발되었습니다. 자세한 내용은 Appendix C를 참조하십시오. 몇몇 데이터 집합에 대해 WER이 최대 50 퍼센트 감소하는 것을 관찰하는데, 이는 주로 innocuous한 차이로 인한 것입니다.

텍스트 표준화 프로세스로 인해 일부 데이터 집합의 참조 전사가 단어와 공백으로 구성된 축약어를 분리한다는 것과 같은 데이터 집합의 특이성이 나타날 수 있습니다. 이러한 표준화 절차는 Whisper 모델의 전사 스타일에 overfitting의 위험이 있다는 점을 주의해야 합니다. 이 문제는 섹션 4.4에서 조사됩니다. 우리는 다른 사람들이 분포에서 음성 인식 시스템의 성능을 연구하고 비교하기 쉽도록 텍스트 정규화 프로그램의 코드를 공개하고 있습니다.

3.3. 영어 음성 인식
2015년에 Deep Speech 2 (Amodei et al., 2015)는 LibriSpeech test-clean 분할을 전사할 때 음성 인식 시스템이 인간 수준의 성능을 보여주었습니다. 그들의 분석 일환으로 "이 결과를 고려할 때, 일반적인 음성 시스템이 추가적인 도메인 적응 없이 깨끗한 읽기 음성에서 더 개선할 여지가 없을 것으로 의심됩니다." 라고 결론 내렸습니다. 그럼에도 불구하고 7년 후에 LibriSpeech test-clean에서의 SOTA WER은 그들의 5.3%에서 1.4%로 더 떨어졌으며 (Zhang et al., 2021), 그들의 인간 수준의 오류율 5.8%에 비해 훨씬 낮아졌습니다. 이러한 대규모이면서 예상치 못한 성능 향상에도 불구하고 LibriSpeech에서 훈련된 음성 인식 모델은 다른 환경에서 사용될 때 여전히 인간 오류율을 크게 상회합니다. 분포 내에서 보고 된 초인간적 성능과 분포 밖에서의 하위인간 성능 사이의 이 차이를 어떻게 설명할까요?

우리는 인간과 기계 퍼포먼스가 테스트 세트에서 측정되는 데 차이가 있는 이유가 테스트 방식이 아니라 훈련 방식에서 발생한다고 의심합니다. 인간들은 일반적으로 연구되는 특정 데이터 분포에 대한 거의 또는 전혀 감독을받지 않고 작업을 수행하도록 요청됩니다. 따라서 인간의 성능은 분포 밖 일반화의 측정입니다. 그러나 기계 학습 모델은 일반적으로 평가 분포에서의 큰 양의 감독 지도를 훈련한 후에 평가되므로 기계의 성능은 분포 내 일반화의 측정입니다. 두 경우 모두 인간과 기계가 동일한 테스트 데이터를 기반으로 평가되지만 훈련 데이터에 차이가 있기 때문에 서로 다른 능력이 측정됩니다.

Whisper 모델은 다양한 종류의 오디오 데이터로 훈련되고 제로샷 환경에서 평가되므로 기존 시스템보다 더 좋은 인간 행동을 보여줄 수 있습니다. 이것이 사실인지 여부를 조사하기 위해 (또는 기계와 인간 퍼포먼스 간의 차이가 아직 이해되지 않은 요인에 기인하는지) Whisper 모델을 인간의 성능과 표준 fine-tuned 기계 학습 모델과 비교하여 어느 쪽에 더 가깝게 일치하는지 확인할 수 있습니다.


Figure 2. 제로샷 Whisper 모델은 인간의 견고성에 가까워집니다. LibriSpeech dev-clean에서 인간과 일치하거나 인간을 능가하더라도 감독되는 LibriSpeech 모델은 다른 데이터셋에서 인간보다 약 두 배 많은 오류를 발생시키며 그의 취약성과 견고성 부족을 보여줍니다. 그러나 제로샷 Whisper 모델의 견고성 추정 경계에는 특정 인간의 95% 신뢰 구간이 포함됩니다.

이러한 차이를 양적으로 측정하기 위해 우리는 전체적인 강인성과 효과적인 강인성을 모두 조사합니다. 전체적인 강인성은 다양한 분포/데이터셋에 걸친 평균 성능을 의미하며, 효과적인 강인성은 Taori et al. (2020)에서 제안한 개념으로, 일반적으로 분포 안에 있는 참조 데이터셋과 분포 밖의 하나 이상의 데이터셋 사이의 기대 성능 차이를 측정합니다. 높은 효과적인 강인성을 가진 모델은 참조 데이터셋에서의 성능을 기반으로 예상보다 더 잘 수행하며 모든 데이터셋에서 동일한 성능을 접근하려고 합니다. 분석을 위해 우리는 최신 음성 인식 연구에서 중요한 역할을 하는 LibriSpeech를 참조 데이터셋으로 사용하고 있으며, 이 데이터셋에서 훈련된 여러 모델의 성능을 비교하여 강한 특성을 파악할 수 있습니다. 우리는 다른 학술 음성 인식 데이터셋 12개의 특성과 분포 밖 특성을 연구하기 위해 이 데이터셋을 사용합니다. 이러한 데이터셋에 대한 자세한 내용은 부록 A에 설명되어 있습니다.

우리의 주요 결론은 Figure 2와 Table 2에 요약되어 있습니다. 최고의 제로샷 Whisper 모델은 상대적으로 무난한 LibriSpeech clean-test WER(2.5)를 보여주며, 이는 현대의 감독된 기준선이나 2019년 중반의 최첨단 기술 수준과 비슷한 성능입니다. 그럼에도 불구하고, 제로샷 Whisper 모델은 감독된 LibriSpeech 모델과는 매우 다른 강인성 특성을 가지며, 다른 데이터셋에서는 모든 벤치마킹된 LibriSpeech 모델을 큰 폭으로 앞지를 수준입니다. 심지어 가장 작은 제로샷 Whisper 모델, 즉 매개 변수가 3900만 개뿐이고 LibriSpeech test-clean에서 6.7의 WER을 가지는 모델조차도 다른 데이터셋에서 평가할 때 최고의 감독된 LibriSpeech 모델과 대략 경쟁력을 가집니다. Figure 2에서 인간과 비교할 때, 최고의 제로샷 Whisper 모델은 정확성과 강인성을 대략적으로 일치시킵니다. 이 큰 강인성 개선에 대한 자세한 내용은 Table 2에서 제로샷 Whisper 모델의 최고 성능과 LibriSpeech test-clean에서 가장 가까운 성능을 가진 감독된 LibriSpeech 모델을 비교하여 설명합니다. 참조 분포에서 그들의 성능이 매우 가까운 경우에도, 제로샷 Whisper 모델은 다른 음성 인식 데이터셋에서 평균 상대적 오류 감소율이 55.2%에 달합니다.

이 결과는 특히 기계 학습 시스템의 능력을 과대 평가하지 않기 위해 모델을 제로샷 및 분포 밖 평가에 중점을 두는 것이 중요함을 시사합니다. 특히 인간 성능과 비교할 때 모델의 능력을 오해하지 않기 위해 이러한 종류의 평가를 강조해야 합니다.

3.4. 다국어 음성 인식

다국어 음성 인식에 대한 이전 연구와 비교하기 위해 두 가지 저데이터 벤치마크에서 결과를 보고합니다: Multilingual LibriSpeech (MLS) (Pratap et al., 2020b) 및 VoxPopuli (Wang et al., 2021)을 Table 3에 보고합니다.
Whisper는 Multilingual LibriSpeech에서 우수한 성능을 발휘하며, 제로샷 설정에서 XLS-R (Babu et al., 2021), mSLAM (Bapna 등, 2022) 및 Maestro (Chen 등, 2022b)를 능가합니다. 그러나 이 결과에 대해서는 직접 비교나 SOTA 성능 주장을 할 수 없도록 간단한 텍스트 표준화기를 사용했음을 주의해야 합니다. 그러나 VoxPopuli에서 Whisper는 이전 연구에 비해 현저히 성능이 낮으며 원래 논문의 VP-10K+FT 기준을 능가합니다. Whisper 모델이 VoxPopuli에서의 성능 저하는 이전 연구가 이 분포를 그들의 비지도 사전 훈련 데이터의 주요 출처로 포함시키고 있으며 데이터셋에는 크게 더 많은 지도 데이터가 있기 때문일 것으로 의심됩니다. MLS는 언어당 10시간의 훈련 데이터를 가지고 있지만, 평균 훈련량은 이 두 가지 벤치마크는 언어 패밀리의 거의 모두 인도-유럽어 언어 패밀리에 속하고 많은 언어가 고자원 언어인 경우가 많기 때문에 다소 좁은 범위입니다. 이러한 벤치마크는 Whisper 모델의 다국어 능력을 연구하기에는 제한된 범위와 공간만 제공하며, Whisper 모델은 음성 인식을 위한 훈련 데이터를 75개 언어에 대해 가지고 있습니다. Whisper의 성능을 보다 넓게 연구하기 위해 Fleurs 데이터셋 (Conneau 등, 2022)에서의 성능도 보고합니다. 특히, 특정 언어에 대한 훈련 데이터 양과 해당 언어의 결과 제로샷 성능 간의 관계를 연구하고자 했습니다. 이 관계를 Figure 3에서 시각화합니다. 우리는 단어 오류율의 로그와 언어당 훈련 데이터 양의 로그 간의 강한 제곱 상관 계수 0.83을 찾았습니다. 이러한 로그-로그 값에 대한 선형 피팅의 회귀 계수를 확인하면 훈련 데이터 양이 16배 증가할 때마다 WER이 절반으로 줄어든다는 추정치가 나옵니다. 또한 이 추세에 따라 예상보다 성능이 나쁜 가장 큰 이상치 중 많은 언어들은 고유한 문자로 표기되며 주로 훈련 데이터셋의 대부분을 이루는 인도-유럽어 언어와는 더 멀리 관련된 언어들입니다. 히브리어 (HE), 텔루구어 (TE), 중국어 (ZH), 한국어 (KO)와 같은 언어들이 이러한 차이의 원인이 될 수 있으며, 언어 간 전이 부족, 바이트 레벨 BPE 토크나이저가 이러한 언어와 부적합하거나 데이터 품질의 변동과 관련이 있을 수 있습니다.


Table 4. X→en 음성 번역 성능. 제로샷 Whisper는 CoVoST2의 전반, 중간 및 낮은 리소스 설정에서 기존 모델을 능가하지만 이전 직접 지도된 작업과 비교할 때 고 리소스 언어에서는 중간 성능을 보이며 아직 어느 정도 성능이 미흡합니다.

Table 5. 언어 식별 성능. 제로샷 Whisper의 언어 식별 정확도는 Fleurs에서 이전 지도 학습 결과와 경쟁력이 없습니다. 이는 Whisper가 Fleurs의 20개 언어에 대한 훈련 데이터가 없어서 부분적으로 발생합니다.


3.5. 번역

Whisper 모델의 번역 능력을 연구하기 위해 CoVoST2 (Wang 등, 2020b)의 X→en 하위 집합에서의 성능을 측정합니다. Maestro, mSLAM 및 XLS-R과 같은 이전 작업 중 성능이 가장 높은 작업과 비교합니다. CoVoST2의 훈련 데이터를 전혀 사용하지 않고도 29.1 BLEU의 새로운 최고 성과를 달성합니다. 이는 노이즈가 있는 훈련 데이터이지만, CoVoST2의 X→en 번역에 대한 861시간의 훈련 데이터보다 훨씬 큰 68,000시간의 X→en 번역 데이터가 우리의 사전 훈련 데이터셋에 포함되어 있기 때문입니다. Whisper의 평가가 제로샷이기 때문에 CoVoST2의 가장 낮은 리소스 그룹에서 특히 잘 수행되어 mSLAM보다 6.7 BLEU가 향상되었습니다. 반면, 최고의 Whisper 모델은 가장 높은 리소스 언어의 평균적인 성능보다 Maestro와 mSLAM을 실제로 개선하지 않습니다.

더 많은 언어를 포함한 더 넓은 범위의 언어에 대한 추가 분석을 위해 Fleurs 데이터셋을 번역 데이터셋으로 다시 활용합니다. 모든 언어에 대해 동일한 문장이 전사되므로 영어 스크립트를 참조 번역으로 사용합니다. Figure 4에서 훈련 데이터 양과 Fleurs에서의 제로샷 BLEU 점수 사이의 상관 관계를 시각화합니다. 훈련 데이터 양이 증가함에 따라 개선되는 분명한 추세가 있지만 제로샷 번역에서 관찰된 0.83과는 훨씬 낮은 제곱 상관 계수 0.24입니다. 이것은 음성 언어 식별의 오류로 인한 노이지 훈련 데이터 일부로 인한 것으로 의심됩니다. 예를 들어 웨일스어 (CY)는 9,000시간의 번역 데이터를 가지고 있으며 13 BLEU로 예상보다 훨씬 나쁜 성능을 나타냅니다. 이런 많은 웨일스어 번역 데이터는 영어 오디오와 영어 캡션인데, 언어 식별 시스템에서 영어 오디오를 웨일스어로 잘못 분류하여 표준화 된 데이터 생성 규칙에 따라 표준화된 데이터로 포함되었습니다.

Figure 5. 추가 백색 노이즈 (왼쪽) 및 술집 노이즈 (오른쪽) 하에서 SNR 기능으로 LibriSpeech test-clean의 WER. LibriSpeech에서 훈련된 모델의 정확도는 최고의 Whisper 모델 (F)보다 빠르게 감소합니다. NVIDIA STT 모델 (•)은 낮은 노이즈 하에서 가장 뛰어납니다만, 높은 노이즈 (SNR < 10 dB) 하에서는 Whisper에 능가합니다. 낮은 노이즈 (H)에서 두 번째로 좋은 모델은 LibriSpeech에서만 파인튜닝되었으며 더 빨리 저하됩니다.

3.6. 언어 식별

언어 식별을 평가하기 위해 Fleurs 데이터셋 (Conneau 등, 2022)을 사용합니다. Whisper의 제로샷 성능은 이전의 지도 학습 작업과 경쟁력이 없으며 지도 학습의 SOTA보다 13.6% 성능이 낮습니다. 그러나 Whisper는 Fleurs의 102개 언어 중 20개 언어에 대한 훈련 데이터가 전혀 없으므로 정확도는 80.4%로 상한이 됩니다. 82개의 겹치는 언어에서 최고의 Whisper 모델은 80.3%의 정확도를 달성합니다.


3.7. 추가 노이즈에 대한 강건성

우리는 추가 노이즈에 대한 Whisper 모델과 LibriSpeech에서 훈련된 14개 모델의 노이즈 강건성을 측정하여 백색 소음 또는 Audio Degradation Toolbox (Mauch & Ewert, 2013)에서 가져온 술집 소음이 오디오에 추가될 때의 WER을 측정했습니다. 술집 소음은 주로 붐비는 레스토랑이나 술집에서 흔한 주변 소음과 알아듣기 어려운 수다를 나타냅니다. 14개 모델 중 12개는 LibriSpeech에서 사전 훈련 및/또는 파인튜닝되었으며 다른 두 모델은 이전과 같이 LibriSpeech를 포함한 혼합 데이터셋에서 훈련된 NVIDIA STT 모델입니다. 특정 신호 대 잡음 비 (SNR)에 해당하는 추가 노이즈의 수준은 개별 예제의 신호 전력을 기반으로 계산됩니다. Figure 5는 추가 노이즈가 더 강렬해짐에 따라 ASR 성능이 어떻게 저하되는지를 보여줍니다. 낮은 소음 (40 dB SNR)에서는 우리의 제로샷 성능을 능가하는 모델이 많이 있지만 모든 모델이 노이즈가 더 강해지면 빠르게 성능이 저하되어 SNR이 10 dB 이하인 추가 술집 소음 하에서 Whisper 모델보다 성능이 나쁘게 나타납니다. 이는 특히 술집 소음과 같은 보다 자연스러운 분포 이동에서 Whisper의 강건성을 보여주며 잘 나타냅니다.

3.8. 장형 전사

Whisper 모델은 30초 오디오 청크에서 훈련되며 한 번에 더 긴 오디오 입력을 처리할 수 없습니다. 대부분의 학술 데이터셋은 짧은 발언으로 구성되어 있어 문제가 되지 않지만 실제 응용 프로그램에서는 종종 몇 분 또는 몇 시간 동안의 오디오를 전사해야 하는 과제가 있습니다. 우리는 모델 예측에 의한 타임 스탬프에 따라 오디오의 30초 세그먼트를 연속으로 전사하고 윈도우를 이동시킴으로써 긴 오디오의 버퍼 전사를 수행하기 위한 전략을 개발했습니다. 긴 오디오를 신뢰성 있게 전사하기 위해 모델 예측의 반복성과 로그 확률에 기반한 빔 서치와 온도 스케줄링이 필요하다는 것을 관찰했습니다. 전체 절차는 Section 4.5에 설명되어 있습니다.
우리는 다양한 길이와 녹음 조건의 음성 녹음을 포함하여 가능한 다양한 데이터 분포를 커버하기 위해 일곱 개의 데이터셋에서 장형 전사 성능을 평가합니다. 이에는 TED-LIUM3 (Hernandez 등, 2018)의 장형 적응, 각 예제가 전체 길이 TED 토크인 TED-LIUM3의 장형 적응, The Late Show with Stephen Colbert (Meanwhile)에서 가져온 전문 용어를 사용한 세그먼트의 컬렉션, 온라인 블로그에서 ASR 벤치마크로 사용된 비디오/팟캐스트 세트 (Rev16 및 Kincaid46), 수익 발표의 녹음 (Del Rio 등, 2021) 및 지역 아프리카계 미국어 언어 말뭉치 (CORAAL) (Gunter 등, 2021)의 전체 인터뷰가 포함됩니다. 장

Figure 6. Whisper는 장형 전사에서 최첨단 상용 및 오픈 소스 ASR 시스템과 경쟁력을 가집니다. 입력 길이가 몇 분에서 몇 시간까지 다양한 일곱 개의 장형 데이터셋에서 여섯 개의 ASR 시스템의 per-example WER의 사분위수와 각 상자에 주석이 달린 per-dataset 집계 WER이 나타납니다. 우리 모델은 모든 데이터셋에서 최고의 오픈 소스 모델 (NVIDIA STT)을 능가하며 대부분의 경우 상용 ASR 시스템도 능가합니다.


또한 NeMo 툴킷 (Kuchaiev et al., 2019)의 NVIDIA STT Conformer-CTC Large 모델을 사용하였으며, 이 모델은 오픈 소스 모델 중에서 가장 우수한 성능을 발휘했습니다. 모든 상용 ASR 서비스는 2022년 9월 1일 현재의 기본 영어 전사 설정을 사용하여 쿼리했으며, NVIDIA STT 모델의 경우 장형 전사를 가능하게 하기 위해 FrameBatchASR 클래스에서 버퍼링된 추론 구현을 사용했습니다. 결과는 Whisper가 대부분의 데이터셋에서 비교 모델보다 성능이 우수함을 보여줍니다, 특히 불평등한 단어가 많은 Meanwhile 데이터셋에서 성능이 뛰어납니다. 또한 일부 상용 ASR 시스템이 이러한 공개 데이터셋 중 일부에서 훈련되었을 가능성이 있으며, 따라서 이러한 결과가 시스템의 상대적인 견고성을 정확하게 반영하지 않을 수 있다는 가능성에 주목해야 합니다.

3.9. 인간 성능과의 비교

모호하거나 분명하지 않은 발화 및 라벨링 오류로 각 데이터셋마다 불가피한 오류 수준이 다르며, ASR 시스템의 WER 메트릭만으로는 각 데이터셋에서 얼마나 개선할 여지가 있는지 이해하기 어렵습니다. Whisper의 성능이 인간 성능에 얼마나 가까운지를 정량화하기 위해 Kincaid46 데이터셋에서 25개 녹음을 선택하고 전문 전사자가 제작한 전사를 얻기 위해 5개의 서비스를 사용했습니다. 이 중 하나는 컴퓨터 지원 전사를 제공하며, 나머지 네 개는 완전히 인간 전사입니다. 오디오 선택은 스크립트 및 비스크립트 방송, 전화 및 VoIP 통화, 회의 등과 같은 다양한 녹음 조건을 포함하고 있습니다. Figure 7은 25개 녹음의 per-example WER 분포와 이를 통합한 WER을 보여줍니다. 여기서 컴퓨터 지원 서비스는 Whisper보다 1.15% 포인트 낮은 통합 WER을 가지며, 순수 인간 성능은 Whisper보다 조금 더 나은 것으로 나타납니다. 이러한 결과는 Whisper의 영어 ASR 성능이 완벽하지는 않지만 인간 수준의 정확도에 아주 가깝다는 것을 나타냅니다.

4. 분석 및 제외

4.1. 모델 스케일링

약한 지도 학습 접근 방식의 많은 장점 중 하나는 전통적인 지도 학습보다 훨씬 큰 데이터셋을 사용할 수 있는 잠재력입니다. 그러나 이것은 금표준 지도보다 데이터가 훨씬 더 노이즈가 많고 낮은 품질일 가능성이 있는 데이터를 사용하는 비용이 따릅니다. 이러한 접근 방식의 걱정 사항 중 하나는 초기에는 유망해 보일 수 있지만 이러한 종류의 데이터의 내재 품질 수준에 모델 성능이 포화될 수 있다는 것이며, 이것은 인간 수준 이하일 수 있습니다. 관련된 걱정 사항은 용량과 데이터에 소비된 컴퓨팅이 증가함에 따라 모델이 데이터의 내재 품질을 훈련하는 방법을 학습할 수 있을 수 있으며, 이러한 접근 방식의 위험입니다.



그림 7. Whisper의 성능은 전문 인간 전사자와 유사합니다. 이 그림은 Whisper에 의해 전사된 Kincaid46 데이터셋의 25개 녹음의 WER 분포를 보여줍니다. 이 데이터는 동일한 4개의 상용 ASR 시스템 (A-D), 컴퓨터 지원 인간 전사 서비스 (E) 및 4개의 인간 전사 서비스 (F-I)에 의해 전사되었습니다. 상자 그림 위에 개별 녹음의 WER을 나타내는 점이 겹쳐져 있으며, 25개 녹음에 대한 통합 WER이 각 상자 위에 주석으로 표시되어 있습니다.

데이터셋 크기의 중요성을 정확하게 어떻게 평가할 수 있을까요? 이를 확인하기 위해 Whisper 데이터셋의 하위 샘플 버전에서 훈련된 일련의 중간 크기 모델을 훈련했으며, 이 버전은 전체 데이터셋 크기의 0.5%, 1%, 2%, 4%, 8%입니다. 그리고 전체 데이터셋에서 훈련된 동일한 중간 크기 모델과의 성능을 비교했습니다. 각 데이터셋 크기에 대해 검증 손실을 기반으로 조기 중단을 사용하여 모델 체크포인트를 선택했습니다. 평가는 지수 이동 평균 파라미터 추정치(Polyak & Juditsky, 1992)를 사용하여 수행되었으며, 학습률이 조기 중단으로 인해 완전히 감소하지 않을 경우를 고려하여 스무딩 비율 0.9999를 사용하여 학습률의 영향을 줄이는 데 도움이 되었습니다. 영어 및 다국어 음성 인식 및 X→en 번역의 성능은 표 6에 보고되었습니다.

Figure 8. 모델 크기가 증가함에 따라 제로샷 Whisper 성능이 작업과 언어 전반에 걸쳐 신뢰성 있게 확장됩니다. 연한 그림자가 있는 선은 개별 데이터셋이나 언어를 나타내며, 집계 성능의 부드러운 추세와 비교해 성능이 더 다양하게 나타납니다. 큰 V2는 이 분석에서 더 작은 모델에는 없는 여러 변경 사항을 포함하므로 점선의 주황색 선으로 구분됩니다.


Table 6. 데이터셋 크기가 증가함에 따라 성능이 향상됩니다. 영어 음성 인식 성능은 12개 데이터셋에 대한 평균입니다. 다국어 음성 인식은 Fleurs의 중첩 언어 하위 집합에 대한 성능을 보고하며, X→en 번역은 CoVoST2의 평균 BLEU 점수를 보고합니다. 데이터셋 크기는 시간 단위로 보고됩니다.

데이터셋 크기를 늘리는 것은 모든 작업에 대한 성능 향상을 가져옵니다. 그러나 작업 및 크기에 따라 향상 속도에 상당한 변동성이 있습니다. 영어 음성 인식에서는 데이터 크기가 3,000에서 13,000 시간까지 빠르게 성능이 향상되며, 그 이후에 13,000에서 54,000 시간 사이에서 성능 개선 속도가 둔화됩니다. 전체 데이터셋을 사용하는 경우, 크기 12.5배 증가에 따라 WER이 1 포인트만 더 낮아집니다. 이것은 영어 음성 인식의 모델 크기 조정에서 관찰된 성능의 미봉책을 반영하며, 인간 수준의 성능에 접근할 때 포화 효과로 설명될 수 있습니다.

다국어 음성 인식의 경우, WER 개선은 54,000 시간까지 거의 거듭나는 힘들었다가 그 이후에 다른 경향을 보이며 전체 데이터셋 크기로 확장해도 7 포인트만 더 개선됩니다. X→en 번역 작업에서는 7,000 시간 미만의 오디오를 학습할 때 성능이 거의 제로이며, 54,000 시간까지 로그 선형 개선 경향을 따르며 그 이후에도 성능 개선이 둔화됩니다.

이러한 결과는 데이터셋 크기를 늘리는 것이 모든 작업에 대한 성능 향상을 가져오지만, 크기 및 작업에 따라 향상 속도가 다르며, 특히 인간 수준의 성능에 접근할 때 포화 효과가 발생할 수 있다는 점을 나타냅니다.


54,000 시간부터 680,000 시간의 전체 데이터셋 크기로 확장할 때 모든 작업에서 성능 향상에 대한 줄어드는 효과가 나타납니다. 이러한 추세는 현재의 최상위 Whisper 모델이 데이터셋 크기에 비해 훈련이 부족하며 더 긴 훈련과 더 큰 모델의 결합으로 성능을 더 향상시킬 수 있음을 시사할 수 있습니다. 또한 음성 인식에 대한 데이터셋 크기 조정에서 성능 향상의 끝에 접근하고 있을 수 있다는 가능성을 제기할 수 있습니다. 음성 인식에 대한 "스케일링 법칙"을 특징화하기 위해 추가 분석이 필요하며, 이를 통해 이러한 설명 사이에서 결정할 수 있습니다.

4.3. 멀티태스크 및 멀티링귀언 전이

다른 많은 작업 및 언어에 대해 단일 모델을 공동으로 훈련시키는 것에 대한 잠재적인 우려는 여러 작업의 학습 간 간섭으로 인해 하나의 작업 또는 언어만 훈련할 때 얻을 수 있는 것보다 성능이 나빠질 수 있다는 가능성입니다. 이러한 현상이 발생하는지 조사하기 위해 영어 음성 인식만 훈련한 모델과 표준 멀티태스크 및 멀티링귀언 훈련 설정에서 훈련한 모델의 성능을 비교하고 영어 음성 인식 벤치마크 모음을 기준으로 평균 성능을 측정했습니다. 영어 음성 인식 작업에 사용한 FLOPs 양을 조정하여, 공동 훈련 설정에서는 전체 계산량의 65%만 이 작업에 사용되기 때문에 이 작업에 대한 부족한 훈련과 동일한 크기의 영어 전용 모델과 비교할 때 분석이 혼란스럽지 않도록 했습니다.

Figure 9에 시각화된 결과는 중간 크기의 모델의 경우, 작업 및 언어 간에 부정적인 전이가 실제로 발생한다는 것을 보여줍니다. 공동 모델은 동일한 양의 계산량으로 훈련한 영어 전용 모델보다 성능이 떨어집니다. 그러나 멀티태스크 및 멀티링귀언의 경우 모델 크기와 훈련 양에 따라 성능 차이가 줄어듭니다.


그림 9. 규모에 따른 멀티태스크 및 멀티링귀언 전이 향상. 작은 모델의 경우, 멀티태스크 및 멀티링귀언 설정에서 공동으로 훈련된 경우 영어 음성 인식 성능이 저하됩니다. 그러나 규모가 커질수록 멀티태스크 및 멀티링귀언 모델은 더 많은 이점을 얻으며 결국 영어 데이터만 훈련한 모델보다 성능이 우수해집니다. 95% 부트스트랩 추정 신뢰 구간이 표시됩니다.


4.4. 텍스트 정규화

Whisper와 함께 개발한 텍스트 정규화가 Whisper의 특이성을 수정하기보다는 전사의 일반적인 변이를 해결하는 데 과적합될 수 있습니다. 이를 확인하기 위해 Whisper를 사용하여 텍스트 정규화를 수행하는 방법과 FairSpeech 프로젝트(Koenecke et al., 2020)에서 독립적으로 개발한 정규화 도구를 비교했습니다. 그림 10에서 차이점을 시각화했습니다. 대부분의 데이터셋에서 두 가지 정규화 도구가 유사한 성능을 발휘하며, Whisper와 비교 대상 오픈 소스 모델 간의 WER 감소에는 중요한 차이가 없습니다. 그러나 일부 데이터셋에서는 특히 WSJ, CallHome 및 Switchboard에서 저희 정규화 도구가 Whisper 모델의 WER을 더 크게 감소시킵니다. 감소의 차이는 차이있는 형식과 두 정규화 도구가 그 형식을 어떻게 패널티로 적용하는지로 추적할 수 있습니다. 예를 들어, CallHome 및 Switchboard의 경우 저희의 정규화 도구는 "you're" 대 "you are"와 같은 일반적인 영어 축약어의 차이를 패널티로 적용하지 않았으며, WSJ의 경우 저희 정규화 도구는 수치 및 통화 표현의 쓰여진 형태와 발음 형태를 표준화했습니다. "sixty-eight million dollars" 대 "$68 million"과 같은 표현입니다.

4.5. 신뢰할 수 있는 장편 전사 전략

Whisper를 사용하여 장편 오디오를 전사하는 경우 정확한 타임스탬프 토큰 예측이 필요하며, 한 창에서의 부정확한 전사는 다음 창에서의 전사에 부정적인 영향을 미칠 수 있습니다. 우리는 장편 전사의 실패 사례를 피하기 위한 일련의 휴리스틱을 개발했으며, 이를 3.8 및 3.9 절에서 보고된 결과에 적용했습니다. 먼저, 그리디 디코딩에서 더 자주 발생하는 반복 루핑을 줄이기 위해 로그 확률을 점수 함수로 사용하여 5개 빔을 사용하는 빔 서치를 사용합니다. 우리는 초기에 온도 0으로 시작합니다. 즉, 항상 가장 높은 확률을 가진 토큰을 선택하고, 생성된 토큰의 평균 로그 확률이 -1보다 낮거나 생성된 텍스트의 gzip 압축률이 2.4보다 높을 때 온도를 0.2씩 증가시킵니다. 이전 창에서 생성된 텍스트를 이전 텍스트 조건으로 제공하면 적용된 온도가 0.5 미만인 경우 성능이 더 개선됩니다. 우리는 토큰의 확률만으로는 음성이 없는 세그먼트를 구별하기에 충분하지 않다는 것을 발견했지만, 음성 활동 감지를 Whisper의 no-speech 확률 임계값 0.6과 평균 로그 확률 임계값 -1을 결합하여 더 신뢰할 수 있게 만듭니다. 마지막으로 모델이 입력의 처음 몇 마디를 무시하는 실패 모드를 피하기 위해 초기 타임스탬프 토큰을 0.0에서 1.0 초 사이로 제한했습니다. 표 7은 위의 각 개입이 전체적으로 WER을 감소시키지만 데이터셋 전반에 일관되게 적용되지 않습니다. 이러한 휴리스틱은 모델의 노이지 예측을 해결하기 위한 일시적인 방법으로, 장편 전사의 신뢰성을 더 개선하기 위해 추가 연구가 필요합니다.


표 7. 추가 디코딩 휴리스틱을 적용함에 따라 장편 전사 성능이 점진적으로 향상됩니다. 각 개입에 대한 자세한 내용은 4.5 절에 설명되어 있습니다.

그림 10. 대부분의 데이터셋에서 우리의 텍스트 정규화 도구는 Whisper 모델과 다른 오픈 소스 모델 간의 WER을 줄이는 데 있어 FairSpeech의 정규화 도구와 유사한 영향을 미칩니다. 각 데이터셋에 대해 상자 그림은 우리의 평가 스위트의 다른 모델 간의 상대적인 WER 감소 분포를 보여줍니다. 우리의 텍스트 정규화를 사용하면 일반적으로 FairSpeech의 것보다 낮은 WER을 얻게됩니다. 일부 데이터셋에서는 우리의 정규화 도구가 WER을 크게 감소시키며, 이는 원문에서 다수의 축약어를 포함하는 CallHome 및 Switchboard와 많은 수치 표현을 포함하는 WSJ와 같은 데이터셋에서 Whisper 모델에 더 큰 영향을 미칩니다.

4.5. 신뢰성 있는 장편 전사를 위한 전략

Whisper를 사용하여 장편 오디오를 전사하기 위해 Whisper의 정확한 타임스탬프 토큰 예측이 필요하며, 한 창의 30초 오디오 컨텍스트 창을 얼마나 이동할지 결정하기 위함입니다. 한 창에서의 부정확한 전사는 후속 창에서의 전사에 부정적인 영향을 미칠 수 있습니다. 우리는 장편 전사의 실패 사례를 피하기 위한 일련의 휴리스틱을 개발했으며, 섹션 3.8과 3.9에 보고된 결과에 적용됩니다. 첫째, 로그 확률을 사용하여 빔 검색(beam search)을 수행하여 반복 루핑이 줄어들도록 하여 greedy 디코딩보다 더 나은 결과를 얻습니다. 우리는 온도(temperature)를 0부터 시작하여 평균 로그 확률이 -1보다 낮거나 생성된 텍스트의 gzip 압축률이 2.4보다 높을 때까지 온도를 0.2씩 증가시킵니다. 이전 창에서 생성된 텍스트를 이전 텍스트 조건으로 제공하는 것은 적용된 온도가 0.5 미만인 경우 전체적인 성능을 더 향상시킵니다. 우리는 또한 모델이 입력의 처음 몇 마디를 무시하는 실패 모드를 피하기 위해 초기 타임스탬프 토큰을 0.0에서 1.0 초 사이로 제한했습니다. 표 7은 위에서 언급한 각 개입을 점진적으로 추가함으로써 전반적으로 WER을 줄이는 것을 보여주며, 데이터셋 전체에 고르게 적용되지 않음을 나타냅니다. 이러한 휴리스틱은 모델의 노이즈 예측을 해결하기 위한 임시 방편으로 작동하며, 장편 전사의 신뢰성을 더 향상시키기 위해 추가 연구가 필요합니다.

관련 연구

음성 인식 확장 음성 인식 연구 전반에 걸쳐 컴퓨팅, 모델 및 데이터셋의 확장의 이점을 문서화하는 일관된 주제였습니다. 초기의 딥러닝을 음성 인식에 적용한 연구에서는 모델의 깊이와 크기가 증가함에 따라 성능이 향상되었으며, 이러한 큰 모델을 훈련하는 것을 가능하게하기 위해 GPU 가속화를 활용하였습니다 (Mohamed et al., 2009). 더 많은 연구는 데이터셋 크기와 함께 딥러닝 접근법의 이점이 나타났으며, 3시간의 TIMIT 훈련 데이터만 사용하여 폰 인식을 위한 모델에서 이전의 GMM-HMM 시스템과 경쟁할 수 있었던 시점부터 2,000시간의 Switchboard 데이터셋을 사용한 경우 단어 오류율을 30% 감소시킬 수 있을 정도로 성능이 향상되었습니다 (Seide et al., 2011). Liao et al. (2013)는 딥러닝 기반 음성 인식 데이터셋의 크기를 1,000 시간 이상 확장하는 것을 가능케하는 조기 연구 사례입니다. 이러한 추세는 Deep Speech 2 (Amodei et al., 2015)로 계속되었으며, 해당 시스템은 16개의 GPU에서 고속 분산 훈련을 개발하고 12,000시간의 훈련 데이터까지 확장하면서 해당 규모에서도 지속적인 개선을 보였습니다. Narayanan et al. (2018)은 반도 지도된 사전 훈련을 활용하여 데이터셋 크기를 훨씬 더 확장하고 162,000 시간의 레이블이 지정된 오디오에 대한 훈련을 연구했습니다. 더 최근의 연구에서는 수십억 개의 매개변수를 사용하는 모델 (Zhang et al., 2020) 및 최대 1,000,000 시간의 훈련 데이터 사용 (Zhang et al., 2021)을 탐색하고 있습니다.

멀티태스크 학습

멀티태스크 학습은 오랜 시간 동안 연구되어온 분야입니다. 음성 인식에서는 멀티링구 모델이 10년 이상 동안 연구되어왔습니다 (Schultz & Kirchhoff, 2006). NLP에서 멀티태스크 학습에 대한 첫 번째 연구는 Collobert et al. (2011)로, 단일 모델로 여러 작업을 수행하는 NLP에서 멀티태스크 학습을 탐구한 첫 연구입니다. 시퀀스 투 시퀀스 프레임워크 내에서의 멀티태스크 학습은 Sutskever et al. (2014)에서 다중 인코더 및 디코더를 사용하여 연구되었습니다. Johnson et al. (2017)는 언어 코드와 공유 인코더/디코더 아키텍처를 사용하여 기계 번역을 위해 언어 코드를 활용한 첫 번째 작업을 수행하였으며, 이를 통해 별도의 인코더 및 디코더가 필요하지 않게 되었습니다. 이 접근 방식은 McCann et al. (2018)의 "텍스트 투 텍스트" 프레임워크로 더 단순화되었으며, Radford et al. (2019) 및 Raffel et al. (2020)의 대규모 트랜스포머 언어 모델의 성공으로 널리 알려지게 되었습니다. Toshniwal et al. (2018)은 최신 딥러닝 음성 인식 시스템을 여러 언어로 동시에 훈련시키는 연구를 처음으로 시연했으며, Pratap et al. (2020a)는 이러한 연구를 큰 언어 모델을 사용하여 50개 언어로 확장시켰습니다. MUTE (Wang et al., 2020c) 및 mSLAM (Bapna et al., 2022)은 텍스트 및 음성 언어 작업을 모두 포함하여 공동 훈련을 연구하며 이들 사이의 전이를 시연하였습니다.

로버스트성

모델이 얼마나 효과적으로 전이하고, 분포 변화 및 다른 유형의 변조에 대한 내구성이 어떻게 되는지, 오랫동안 연구되어온 주제로, 기계 학습의 여러 분야에서 활발한 연구가 진행 중입니다. Torralba & Efros (2011)는 한때 부족했던 기계 학습 모델의 데이터셋 간 일반화 부족을 강조했습니다. 다른 많은 연구에서도 IID(test set의 분포가 동일한) 테스트 세트에서 높은 성능을 보여도 기계 학습 모델이 조금만 다른 환경에서 평가될 때 여전히 많은 오류를 발생시킬 수 있음을 보여주었습니다 (Lake et al., 2017; Jia & Liang, 2017; Alcorn et al., 2019; Barbu et al., 2019; Recht et al., 2019). 더 최근에는 Taori et al. (2020)가 이미지 분류 모델의 내구성을 연구하였으며, Miller et al. (2020)은 질문 응답 모델에 대한 내구성을 조사하였습니다. 중요한 결과 중 하나는 소개 부분에서 논의한 대로 다중 도메인 훈련이 내구성과 일반화를 증가시킨다는 것입니다. 이러한 결과는 음성 인식뿐만 아니라 NLP (Hendrycks et al., 2020) 및 컴퓨터 비전 (Radford et al., 2021)을 포함한 다양한 분야에서 재현되었습니다.



제한 사항 및 향후 연구

우리의 실험 결과, 분석 및 미련한 영역과 향후 연구를 위해 몇 가지 제한 사항과 영역을 주목했습니다.
개선된 디코딩 전략. Whisper를 확장하면서 큰 모델이 비슷한 소리나는 단어를 혼동하는 것과 같은 인식 관련 오류를 줄이는 데 지속적이고 신뢰할 수 있는 진전을 보였습니다. 그러나 여전히 많은 오류, 특히 장편 전사에서는 더 고집스럽고 명백하게 비인간/지각적인 성격을 가지고 있습니다. 이러한 오류는 seq2seq 모델, 언어 모델 및 텍스트-오디오 정렬의 실패 모드인 반면, 무한 반복 루프에 갇히는 문제, 오디오 세그먼트의 처음이나 마지막 몇 마디를 전사하지 않는 문제 또는 모델이 실제 오디오와 관련이 없는 전사를 출력하는 완전한 환각 등을 포함합니다. 섹션 4.5에서 논의한 디코딩 세부 정보는 큰 도움이 되지만, 우리는 Whisper 모델을 고품질 감독형 데이터셋에 대한 미세 조정하거나 디코딩 성능을 더 직접 최적화하기 위해 강화 학습을 사용하여 이러한 오류를 더 줄일 수 있을 것으로 예상합니다.
낮은 리소스 언어용 훈련 데이터 증가

Figure 3에서 확인할 수 있듯이 Whisper의 음성 인식 성능은 여전히 많은 언어에서 상당히 낮습니다. 동일한 분석은 각 언어의 훈련 데이터 양으로 언어의 성능을 아주 잘 예측한다는 명확한 향상 경로를 제시합니다. 현재 우리의 사전 훈련 데이터셋은 주로 인터넷의 영어 중심 부분에서 가져온 데이터 수집 파이프라인의 편향으로 인해 영어가 매우 중요한 비중을 차지하고 있으므로 대부분의 언어는 1000시간 미만의 훈련 데이터만 가지고 있습니다. 이러한 드문 언어의 데이터 양을 늘리는 것에 대한 목표 지향적 노력은 전체 훈련 데이터셋 크기의 작은 증가만 있어도 평균 음성 인식 성능을 크게 향상시킬 수 있을 것입니다.

미세 조정 연구

이 연구에서는 음성 처리 시스템의 견고성 특성에 중점을 두어 Whisper의 제로샷 전송 성능만 연구했습니다. 이것은 일반적인 신뢰성을 나타내기 때문에 연구해야하는 중요한 설정입니다. 그러나 고품질의 감독형 음성 데이터가 존재하는 많은 도메인에서는 미세 조정을 통해 결과를 더 개선할 수 있을 것으로 예상됩니다. 미세 조정 연구를 연구하는 추가 이점은 훨씬 더 일반적인 평가 설정이기 때문에 직접적인 비교를 허용한다는 것입니다.

로버스트성에 대한 언어 모델의 영향 연구

소개에서 논의한대로, Whisper의 로버스트성은 그 강력한 디코더에 부분적으로 기인한다고 의심합니다. 현재 Whisper의 이점이 어디에서 유래되었는지에 대한 정도는 아직 알려져 있지 않습니다. 이것은 Whisper의 여러 디자인 구성 요소를 삭제하거나 CTC 모델에서 디코더를 훈련시키는 것과 같은 방법으로 연구하거나 언어 모델과 함께 사용될 때 wav2vec 2.0과 같은 기존 음성 인식 인코더의 성능이 어떻게 변하는지 연구함으로써 연구할 수 있습니다.

보조 훈련 목표 추가

Whisper는 최근 대규모 음성 인식 시스템의 상당한 부분과 달리 자율적 사전 훈련이나 자기 교육 기술이 부족한 점에서 주목할만한 차이점이 있습니다. 우리는 좋은 성능을 얻기 위해 이러한 기술이 필요하지 않다고 판단했지만, 이러한 기술을 통합함으로써 결과를 더 개선할 수 있을 것입니다.

결론

Whisper는 음성 인식 연구에서 약점 있는 감독형 사전 훈련의 스케일링이 지금까지 미묘하게 인식되어 왔음을 시사합니다. 우리는 최근 대규모 음성 인식 연구의 핵심이었던 자체 감독 및 자체 훈련 기술이 필요하지 않고, 대규모이고 다양한 지도 데이터셋에서 훈련하고 제로샷 전송에 중점을 두면 음성 인식 시스템의 견고성을 크게 향상시킬 수 있음을 보여주었습니다.

감사의 말

Whisper 데이터 생성에 참여한 수백만 명의 사람들에게 감사드립니다. 또한 이 프로젝트를 영감을 주는 폭포 하이킹에서의 대화를 위해 Nick Ryder, Will Zhuk 및 Andrew Carr에게 감사드립니다. 이 프로젝트에서 사용한 소프트웨어 및 하드웨어 인프라에 대한 중요한 역할을 한 OpenAI의 가속 및 슈퍼컴퓨팅 팀에 감사드립니다. 또한 정책 관점에서 이 프로젝트를 조언해 준 Pamela Mishkin에게 감사드립니다. 마지막으로, 이 프로젝트 전반에 걸쳐 사용된 다양한 소프트웨어 패키지, Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020) 및 scikit-learn (Pedregosa et al., 2011)를 비롯한 소프트웨어 패키지 개발자들에게 감사드립니다.




모델
핀란드어 프랑스어 힌디어
헝가리어 인도네시아어
이탈리아어 일본어
리투아니아어
라트비아어
말라얄람어 몽골어
네덜란드어 폴란드어

휘스퍼타이니 68.5 49.7 108.3 87.0 49.6 44.5 36.1 103.5 87.8 102.7 123.0 43.6 45.3
휘스퍼베이스 52.9 37.3 106.5 71.9 36.1 30.5 24.2 91.3 78.0 122.9 137.0 29.5 32.8
휘스퍼스몰 30.5 22.7 43.6 44.4 18.4 16.0 14.0 72.8 54.6 104.8 225.8 14.2 16.9
휘스퍼미디엄 18.8 16.0 31.5 26.9 11.6 9.4 10.5 49.4 37.2 137.8 113.4 8.0 10.1
휘스퍼라지 17.0 14.7 25.0 23.5 10.6 8.1 9.4 43.9 34.8 107.1 117.4 7.1 9.0
휘스퍼라지-v2 14.4 13.9 21.9 19.7 8.5 7.1 9.1 35.2 25.5 103.2 128.4 5.8 7.6

모델
포르투갈어 루마니아어
러시아어 슬로바키아어
슬로베니아어
세르비아어 스웨덴어 타밀어 태국어 터키어 우르두어

베트남어
중국어

휘스퍼타이니 35.2 68.2 40.6 104.0 82.0 106.1 58.2 105.7 55.9 53.6 74.7 69.3 52.4
휘스퍼베이스 23.7 55.9 28.8 87.2 70.3 103.0 42.4 49.5 32.1 38.6 58.6 51.6 44.9
휘스퍼스몰 12.5 33.2 15.0 60.4 45.5 101.3 22.1 28.7 18.1 23.7 39.1 33.3 29.4
휘스퍼미디엄 8.1 21.5 9.3 42.0 29.8 85.6 13.7 19.6 10.5 17.7 29.9 24.4 23.2
휘스퍼라지 7.1 19.8 8.2 37.9 25.1 87.4 12.4 17.6 8.8 16.6 28.1 19.9 29.1
휘스퍼라지-v2 6.3 15.8 7.1 31.9 20.6 70.5 10.6 16.1 8.0 14.5 24.2 18.2 26.8

테이블 11. CommonVoice9에서의 WER(%)

D.2.3. 대중의 목소리

모델
체코 독일 영어
비강세
스페인 에스토니아 핀란드 프랑스 크로아티아 헝가리 이탈리아 리투아니아 네덜란드 폴란드 루마니아 슬로바키아 슬로베니아
Whispertiny 73.5 27.4 11.6 18.8 19.7 99.2 54.1 32.9 72.4 74.5 40.5 93.1 41.9 31.4 65.9 78.7 81.9
Whisperbase 54.7 20.6 9.5 17.5 14.4 83.0 39.7 24.9 53.6 52.6 30.8 82.1 29.4 22.1 49.3 63.7 70.5
Whispersmall 28.8 14.8 8.2 19.2 11.1 59.2 24.9 15.7 33.7 31.3 22.9 60.1 18.8 13.3 28.6 37.3 50.8
Whispermedium 18.4 12.4 7.6 19.1 9.6 38.2 16.6 12.2 23.9 19.3 19.7 39.3 14.9 10.1 18.4 23.0 36.3
Whisperlarge 15.9 11.9 7.2 20.8 8.8 33.3 15.5 11.0 19.0 16.8 18.4 35.0 14.0 9.0 17.0 19.1 31.3
Whisperlarge-v2 12.6 11.2 7.0 18.6 8.2 28.7 12.4 11.4 16.1 13.8 19.0 33.2 12.9 7.8 14.4 15.4 27.9

테이블 12. VoxPopuli에서의 WER(%) 
대규모 약한 지도를 통한 강건한 음성 인식       24

D.2.4. 꽃들

모델
아프리칸스어 아무하어 아랍어 아사메어 아제르바이잔어 벨라루스어 불가리아어 벵골어 보스니아어 카탈로니아어 중국어 체코어 웨일스어 덴마크어
Whispertiny 91.2 122.9 63.4 102.0 93.1 94.0 81.0 101.6 82.1 42.8 40.5 82.8 101.3 82.0
Whisperbase 81.5 196.8 48.8 102.0 76.4 91.3 65.1 100.6 66.7 29.0 34.1 66.0 85.3 57.6
Whispersmall 61.1 120.2 30.6 108.0 49.1 75.1 37.3 104.4 39.4 16.2 20.8 37.6 59.3 32.8
Whispermedium 44.9 229.3 20.4 102.3 33.1 60.4 21.4 100.6 23.9 9.6 12.1 21.3 40.8 19.5
Whisperlarge 42.6 129.3 18.1 105.6 28.7 56.6 18.4 104.9 20.7 8.0 19.6 17.4 36.6 16.8
Whisperlarge-v2 36.7 140.3 16.0 106.2 23.4 45.4 14.6 104.1 15.7 7.3 14.7 13.3 33.0 13.8

모델
독일어 그리스어 영어 스페인어 에스토니아어 페르시아어 핀란드어 타갈로그어 프랑스어 갈리시아어 구자라트어 하우사어 히브리어 힌디어
Whispertiny 27.8 67.4 12.4 15.9 94.8 101.8 59.5 65.6 41.4 54.8 101.2 100.2 71.6 102.3
Whisperbase 17.9 53.5 8.9 9.9 77.9 86.1 43.1 45.8 28.5 47.4 101.4 98.6 61.7 101.1
Whispersmall 10.2 30.8 6.1 5.6 51.3 55.8 24.0 27.7 15.0 30.2 106.4 90.1 44.4 38.4
Whispermedium 6.5 19.0 4.4 3.6 29.8 41.0 13.9 19.1 8.7 21.2 104.8 106.6 33.1 26.8
Whisperlarge 5.5 18.7 4.5 3.5 25.5 36.1 12.2 15.8 7.7 19.0 103.9 87.0 30.2 26.9
Whisperlarge-v2 4.5 12.5 4.2 3.0 21.9 32.9 9.7 13.8 8.3 15.4 102.7 88.9 27.1 21.5

모델
크로아티아 헝가리 아르메니아 인도네시아 아이슬란드 이탈리아 일본 자바어 자바어 그루지야 카자흐스탄 캄보디아 칸나다어 한국어

룩셈부르크어
Whispertiny 79.0 83.8 118.6 51.7 113.3 29.8 37.0 107.3 123.0 165.2 100.6 100.7 36.1 99.1
Whisperbase 59.1 65.0 126.3 33.1 95.5 17.9 22.8 89.5 114.7 109.2 101.6 107.2 27.8 100.7
Whispersmall 33.4 38.9 86.6 16.3 72.6 9.8 12.0 88.6 118.3 70.3 104.4 100.4 19.6 100.1
Whispermedium 19.3 24.3 60.1 10.2 49.9 5.2 7.1 67.9 117.3 48.8 98.9 77.7 16.4 90.0
Whisperlarge 16.7 21.0 53.7 8.5 43.0 4.2 6.4 87.0 100.5 43.8 96.0 69.8 15.2 86.5
Whisperlarge-v2 13.4 17.0 44.6 7.1 38.2 4.0 5.3 nan 105.0 37.7 99.7 37.0 14.3 88.0

모델
링갈라어
라오어
리투아니아어
라트비아어
마오리어
마케도니아어
말라얄람어
몽골어

마라티어 말라이 말티어

미얀마 노르웨이어
네팔

