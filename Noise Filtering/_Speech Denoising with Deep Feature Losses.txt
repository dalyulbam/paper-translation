사전 인쇄

깊은 특징 손실을 이용한 음성 노이즈 제거

프랑수아 G. 제르맹, 치펑 첸, 그리고 블라들렌 콜툰

요약 - 우리는 원시 파형을 직접 처리하여 음성 신호의 잡음 제거를 위한 end-to-end 딥러닝 접근 방식을 제시합니다. 음성이 첨가된 배경 신호로 손상된 입력 오디오가 주어지면, 시스템은 음성 내용만을 포함하는 처리된 신호를 생성하도록 목표로 합니다. 최근의 접근 방식은 다양한 딥 네트워크 구조를 사용하여 유망한 결과를 보여주고 있습니다. 본 논문에서는 깊은 특징 손실을 사용하여 완전 합성곱 컨텍스트 집계 네트워크를 훈련시키는 것을 제안합니다. 이 손실은 음향 환경 감지 및 가정용 오디오 태깅을 위해 훈련된 다른 네트워크에서 내부 특징 활성화를 비교함에 기반합니다. 우리의 접근 방식은 객관적인 음성 품질 측정 및 인간 청취자와의 대규모 지각 실험에서 최신 기술을 능가합니다. 또한 전통적인 회귀 손실을 사용하여 훈련된 동일한 네트워크보다 우수한 성능을 보입니다. 새로운 접근 방식의 장점은 가장 강력한 배경 잡음이 있는 가장 어려운 데이터에 대해 특히 두드러지며, 이는 잡음 제거가 가장 필요하고 도전적인 경우입니다.

색인 용어 - 음성 노이즈 제거, 음성 개선, 딥러닝, 문맥 집계 네트워크, 딥 피처 손실

I. 소개

음성 노이즈 제거(또는 개선)는 음성 신호에서 배경 콘텐츠를 제거하는 것을 의미합니다 [1]. 이 오디오 손상의 보편성으로 인해, 노이즈 제거는 인간-인간 (예: 보청기) 및 인간-기계 (예: 자동 음성 인식) 커뮤니케이션의 개선에 중요한 역할을 합니다. 문제의 특히 어려운 형태 중 하나는 음성 프로세스의 복잡성과 비음성 자료의 미지성으로 인한 단일 채널 음성 노이즈 제거의 미결 문제입니다. 데이터의 복잡성은 오디오 자료에 데이터 샘플의 높은 밀도 (예: 초당 16,000개의 샘플)가 포함되어 있기 때문에 더욱 복잡해집니다. 인지 메커니즘에 의해 작은 오류가 평균 사용자에게 여전히 눈에 띌 수 있기 때문에 중재된 인간-인간 커뮤니케이션에서도 문제가 발생할 수 있습니다 [2].
이 연구에서는 음성 노이즈 제거에 대한 end-to-end 딥러닝 접근 방식을 제시합니다. 저희 방법은 깊은 특징 손실을 사용하여 완전 합성곱 노이즈 제거 네트워크를 훈련시킵니다. 두 웨이브폼 간의 손실을 계산하기 위해, 우리는 각 웨이브폼에 사전 훈련된 오디오 분류 네트워크를 적용하고 두 신호에 의해 네트워크에서 유발된 내부 활성화 패턴을 비교합니다. 이는 두 웨이브폼의 다양한 스케일에서 다양한 특징을 비교합니다. 저희는 노이즈 제거에 대한 최근 end-to-end 딥러닝 기술과 저희 방법을 비교하는 광범위한 실험을 수행합니다. 저희 방법은 객관적인 음성 품질 지표와 인간 청취자를 대상으로 한 대규모 인지 실험에서 기준선보다 우수한 성능을 보여주므로 저희 방법이 더 효과적임을 나타냅니다. 제시된 방법의 장점은 다음과 같습니다.

F. Germain은 Stanford 대학교 CCRMA 센터에 소속되어 있으며, 주소는 CA 94305 Stanford입니다. 이메일: francois@ccrma.stanford.edu. 이 연구는 그가 Intel Labs에서 인턴으로 활동하던 동안 수행되었습니다. Q. Chen과 V. Koltun은 Intel Labs의 Intelligent Systems Lab에 소속되어 있으며, 주소는 CA 95054 Santa Clara입니다.

접근 방식은 가장 어렵고 시끄러운 입력에 대해 특히 두드러집니다. 이러한 경우에는 노이즈 제거가 가장 어렵습니다.

관련 연구

인기 있는 심층 신경망이 보급되기 전에는, 노이즈 제거 시스템은 스펙트로그램 도메인의 통계 신호 처리 방법에 의존했습니다 [1], 최근에는 스펙트로그램 분해 기반의 방법을 따랐습니다 [3]. 현재의 노이즈 제거 파이프라인은 최첨단 성능을 위해 심층 신경망에 의존합니다. 그러나 대부분의 파이프라인은 여전히 스펙트로그램 도메인에서 작동합니다 [4]–[11]. 따라서 시간 도메인 향상 신호를 생성하기 위해 역 단시간 푸리에 변환을 사용할 때 시간 에일리어싱으로 인한 신호 아티팩트가 발생합니다. 이 특정 문제는 약간 완화될 수 있지만, 계산 비용과 시스템 복잡성이 증가합니다 [12]–[18].
최근에는 원시 웨이브폼에서 최적화된 엔드 투 엔드 노이즈 제거 파이프라인의 설계에 대한 관심이 증가하고 있습니다. 이러한 접근 방식은 비싼 시간-주파수 변환이나 위상 정보의 손실을 피하면서 심층 신경망의 표현력을 완전히 활용하기 위해 고안되었습니다 [19]–[22]. 이러한 접근 방식 중 일부는 네트워크를 훈련시키기 위해 간단한 회귀 손실 함수를 사용합니다 [19], [20] (예: 원시 웨이브폼의 L1 손실), 반면에 더 고급 손실 함수를 사용한 접근 방식은 불일치 조건에서 제한된 성능 향상을 보였습니다 [21], [22].
우리의 손실 함수는 컴퓨터 비전 연구에서 영감을 받았습니다. 사전 훈련된 분류 신경망의 활성화는 이미지 스타일화와 합성을 위한 효과적인 손실 함수로 발견되었습니다 [23], [24]. 이러한 접근 방식은 두 이미지 사이의 손실을 계산하기 위해 사전 훈련된 이미지 분류 신경망을 적용합니다. 각 이미지는 네트워크 내부의 활성화 패턴을 유발하여 비교되며, 손실은 그들의 불일치로 정의됩니다. 이러한 복잡한 훈련 손실은 전문 지식이나 처리 네트워크 자체의 추가 복잡성 없이 최첨단 알고리즘을 제공하는 것으로 입증되었습니다. 또한, 작업 특정 손실 네트워크 없이도 성능을 향상시킬 수 있습니다 [25]. 우리의 연구는 이 아이디어를 음성 처리의 맥락에서 발전시킵니다.

II. 방법

노이즈 제거 네트워크

음성 s에 해당하는 오디오 신호 x가 첨가된 배경 신호 n에 의해 손상되었다고 가정하자. 즉, x = s + n이다. 우리의 목표는 g(x) ≈ s인 노이즈 제거 연산자 g를 찾는 것이다. 우리는 컨텍스트 집계 네트워크 [26]를 기반으로 한 완전 합성 네트워크 아키텍처를 사용한다. 입력을 따라 네트워크를 이동하면서 출력 신호를 샘플 단위로 합성한다. 컨텍스트 집계 네트워크는 이전에 WaveNet 아키텍처에서 음성에 사용되었다.

합성 [27]. 우리의 아키텍처는 WaveNet보다 간단합니다 - 레이어 간 스킵 연결, 조건부 없음, 게이트된 활성화 없음 - 그러나 우리의 손실 함수는 더 발전된 것입니다. II-B절에서 설명한 대로.
a) 문맥 집계: 우리의 네트워크는 16개의 합성곱 레이어로 구성됩니다. 첫 번째와 마지막 레이어 (손상된 입력 신호 및 향상된 출력 신호)는 각각 N × 1 차원의 1차원 텐서입니다. 입력 신호의 샘플 수 N은 가변적이며 미리 주어지지 않습니다. 신호 샘플링 주파수 fs는 16kHz로 가정됩니다. 각 중간 레이어는 N × W 차원의 2차원 텐서이며, 여기서 W는 각 레이어의 특징 맵 수입니다. (W = 64로 설정합니다.) 각 중간 레이어의 내용은 이전 레이어에서 3 × 1 합성곱 커널 [26]을 통해 계산된 다음 적응적 정규화 (아래 참조)와 점별 비선형 누수 정류 유닛 (LReLU) [28] max (0.2x, x)를 거쳐 계산됩니다. 정규화 때문에 중간 레이어에는 편향 항이 사용되지 않습니다. 모든 레이어를 N의 "유효" 길이가 일정하도록 제로 패딩합니다. 그런 다음 우리의 네트워크는 음성 콘텐츠가 시퀀스 가장자리에 가까울 때에도 오디오 파일의 시작과 끝을 처리할 수 있도록 훈련됩니다.
확장 연산자는 레이어 간 샘플링 주파수를 변경하지 않고 장거리 문맥 정보를 집계합니다 [26], [27]. 여기서 우리는 1번째 중간 레이어에 대해 20에서 13번째 중간 레이어에 대해 212까지 지수적으로 확장 계수를 증가시킵니다. 14번째와 마지막 레이어에는 확장을 사용하지 않습니다. 출력 레이어에는 선형 변환 (1 × 1 합성곱과 정규화 및 비선형성이 없는 편향)을 사용하여 출력 신호의 샘플을 합성합니다. 파이프라인의 수용 영역은 214 + 1 샘플로, fs = 16kHz의 약 1초 오디오에 해당합니다. 따라서 우리는 시스템이 말로 된 단어의 시간 스케일에서 문맥을 포착할 것으로 기대합니다. 유사한 네트워크 아키텍처는 이미지 처리에서 간결성과 실행 시간 측면에서 유리하다는 것이 입증되었습니다 [29].
b) 적응적 정규화: 우리 네트워크에서 사용하는 적응적 정규화 연산자는 [29]에서 제안된 연산자와 일치하며 성능과 훈련 속도를 향상시킵니다. 입력 x의 배치 정규화와 항등 매핑을 가중 합 αkx + βkBN(x)로 적응적으로 결합합니다 (여기서 αk, βk는 가중치입니다).

∈ R는 k번째 레이어의 스칼라 가중치이며, BN은 배치 정규화 연산자입니다 [30]). 가중치 α, β는 네트워크 매개변수로 역전파를 통해 학습됩니다.

B. 특징 손실
우리의 실험에서는 간단한 훈련 손실 (예: L1)이 신호 대 잡음 비율 (SNR)이 낮은 경우에는 뚜렷하게 저하된 출력 품질을 보였습니다. 네트워크는 인지적 중요성을 가진 저에너지 음성 정보를 부적절하게 처리하는 것으로 보였습니다. 대신, 우리는 비교되는 신호에 적용되는 사전 훈련된 심층 네트워크의 내부 활성화의 차이를 벌점으로 주는 심층 특징 손실을 사용하여 노이즈 제거 네트워크를 훈련시킵니다. 계층화된 네트워크의 특징 활성화는 손실 네트워크의 다른 깊이에서 다른 시간 스케일에 해당합니다. 이러한 활성화의 차이를 벌점으로 주는 것은 다양한 오디오 스케일에서 많은 특징을 비교합니다.
컴퓨터 비전에서는 VGG-19 [31]과 같은 표준 분류 네트워크가 있으며, 이는 표준 분류 작업에 대해 사전 훈련되었습니다.

sification 데이터셋은 ImageNet [32]과 같은 것들이 있다. 이러한 표준 분류 네트워크는 오디오 처리 분야에는 아직 존재하지 않으므로, 우리는 직접 특징 손실 네트워크를 설계하고 훈련시킵니다.
a) 특징 손실 네트워크: 우리는 컴퓨터 비전에서 VGG 아키텍처에서 영감을 받은 간단한 오디오 분류 네트워크를 설계합니다. 이는 특히 효과적인 특징 손실 아키텍처로 알려져 있기 때문입니다. 이 네트워크는 3 × 1 커널을 가진 15개의 합성곱 레이어, 배치 정규화, LReLU 유닛, 그리고 제로 패딩으로 구성되어 있습니다. 각 레이어는 2로 감소하여, 이전 레이어에 비해 다음 레이어의 길이를 절반으로 줄입니다. 채널의 수는 5개의 레이어마다 두 배로 증가하며, 첫 번째 중간 레이어에는 32개의 채널이 있습니다. 마지막 특징 레이어의 각 채널은 평균 풀링되어 출력 특징 벡터를 생성합니다. 수용 영역은 215입니다.

−
1 샘플.
우리는 네트워크를 훈련시키기 위해 역전파를 사용하여 출력 벡터를 하나 이상의 로지스틱 분류기에 특징으로 제공하고 하나 이상의 분류 작업에 대한 교차 엔트로피 손실을 사용합니다.
b) 노이즈 제거 손실 함수: Φm을 특징 손실 네트워크의 m번째 특징 레이어로 정의하며, 다른 깊이의 레이어는 다양한 시간 해상도의 특징에 해당합니다. 특징 손실 함수는 훈련 중인 노이즈 제거 네트워크의 출력 g(x)와 깨끗한 참조 신호 s에 의해 네트워크의 다른 레이어에서 유발된 특징 활성화 사이의 차이에 대한 가중치가 있는 L1 손실로 정의됩니다.

Ls, x(θ) = M

X
m=1λm kΦm(s)
−
Φm(g(x;θ))
k1
,  (1)

X
m=1λm kΦm(s)
−
Φm(g(x;θ))
k1
,  (1)

θ는 노이즈 제거 네트워크의 매개변수입니다. 
가중치 λm은 각 레이어의 손실에 기여하는 정도를 균형있게 조절하기 위해 설정됩니다. 
이 값은 10번의 훈련 에포크 후 kΦm(s) − Φm(g(x;θ)) k1의 상대적인 값의 역수로 설정됩니다. 
(처음 10번의 에포크 동안 가중치는 1로 설정됩니다.)

III. 훈련

특징 손실

a) 작업: 일반적인 용도의 특징 손실 네트워크를 생성하기 위해, 우리는 여러 오디오 분류 작업에 대해 함께 훈련시킵니다 (로지스틱 분류기 매개변수만 작업에 따라 훈련됩니다). 우리는 DCASE 2016 챌린지 [33]에서 두 가지 작업을 사용합니다: 음향 장면 분류 작업과 가정용 오디오 태깅 작업. 첫 번째 작업에서는 다양한 장면 (예: 해변)이 포함된 오디오 파일이 제공됩니다. 각 파일에 대해 장면 유형을 결정하는 것이 목표입니다. 두 번째 작업에서는 관심 있는 이벤트 (예: 아이가 말하는 것)가 포함된 오디오 파일이 제공됩니다. 각 파일에서 어떤 이벤트가 발생했는지 결정하는 것이 목표입니다 (하나의 파일에 여러 이벤트가 있을 수 있음).

b) 데이터: 장면 분류 작업을 위한 훈련 세트 [34]는 44.1kHz로 샘플링된 30초 길이의 오디오 파일로 구성되어 있으며, 15개의 다른 장면 (즉, 클래스)으로 분할되어 있습니다. 우리는 샘플링 주파수를 16kHz로 줄이기 위해 특징 손실을 개발해야 하므로 데이터를 재샘플링합니다. 오디오 파일은 스테레오이므로 두 개의 모노 파일로 분할합니다. 훈련 세트에는 2,340개의 파일이 포함되어 있습니다. 태깅 작업을 위한 훈련 세트 CHiME-Home-refine [35]는 16kHz로 샘플링된 4초 길이의 모노 오디오 파일로 구성되어 있으며, 7개의 다른 태그 (즉, 레이블)가 있습니다. 훈련 세트에는 1,946개의 파일이 포함되어 있습니다.

c) 훈련: 네트워크 가중치는 제이비어 초기화 [36]로 초기화됩니다. 우리는 학습률이 10^-4인 아담 옵티마이저 [37]를 사용합니다. 모델은 2,500 에포크 동안 훈련됩니다.
각 에포크에서는 각 작업에 대해 훈련 데이터를 반복하며, 각 작업의 파일들 사이에서 번갈아가며 진행합니다. 파일의 순서는 각 에포크마다 독립적으로 무작위로 섞입니다. 첫 번째 작업을 위한 데이터셋은 두 번째 작업을 위한 것보다 크기가 크므로, 엄격한 번갈아가기를 유지하기 위해 두 번째 데이터셋의 일부 파일들을 (무작위로 선택하여) 두 번째로 다시 제시합니다. 1 에포크는 4,680 번의 반복 (1 파일 당 반복)으로 구성됩니다.
데이터 증강 절차로, 우리는 전체 클립을 제시하지 않고, 각 반복마다 무작위로 선택된 최소한의 지속 시간 215개 샘플의 연속적인 섹션을 제시합니다.

B. 음성 노이즈 제거

a) 데이터: 우리는 [38]에서 제공된 잡음이 섞인 데이터셋을 사용합니다. 우리의 지식으로는, 이는 명확하게 문서화된 혼합 절차를 가진 노이즈 제거를 위한 가장 큰 데이터셋입니다. 또한 우리가 기준으로 사용하는 최근 두 작업에서 사용된 데이터셋의 이점이 있습니다. 데이터에 관한 모든 세부 정보는 [38]에서 찾을 수 있습니다. 훈련 세트는 28명의 화자(14명의 남성/14명의 여성)의 음성 데이터와 10개의 고유한 배경 유형의 배경 데이터에서 생성됩니다. 각 잡음 세그먼트는 0, 5, 10, 15dB SNR로 4개의 파일을 생성하는 데 사용됩니다. 게시된 파일은 48kHz로 샘플링되고, 깨끗한 음성 파일의 최대 절대 진폭이 0.5가 되도록 정규화됩니다. 우리는 이를 16kHz로 재샘플링합니다. 전체 데이터셋은 11,572개의 파일로 구성됩니다.

b) 훈련: 네트워크의 가중치와 편향은 각각 제로와 제로로 초기화됩니다. 적응적 정규화 매개변수는 각각 α = 1, β = 0으로 초기화됩니다. 특성 손실은 첫 번째 M = 6개의 레이어를 사용하여 계산됩니다. 우리는 학습률이 10^-4인 Adam 옵티마이저를 사용합니다. 우리는 Titan X GPU에서 320 에포크(80시간) 동안 훈련합니다. 각 에포크에서 전체 데이터셋을 무작위로 순서대로 제시하며, 파일은 전체적으로 제시됩니다.

IV. 실험 설정

기준선

기준으로, 우리는 Wiener 필터링 파이프라인을 사용하여 사전에 노이즈 SNR 추정을 수행하며([39]에서 구현된 대로), 최근에 개발된 최첨단 기술 두 가지를 사용하여 원시 파형에 직접적으로 엔드 투 엔드 노이즈 제거를 수행하는 딥 네트워크를 사용합니다: 음성 개선 생성적 적대 신경망(SEGAN) [21]과 WaveNet 기반 네트워크 [20]. 마지막 하나는 [27]의 아키텍처에 소규모 수정을 가한 것입니다. 이는 게이트 활성화 유닛, 스킵 연결 및 조건부 메커니즘을 사용하는 적용된 컨텍스트 집계 모듈을 사용합니다. 수정 사항에는 분류 손실이 아닌 회귀 손실(원시 파형의 L1)로 훈련하는 것이 포함됩니다. 레이어 수는 우리의 네트워크보다 크지만(30), 수용 영역은 더 작습니다(3).

· 211개의 샘플을 사용하여,
더 제한된 시간 범위에서 맥락 정보를 수집합니다.
네트워크 구조는 또한 우리보다 더 복잡합니다.
깊은 학습 기준선 모델에 대해서는, 각각의 저자가 게시한 코드와 모델을 사용합니다.
이 모델들은 저자들에 의해 정확히 동일한 훈련 데이터셋으로 최적화되어 있으므로, 공정한 비교가 가능합니다.

1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 -> 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5

BAK
0
20 40 파일 수

그림 1. 복합 배경 점수에 따른 테스트 세트의 분포.
테스트 세트는 빨간 점선으로 구분된 8개의 구간으로 분할되었습니다.

B. 데이터

우리의 모든 테스트는 맞지 않는 조건에서 수행됩니다. 데이터 소스는 섹션 III-B와 동일합니다. 음성은 2명의 화자(1명의 남성/1명의 여성)로부터 얻어집니다. 배경 데이터는 5개의 서로 다른 배경 유형에서 얻어집니다. 훈련 중에는 테스트 시간에 사용되는 화자나 배경을 본 적이 없습니다. 각 배경 세그먼트는 2.5, 7.5, 12.5, 17.5 dB SNR로 4개의 파일을 생성하는 데 사용됩니다. 완전한 테스트 세트는 824개의 파일로 구성됩니다. 우리의 노이즈 제거 파이프라인은 우리의 구성에서 1초의 오디오를 처리하는 데 약 12ms가 소요됩니다.

C. 양적 측정

a) 객관적인 품질 측정 지표: 각 시스템을 평가하기 위해, 우리는 그 시스템의 출력을 원본 음성 신호(즉, 깨끗한 음성만)와 비교합니다. 원본을 기준으로 음성 품질을 측정하는 일반적인 지표는 [1]에서 비교됩니다. 우리는 여기서 인간 청취자 평가와 가장 관련성이 높은 것으로 판명된 [39]의 종합 점수를 사용합니다. 이 점수는 전체 (OVL), 신호 (SIG) 및 배경 (BAK) 점수로 구성되며, 각각 1.0에서 5.0까지의 척도로 측정되며, 전체 신호 품질, 음성 신호 저하만을 고려한 품질 측정 및 배경 신호 침입성만을 고려한 품질 측정에 해당합니다 [40]. 또한 SNR [41]을 보고합니다. 이는 주어진 신호에서 잔여 배경과 음성의 상대 에너지를 데시벨 (dB)로 측정한 원시 지표입니다. 우리는 [1]의 구현을 사용합니다. 모든 지표에서 높은 점수는 더 나은 성능을 나타냅니다.
테스트 데이터셋은 4개의 혼합 SNR 하위 그룹으로 나누어집니다 (IV-B 섹션 참조). 우리는 SNR이 손상 수준에 대한 인간의 인식과 상관관계가 낮기 때문에 데이터셋을 손상의 연속적인 분포로 간주하는 것이 더 적절하다고 주장합니다 [1]. 손상 수준의 연속성은 배경 침입성 BAK 점수의 분포에서 더 잘 나타납니다. (SIG 점수는 왜곡되지 않은 음성 신호가 추가되기 때문에 정보성이 덜합니다.) 입력 손상의 크기에 따른 성능을 평가하기 위해, 우리는 테스트 세트를 BAK 점수 분포의 8개 옥타일에 해당하는 크기가 같은 8개의 구간으로 분할합니다. 이는 Figure 1에 나타납니다. 각 구간은 다른 노이즈 제거의 난이도를 나타냅니다.

b) 결과: 표 I는 우리의 접근 방식과 기준선에 대한 이러한 지표를 테스트 세트에서 평가한 결과를 보고합니다. 우리의 방법

표 I
목표 품질 측정에 따른 다른 접근 방식의 성능. (높을수록 좋음.)

SNR SIG BAK OVL - SNR 신호 BAK OVL

시끄러운 8.45 3.34 2.44 2.63
위너 12.28 3.23 2.68 2.67
SEGAN 14.82 3.21 2.76 2.56
WaveNet 18.18 2.87 3.08 2.43
우리들의 19.00 3.86 3.33 3.22
4                                                                 사전 인쇄

1 2 3 4 5 6 7 8 트랜치
10
20

S N R

1 2 3 4 5 6 7 8 트랜치
234
S I G

1 2 3 4 5 6 7 8 트랜치
234
B A K

우리들
웨이브넷

1 2 3 4 5 6 7 8 트랜치
234
O V L

세간
위너

그림 2. 테스트 세트의 각 트랜치에 대해 SNR, SIG, BAK 및 OVL과 같은 4가지 목적 품질 측정에 따른 다양한 노이즈 제거 접근 방식의 성능. 모든 측정 항목에서 높을수록 좋습니다.

모든 측정에 따라 기준선을 훨씬 능가합니다. 그림 2의 그래프는 우리의 네트워크가 트랜치로 분리된 모든 배경 침입 수준에 대해 최고의 품질을 제공함을 보여줍니다. 특히 인지적 동기를 갖춘 복합 측정에 따라 상당한 차이가 있습니다. 표 II는 특징 손실을 사용하는 것과 L1 또는 L2 손실을 사용하여 동일한 데이터와 동일한 절차로 동일한 노이즈 제거 네트워크를 훈련하는 것의 이점을 보여줍니다. 특징 손실로 훈련하는 것은 다른 손실로 훈련된 네트워크보다 우수한 성능을 발휘합니다. 특히, L1 손실은 우리의 특징 손실과 유사한 SNR 점수를 달성하지만, 특징 손실은 BAK 및 OVL 측정에 명확한 개선을 보여줍니다. 또한, 특히 노이즈가 있는 트랜치에서 SIG 측정에 대해 우수한 점수를 얻으며, 중요한 단서가 노이즈에 숨겨져 있을 때 의미 있는 특징을 포착할 수 있는 능력을 보여줍니다.

D. 지각 실험

실험 설계: 목적 지표는 인간의 오디오 품질 평가와 부분적으로 상관 관계가 있다는 것이 알려져 있다 [1]. 따라서, 우리는 인간 청취자와 함께 신중하게 설계된 지각적 실험도 진행한다. 이 절차는 Amazon Mechanical Turk 플랫폼에서 규모에 맞게 A/B 테스트를 기반으로 한다. A/B 테스트는 Human Intelligence Tasks (HITs)로 그룹화된다. 각 HIT는 100개의 "우리 방법 vs 기준선" 쌍 비교로 구성된다. 각 비교는 작업자가 임의로 재생할 수 있는 두 개의 오디오 클립을 제시한다. 클립은 어떤 순서로든 여러 번 재생될 수 있다. 클립 중 하나는 우리의 접근 방식의 출력이고 다른 하나는 테스트 세트의 동일한 입력에 대한 기준선의 출력이다. 파일은 임의의 순서로 제시되므로 작업자는 클립의 출처에 대한 정보를 알 수 없다. 작업자는 각 쌍에서 더 깨끗한 음성을 가진 클립을 선택하도록 요청된다. 각 HIT에는 부주의하거나 부주의한 작업자를 방지하기 위해 답이 명백한 10개의 '감시' 비교도 포함된다. 이러한 감시 쌍은 임의의 순서로 HIT에 섞여진다. 작업자가 두 개 이상의 감시 쌍에 잘못된 답을 제시하면 전체 HIT가 폐기된다. 각 HIT에는 총 110개의 쌍 비교가 포함된다. 작업자는 HIT를 완료하기 위해 1시간을 주어진다. 각 HIT는 10명의 구별된 작업자에 의해 완료된다.

표 II
다른 손실 함수로 동일한 네트워크를 훈련시키기. 모든 지표에서 높을수록 좋다.

SNR  SIG BAK  OVL - SNR  SIG BAK  OVL

시끄러운 8.45 3.34 2.44 2.63
L2    18.46 3.70 3.21 3.07
L1    18.98 3.75 3.27 3.11
특징 손실 19.00 3.86 3.33 3.22

결과: 결과는 표 III에 요약되어 있습니다. 이 표는 청취자가 우리의 네트워크에 의해 소음 제거된 클립을 기준선에 의해 소음 제거된 클립보다 깨끗하다고 평가한 조합의 비율을 보여줍니다. 선호도는 4개의 구간에 걸쳐 각 기준선과 비교하여 제시됩니다. 가장 주목할만한 결과는 가장 어려운 구간에서 우리의 접근 방식의 출력이 최근 최첨단 심층 신경망의 출력보다 83% 이상의 비교에서 더 깨끗하다고 평가되었다는 것입니다. 모든 결과는 p < 10^-3으로 통계적으로 유의미합니다. 이는 우리의 알고리즘이 배경 신호로부터의 저하가 훨씬 더 눈에 띄는 이 지역에서 더 견고하며, 소음 제거가 특히 유용한 이 지역에서 더 견고하다는 것을 보여줍니다. 입력의 저하 수준이 낮은 더 쉬운 구간에서는 우리의 방법과 기준선 모두 일반적으로 만족스럽게 수행되며, 청취자는 다른 처리된 파일들 사이를 구별하는 데 어려움을 겪을 수 있지만, 우리의 접근 방식에 대한 선호도는 모든 기준선과 모든 구간에서 통계적으로 유의미한 수준에서 우수한 수준을 유지합니다.

V. 결론

우리는 완전 합성곱 네트워크를 사용하는 end-to-end 음성 노이즈 제거 파이프라인을 제시했습니다. 이 네트워크는 훈련을 위해 여러 관련 오디오 분류 작업에 사전 훈련된 깊은 특징 손실 네트워크를 사용합니다. 이 접근 방식은 노이즈 제거 시스템이 다양한 스케일에서 음성 구조를 포착하고 복잡성을 추가하지 않거나 손실 디자인에 전문 지식이 필요하지 않고 더 나은 노이즈 제거 성능을 달성할 수 있도록 합니다. 실험 결과는 우리의 접근 방식이 객관적인 음성 품질 측정 및 대규모 인지 실험에서 인간 청취자와 비교하여 최근의 최첨단 기준을 크게 능가한다는 것을 보여줍니다. 특히, 제시된 접근 방식은 음성 노이즈 제거가 가장 어려운 가장 노이즈가 심한 조건에서 훨씬 더 우수한 성능을 발휘합니다. 우리의 논문은 합성곱 컨텍스트 집계 네트워크와 특징 손실의 결합 사용으로 최첨단 성능을 달성하는 것을 검증합니다.

각 셀은 시각적 실험 결과를 나열합니다. 각 셀은 랜덤화된 쌍별 비교에서 맹인의 비율을 나타냅니다.

청취자는 우리의 접근 방식의 출력물을 기준선의 출력물보다 깨끗하다고 평가했습니다. 각 행은 특정 기준선에 대한 결과를 나열합니다. 각 열은 테스트 세트의 일부에 대한 결과를 나열합니다. (확률은 50%이며, 높을수록 좋습니다.)

트란체: 1 (어려움) 3 (보통) 5 (쉬움) 7 (매우 쉬움)

우리 것 > 위너 96.1% 89.4% 81.7% 90.2%
우리 것 > 세간 83.5% 70.5% 64.1% 61.4%
우리 것 > 웨이브넷 83.9% 67.0% 61.4% 55.8%
미리 인쇄: GERMAIN et al.: 깊은 특징 손실로 음성 노이즈 제거   5

참고문헌

[1] P. C. Loizou, 음성 개선: 이론과 실제, 제2판, CRC Press, 2013.
[2] M. Bosi와 R. E. Goldberg, 디지털 오디오 코딩과 표준 소개, Springer, 2002.
[3] P. Smaragdis, C. Fevotte, G. J. Mysore, N. Mohammadiha, 그리고 M. Hoff-
man, "비음수 인수분해를 사용한 정적 및 동적 소스 분리: 통합된 관점," IEEE 신호처리 매거진, 제31권, 제3호, 2014.
[4] Y. Wang와 D. Wang, "구조화된 예측을 통한 칵테일 파티 처리," Neural Information Processing Systems (NIPS), 2012.
[5] X. Lu, Y. Tsao, S. Matsuda, 그리고 C. Hori, "심층 노이즈 제거 오토인코더를 기반으로 한 음성 개선," Interspeech, 2013.
[6] A. Narayanan과 D. Wang, "로버스트 음성 인식을 위한 심층 신경망을 사용한 이상적인 비율 마스크 추정," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), 2013.
[7] F. Weninger, J. R. Hershey, J. L. Roux, 그리고 B. Schuller, "단일 채널 음성 분리를 위한 식별적으로 훈련된 순환 신경망," IEEE 글로벌 신호 및 정보 처리 학회, 2014.
[8] Y. Xu, J. Du, L.-R. Dai, 그리고 C.-H. Lee, "심층 신경망을 기반으로 한 회귀 접근 방식을 통한 음성 개선," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, 제23권, 제1호, 2015.
[9] A. Kumar와 D. Florencio, "심층 신경망을 사용한 다중 잡음 환경에서의 음성 개선," arXiv:1605.02427, 2016.
[10] X.-L. Zhang와 D. Wang, "단일 음성 분리를 위한 심층 앙상블 학습 방법," IEEE/ACM 음향, 음성, 언어 처리 트랜잭션, 제24권, 제5호, 2016.
[11] J. Chen과 D. Wang, "지도된 음성 분리에서 확장성을 위한 장단기 기억망," Journal of the Acoustical Society of America, 제141권, 제6호, 2017.
[12] J. L. Roux와 E. Vincent, "오디오 소스 분리를 위한 일관된 와이너 필터링," IEEE 신호 처리 레터, 제20권, 제3호, 2013.
[13] F. G. Germain, G. J. Mysore, 그리고 T. Fujioka, "실제 환경에서의 음성 녹음의 평준화 일치," IEEE 국제 음향, 음성, 신호 처리 학회 (ICASSP), 2016.
[14] T. Gerkmann, M. Krawczyk-Becker, 그리고 J. L. Roux, "단일 채널 음성 개선을 위한 위상 처리: 역사와 최근의 발전," IEEE 신호 처리 매거진, 제32권, 제2호, 2015.
[15] Y. Wang와 D. Wang, "시간 영역 신호 재구성을 위한 심층 신경망," IEEE 국제 음향, 음성, 신호 처리 학회 (ICASSP), 2015.
[16] H.Erdogan, J.R.Hershey, S.Watanabe, 그리고 J.L.Roux, "심층 순환 신경망을 사용한 위상 민감 및 인식 강화 음성 분리," IEEE 국제 음향, 음성, 신호 처리 학회 (ICASSP), 2015.
[17] D. S. Williamson과 D. Wang, "음성 감쇠 및 노이즈 제거를 위한 복소 도메인에서의 시간-주파수 마스킹," IEEE/ACM 음향, 음성, 언어 처리 트랜잭션, 제25권, 제7호, 2017.
[18] J. A. Moorer, "단기 푸리에 변환을 통한 오디오 처리 구현에 대한 주의사항," IEEE 음향 및 음향학에 신호 처리 응용 워크샵 (WASPAA), 2017.
[19] S.-W. Fu, Y. Tsao, X. Lu, 그리고 H. Kawai, "완전 합성 신경망을 통한 원시 파형 기반 음성 개선," arXiv:1703.02205, 2017.
[20] D. Rethage, J. Pons, 그리고 X. Serra, "음성 노이즈 제거를 위한 WaveNet," IEEE 국제 음향, 음성, 신호 처리 학회 (ICASSP), 2018.
[21] S. Pascual, A. Bonafonte, 그리고 J. Serr` a, "SEGAN: 음성 개선 생성적 적대 신경망," Interspeech, 2017.
[22] K. Qian, Y. Zhang, S. Chang, X. Yang, D. Florencio, 그리고 M. Hasegawa-Johnson, "베이지안 WaveNet을 사용한 음성 개선," Interspeech, 2017.
[23] J. Johnson, A. Alahi, 그리고 L. Fei-Fei, "실시간 스타일 전이 및 초해상도를 위한 지각적 손실," European Conference on Computer Vision (ECCV), 2016.
[24] Q. Chen와 V. Koltun, "단계별 개선 네트워크를 사용한 사진 이미지 합성," International Conference on Computer Vision (ICCV), 2017.
[25] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, 그리고 O. Wang, "깊은 특징의 비합리적인 효과적인 인지적 측정으로서의 역할," Computer Vision and Pattern Recognition (CVPR), 2018.

[26] F. Yu와 V. Koltun, "Dilated convolutions에 의한 다중 스케일 컨텍스트 집계," 국제 학습 표현 대회 (ICLR), 2016.
[27] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. W. Senior, 및 K. Kavukcuoglu, "WaveNet: 원시 오디오를 위한 생성 모델," arXiv:1609.03499, 2016.
[28] A. L. Maas, A. Y. Hannun, 및 A. Y. Ng, "렉티파이어 비선형성은 신경망 음향 모델을 개선시킵니다," 딥러닝을 위한 ICML 워크샵, 2013.
[29] Q. Chen, J. Xu, 및 V. Koltun, "완전 합성곱 네트워크를 사용한 빠른 이미지 처리," 국제 컴퓨터 비전 대회 (ICCV), 2017.
[30] S. Ioffe와 C. Szegedy, "배치 정규화: 내부 공변량 변화를 줄여 깊은 네트워크 훈련 가속화," 기계 학습 국제 학회 (ICML), 2015.
[31] K. Simonyan과 A. Zisserman, "대규모 이미지 인식을 위한 매우 깊은 합성곱 네트워크," 국제 학습 표현 대회 (ICLR), 2015.
[32] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, 및 F. Li, "ImageNet 대규모 시각 인식 챌린지," 국제 컴퓨터 비전 저널 (IJCV), vol. 115, no. 3, 2015.
[33] A. Mesaros, T. Heittola, E. Benetos, P. Foster, M. Lagrange, T. Virtanen, 및 M. D. Plumbley, "음향 장면 및 이벤트의 감지 및 분류: DCASE 2016 챌린지 결과," IEEE/ACM 음성 및 언어 처리 트랜잭션, vol. 26, no. 2, 2018.
[34] A. Mesaros, T. Heittola, 및 T. Virtanen, "음향 장면 분류 및 사운드 이벤트 감지를 위한 TUT 데이터베이스," 유럽 신호 처리 컨퍼런스 (EUSIPCO), 2016.
[35] P. Foster, S. Sigtia, S. Krstulovic, J. Barker, 및 M. D. Plumbley, "CHiMe-Home: 가정 환경에서 소리 원천 인식을 위한 데이터셋," IEEE 신호 처리 및 음향 응용 워크샵 (WASPAA), 2015.
[36] X. Glorot와 Y. Bengio, "깊은 피드포워드 신경망 훈련의 어려움 이해," 인공 지능 및 통계 국제 학회 (AISTATS), 2010.
[37] D. P. Kingma와 J. Ba, "Adam: 확률적 최적화를 위한 방법," 국제 학습 표현 대회 (ICLR), 2015.
[38] C. Valentini-Botinhao, X. Wang, S. Takaki, 및 J. Yamagishi, "잡음 내성 텍스트 음성을 위한 RNN 기반 음성 개선 방법 조사," ISCA 음성 합성 워크샵, 2016.
[39] Y. Hu와 P. C. Loizou, "음성 개선 알고리즘의 주관적 비교," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2006.
[40] ITU-T, "잡음 억제 알고리즘을 포함하는 음성 통신 시스템을 평가하기 위한 주관적 테스트 방법론," ITU-T 권고안 P.835, 기술 보고서, 2003.
[41] S. R. Quackenbush, T. P. Barnwell, 및 M. A. Clements, "음성 품질의 객관적 측정," Prentice Hall, 1988.

부록

이 부록은 제2절에서 제시된 노이즈 제거 및 특징 손실 네트워크 구조에 대한 추가적인 세부 정보를 제공합니다.

노이즈 제거 네트워크

의 차원은 N × 1인 1차원 텐서입니다. N의 샘플 수는 미리 주어지지 않습니다. 각 중간 레이어 Λk

∈
{Λ1,...,Λ15
}
는 차원이 N × W인 2차원 텐서입니다. 여기서 W는 각 레이어의 너비(즉, 특징 맵의 개수)입니다. k = 1,...,14에 대해 각 중간 레이어 Λk의 내용은 이전 레이어 Λk-1을 통해 계산됩니다.

j is the j-th feature map of layer Λk-1, Ψ is the activation function, Γk is the convolutional kernel, Xj is the input feature map, rk is the receptive field, and Kk i,j is the convolutional weight.

j
j번째
feature map은 Λk −1, Kk 레이어의
입니다.

i, j는 학습된 3 × 1 합성곱 커널입니다. Γk는 적응적 정규화 연산자이고, Ψ는 점별 비선형성입니다. 적응적 정규화의 존재로 인해 이러한 레이어에는 편향 항이 사용되지 않습니다. 연산자 ∗r은 확장된 합성곱입니다. [26] 즉,

(Λj ∗r Ki,j)[n] = (Λj ∗r Ki,j)[n]

+1

X m = −1Ki,j[m]Λj[n − rm]. (3)
k-번째 레이어의 확장 계수는 rk = 2k −1로 설정되어 있습니다.
k ∈ {1,...13}에 대해서만 적용됩니다. 레이어 Λ13과 Λ14 사이에서는 확장을 사용하지 않습니다 (즉, r14 = 1). 출력 레이어 Λ15에서는 선형 변환 (비선형성이 없는 1 × 1 컨볼루션)을 사용하여 출력 신호의 샘플을 합성합니다.

Λ15 = XjΛ14j×K14j+ b, (4)

b는 학습된 편향 항입니다. 네트워크의 수용 영역은 214 + 1 = 16385 샘플입니다.
b) 비선형 유닛: 점별 비선형성 Ψ에는 leaky rectified linear unit (LReLU) [28]을 사용합니다.

Ψ(x) = max(δx,x) with δ = 0.2. (5)
Ψ(x) = max(δx,x)이고, 여기서 δ는 0.2입니다. (5)

c) 적응 정규화: Γk는 섹션 II-A에서 설명된 적응 정규화 작업에 해당합니다. k ∈ {1,...13}에 대해, 연산자는 배치 정규화와 항등 매핑을 적응적으로 결합합니다.

Γk(x) = αkx + βkBN(x), (6)
Γk(x) = αkx + βkBN(x), (6)

a) αk, βk ∈ R로 학습된 스칼라 가중치이고, BN은 배치 정규화 연산자입니다 [30].
d) 제로 패딩: 우리의 알고리즘은 각 레이어에서 제로 패딩을 사용하여 각 레이어 텐서의 "유효" 길이를 일정하게 유지하고 N과 동일하게 만듭니다.
e) 훈련 손실: 네트워크는 우리의 딥 피처 손실을 사용하여 역전파를 통해 훈련됩니다. 이는 섹션 II-B에서 설명한 대로 이루어집니다 (특히 Equation 1을 참조하십시오). 피처 손실 분류 네트워크는 다음 섹션에서 자세히 설명됩니다.

B. 특징 손실 네트워크

의 특징적인 레이어 구조: II-B 섹션에서 언급한 대로, 이 네트워크는 컴퓨터 비전에서 영감을 받은 VGG 아키텍처에 기반합니다. 우리는 이를 Φ0,...,Φ14로 표기하는 15개의 (연속적인) 레이어로 나타냅니다. 첫 번째 레이어인 Φ0은 N × 1 차원의 1차원 텐서이며 입력 신호에 해당합니다. 샘플의 수 N은 미리 주어지지 않습니다. 각 중간 레이어 Φm은

∈
{Φ1,...,Φ14
}
는 차원 N의 2차원 텐서입니다.

2m
×Wm, Wm은 각 레이어의 너비이며, Wm = 32 ×2bm−1 5 c로 설정됩니다 (즉, 특징의 수는 매 5개 레이어마다 두 배로 증가합니다). 각 중간 레이어 Φm의 내용은 이전 레이어 Φm −1을 통해 다음과 같은 연산으로 계산됩니다.

i
는 이전 레이어 Φm의 i번째 특징 맵입니다.

j번째 특징 맵은 레이어 Φm −1, Lm의 j번째 특징 맵입니다. i,j는 학습된 3×1 합성곱 커널입니다. BN은 배치 정규화 연산자이고, Ψ는 식 5와 동일한 점별 선형성입니다. 배치 정규화가 존재하기 때문에 이러한 레이어에는 편향 항이 사용되지 않습니다. 이후에는 감소 연산이 이어집니다.

Φm[i] = ˜ Φm[i][2n]

다음 레이어의 길이는 이전 레이어의 절반입니다. 네트워크의 수용 영역은 215-1 = 32767 샘플입니다. 각 레이어마다 필요에 따라 네트워크는 제로 패딩됩니다. 이로 인해 ˜ Φm과 Φm-1은 동일한 "유효" 길이를 가지게 됩니다.
b) 분류 레이어: 우리는 관심 있는 p번째 분류 작업을 수행하기 위해 마지막 특징 레이어 ˜ Φ14의 각 채널을 평균 풀링하여 차원이 1 × W14인 출력 특징 벡터 Φ15,p를 얻습니다. 이 벡터는 선형 레이어에 공급되어 차원이 1 × Cp인 로짓 벡터 Φ15,p를 형성합니다 (여기서 Cp는 p번째 작업과 관련된 클래스의 수입니다).

Φ16, p, 나는 같은 의미입니다.

Xj
Φ15,p
j
×L16,p
i,j
+˜bp i,   (9)

Xj
Φ15,p
j
×L16,p
i,j
+˜bp i,   (9)

어디에 L16,p
i,j
는 학습된 스칼라 가중치이고 ˜bp는

나는 학습된 편향 용어입니다. 우리는 마침내 네트워크의 출력 분류 벡터 Φ17,p를 이 작업을 통해 얻습니다.

Φ17,p = ∆(Φ16,p),   (10) 

Φ17,p = ∆(Φ16,p),   (10)

∆는 p번째 작업에 대한 로지스틱 비선형성을 나타내는 것이다 (즉, 각 오디오 파일에 대해 고유한 레이블을 요구하는 작업의 경우 벡터 소프트맥스 비선형성, 각 오디오 파일에 대해 임의의 수의 레이블을 허용하는 작업의 경우 점별 시그모이드). Φ17,p는 1 × Cp 차원이며 그 요소는 [0,1] 범위에 있다.
c) 훈련 손실: 훈련은 현재 파일에 연관된 벡터 Φ17,p와 해당하는 실제 분류 벡터 (즉, 파일과 연관된 c번째 분류 레이블이면 c번째 요소가 1이고 그렇지 않으면 0인 1 × Cp 차원의 벡터) 사이의 교차 엔트로피 손실을 사용하여 역전파를 통해 수행된다.

