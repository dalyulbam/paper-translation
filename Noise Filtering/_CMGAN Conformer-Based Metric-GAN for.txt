1. 안녕하세요, 저는 한국어를 배우고 있습니다.

CMGAN: 콘포머 기반 메트릭-GAN을 사용한 단일 음성 개선

셰리프 압둘라티프, 루이즈 카오, 그리고 빈 양 IEEE 고급 회원

요약 - 합성곱 증강 트랜스포머 (Conformers)는 최근에 자동 음성 인식 (ASR) 및 음성 분리와 같은 다양한 음성 도메인 응용 프로그램에서 제안되었습니다. 이들은 로컬 및 글로벌 종속성을 모두 포착할 수 있기 때문입니다. 본 논문에서는 시간-주파수 (TF) 도메인에서 음성 개선 (SE)을 위한 conformer 기반의 측정 생성적 적대 신경망 (CMGAN)을 제안합니다. 생성자는 시간 및 주파수 종속성을 모델링하기 위해 두 단계의 conformer 블록을 사용하여 크기와 복소 스펙트로그램 정보를 인코딩합니다. 디코더는 추정을 크기 마스크 디코더 분기와 복소 정제 분기로 분리하여 원치 않는 왜곡을 걸러내고 크기 추정을 더욱 개선하고 위상 정보를 암묵적으로 향상시킵니다. 또한, 측정 판별자를 포함하여 생성자를 해당 평가 점수에 대해 최적화하여 측정 불일치를 완화합니다. 객관적 및 주관적 평가 결과, CMGAN은 세 가지 음성 개선 작업 (소음 제거, 반향 제거 및 초해상도)에서 최첨단 방법과 비교하여 우수한 성능을 보여줄 수 있습니다. 예를 들어, Voice Bank+DEMAND 데이터 세트에서의 정량적인 소음 제거 분석은 CMGAN이 여러 이전 모델보다 우수한 성능을 보여준다는 것을 나타냅니다. PESQ는 3.41이고 SSNR은 11.10 dB입니다.

색인 용어 - 음성 개선, 딥 러닝, 어텐션 모델, 생성적 적대 신경망, 측정 판별자.

I. 소개
실제 음성 응용 프로그램에서 인식된 음성 품질과 이해도는 음성 개선(SE) 시스템의 성능에 의존합니다. 예를 들어, 음성 소음 제거, 반향 제거 및 음향 에코 캔슬링 등이 있습니다. 따라서 SE 프레임워크는 현대의 자동 음성 인식(ASR), 통신 시스템 및 보청기 장치에서 필수적인 구성 요소입니다 [2]–[4]. 이는 현재 SE 시스템의 성능 경계를 끊임없이 미는 연구의 양이 점점 더 많아지는 것으로 나타납니다 [5], [6]. 이러한 접근 방식의 대부분은 최근의 심층 학습(DL) 기술의 발전과 점점 더 많이 사용 가능한 음성 데이터셋을 활용합니다 [7]–[10].
SE 기술은 대략적으로 두 가지 주요 접근 방식으로 분류될 수 있습니다. 연대적으로, 음성 시간-주파수(TF) 표현(스펙트로그램)을 향상시키는 것은 대부분의 모델 기반 및 최근의 DL 접근 방식을 포함하는 고전적인 SE 패러다임을 구성합니다 [5], [11]–[13]. 최근에는 변환 오버헤드 없이 원시 음성 시간 영역 웨이브폼을 직접 향상시키기 위한 새로운 접근 방식이 소개되었습니다 [14]–[18].
각 패러다임은 독특한 장점과 단점을 가지고 있습니다. 시간 영역 패러다임은 TF 도메인 변환 또는 재구성 요구 사항 없이 왜곡된 상대의 깨끗한 웨이브폼 조각을 직접 추정하는 생성 모델에 기반합니다 [15], [16]. 그러나 직접적인 주파수 표현의 부재로 인해 이러한 프레임워크는 주파수 도메인에서의 음성 음운을 포착하는 데 어려움을 겪습니다. 이 제한은 주로 아티팩트로 나타납니다. 저자들은 슈투트가르트 대학교 신호 처리 및 시스템 이론 연구소에 소속되어 있습니다 (이메일: sherif.abdulatif@iss.uni-stuttgart.de; ruizhe.cao96@gmail.com; bin.yang@iss.uni-stuttgart.de).
더 짧은 버전은 https://arxiv.org/abs/2203.15149 [1]에서 확인할 수 있습니다.

재구성된 연설. 이 패러다임의 또 다른 단점은 원시 파형과 관련된 넓은 입력 공간으로, 이는 종종 깊은 계산적으로 복잡한 프레임워크의 활용을 필요로 한다 [14], [17].
TF 도메인에서 대부분의 전통적인 모델 기반 또는 DL 기술은 크기 구성 요소를 활용하면서 위상을 무시한다. 이는 사용된 아키텍처에 도전을 주는 비구조적인 위상 구성 요소로 설명된다 [19], [20]. 이 도전을 극복하기 위해, 몇 가지 접근 방식은 복소 스펙트로그램 (실수 및 허수 부분)을 향상시키는 전략을 따른다. 이는 크기와 위상을 모두 암묵적으로 향상시킨다 [21], [22]. 그러나 크기와 위상 간의 보상 효과는 종종 부정확한 크기 추정으로 이어진다 [23]. 이 문제는 섹션 II-A에서 자세히 논의될 것이다. 최근 연구에서는 크기를 향상시킨 다음 복소 스펙트로그램을 개선하는 방식을 제안하여 이 보상 문제를 효과적으로 완화할 수 있다 [13], [24]. 또한, SE에서 일반적으로 사용되는 목적 함수는 추정된 스펙트로그램과 대상 스펙트로그램 사이의 Lp-노름 거리에 단순히 기초한다. 그러나 더 낮은 거리가 항상 더 높은 음성 품질로 이어지지는 않는다. MetricGAN은 판별자가 학습할 수 있는 평가 지표 점수에 대해 생성자를 최적화하여 이 문제를 극복하기 위해 제안되었다 [11].
또한, 많은 접근 방식에서는 웨이브폼이나 스펙트로그램에서 장기 의존성을 포착하기 위해 변환기 [25]를 활용한다 [13], [16], [26]. 최근에는 로컬 컨텍스트와 글로벌 컨텍스트를 모두 포착할 수 있는 컨포머가 ASR 및 음성 분리 작업에서 변환기의 대안으로 소개되었다 [27], [28]. 따라서 시간 도메인 SE에도 사용되었다 [18]. 우리의 지식으로는 컨포머는 아직 TF 도메인 SE에 대해 명시적으로 조사되지 않았다.
언급된 문제와 이전 연구들에 영감을 받아, 우리는 다양한 단일 음성 향상 작업을 위한 첫 번째 컨포머 기반 MetricGAN (CMGAN)을 제안한다. CMGAN은 생성자와 메트릭 판별자로 구성된다. 생성자는 TF 도메인의 두 단계 컨포머 블록을 기반으로 하며, 판별자는 미분 불가능한 메트릭을 추정하는 역할을 한다. 결합된 크기, 실수 및 허수 구성 요소는 생성자로 전달되며, 이는 인코더, 마스크 디코더 및 복소 디코더로 구성된 두 단계 컨포머 블록을 포함한다. 인코더는 입력의 간결한 특징 표현을 학습하기 위해 사용된다. 마스크 디코더는 입력 크기에 대한 마스크를 추정하고, 복소 디코더는 실수 및 허수 부분을 개선한다.
컨포머의 중요한 계산 복잡성을 줄이기 위해, 우리는 두 단계 컨포머 블록에 이중 경로 변환기 [16], [26], [29]를 채택하여 시간 차원과 주파수 차원을 순차적으로 포착할 수 있다. 요약하면, 이 연구의 기여는 다음과 같이 요약된다:

• 우리는 두 단계 형태의 블록의 성능을 조사하고, 그들이 상대적으로 낮은 계산 복잡성으로 시간 및 주파수 의존성을 포착하는 능력을 확인합니다.

0.5 1 1.5 2 2.5 3 3.5 4
02468
시간 [초]
주파수 [k
H z ]

(a) 참고 트랙

0.5 1 1.5 2 2.5 3 3.5 4
02468
시간 [초]
주파수 [k
H z ]

(b) 시끄러운 트랙 (SNR 0 dB)

0.5 1 1.5 2 2.5 3 3.5 4
02468
시간 [초]
주파수 [k
H z ]

(c) 리버브 트랙 (τ = 0.5초)

0.5 1 1.5 2 2.5 3 3.5 4
02468
시간 [초]
주파수 [ k
H z ]

저해상도 트랙 (s = 4)

그림 1: 다른 SE 작업에 대한 왜곡된 음성의 TF-크기 표현, 즉, 노이즈 제거, 반향 제거 및 대역폭 확장 (슈퍼-해상도). 변수 τ는 60 dB의 반향 시간을 나타내며, s는 대역폭 스케일 업 비율입니다.

• 우리는 네트워크에 메트릭 판별자를 도입하여, 다른 메트릭에 부정적인 영향을 미치지 않으면서 해당 평가 메트릭을 개선하는 데 도움이 됩니다.

제안된 모델은 다양한 SE 작업에 대해 테스트되었습니다: 음성 노이즈 제거, 에코 제거 및 대역폭 확장 (고해상도)과 관련된 데이터셋을 사용하여 모델이 최첨단 접근법보다 우수한 성능을 보였습니다.

• 우리의 디자인 선택의 효과를 확인하기 위해 포괄적인 소작용 연구가 진행되었습니다.

II. 문제 명세 및 관련 문헌
본 논문에서는 제안된 CMGAN이 다양한 SE 작업, 즉 음성 노이즈 제거, 반향 제거 및 초해상도에 대해 평가될 것입니다. 따라서, 어떤 음향 환경에서도 위에서 언급한 SE 작업은 다음과 같이 모델링될 수 있습니다.

y(t) = x(t) ∗ h(t) + n(t) (1)

y(t)이 왜곡된 음성이고, x(t)가 필요한 깨끗한 음성이며, n(t)가 배경 소음이며, '*'는 필터 h(t)와의 합성 연산을 나타냅니다. 그러나 공간 제약으로 인해 본 연구는 각 작업을 개별적으로 평가하고, 상호 작용 효과는 고려하지 않을 것입니다. 이는 그림 1에 나와 있습니다. 따라서, 추가된 배경 소음 n(t)을 제거하기 위해서만 고려될 것입니다 (그림 1b). 반향 제거를 위해서 (그림 1c), 필터 h(t)는 방 반향 응답 (RIR) 필터를 나타낼 것입니다. 마지막으로, h(t)는 초고해상도 작업에서 낮은 샘플링 주파수의 영향을 모방하기 위해 저역 통과 필터 (LPF)로 작동할 것입니다 (그림 1d). 각 작업에 대한 관련 문헌은 다음 소절에서 제시될 것입니다.

소음 제거

음성 노이즈 제거는 소스 분리 문제로 간주되며, 목표는 배경 노이즈 n(t)를 억제하고 최대한의 품질과 이해 가능성을 가진 원하는 음성 ˆ x(t)를 예측하는 것입니다. 따라서, 이 문제의 어려움은 원하는 음성과 배경 노이즈의 특성에 크게 의존할 것입니다. 예를 들어, 음성 신호는 매우 비정상적입니다. 노이즈 구성 요소에 대해서는 정상적인 시나리오 (예 : 컴퓨터 팬 소음 및 에어컨)와 비정상적인 시나리오 (예 : 떠들기 및 거리 소음)로 나눌 수 있습니다. 일반적으로, 후자의 시나리오가 더 어렵습니다. 이 경우에는 노이즈가 원하는 음성과 유사한 주파수 대역을 차지하기 때문입니다 [19].
음성 노이즈 제거 문헌에서는 문제의 비정상적인 성질로 인해, 시간에 따라 변하는 주파수 특성을 반영하기 위해 TF 표현을 탐색하는 것이 일반적인 접근 방식입니다 [5], [30], [31]. TF 도메인 노이즈 제거에서 발생하는 유일한 제한은 구조화되지 않은 위상 표현입니다. 그러나 오랜 시간 동안 위상은 노이즈에 둔감하다고 여겨졌습니다 [32]. 결과적으로, 연구는 대부분 크기 노이즈 제거에 초점을 맞추었습니다.

소음이 있는 상태를 유지하면서 [6]. 최근에는 많은 연구들이 소음 제거된 음성의 품질에 위상의 중요성을 지적하고 있다 [21], [33]. 이를 위해, TF 음성 소음 제거는 매핑 기반 및 마스킹 기반 방법으로 분류될 수 있다. 매핑 기반 방법의 경우, 비선형 함수를 사용하여 소음이 있는 음성을 해당하는 소음 제거된 음성으로 매핑하는 것이다. 이러한 방법들은 처음에 시간 영역 음성 소음 제거에서 사용되었다 [15], [34]–[36]. 예를 들어, SEGAN [14]은 소음이 있는 웨이브폼을 해당하는 소음 제거된 음성으로 매핑하기 위한 적대적인 프레임워크로 소개되었다. SEGAN의 변형들은 생성자의 용량을 늘리기 위해 제안되기도 하였고 [37], 또는 양 영역의 손실을 이용하여 양 영역의 이점을 얻기 위해 추가적인 TF 영역 손실을 사용하기도 하였다 [38]. 이러한 시도들을 바탕으로, 다양한 매핑 기반 적대적인 프레임워크가 TF 영역 음성 소음 제거에서도 조사되었으며, 더욱 유망한 결과를 얻었다 [19], [39]–[41].

반면, 마스킹 기반 방법은 대부분 TF 영역에서 사용되며, 시간 영역 음성 소음 제거에서는 몇 가지 시도만 있었다 [42]. TF 영역 마스킹 기반 방법은 두 신호가 서로 겹치지 않는다고 가정하는 W-분리 직교성을 가지고 작동한다 [43]. 따라서, 각 TF 유닛에서 활성 소스를 결정하여 신호를 분리할 수 있다. 청각 마스킹 현상과 청각 장면 분석에서의 배타적 할당 원칙에 영감을 받아 이상적인 이진 마스킹 (IBM)은 지도 학습 음성 소음 제거에서 처음으로 사용되는 마스킹 기반 방법이다 [45]. IBM에서는 TF 유닛의 신호 대 잡음 비율 (SNR)이 미리 정의된 임계값 (필요한 음성)을 초과하면 해당 유닛에 1의 값을 할당하고, 그렇지 않으면 0의 값을 할당하여 마스크를 생성한다 (억제할 잡음). 다시 말해, IBM은 이진 분류 문제로 처리될 수 있다 [46], [47]. IBM은 음성의 명료성을 상당히 향상시킬 수 있지만, 음악적인 잡음 왜곡을 도입하여 음성 품질을 저하시킬 수 있다 [48]. 이상적인 비율 마스킹 (IRM)은 이를 해결하기 위해 도입되었으며, 각 TF 유닛은 해당 신호 및 잡음 파워에 따라 0과 1 사이의 값을 가질 수 있는 부드러운 버전으로 볼 수 있다 [49], [50]. 스펙트럼 크기 마스크 (SMM)는 IRM의 무제한 변형으로 간주된다 [51].

앞서 언급한 마스킹 기반 방법들은 크기를 향상시키고 소음이 있는 위상을 변경하지 않는다. 따라서, 위상을 다루는 것은 위상 재구성 및 위상 소음 제거 접근법으로 나뉜다. 위상 재구성을 위해, 심층 신경망 (DNN)은 크기를 추정하기 위해 훈련되고, 이는 반복적인 위상 재구성 (IPR)에 사용된다 [52]–[55]. 위상 소음 제거에 대해서는, [56]의 저자들이 SMM의 변형인 위상 감도 마스크 (PSM)를 처음으로 소개하였으며, 음성 품질의 상당한 향상을 주장하였다. IRM을 기반으로 한 복소 이상적인 비율 마스킹 (cIRM) 접근법이 제안되었으며, 이는 0과 1 사이의 값을 가질 수 있는 각 TF 유닛에 대해 복소수 형태로 처리된다 [49], [50].

실수부와 허수부에 작용하여 크기와 위상을 암묵적으로 처리합니다 [21]. 그러나 실수부와 허수부는 반드시 양수일 필요는 없으므로, 저자들은 tanh 활성화 함수를 사용하여 cIRM을 -1과 1 사이의 값으로 압축합니다. cIRM의 개념은 심층 복소값 순환 신경망(DCCRN)과 새로운 손실 함수를 통해 관련된 마스크를 추정하는 방식으로 확장됩니다 [57].
이러한 접근 방식의 주요 단점은 [23]에서 논의된 크기와 위상 보상 효과입니다. 이 경우, 복소 표현을 복소 손실만 사용하여 노이즈 제거하는 것은 실수부와 허수부를 벌점으로 적용하는 것을 의미하며, 이는 훈련된 모델에 크기와 위상을 추정하는 일정한 자유도를 암묵적으로 제공합니다. 위상은 구조화되지 않으며 항상 추정이 어려우므로, 이는 도전적인 위상을 보상하기 위해 부정확한 크기 추정 결과를 초래할 수 있습니다. 이 문제는 복소 손실과 크기 손실을 모두 포함하거나 복소 정제 접근 방식을 통해 완화될 수 있습니다. 복소 정제 접근 방식은 기본적으로 크기에 대한 제한된 마스크를 추정한 후, 노이즈 제거된 복소 표현에서 크기를 더 개선하고 위상을 추정하는 복소 정제 분기로 문제를 분리합니다 [13], [24], [58]–[60]. 그러나 최근 연구에서는 복소 스펙트로그램 추정을 위해 기존의 마스킹 기반 접근 방식보다 매핑 기반 방법을 권장하고 있으므로, 복소 정제 분기는 매핑 기반 접근 방식을 따를 것입니다. 이런 의미에서 모델은 마스킹 기반 및 매핑 기반 방법의 단편적인 이점을 결합할 수 있습니다.

B. 소리 반향 제거

폐쇄된 음향 환경에서는 소리가 세 가지 구성 요소의 중첩으로 인식됩니다: 직접 경로, 초기 반사 및 지연된 반향. 이는 식 1의 컨볼루션 RIR 필터 h(t)로 모델링될 수 있습니다. 따라서 음성 감쇠제는 주로 원하지 않는 반사를 억제하고 추정된 원하는 음성 ˆ x(t)를 나타내는 직접 경로를 유지하는 데 초점을 맞출 것입니다. 초기 반사는 특정 방향에서 오기 때문에 마이크로폰에는 일반적으로 짧은 시간(50ms)에 도달하며, 직접 경로의 감쇠된 복사본으로 처리될 수 있습니다. 반면, 지연된 반향은 다른 방향에서 지연 및 감쇠된 중첩 신호를 나타내므로 더 늦게 도착합니다. 감쇠제 문제의 어려움은 다양한 요소로 설명됩니다. 예를 들어, 방의 크기와 표면 특성은 주로 반사의 양과 감쇠 정도에 기여합니다. 또한, 마이크로폰과 스피커 사이의 거리는 반사 강도에 영향을 미칠 것입니다. 즉, 거리가 길수록 반사가 강해집니다.
우리의 지식으로는, 감쇠제 문제는 주로 시간-주파수 영역에서 다루어지며, 시간 영역에서는 제한된 시도만 있습니다. 이는 시간 영역 모델이 반향 조건에서 심한 시간 왜곡에 취약하기 때문입니다. 소음 제거와 마찬가지로, TF-도메인 마스킹 기반 방법은 감쇠제에도 확장되었습니다. 예를 들어, [67]에서는 직접 경로와 초기 반사를 원하는 음성으로 간주하고 IBM을 사용하여 지연된 반향을 억제합니다. 소음 제거와 달리, 각 TF 단위에 0과 1을 할당하기 위한 SNR 기준은 [68]에서 음성 존재 확률을 처리하기 위해 수정되었습니다. 그러나 IBM은 원래 무반향 조건에서 첨가 잡음에 대해 정의되었습니다. 반향에서는 결과적인 TF 표현에서 음성의 시간적 흐림이 관찰됩니다(그림 1c 참조). 따라서 경계가 있는 IBM은 성능 저하를 일으킬 수 있습니다.

결과적인 음질 [69]과 부드러운 IRM은 일반적으로 이 경우에 선호되는 방법이다 [51], [70]–[72]. 소음 제거 경로를 따라, IRM은 위상을 포함한 cIRM으로 확장되어 감청 제거 과정에 포함된다 [73]–[75].
또한, 매핑 기반 방법도 음성 감청 제거에서 조사되고 있다. 예를 들어, Han et al. [52]은 간단한 완전 연결 네트워크를 사용하여 음향 감청 제거에 스펙트럼 매핑을 조사한 첫 번째 연구 중 하나이다. 이후, [76]의 저자들은 중간 스킵 연결을 사용한 완전 합성곱 U-Net (인코더-디코더) 아키텍처를 이 작업에 적용했다. SkipConvNet은 각 스킵 연결을 여러 합성곱 모듈로 대체하여 디코더에 직관적인 특징 맵을 제공하는 U-Net 아키텍처를 변경했다 [77]. 또한, [78]에서는 넓은 잔여 네트워크가 STFT의 크기, Mel 필터뱅크 및 켑스트럼이라는 다른 음성 표현을 처리하기 위해 도입되었다. 일부 접근 방식은 DNN을 지연-합 빔포밍 및 스펙트럼 차감에 의한 지연 감쇠 제거와 결합하여 상당한 성능 향상을 제공할 수 있다 [79].

C. 초고해상도

슈퍼 해상도 문제는 이전 SE 사용 사례와 약간 다릅니다. 노이즈 제거와 에코 제거에서는 원하는 음성이 불필요한 노이즈나 반사와 함께 제공되며, 이러한 효과를 억제하면서 음성을 보존하는 작업이 필요합니다. 그러나 슈퍼 해상도는 낮은 샘플링 주파수 입력 신호에서 누락된 샘플을 재구성하는 작업입니다. 따라서이 문제는 입력 도메인을 기반으로 두 가지 다른 관점에서 정의 될 수 있습니다. 시간 도메인에서는 문제가 자연 이미지의 슈퍼 해상도와 밀접한 관련이 있습니다. 여기서 작업은 K × 1 샘플의 입력 신호를 M × 1 샘플 (K <M)의 출력 신호로 업샘플링하는 것입니다. 이 경우 DNN은 보간 작업을 위해 훈련 될 수 있습니다. 반면에 TF 도메인의 경우, 작업은 자연 이미지 인페인팅과 유사 할 것입니다. 여기서는 이미지 또는 스펙트로그램의 일부가 누락되고 DNN이 이미지를 완성하거나 누락 된 고주파 대역을 재구성하도록 훈련됩니다. 이전 설명을 기반으로하면 매핑 기반 접근 방식이 슈퍼 해상도에서 유일하게 관련이있는 접근 방식임을 추론 할 수 있습니다.
기존의 오디오 처리에서 슈퍼 해상도는 대역 확장의 이름으로 조사되었습니다. 최근의 DL 기반 오디오 슈퍼 해상도 연구는 전통적인 방법과 비교하여 우수한 성능을 보였습니다. 2017 년에 Kuleshov et al. [83]은 U-Net과 스킵 연결 아키텍처를 사용하여 웨이브폼을 재구성하는 것을 제안했습니다. TFiLM [85] 및 AFiLM [86]은 장기 시간 종속성을 캡처하기 위해 순환 모델과 어텐션 블록을 사용했습니다. 그러나 주파수 구성 요소의 부족으로 인해 성능 향상이 제한됩니다. TFNet [84]은 두 개의 브랜치를 사용하여 시간 및 주파수 도메인을 모두 활용하며, 하나의 브랜치는 스펙트럼 크기의 재구성을 모델링하고 다른 브랜치는 웨이브폼을 모델링합니다. 그러나 주파수 브랜치에서 위상 정보는 무시됩니다. Wang et al. [87]은 시간 도메인 수정된 오토인코더 (AE)와 크로스 도메인 손실 함수를 제안하여 하이브리드 프레임 워크를 최적화합니다. 최근에는 [88]에서 슈퍼 해상도 작업을 위한 신경 보코더 기반 프레임 워크 (NVSR)가 제안되었습니다. 위의 연구들은 유망한 결과를 보여주지만, 많은 연구들이 시간 도메인이나 하이브리드 시간 도메인 및 TF 도메인 크기 표현에 초점을 맞추고 있습니다. 그러나 복잡한 TF 도메인 슈퍼 해상도에 대한 연구는 아직 다루지 않았습니다.

(a) 인코더-디코더 생성자 아키텍처

두 단계 형태 (TS-Conformer)

(c) 미터법 구별자

그림 2: 제안된 CMGAN 아키텍처 개요

III. 방법론
A. 생성기 아키텍처

CMGAN의 생성자 아키텍처 개요는 그림 2a에 나와 있습니다. 왜곡된 음성 파형 y에 대해

∈
RL×1,
STFT 연산은 먼저 웨이브폼을 복소 스펙트로그램 Yo
∈
RT×F×2로 변환합니다. 여기서 T와 F는 각각 시간과 주파수 차원을 나타냅니다. 그런 다음, 스펙트로그램 Y는 거듭제곱 압축을 통해 얻어집니다.

Y = |Yo|cejYp = YmejYp = Yr + jYi (2)
여기서 Ym, Yp, Yr 및 Yi는 압축된 스펙트로그램의 크기, 위상, 실수 및 허수 성분을 나타냅니다. c는 0에서 1까지 범위하는 압축 지수이며, 여기서는 Braun et al. [89]의 방법을 따라 c = 0.3으로 설정합니다.
크기의 거듭제곱 압축은 큰 소리에 비해 조용한 소리의 중요성을 동등하게 만들어 인간의 소리 지각에 더 가까워집니다 [90], [91]. 실수 및 허수 부분인 Yr 및 Yi는 생성기의 입력으로 크기인 Ym과 연결됩니다.
1) 인코더: 입력 특징 Yin이 주어졌을 때

∈
RB×T×F×3,
여기서 B는 배치 크기를 나타내며, 인코더는 두 개의 컨볼루션 블록과 그 사이에 확장된 DenseNet [92]으로 구성됩니다. 각 컨볼루션 블록은 컨볼루션 레이어, 인스턴스 정규화 [93] 및 PReLU 활성화 [94]로 구성됩니다. 첫 번째 컨볼루션 블록은 세 개의 입력 특성을 C 채널을 가진 중간 특성 맵으로 확장하는 데 사용됩니다. 확장된 DenseNet은 밀집 잔여 연결을 가진 네 개의 컨볼루션 블록을 포함하며, 각 블록의 확장 요소는 {1, 2, 4, 8}로 설정됩니다. 밀집 연결은 이전의 모든 특성 맵을 집계하여 다른 특성 수준을 추출할 수 있습니다. 확장된 컨볼루션은 수용 영역을 증가시키는 데 사용됩니다.

효과적으로 커널과 레이어 수를 보존하면서 작동합니다. 마지막 합성곱 블록은 주파수 차원을 F/2로 줄여 복잡성을 감소시킵니다.

2) 두 단계의 콘포머 블록: 콘포머 [27], [28]은 변환기와 합성곱 신경망(CNN)의 장점을 결합하여 음성 인식과 분리에서 큰 성공을 거두었습니다. 변환기는 장거리 종속성을 포착할 수 있으며, CNN은 지역적인 특징을 효과적으로 활용합니다. 여기서는 첫 번째 단계에서 시간 종속성을 포착하고, 두 번째 단계에서 주파수 종속성을 포착하기 위해 두 개의 콘포머 블록을 순차적으로 사용합니다. 그림 2b에서 보여지듯이, 특징 맵 D가 주어졌을 때

∈ RB×T×F(cid:48)×C, 입력 특성 맵 D는 먼저 시간 종속성을 포착하기 위해 DT ∈ RBF(cid:48)×T×C로 재구성됩니다. 그런 다음 출력 DT는

o
입력과 원소별로 더해진다
DT (잔차 연결)와 함께 새로운 특성 맵으로 재구성된다
DF ∈ RBT×F(cid:48)×C. 두 번째 conformer는 주파수 종속성을 포착한다. 잔차 연결 후 최종 출력 Do는 입력 크기로 다시 재구성된다.
[27]와 유사하게 각 conformer 블록은 두 개의 반스텝 피드포워드 신경망 (FFNNs)을 사용한다. 두 개의 FFNNs 사이에는 4개의 헤드를 가진 멀티헤드 셀프 어텐션 (MHSA)이 사용되며, 그 뒤에는 합성곱 모듈이 따른다. 그림 2b에 나타난 합성곱 모듈은 레이어 정규화, 포인트별 합성곱 레이어 및 소멸 그래디언트 문제를 줄이기 위한 게이트 선형 유닛 (GLU) 활성화로 시작한다. GLU의 출력은 스위시 활성화 함수를 가진 1D-깊이별 합성곱 레이어를 거쳐 다른 포인트별 합성곱 레이어로 전달된다. 마지막으로 드롭아웃 레이어가 네트워크를 정규화한다. 또한 잔차 경로가 입력을 출력에 연결된다.

3) 디코더: 디코더는 N개의 이중 단계 콘포머 블록에서 출력을 분리하여 추출합니다. 이는 마스크 디코더와 복소 디코더 두 가지 경로로 이루어져 있습니다. 마스크 디코더는 입력 크기 Ym에 원소별 곱셈을 수행할 마스크를 예측하여 ˆ X(cid:48) m을 예측하는 것을 목표로 합니다. 반면, 복소 디코더는 실수부와 허수부를 직접 예측합니다. 마스크 디코더와 복소 디코더는 모두 인코더와 유사한 확장된 DenseNet으로 구성됩니다. 주파수 차원을 F로 다시 업샘플링하기 위해 두 경로 모두에서 서브픽셀 컨볼루션 레이어가 사용됩니다. 마스크 디코더의 경우, 채널 수를 1로 압축하기 위해 컨볼루션 블록이 사용되며, 이후에는 PReLU 활성화 함수를 가진 또 다른 컨볼루션 레이어가 사용되어 최종 마스크를 예측합니다. PReLU 활성화 함수는 각 주파수 대역마다 다른 기울기를 학습하며, 초기에는 고정된 양수 값(0.2)으로 정의됩니다. 사후 훈련 평가 결과, 모든 기울기가 다른 음수 값을 반영하며, 즉, 출력 마스크는 항상 양수 1사분면과 2사분면에 투영됩니다(그림 3 참조). 복소 디코더의 경우, 아키텍처는 마스크 디코더와 동일하지만, 복소 출력에는 활성화 함수가 적용되지 않습니다. [13], [24]와 마찬가지로, 마스크된 크기 ˆ X(cid:48) m은 먼저 노이즈가 있는 위상 Yp와 결합하여 크기가 향상된 복소 스펙트로그램을 얻습니다. 그런 다음 복소 디코더의 출력 ( ˆ X(cid:48) r, ˆ X(cid:48) i)와 원소별로 합산하여 최종 복소 스펙트로그램을 얻습니다.

ˆ Xr = ˆ X0 mcos(Yp) + ˆ X0 r 
ˆ Xi = ˆ X0 msin(Yp) + ˆ X0 i (3)

그런 다음 복소 스펙트로그램 (ˆ Xr, ˆ Xi)에 거듭제곱 압축을 반전시키고 역 단시간 푸리에 변환 (ISTFT)을 적용하여 시간 영역 신호 ˆ x를 얻습니다. 그림 4a에 표시된 대로. 더 나아가, 디코더 브랜치 양쪽에 대한 크기 손실을 개선하기 위해 ˆ Xm에서 크기 손실을 계산합니다.

Xm = X2r + X2i

(4) 

Please give me a glass of water.
I am going to the store to buy some groceries.
What time is the meeting?
Can you help me with this problem?
I like to listen to music in my free time.

B. 미터법 구별자

SE에서는 목적 함수가 종종 평가 지표와 직접적으로 상관되지 않습니다. 따라서 목적 손실이 최적화되어도 평가 점수가 아직도 만족스럽지 않을 수 있습니다. 또한, PESQ (perceptual evaluation of speech quality) [96] 및 STOI (short-time objective intelligibility) [97]와 같은 일부 평가 지표는 미분 가능하지 않기 때문에 손실 함수로 사용할 수 없습니다. 따라서 CMGAN의 판별자는 메트릭 점수를 모방하고 손실 함수의 일부로 사용하려고 합니다. 여기서는 MetricGAN을 따라 PESQ 점수를 레이블로 사용합니다 [11]. 그림 2c에 나와 있는 것처럼, 판별자는 4개의 컨볼루션 블록으로 구성됩니다. 각 블록은 컨볼루션 레이어로 시작하여 인스턴스 정규화와 PReLU 활성화가 이어집니다. 컨볼루션 이후에는

−1            1
1

-1            1
1

훈련 전

−1            1
1

-1            1
1

훈련 후

그림 3: 결과적인 크기 마스크의 PReLU 기울기.

블록, 전역 평균 풀링 다음에는 두 개의 피드포워드 레이어와 시그모이드 활성화 함수가 따릅니다. 판별자는 두 입력을 깨끗한 크기로 취하여 최대 정규화된 PESQ 점수(= 1)를 추정하도록 훈련됩니다. 추가로, 판별자는 깨끗한 스펙트럼과 향상된 스펙트럼을 함께 입력으로 사용하고 해당하는 PESQ 레이블을 추정하도록 훈련됩니다. 그림 4b에 나와 있습니다. 반면에, 생성자는 깨끗한 음성과 유사한 향상된 음성을 생성하도록 훈련되어 PESQ 레이블 1에 가까워지도록 합니다. 그림 4c에 나와 있습니다.

C. 손실 함수

Braun et al. [89]에 영감을 받아, 우리는 TF 도메인에서 크기 손실 LMag.와 복소 손실 LRI의 선형 조합을 사용합니다:
LTF = α LMag. + (1 − α) LRI LMag. = E Xm, ˆ Xm(cid:2) (cid:107)Xm − ˆ Xm(cid:107)2(cid:3)
LRI = E Xr, ˆ Xr(cid:2) (cid:107)Xr − ˆ Xr(cid:107)2(cid:3) + E Xi, ˆ Xi(cid:2) (cid:107)Xi − ˆ Xi(cid:107)2(cid:3)

1. Can you help me with this?
2. I don't understand what you're saying.
3. Where is the nearest bus stop?
4. How much does this cost?
5. What time does the movie start?

α가 선택된 가중치인 경우. 그리드 서치에 따르면, α = 0.7이 가장 좋은 성능을 보여줍니다. 최소 제곱 GAN과 유사하게 [98], 적대적 훈련은 다음과 같이 표현되는 판별자 손실 LD와 해당 생성자 손실 LGAN을 통해 최소-최소 최적화 작업을 따릅니다:
LGAN = E Xm, ˆ Xm(cid:2) (cid:107)D(Xm, ˆ Xm) − 1 (cid:107)2(cid:3)
LD = E Xm(cid:2) (cid:107)D(Xm,Xm) − 1 (cid:107)2(cid:3)
+ E Xm, ˆ Xm(cid:2) (cid:107)D(Xm, ˆ Xm) − QPESQ(cid:107)2(cid:3)

1. Can you please help me with this?
2. I am sorry for the inconvenience.
3. What time is the meeting?
4. How much does this cost?
5. Where is the nearest hospital?
6. I don't understand.

D는 판별자를 나타내며 QPESQ는 정규화된 PESQ 점수를 나타냅니다. 여기서는 PESQ 점수를 [0,1] 범위로 정규화합니다. 또한, 결과 웨이브폼 LTime에 추가적인 벌칙화가 복원된 음성 품질을 향상시키는 것으로 입증되었습니다 [20]:
LTime = E x,ˆ x(cid:2) (cid:107)x − ˆ x (cid:107)1(cid:3) (7)
여기서 ˆ x는 향상된 웨이브폼이고 x는 깨끗한 목표 웨이브폼입니다. 최종 생성자 손실은 다음과 같이 정의됩니다:

LG = γ1 LTF + γ2 LGAN + γ3 LTime (8)
γ1, γ2, γ3은 각각에 해당하는 손실의 가중치이며, 동등한 중요성을 반영하기 위해 선택됩니다.

IV. 실험
A. 데이터셋

1) 노이즈 제거: 우리는 공개적으로 사용 가능한 Voice Bank+DEMAND 데이터셋 [7]에서 제안한 접근법을 조사합니다. 깨끗한 트랙은 Voice Bank 말뭉치 [99]에서 선택되었으며, 이는 훈련 세트의 28명의 화자로부터 11,572개의 발화와 테스트 세트의 2명의 보이지 않는 화자로부터 872개의 발화를 포함합니다. 훈련 세트에서 깨끗한 발화는 배경 노이즈 (DEMAND 데이터베이스 [100]의 8가지 노이즈 유형과 2가지 인공 노이즈 유형)와 0 dB, 5 dB, 10 dB 및 15 dB의 SNR에서 혼합됩니다. 테스트 세트에서 깨끗한 발화는 DEMAND 데이터베이스의 5가지 보이지 않는 노이즈 유형과 2.5 dB, 7.5 dB, 12.5 dB 및 17.5 dB의 SNR에서 혼합됩니다. 노이즈 유형은 대부분 도전적이며, 예를 들어 공공장소의 소음 (카페테리아, 레스토랑 및 사무실), 가정의 소음 (부엌과 거실) 및 교통/도로 소음 (자동차, 지하철, 버스, 교통 체증, 공공 광장 및 지하철 역)이 포함됩니다. 모든 발화는 실험에서 16 kHz로 재샘플링됩니다.

비적대적 생성자 손실

(b) 판별자 손실

적대적 생성자 손실

그림 4: CMGAN 아키텍처에서 전파된 손실 함수의 설명. 간단하게 설명하기 위해, X와 ˆ X는 깨끗한 대상과 추정된 출력 스펙트로그램의 세 채널 크기와 복소수 표현을 나타냅니다.

2) 디리버베레이션: 우리는 REVERB 챌린지 데이터셋 [8]을 선택합니다. 발화는 시뮬레이션과 실제 녹음으로 나뉩니다. 시뮬레이션 데이터는 측정된 RIR과 SNR = 20 dB의 정지된 주변 소음으로 왜곡된 월스트리트 저널 코퍼스 (WSJCAM0) [101]를 기반으로 합니다. 측정된 RIR은 작은 방 - 방 1, 중간 방 - 방 2 및 큰 방 - 방 3을 나타내며, 각각 0.3, 0.6 및 0.7 초의 60 dB의 반향 시간 (τ)을 가지고 있습니다. 각 방에 대해 마이크는 가까운 상태 (0.5 m)와 먼 상태 (2 m)에 배치됩니다. 실제 데이터는 멀티채널 월스트리트 저널 오디오-비주얼 (MC-WSJ-AV) 코퍼스 [102]를 기반으로 하며, 화자들은 τ = 0.7 초의 큰 방에서 가까운 (1 m) 및 먼 (2.5 m) 마이크 조건에서 녹음됩니다. 훈련 세트에는 시뮬레이션 데이터에서 7861개의 쌍으로 이루어진 발화가 포함됩니다. 테스트 세트에는 시뮬레이션 쌍 발화 (2176개)와 실제 반향 발화 (372개)가 포함됩니다. 훈련 및 테스트 세트에는 서로 다른 방 녹음이 사용됩니다. 데이터셋은 16 kHz 샘플링 주파수로 단일 채널, 이중 채널 및 팔 채널 구성으로 원래 캡처되었습니다. 그러나 이 연구의 범위를 위해 우리는 단일 채널 구성만 사용합니다.
3) 초고해상도: 비교 분석을 위해 영어 멀티스피커 코퍼스 (VCTK) [103]를 활용합니다. VCTK 데이터셋에는 다양한 영어 강세를 가진 108명의 화자로부터 44시간의 녹음이 포함되어 있습니다. 초고해상도 실험에서는 [83]의 설계 선택을 따릅니다. 저해상도 오디오 신호는 원본 트랙의 16 kHz에서 원하는 업스케일링 비율 (s)로 신호를 하위 샘플링하여 생성됩니다. 첫 번째 작업은 단일 VCTK 화자 (p225)를 사용하며, 처음 223개의 녹음은 훈련에 사용되고 마지막 8개의 녹음은 테스트에 사용됩니다. 두 번째 작업은 처음 100개의 VCTK 화자를 훈련 세트로 사용하고 마지막 8개의 화자에서 테스트합니다. 단일 화자 및 멀티스피커 작업의 업스케일링 비율은 {2, 4, 8}로 설정되어 8 kHz, 4 kHz, 2 kHz에서 16 kHz로 재구성을 나타냅니다.

B. 실험 설정

훈련 세트의 발화는 2초로 잘립니다.
반면에 테스트 세트에서는 잘림 없이 길이가 가변적으로 유지됩니다. 25ms 윈도우 길이 (400포인트 FFT)와 6.25ms (75% 겹침)의 호핑 크기를 가진 햄밍 창이 사용됩니다. 따라서 결과적인 스펙트로그램은 200개의 주파수 바인 F를 가지며, 시간 차원 T은 가변적인 트랙 지속 시간에 따라 달라집니다. 생성기의 두 단계 컨포머 블록 수 N, 배치 크기 B 및 채널 수 C는 각각 4, 4 및 64로 설정됩니다. 측정 판별기의 채널 수는 {16,

32, 64, 128}. 훈련 단계에서 생성자와 판별자 모두에게 AdamW 옵티마이저 [106]를 사용하여 50 에포크 동안 훈련합니다. 생성자의 학습률은 5 ×10−4로 설정되고, 판별자의 학습률은 1 ×10−3로 설정됩니다. 학습률 스케줄러는 12 에포크마다 감소 계수 0.5로 적용됩니다. 생성자 손실 LG에서 가중치는 {γ1 = 1,γ2 = 0.01,γ3 = 1}로 설정됩니다. 오디오 샘플과 CMGAN 구현은 온라인에서 이용할 수 있습니다.

V. 결과 및 토론
A. 노이즈 제거

목표 점수: 우리는 소음 제거된 음성 품질을 평가하기 위해 일반적으로 사용되는 일련의 지표를 선택합니다. 즉, PESQ는 -0.5에서 4.5까지의 점수 범위를 가지며, 세그멘탈 신호 대 잡음 비율 (SSNR) 및 복합 평균 의견 점수 (MOS) [107] 기반 지표: 신호 왜곡의 MOS 예측 (CSIG), 배경 잡음의 침입성의 MOS 예측 (CBAK) 및 전반적인 효과의 MOS 예측 (COVL)이 모두 1에서 5의 점수 범위 내에 있습니다. 추가로, 우리는 STOI를 사용하여 0에서 1의 점수 범위로 말의 명료성을 판단합니다. 모든 지표에 대해 높은 값은 더 나은 성능을 나타냅니다.

결과 분석: 우리가 제안한 CMGAN은 표 I에 나와 있는 다른 최첨단 (SOTA) 소음 제거 기준과 객관적으로 비교됩니다. 시간 영역 방법의 경우, 우리는 표준 SEGAN [14]과 최근의 세 가지 방법을 포함시켰습니다. TSTNN [16], DEMUCS [17] 및 SE-Conformer [18]. TF 영역 방법의 경우, 우리는 MetricGAN [11], PHASEN [12], PFPL [104], MetricGAN+ [105], DB-AIAT [13] 및 DPT-FSNet [26]의 최근 SOTA 방법을 평가합니다. 대부분의 TF 영역 방법이 모든 사용된 지표에서 시간 영역 상대 방법보다 우수한 성능을 보여줍니다. 또한, 우리가 제안한 TF Conformer 기반 접근 방식은 시간 영역 SE-Conformer보다 큰 개선을 보여줍니다. 지표 판별기를 포함하는 프레임워크 (MetricGAN+)와 비교했을 때, PESQ, CSIG, CBAK 및 COVL 점수에서 각각 0.26, 0.49, 0.78 및 0.48의 개선이 있습니다. 마지막으로, 우리의 프레임워크는 DB-AIAT 및 DPT-FSNet과 같은 최근 개선된 트랜스포머 기반 방법보다 상대적으로 낮은 모델 크기인 1.83 M 매개변수로 모든 평가 점수에서 우수한 성능을 보입니다.

제거 연구: 우리의 설계 선택 사항을 검증하기 위해 제거 연구가 수행되었습니다. 표 II에 나와 있습니다. 우리는 먼저 다른 입력의 영향을 조사합니다. Magnitude-only는 오직 크기만 입력으로 사용되며, 향상된 크기는 ISTFT를 위해 노이즈가 있는 위상과 결합됩니다. 1https://github.com/ruizhecao96/CMGAN/ 7

표 I: Voice Bank+DEMAND 데이터셋 [7]에서의 성능 비교. "-"는 원본 논문에서 결과가 제공되지 않음을 의미함. 모델 크기는 백만 개의 학습 가능한 매개변수 수를 나타냄.

방법      연도 입력       모델 크기 (M) PESQ CSIG CBAK COVL SSNR STOI

시끄러운

SEGAN [14]  2017 시간           97.47   2.16 3.48 2.94 2.80 7.73 0.92
MetricGAN [11] 2019 크기     -     2.86 3.99 3.18 3.42  -    -
PHASEN [12] 2020 크기+위상  -     2.99 4.21 3.55 3.62 10.08 -
TSTNN [16]  2021 시간            0.92   2.96 4.10 3.77 3.52 9.70 0.95
DEMUCS [17] 2021 시간            128    3.07 4.31 3.40 3.63  -   0.95
PFPL [104]  2021 복소수          -     3.15 4.18 3.60 3.67  -   0.95
MetricGAN+ [105] 2021 크기   -     3.15 4.14 3.16 3.64  -    -
SE-Conformer [18] 2021 시간       -     3.13 4.45 3.55 3.82  -   0.95
DB-AIAT [13] 2021 복소수+크기 2.81 3.31 4.61 3.75 3.96 10.79 0.96
DPT-FSNet [26] 2021 복소수      0.91   3.33 4.58 3.72 4.00  -   0.96
CMGAN       2022 복소수+크기 1.83 3.41 4.63 3.94 4.12 11.10 0.96

작업. 네트워크 아키텍처는 복잡한 디코더를 제외하고는 동일합니다. 따라서 복소수만 복소 스펙트로그램을 입력으로 사용하고 마스크 디코더를 제거하는 것을 의미합니다. 비교 결과, 위상 향상이 없으면 PESQ 점수가 0.18 감소하고, 순수한 복소 스펙트로그램을 사용하면 SSNR 점수가 1.91 dB 감소합니다. 이 결과는 복소 스펙트로그램이 크기 정보를 포함하고 있지만, 크기를 암묵적으로 향상시키는 것은 사용된 프레임워크에 도전적입니다. 또한, 손실 함수에서 크기를 명시적으로 다루면 II-A 절에서 언급한 보상 효과를 완화할 수 있습니다. 또한, 복소 정제 분기에서 선택한 매핑 기반 접근 방식을 검증하기 위해, 마스크 디코더를 변경하지 않고 복소 디코더를 [21]과 유사한 cIRM을 포함하도록 수정합니다. CMGAN-cIRM과 CMGAN 간의 비교 결과, PESQ 및 SSNR 점수가 크게 감소하는 것을 보여줍니다.

반면, 결과는 시간 손실의 부재(w/o Time loss)가 PESQ 점수를 3.45로 향상시키는 반면, SSNR은 원래 CMGAN보다 약간 낮습니다. 이는 시간 손실이 PESQ 및 SSNR 점수의 성능을 균형있게 조정하는 데 효과적임을 나타냅니다. 

판별자 선택을 보여주기 위해 판별자를 제거(w/o Disc.)하거나 메트릭 판별자를 이미지 생성 작업에서 일반적으로 사용되는 패치 판별자로 대체하는 두 가지 테스트를 수행했습니다. 판별자를 제거하는 것은 모든 주어진 점수에 부정적인 영향을 미친 것으로 나타납니다. 마찬가지로 패치 판별자를 추가하는 것은 약간의 개선만 보여주었으며, 이는 패치 판별자가 주어진 점수에 미치는 영향이 제한적임을 반영합니다.

표 II: 노이즈 제거 실험 결과.

방법   PESQ CSIG CBAK COVL SSNR STOI

CMGAN 3.41 4.63 3.94 4.12 11.10 0.96
Magnitude-only 3.23 4.60 3.76 4.00 9.82 0.95
Complex-only 3.35 4.56 3.79 4.05 9.19 0.96
CMGAN-cIRM 3.28 4.60 3.83 4.03 10.40 0.96
w/o Time loss 3.45 4.56 3.86 4.11 9.71 0.96
w/o Disc. 3.24 4.46 3.82 3.93 10.56 0.96
Patch Disc. 3.28 4.48 3.85 3.96 10.75 0.96
Parallel-Conf. 3.35 4.54 3.87 4.03 10.63 0.96
Freq. → Time 3.39 4.56 3.91 4.07 10.84 0.96
Single Dec. 3.38 4.54 3.86 4.05 10.19 0.96

크기 마스크 활성화 함수

시그모이드 3.34 4.52 3.80 4.02 10.70 0.96
ReLU 3.32 4.54 3.80 4.04 10.69 0.96
소프트플러스 3.43 4.58 3.83 4.02 10.75 0.96

생성자는 일반적인 패치 판별기의 도움 없이 트랙을 향상시키는 데 완전히 능력이 있습니다. 그러나 평가 점수를 직접적으로 향상시키기 위해 측정 판별기를 사용하는 것이 유익하다는 것이 입증되었습니다.
또한, 우리는 두 단계의 형태로 아웃라인의 영향을 조사했습니다. 입력 특성 맵이 주어지면, 두 단계의 형태는 시간과 주파수 차원에 각각 초점을 맞출 것입니다. 이를 위해 두 가지 다른 구성이 제안될 수 있으며, 순차적 또는 병렬적일 수 있습니다. 따라서 우리는 순차적 CMGAN을 추가 수정 없이 병렬 연결 대조군인 Parallel-Conformer와 비교합니다. 결과는 병렬 접근 방식이 제안된 순차적인 방식보다 뒤쳐진다는 것을 보여줍니다. 즉, PESQ와 SSNR 점수가 각각 0.06 dB와 0.47 dB 감소합니다. 또한, 우리는 순차적인 형태의 블록 순서를 뒤집었습니다 (주파수).

시간)
그리고 우리는 점수가 비슷하며 약간의 개선이 표준 CMGAN (시간 → 주파수)에 유리하다는 결론을 내릴 수 있습니다. 하지만, 시간과 주파수를 모두 고려하는 단일 컨포머를 설계하는 것은 이론적으로 가능합니다. 그러나 이 경우에는 복잡성이 기하급수적으로 증가할 것입니다 [109]. 복잡한 세부 조정에서 디코더의 분리 요구를 보여주기 위해 원래의 마스크/복소 디코더를 단일 경로 디코더로 대체합니다. 최종 출력은 세 개의 채널을 나타내며, 첫 번째 채널은 PReLU 활성화 (크기)를 따르고 다른 두 채널에는 활성화가 없습니다 (복소). 단일 경로 디코더와 마스크/복소 디코더를 비교하면 모든 지표에서 저하가 나타나며, 특히 SSNR 점수 (0.91 dB)에서 저하가 큽니다.
예비 문헌 대부분은 예측된 크기 마스크가 0과 1 사이에 있을 것으로 가정합니다. 따라서 이 간격을 반영하기 위해 시그모이드 활성화가 일반적으로 선호됩니다 [11]–[13], [57], [105]. 비록 이것이 사실이지만, 바운드된 시그모이드 함수는 모델이 0과 사이의 값을 할당하는 것을 제한할 것입니다.

0    2
4   6
8

0    2
4   6
8

영
0.5
1

주파수 [kHz]
활성화 [A.U.]
P r
o
b a
b i l i t
y

그림 5: 마스킹된 PReLU 활성화의 히스토그램.
8

0    1     2    3     4    5
2.9
3.1 3.3
3.5

TS-Conformer 블록
P E S Q

0. 안녕하세요.
1. 오늘은 날씨가 좋네요.
2. 저는 한국어를 배우고 있어요.
3. 한국 음식이 맛있어요.
4. 저는 한국에 살고 있어요.

510.0 - 오백 십 점 영
10.4 10.8 - 십 점 사 십 점 팔
11.2 - 십 일 점 이

S
S N R [ d B ]
PESQ
SSNR

그림 6: TS-Conformer 블록이 목표 점수에 미치는 영향.

그리고 이전 레이어의 모든 집계된 부정적인 활성화에 0.5를 더합니다. 반면에, PReLU와 같은 무제한 활성화 함수는 각 주파수 대역에 적절한 기울기를 학습하여 부정적인 활성화 문제를 완화하면서 이 간격을 자동으로 학습할 수 있습니다. 이 가정을 확인하기 위해, 다른 잡음 트랙에서 여러 크기의 마스크의 히스토그램을 구성합니다. 그림 5에서 보여지듯이, PReLU 활성화는 항상 0에서 1 사이에 위치합니다. 게다가, 대부분의 낮은 활성화는 5 kHz 이상의 주파수 대역에 할당됩니다 (인간의 말을 넘어서는 범위) [110]. 또한, 우리는 마스크 디코더에 다른 유계 및 무제한 활성화인 시그모이드, ReLU 및 ReLU의 소프트 버전 (소프트플러스)를 포함한 실험을 확장합니다 [111]. 표 II에 따르면, 시그모이드와 ReLU 활성화는 비교 가능하며, PReLU 활성화를 사용한 CMGAN보다 낮은 점수를 보고합니다. 소프트플러스는 약간 더 높은 PESQ를 달성하지만, 다른 메트릭에 대한 대가로 지불해야 합니다.
마지막으로, TS-Conformer 블록의 개수에 대한 실험을 진행합니다. 그림 6에서 보여지듯이, 어떤 conformer 블록도 없는 CMGAN의 성능은 허용할만하며, MetricGAN과 같은 다른 SOTA 방법과 비교 가능합니다. 그러나 conformer 블록이 하나만 있어도 PESQ가 0.4 향상됩니다. 성능은 블록이 더 많아질수록 점진적으로 증가하지만, 네 개의 블록 이후에는 더 이상 개선이 관찰되지 않습니다. 공간 제약으로 인해, 원래 CMGAN은 관련된 몇 가지 유의미한 실험 연구를 포함한 다가오는 작업에 대해 고려될 것입니다.

B. 디리버베레이션
목표 점수: 디리버베레이션에 대해, 우리는 REVERB 챌린지 논문 [8]에서 권장하는 측정 방법을 사용합니다: 켑스트럼 거리 (CD) [112], 로그 우도 비율 (LLR) [113],

주파수 가중 세그멘탈 SNR (FWSegSNR) [114] 및
음성 대 감쇠 제거 변조 에너지 비율 (SRMR)
[115]. 논문은 또한 대체 측정 항목으로 PESQ를 권장했지만, 최근의 대감쇠 문헌 대부분은 이를 고려하지 않았다. 이상치 감소를 위해 [107]의 저자들은 CD의 범위를 [0,10]으로 제한하고
LLR의 범위를 [0,2]로 제한하는 것을 제안했다. CD와 LLR에 대해 낮은 값은 좋은 점수를 나타내고,
FWSegSSNR, PESQ 및 SRMR에 대해서는 높은 값이 좋은 음성 품질을 나타낸다. CD, LLR, FWSegSNR
및 PESQ는 청취 테스트와 상관관계가 있으므로 선택되었으며, 이들은 모두 침입적인 점수이므로 개선된 음성과 깨끗한 참조가 필요하다. 따라서 SRMR은 깨끗한 참조 없이 개선된 음성에 적용되는 비침입적인 점수로 사용된다. 따라서 개선된 비짝지어진 실제 녹음의 품질과 명료성을 측정하는 것은 매우 중요하다.
결과: 양적 분석을 위해 CMGAN은 최근 대감쇠 방법과 비교되었다. II-B절에서 논의된 대로, 대감쇠에서 시간 영역 접근법은 제한적이며, 이러한 방법들은 REVERB 챌린지 데이터를 사용하지 않았다. 따라서 선택된 방법들은 모두 TF-도메인 분석을 고려한다. 공정한 비교를 위해 개별 방 점수를 기록한 논문들만 고려된다. 이 기준에 따라, 우리는 네 가지 최근 방법과 비교한다: Xiao et al. [79], U-Net [76],
와이드 잔여 네트워크 (WRN) [78] 및 SkipConvNet [77].
불행히도, 이 논문들 중 어느 것도 PESQ 점수를 보고하지 않았으므로, 비교 분석에서 제외되었다. 그러나, PESQ는 여전히 CMGAN의 측정 판별자에 의해 극대화되어야 하는 목적 점수로 사용된다.
근거리 및 원거리 마이크로폰 케이스에 대한 결과는 각각 표 III 및 IV에 나와 있다. 첫 네 열은 세 가지 다른 방 크기 (작은 - 방 1, 중간 - 방 2, 큰 - 방 3 및 평균 점수)에 대한 시뮬레이션 데이터 결과를 나타낸다. 마지막 열은 실제 녹음의 SRMR을 나타낸다. 예상대로, 더 큰 방과 더 멀리 떨어진 마이크 배치는 음성에 더 많은 왜곡을 도입하므로 점수가 낮아진다. 근거리 시뮬레이션 마이크로폰 케이스에서 제안된 CMGAN은 대부분의 메트릭에서 다른 방법들보다 우수한 성능을 보여준다, 특히 FWSegSNR에서. SRMR의 경우, Xiao et al.은 시뮬레이션 근거리 데이터에서 더 높은 SRMR 점수를 보고하지만, 근거리 실제 녹음에서는 상당한 점수 하락이 관찰된다. SkipConvNet은 근거리 실제 SRMR 점수에서 더 좋은 성적을 거두었다.

표 III: 가까운 마이크로폰 케이스에서 시뮬레이션 및 실제 데이터 결과.

CD
↓
LLR
↓
FWSegSNR
↑
SRMR
↑
SRMR-real
↑
방       1  2  3 평균 1 2  3  평균 1 2   3  평균 1 2  3 평균   -
울림이 있는 말 1.99 4.63 4.38 3.67 0.35 0.49 0.65 0.50 8.12 3.35 2.27 4.58 4.50 3.74 3.57 3.94 3.17
Xiao et al. [79] 1.58 2.65 2.68 2.30 0.37 0.50 0.52 0.46 9.79 7.27 6.83 7.96 5.74 6.49 5.86 6.03 4.29
WRN [78]  2.02 4.61 4.15 3.59 0.36 0.46 0.60 0.47 8.28 3.57 2.54 4.80 4.04 3.46 3.27 3.59 -
U-Net [76] 1.75 2.58 2.53 2.28 0.20 0.41 0.45 0.35 13.32 10.87 10.40 11.53 4.51 5.09 4.94 4.85 5.47
SkipConvNet [77] 1.86 2.57 2.45 2.29 0.19 0.30 0.35 0.28 13.07 10.96 10.22 11.42 4.99 4.75 4.56 4.77 7.27

CMGAN     1.46 2.14 2.27 1.96 0.14 0.25 0.34 0.24 14.36 13.49 11.69 13.18 5.42 5.74 5.29 5.48 6.49
CMGAN-LLR 1.69 2.56 2.43 2.23 0.15 0.25 0.25 0.22 14.48 12.49 11.03 12.67 5.48 5.80 6.02 5.77 7.71

표 IV: 원격 마이크로폰 케이스에서 시뮬레이션 및 실제 데이터 결과.

CD
↓
LLR
↓
FWSegSNR
↑
SRMR
↑
SRMR-real
↑
방       1  2  3 평균 1 2  3  평균 1  2  3  평균 1 2  3 평균  -
반향음성 2.67 5.21 4.96 4.28 0.38 0.75 0.84 0.66 6.68 1.04 0.24 2.65 4.58 2.97 2.73 3.43 3.19
Xiao et al. [79] 1.92 3.17 2.99 2.69 0.41 0.61 0.58 0.53 9.12 6.31 5.97 7.13 5.67 5.80 5.03 5.50 4.42
WRN [78]  2.43 4.99 4.56 3.99 0.35 0.59 0.67 0.54 7.54 1.79 0.88 3.40 4.48 3.32 2.84 3.55 -
U-Net [76] 2.05 3.19 2.92 2.72 0.26 0.57 0.56 0.46 12.08 9.00 9.05 10.04 4.76 5.27 4.71 4.91 5.68
SkipConvNet [77] 2.12 3.06 2.82 2.67 0.22 0.46 0.46 0.38 11.80 8.88 8.16 9.61 5.10 4.76 4.25 4.70 6.87

CMGAN     1.88 2.90 2.85 2.54 0.24 0.43 0.47 0.38 11.65 10.34 8.91 10.30 5.78 5.87 4.69 5.45 6.61
CMGAN-LLR 2.07 3.32 3.05 2.81 0.24 0.46 0.40 0.37 11.21 9.22 9.48 9.97 5.93 5.54 5.19 5.55 7.62
9

CMGAN     1.88 2.90 2.85 2.54 0.24 0.43 0.47 0.38 11.65 10.34 8.91 10.30 5.78 5.87 4.69 5.45 6.61
CMGAN-LLR 2.07 3.32 3.05 2.81 0.24 0.46 0.40 0.37 11.21 9.22 9.48 9.97 5.93 5.54 5.19 5.55 7.62
9

실험 데이터에서는 U-Net과 SkipConvNet보다 나쁜 결과를 보입니다. CMGAN은 평균 CD와 FWSegSNR에서 각각 0.3dB와 1.65dB로 우수한 성능을 보이지만, 전반적인 경쟁력 있는 점수를 보고합니다. 먼 거리 마이크로폰의 경우, CMGAN은 특히 FWSegSNR에서 전반적인 점수 향상을 보입니다. Xiao et al.은 실험 데이터에서 여전히 SRMR에서 약간 우수한 성능을 보이지만, 가까운 거리 마이크로폰의 경우에만 0.05로 차이가 매우 작습니다. SkipConvNet도 제안된 CMGAN보다 실제 SRMR 점수가 약간 우수합니다.

제거 연구: 메트릭 판별자로 PESQ를 선택한 것을 검증하기 위해, 우리는 목적 메트릭 판별자 점수로 LLR을 사용하는 CMGAN 변형인 CMGAN-LLR을 소개합니다. LLR은 한정된 메트릭을 반영하기 때문에 선택되었으며, LS-GAN 공식에 기반하여 최적화 공간이 정규화된 점수에 의해 제한될 때 메트릭 판별자가 더 견고해집니다. 따라서, Eq. 6을 수정하여 정규화된 LLR 점수 QLLR을 QPESQ 대신에 사용하고, LGAN과 LD에서 1 대신에 0으로 변경합니다. 따라서, 점수는 1 대신에 0으로 최소화됩니다. Table III와 IV에서 확인할 수 있듯이, LLR 점수는 PESQ로 훈련된 원래 CMGAN보다 약간 우수합니다. 그러나, 가까운 거리와 먼 거리 마이크로폰의 실제 녹음에서는 SRMR 점수에서 상당한 개선이 관찰됩니다. 또한, CMGAN-LLR 변형은 가까운 거리와 먼 거리 마이크로폰의 실제 녹음에서 SkipConvNet보다 각각 0.44와 0.75로 우수한 성능을 보입니다. CMGAN과 CMGAN-LLR을 비교하면 대부분의 지표에서 균형 잡힌 성능을 보이며, 이는 PESQ가 최적화하기에 견고한 메트릭이며 대부분의 주어진 품질 메트릭과 높은 상관관계를 가지고 있다는 것을 나타냅니다.

C. 초고해상도
목표 점수: 두 가지 지표, 로그-주파수 거리(LSD)와 신호 대 잡음 비율(SNR)을 사용하여 초고해상도를 평가합니다. 문헌 검토에 따르면, LSD의 정의는 모든 논문에서 동일하지 않습니다. 수학적으로, LSD는 개선된 음성의 크기 스펙트로그램 구성 요소와 깨끗한 참조 사이의 로그 거리를 측정합니다. 일부 논문에서는 자연로그를 사용하고, 다른 논문에서는 상용로그를 사용합니다. 두 정의 모두 STFT는 2048개 샘플의 한닝 창과 512의 hop 크기로 평가됩니다. 공정한 비교를 위해 동일한 STFT 매개변수화가 사용되며, 문헌에서 두 가지 다른 정의에 기반한 LSD 결과가 제시됩니다. 낮은 LSD와 높은 SNR은 더 나은 음성 품질을 나타냅니다.
결과: 마스킹 기반 방법은 초고해상도 작업에 관련이 없으므로, 이전에 섹션 II-C에서 언급한 대로 CMGAN 마스크 디코더 부분은 원소별 곱셈 대신 원소별 덧셈을 포함하여 수정됩니다. 이는 다음과 같이 식 3에 반영됩니다.

ˆ Xr = (M0 + Ym)cosYp + ˆ X0

r
ˆ Xi = (M0 + Ym)sinYp + ˆ X0 i

(9) 
나는 한국 음식을 좋아해요. 
나는 매일 한국어를 공부해요. 
나는 한국에 가고 싶어요. 
한국 문화에 대해 배우고 싶어요. 
한국 드라마를 좋아해요.

M은 마스크 디코더의 수정된 출력을 나타냅니다. 
노이즈 제거와 반향 제거의 이전 사례와 달리, 네트워크는 노이즈를 억제하고 음성을 보존하기 위해 0과 1 사이의 마스크 활성화를 학습하는 것이 아니라, 주어진 저주파 대역을 보존하면서 누락된 고주파 대역을 완성할 수 있는 활성화를 학습합니다.
표 V에서는 우리의 접근 방식을 Kuleshov 등이 제안한 U-Net 아키텍처 [83], TFiLM [85], AFiLM [86], hybrid TFNet [84], hybrid AE [87] 및 NVSR [88]와 비교합니다. 모든 점수는에서 가져온 것입니다.

표 V: 초해상도에 대한 성능 비교, "-"는 원본 논문에서 결과가 제공되지 않음을 의미합니다.

VCTK-단일 VCTK-다중.
방법 s LSDe

LSD10
↓
SNR
↑
LSDe
↓
LSD10
↓
SNR
↑
U-Net [83] 2 3.2 - 21.1 3.1 - 20.7
TFiLM [85] 2 2.5 - 19.5 1.8 - 19.8
AFILM [86] 2 2.3 - 19.3 1.7 - 20.0
AE [87] 2 -   0.9 22.4 -  0.9 22.1
NVSR [88] 2 - -   -   -   0.8  -
CMGAN  2 1.7  0.7 24.7 1.6 0.7 24.4
CMGAN-Mag. 2 1.4 0.6 22.2 1.3 0.6 23.4
U-Net [83] 4 3.6 - 17.1 3.5 - 16.1
TFiLM [85] 4 3.5 - 16.8 2.7 - 15.0
AFILM [86] 4 3.1 - 17.2 2.3 - 17.2
TFNet [84] 4 - 1.3 18.5 - 1.3 17.5
AE [87] 4 -   0.9 18.9 -  1.0 18.1
NVSR [88] 4 - -   -   -   0.9  -
CMGAN  4 2.3  1.0 18.6 2.2 1.0 19.1
CMGAN-Mag. 4 1.7 0.7 16.9 1.8 0.8 16.1
TFiLM [85] 8 4.3 - 12.9 2.9 - 12.0
AFILM [86] 8 3.7 - 12.9 2.7 - 12.0
TFNet [84] 8 - 1.9 15.0 - 1.9 12.0
NVSR [88] 8 - -   -   -   1.1  -
CMGAN  8 2.6  1.1 12.9 2.7 1.2 14.1
CMGAN-Mag. 8 1.9 0.8 10.9 2.0 0.9 10.9

해당 원본 논문들. 값 s = 2/4/8은 8 kHz/4 kHz/2 kHz에서 16 kHz 음성으로 업샘플링 스케일을 의미합니다. VCTK-Single 실험에서, 우리의 방법은 8 kHz에서 16 kHz로 오디오 신호를 변환할 때 스케일 2에서 모든 세 가지 지표에서 최고의 점수를 달성했습니다. 특히 SNR에서는 SOTA AE 방법과 비교하여 2.3 dB의 개선이 있었습니다. 스케일 4에서는 AE 방법이 SNR과 LSD10에서 약간의 개선을 보였습니다. 스케일 8 작업에서는 LSDe와 LSD10 측면에서 우리의 방법이 다른 방법들보다 우수했습니다. 그러나 SNR은 TFNet보다 낮고 TFiLM과 AFiLM 접근법과 유사합니다. 이는 VCTK-Single 데이터셋의 제한된 학습 샘플로 인한 모델 오버피팅으로 설명될 수 있다고 가정합니다. 반면, VCTK-Multi 평가에서는 우리의 방법이 모든 메트릭에서 모든 업스케일링 비율에서 다른 접근법들보다 우수한 성능을 보였습니다. 특히, 스케일 2/4/8에서 SNR이 2.3 dB, 1.0 dB, 2.1 dB 개선되었습니다. VCTK-Single 평가에서와 비교하여 CMGAN은 스케일 8에서 훨씬 더 좋은 성능을 보여주어 오버피팅 가정을 확인합니다. 실험 결과: 복잡한 TF 도메인 초고해상도의 효과를 입증하기 위해, CMGAN은 복잡한 디코더와 메트릭 판별자를 제거하고 크기 손실만 남겨둔 CMGAN-Mag.로 수정되었습니다. 복잡한 분기가 제거되면 LSDe와 LSD10에서 상당한 개선이 관찰되며, 이는 LSD가 크기 구성 요소만을 고려하기 때문에 예상되는 결과입니다. 이 LSD 획득은 재구성된 시간 도메인 신호를 고려하는 SNR 점수의 상당한 하락을 동반합니다. 따라서 복잡한 분기를 제거하면 네트워크가 크기 구성 요소만을 향상시키기 때문에 LSD에 푸시가 생기지만 전체 신호 품질은 저하됩니다. 스케일 4 예제의 입력, 예측 및 참조 트랙의 그림은 그림 7에 나와 있습니다. 출력 마스크 M(cid:48)에서 고주파 대역의 활성화가 명확하게 나타납니다. 그림 7c와 7d를 비교하면 CM-GAN이 훈련 데이터에서 다른 음성 음운을 관찰함으로써 놓친 고주파 대역을 구성하는 능력이 나타납니다. 이 성능은 시간 도메인 그림 7e, 7f 및 7g에서 중간 샘플의 정확한 보간으로도 나타납니다.

0.5 1 1.5 2 2.5 3
02468
시간 [초]
주파수 [k
Hz]

저해상도 입력

0.5 1 1.5 2 2.5 3
02468
시간 [초]
주파수 [k
Hz]

예측된 마스크 M(cid:48)

0.5 1 1.5 2 2.5 3
02468
시간 [초]
주파수 [k
Hz]

(c) 예측된 출력

0.5 1 1.5 2 2.5 3
02468
시간 [초]
주파수 [k
Hz]

고해상도 참조

−101
시간 [밀리초]
값
[
A .
U . ]

(e) 4 kHz 입력 세그먼트

−101
시간 [밀리초]
값
[
A .
U . ]

(f) 16 kHz 예측 세그먼트

−101
시간 [밀리초]
값
[
A .
U . ]

(g) 16 kHz 참조 세그먼트

그림 7: 4 kHz 슈퍼 해상도의 스케일 4 예시

16 kHz). 상단 행은 관련 스펙트로그램의 TF-크기 표현을 나타냅니다. 하단 행은 해당 시간 영역 신호의 20 ms 세그먼트를 보여줍니다.

VI. 주관적 평가

지금까지 제안된 아키텍처는 객관적인 지표 점수를 사용하여 다른 SOTA 방법과 양적으로 비교되었습니다. 이러한 점수는 제안된 방법의 우수성을 나타내는 지표로 사용될 수 있지만, 주관적인 품질 측정을 완전히 대체할 수는 없습니다. 주관적 청취 테스트는 많은 참가자와 이상적인 청취 환경이 필요하기 때문에 비용이 많이 들고 시간이 많이 소요됩니다. 따라서 주관적 품질 점수와 높은 상관관계를 가지는 객관적인 측정 방법을 찾는 것은 여전히 연구 주제입니다. 이 분야에서 가장 주목할만한 작업은 [107]에서 소개된 것으로, 저자들은 전통적인 회귀 분석 방법을 기반으로 한 복합 평균 의견 점수(MOS)를 제안했습니다. 이러한 점수는 음성 노이즈 제거 성능을 평가하기 위해 섹션 V-A에서 사용됩니다. 이 연구에는 ITU-T P.835 표준 [118]에 따라 평가된 1792개의 음성 샘플이 포함되어 있으며, PESQ, 세그멘탈 SNR, LLR 및 가중 스펙트럴 슬로프(WSS)와 같은 잘 알려진 객관적인 측정 방법이 신호 왜곡, 배경 잡음 및 전체 품질을 반영하는 세 가지 다른 복합 점수의 기반 함수로 사용됩니다. 제안된 복합 측정은 주관적 평가와 0.9에서 0.91의 상관관계를 보고하며, 저자들은 PESQ의 중요성을 강조하고 있습니다(0.89의 상관관계를 보임). 그러나 이 연구는 두 가지 SNR 조건(5dB 및 10dB)에서 네 가지 배경 잡음 유형에만 한정되어 있으며, 무엇보다도 제안된 점수는 침입적입니다(청정 및 개선된 음성이 모두 필요함).

최근에는 DNN(DNNs)이 주관적 대체 점수를 찾기 위해 활용되고 있습니다. 이전의 복합 측정과 달리, 이러한 방법 중 대부분은 입력으로 트랙을 사용하고 네트워크는 주관적 평가를 모방하도록 훈련됩니다. 따라서 이러한 점수는 비최적의 객관적 점수가 아닌 전체 트랙에 의존하지 않습니다. 또한, 이러한 점수는 침입적이지 않으므로 청정 참조가 필요하지 않은 개선된 트랙을 평가할 수 있습니다. 최근 연구에서 주관적 기준으로 사용되는 표준 점수는 Microsoft에서 제안한 DNSMOS입니다. DNSMOS는 평가된 음성 75시간에 대해 훈련되었습니다. ITU-T P.835에 따라 청취자는 신호 왜곡, 배경 잡음 및 전체 품질에 대해 1에서 5까지의 점수를 할당합니다(높을수록 좋음). 세 가지 주어진 품질 평가 점수에 대해 0.94에서 0.98의 상관관계가 보고되었습니다.

문헌에 따르면, DNSMOS는 우리의 주관적 평가 척도로 평가될 것입니다. 제한된 공간과 오픈 소스 구현의 부재로 인해 특히 소음 제거에서 주관적 평가는 SE 문제의 소음 제거 측면에 초점을 맞출 것입니다. 따라서, 이 연구에는 네 가지 다른 소음 제거 유즈 케이스가 포함되어 있으며, 네트워크의 일반화 능력을 보여주기 위해 훈련에 포함되지 않은 보지 못한 소음 조건, 실제 소음 샘플 및 추가적인 왜곡에 대해 평가될 것입니다. 이를 위해, 프레임워크는 단일 유즈 케이스 (Voice Bank+DEMAND)에서 모두 훈련되고, 그런 다음 모델은 네 가지 다른 데이터셋에서 평가될 것입니다:
(a) Voice Bank+DEMAND 테스트 세트 [7]: 훈련에 포함되지 않은 DEMAND 데이터셋 [100]에서 얻은 두 명의 보지 못한 화자의 35분 (824개 트랙)의 소음이 섞인 음성을 포함합니다.
(b) CHiME-3 [9]: 버스, 카페, 보행자 지역 및 도로 교차로의 네 가지 다른 환경에서 12명의 화자로부터 얻은 실제 소음이 섞인 음성 녹음 7.8시간 (4560개 트랙)을 포함합니다. 이 데이터에는 깨끗한 참조 트랙이 없습니다.
(c) DNS challenge [10]: 원본 데이터에는 Librivox2에서 영어 화자가 읽은 1934개의 연설 샘플과 Audio Set [126] 및 Freesound3에서 얻은 150가지 다른 소음 유형의 181시간이 포함되어 있습니다. 이 데이터셋을 기반으로 0에서 10 dB의 SNR을 가진 9시간 (3240개 트랙)의 소음이 섞인 음성을 구성합니다.
(d) DNS challenge+Reverb.: 동일한 9시간을 사용하지만 음성에 감쇠 조건을 시뮬레이션하고 DNS challenge 부분에 동일한 소음을 추가합니다. RIRs는 openSLR26/28 [127]에서 선택되었으며, 248개의 실제 조건과 60k개의 합성 조건이 포함되어 있습니다. RIRs는 0.3-1.3초의 60 dB 감쇠 시간을 가진 세 가지 다른 방 크기에서 녹음되었습니다. 모든 트랙은 16 kHz로 리샘플링되었으며, 남성과 여성 화자의 비율은 50%입니다. Table I에서 각 소음 제거 패러다임을 대표하는 방법을 선택합니다. 방법은 오픈 소스 구현의 가용성과 해당 논문에서 보고된 결과의 재현성을 기반으로 선택되었습니다. 메트릭 디스크리미네이터를 대표하는 방법으로 MetricGAN+ [105]를 사용했습니다. 시간 영역 방법으로는 DEMUCS [17]가 선택되었습니다. TF 영역 복소수 소음 제거를 위해 PHASEN [12]이 선택되었습니다. 또한, 깊은 PFPL [104]을 활용한 추가적인 방법도 사용되었습니다.

MetricGAN+ 메트릭간+
DEMUCS 디뮤크스
PHASEN 페이즌
PFPL 피플
CMGAN 씨엠간

3.26 3.56 3.57 3.57 3.72
2.4
2.8
3.2
3.64

3.26 3.56 3.57 3.57 3.72
2.4
2.8
3.2
3.64

의미
D N S
M O
S

보이스 뱅크 + 수요

3.05 3.48 3.58 3.62 3.77
1.8
2.2
2.63
3.4
3.8
4.2

3.05 3.48 3.58 3.62 3.77
1.8
2.2
2.63
3.4
3.8
4.2

의미
D N S
M O
S

(b) CHiME-3 (실제 소음)

3.1 3.12 3.71 3.75 3.9
1.4
1.8
2.2
2.63
3.4
3.8
4.2

3.1 3.12 3.71 3.75 3.9
1.4
1.8
2.2
2.63
3.4
3.8
4.2

의미
D N S
M O
S

(c) DNS 도전

2.47 2.68 3.28 3.22 3.43
1.2
1.62
2.4
2.8
3.2
3.64

2.47 2.68 3.28 3.22 3.43
1.2
1.62
2.4
2.8
3.2
3.64

의미
D N S
M O
S

(d) DNS 도전 + 리버브.

그림 8: 네 가지 다른 데이터셋에서 테스트된 주관적 평가 방법의 DNSMOS. 상자 그림에서 평균은 ( )로 표시되고, 중앙값은 (−)로 표시되며, 각 상자의 너비는 사분위 범위(25번째 백분위수와 75번째 백분위수)를 나타냅니다. 수염은 이상치를 제외한 최대값과 최소값을 보여줍니다 ( ). 각 방법의 평균 값은 x축에 표시됩니다.

복소값 네트워크는 실수와 허수 부분을 모두 향상시키기 위해 사용됩니다. 대부분의 논문들은 사전 훈련된 모델과 함께 공식 구현을 제공합니다. PHASEN은 비공식 코드를 사용하며, 우리는 논문의 결과를 재현하기 위해 모델을 훈련시켰습니다. DEMUCS의 경우, 사용 가능한 모델은 Voice Bank+DEMAND와 DNS 챌린지 데이터 모두에 대해 사전 훈련되었습니다. 따라서, 우리는 모든 제시된 모델들 간의 공정한 비교를 위해 Voice Bank+DEMAND 데이터에서 권장되는 구성으로 DEMUCS를 다시 훈련시켰습니다.

공간 제한으로 인해 전체 음질의 DNSMOS만 보고되었습니다. 그림 8에 표시된대로 CMGAN은 네 가지 사용 사례에서 모든 방법을 능가하고 있습니다. 예를 들어, CMGAN은 첫 번째 세 가지 사용 사례에서 가장 경쟁력 있는 접근 방식 (PFPL)과 비교하여 평균 개선량이 0.15입니다. 또한, CMGAN의 사분위 범위는 다른 모든 방법보다 훨씬 좁으며, 이는 분산이 낮고 따라서 DNS 도전과제에서 자신감있는 예측을 나타냅니다 (그림 8c). 반면에, MetricGAN+은 모든 사용 사례에서 최악의 성능을 보여줍니다. PESQ 점수가 비교적 높은 (3.15) 반면, 우리가 계산한 SSNR 점수는 1 dB 미만으로, MetricGAN+의 경우 메트릭 판별자가 다른 메트릭을 희생하여 PESQ를 향상시키기만 하는 것으로 보입니다. SSNR 점수는 원래 논문에서 보고되지 않았습니다. 시간 영역 패러다임을 나타내는 DEMUCS는 Voice Bank+DEMAND 및 실제 CHiME-3 사용 사례에서 견고한 성능을 보여줍니다. 그러나 DNS 도전 데이터셋에는 일반화되지 않습니다. 이 일반화 문제는 TF-도메인 복잡한 노이즈 제거 방법 (PHASEN, PFPL 및 CMGAN)에서 명확하게 완화되었습니다. 그림 8d에서 추가적인 반향이 있는 DNS 도전의 전체 DNSMOS는 DNS 도전 (그림 8c)과 비교하여 평균적으로 0.5 하락했습니다. 이는 반향과 같은 보이지 않는 효과에 대한 일반화가 보이지 않는 노이즈 유형에 대한 일반화보다 더 어렵기 때문에 예상되는 결과입니다. 이 하락에도 불구하고, CMGAN은 여전히 경쟁력 있는 다른 TF-도메인 접근 방식 (PHASEN 및 PFPL)보다 우수한 성능을 보여줍니다. 제시된 주관적 평가 모델의 추가 시각화는 부록 섹션에서 제공됩니다. 모든 주관적 평가 방법의 오디오 샘플은 온라인에서 관심 있는 독자들을 위해 사용할 수 있습니다. 4https://sherifabdulatif.github.io/

위의 결과에도 불구하고, 이 연구에는 제한 사항이 있습니다. 예를 들어, CMGAN은 아직 실시간 음성 개선을 위해 테스트되지 않았습니다. 즉, CMGAN은 전체 트랙에 액세스할 수 있습니다. 앞으로는 CMGAN을 수정하여 이전 샘플의 몇 개의 TF bin에만 액세스하고 전체 트랙에는 액세스하지 않도록 함께 실시간 시나리오에서의 부동 소수점 연산의 정확한 양에 대한 광범위한 연구가 필요합니다. 공간 제약으로 인해 우리는 각 작업을 개별적으로 실험하는 데 초점을 맞추었습니다. 슈퍼임포즈 효과 (소음 제거 및 반향 제거)는 주관적 평가 부분에서만 간단히 다루어졌으므로, 이용 사례에 대해 CMGAN을 훈련하고 평가하는 것은 우리의 작업의 중요한 확장이 될 것입니다.

VII. 결론
본 논문은 CMGAN을 소음 제거, 반향 제거 및 초해상도 등 다양한 음성 개선 작업에 대해 크기 및 복소 스펙트로그램 구성 요소 모두에서 작동하는 통합 프레임워크로 소개합니다. 저희 접근 방식은 최근에 나온 컨포머를 사용하여 시간 및 주파수 차원에서 장기적인 의존성과 지역적인 특징을 모두 포착할 수 있으며, 비미분 가능한 평가 점수를 직접 향상시켜 메트릭 불일치를 해결하는 메트릭 판별자와 결합합니다. 실험 결과는 제안된 방법이 상대적으로 적은 매개변수(1.83 M)로 각 작업에서 SOTA 방법과 우수하거나 경쟁력 있는 성능을 달성한다는 것을 보여줍니다. 또한, 우리는 각 사용된 구성 요소와 손실의 조각화된 이점 및 제안된 CMGAN 프레임워크에서의 손실을 검증하기 위해 소거 연구를 수행합니다. 마지막으로, 주관적 평가는 CMGAN이 다른 방법들보다 보다 강력한 일반화 능력으로 보이지 않는 잡음 유형과 왜곡에 대해 우수한 성능을 발휘한다는 것을 보여줍니다.

감사의 글
이 연구를 지원하기 위해 유용한 데이터셋을 제공해준 슈투트가르트 대학교 자연어 처리 연구소에 감사드립니다.

부록
이 섹션은 주관적 평가 방법과 비교하여 CMGAN의 시각화를 제공합니다. DEMAND 데이터셋에서의 넓은 대역 비정상적인 카페 소음 (SNR = 0 dB)과 좁은 대역 고주파 정상적인 초인종 소음 12에 대한 시각화입니다.

Freesound 데이터셋(SNR = 3 dB)에서는 방법을 평가하는 데 사용됩니다.
두 가지 잡음은 DNS 도전에서 문장에 추가됩니다.
시간 영역, TF-크기 및 TF-위상 표현을 종합적인 성능 분석을 위해 비교합니다.
위상이 구조화되지 않았기 때문에, 우리는 위상 시각화를 향상시키기 위해 [128]에서 제안된 기저대역 위상 차이(BPD) 접근법을 사용합니다.
그림 9에서 MetricGAN+, DEMUCS 및 PHASEN은 음성과 잡음을 혼동하여 최악의 성능을 보여줍니다, 특히 1.5에서 2초 사이(유사한 음성과 잡음 파워).
왜곡 및 누락된 음성 세그먼트는 시간 및 TF-크기 표현에서 ( ) 및 ( )로 주석이 달립니다.
또한, 오직 크기(MetricGAN+)와 시간 영역(DEMUCS)만 사용하는 방법에서는 노이즈 입력과 매우 유사한 노이즈가 제거된 위상이 나타납니다. 이에 반해 복잡한 TF-도메인 방법(PHASEN, PFPL 및 CMGAN)에서는 명확한 개선이 있습니다. PFPL과 CMGAN은 최고의 성능을 보여주며, CMGAN에서는 위상 재구성이 더 잘 이루어집니다(1.5에서 2초 사이).
일반적으로 정지 잡음은 비정지 잡음보다 도전적이지 않습니다. 그러나 정지 잡음은 훈련 데이터에 표현되지 않습니다. 그림 10에서 MetricGAN+ 및 PHASEN과 같은 방법은 일반화 성능이 낮으며, 도어벨 왜곡이 뚜렷하게 보입니다(3.5, 5 및 7 kHz에서). 반면에 DEMUCS와 PFPL에서는 성능이 약간 더 좋습니다. CMGAN은 모든 왜곡을 완벽하게 감쇠시킵니다. 높은 주파수 왜곡은 시간 영역에서는 TF-크기 및 TF-위상 표현보다 더 어렵게 감지됩니다.

참고문헌

[1] R. Cao, S. Abdulatif 및 B. Yang, "음성 개선을 위한 Conformer 기반의 측정 GAN (CMGAN)," Interspeech 학회 논문집, 2022, pp. 936-940. 
[2] F. Weninger 등, "LSTM 순환 신경망을 이용한 음성 개선 및 잡음 강인 ASR에의 응용," 잠재 변수 분석과 신호 분리 국제 학회, 2015, pp. 91-99. 
[3] C. Zheng 등, "음성 개선을 위한 상호작용적인 음성 및 잡음 모델링," 인공지능 AAAI 학회 논문집, vol. 35, no. 16, 2021, pp. 14549-14557. 
[4] J. L. Desjardins 및 A. K. Doherty, "청력장애인의 청력 보조기 잡음 감소가 청취 노력에 미치는 영향," Ear and hearing, vol. 35, no. 6, pp. 600-610, 2014. 
[5] D. Wang 및 J. Chen, "심층 학습을 기반으로 한 지도 음성 분리: 개요," IEEE/ACM 음성 및 언어 처리 트랜잭션, vol. 26, no. 10, pp. 1702-1726, 2018. 
[6] P. C. Loizou, 음성 개선: 이론과 실제, CRC Press, Inc., 미국, 2판, 2013. 
[7] C. Valentini-Botinhao, X. Wang, S. Takaki 및 J. Yamagishi, "잡음 강인 텍스트 음성 변환을 위한 RNN 기반 음성 개선 방법 연구," 제9회 ISCA 음성 합성 워크샵 (SSW), 2016, pp. 146-152. 
[8] K. Kinoshita 등, "리버브 챌린지 개요: 강인한 환청 음성 처리 연구의 최신 동향과 남은 과제," 신호 처리에 대한 진보 저널, vol. 7, no. 01, pp. 1-19, 2016. 
[9] J. Barker, R. Marxer, E. Vincent 및 S. Watanabe, "세 번째 'CHiME' 음성 분리 및 인식 챌린지: 데이터셋, 과제 및 기준선," IEEE 자동 음성 인식 및 이해 워크샵 (ASRU), 2015, pp. 504-511. 
[10] H. Dubey 등, "ICASSP 2022 심층 잡음 억제 챌린지," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 2022. 
[11] S.-W. Fu, C.-F. Liao, Y. Tsao 및 S. D. Lin, "음성 개선을 위한 MetricGAN: 측정 점수 최적화를 위한 생성적 적대 신경망," 기계 학습 국제 학회, PMLR, 2019, pp. 2031-2041. 
[12] D. Yin, C. Luo, Z. Xiong 및 W. Zeng, "PHASEN: 위상 및 고조파를 고려한 음성 개선 네트워크," 인공지능 AAAI 학회 논문집, vol. 34, no. 05, 2020, pp. 9458-9465. 
[13] G. Yu 등, "단일 채널 음성 개선을 위한 이중 분기 주의-주의 트랜스포머," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 2022, pp. 7847-7851.

[14] S. Pascual, A. Bonafonte 및 J. Serra, "SEGAN: 음성 개선 생성적 적대 신경망," Interspeech 논문집, 2017, pp. 3642-3646.
[15] C. Macartney 및 T. Weyde, "Wave-U-Net을 사용한 개선된 음성 개선," arXiv, vol. abs/1811.11307, 2018.
[16] K. Wang, B. He 및 W. P. Zhu, "시간 영역에서의 음성 개선을 위한 두 단계 변압기 기반 신경망인 TSTNN," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2021, pp. 7098-7102.
[17] A. Defossez, G. Synnaeve 및 Y. Adi, "파형 영역에서의 실시간 음성 개선," Interspeech 논문집, 2020, pp. 3291-3295.
[18] E. Kim 및 H. Seo, "Conformer를 사용한 시간 영역 음성 개선인 SE-Conformer," Interspeech 논문집, 2021, pp. 2736-2740.
[19] S. Abdulatif 등, "생성적 적대 신경망을 통한 시간-주파수 음성 소음 제거인 AeGAN," 28회 유럽 신호 처리 회의 (EUSIPCO), 2020, pp. 451-455.
[20] S. Abdulatif 등, "음성 개선을 위한 교차 도메인 손실 조사," 29회 유럽 신호 처리 회의 (EUSIPCO), 2021, pp. 411-415.
[21] D. S. Williamson, Y. Wang 및 P. Wang, "단일 음성 분리를 위한 복소 비율 마스킹," IEEE 음향, 음성 및 언어 처리 트랜잭션, vol. 24, no. 3, pp. 483-492, 2016.
[22] K. Tan 및 D. Wang, "단일 음성 개선을 위한 합성곱 순환 네트워크와 복소 스펙트럼 매핑," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2019, pp. 6865-6869.
[23] Z. Q. Wang, G. Wichern 및 J. Le Roux, "음성 분리에서 크기와 위상 간 보상에 대한 연구," IEEE 신호 처리 레터, vol. 28, pp. 2018-2022, 2021.
[24] A. Li, C. Zheng, L. Zhang 및 X. Li, "단일 채널 음성 개선을 위한 협업 학습 프레임워크인 Glance and gaze," Applied Acoustics, vol. 187, 2022.
[25] A. Vaswani 등, "Attention is all you need," 신경 정보 처리 시스템의 진보, vol. 30, 2017.
[26] F. Dang, H. Chen 및 P. Zhang, "시간 영역에서의 실시간 음성 개선을 위한 이중 경로 변압기 기반 전체 및 하위 대역 융합 네트워크인 DPT-FSNet," arXiv, vol. abs/2104.13002, 2021.
[27] A. Gulati 등, "음성 인식을 위한 합성곱 증강 변압기인 Conformer," Interspeech 논문집, 2020, pp. 5036-5040.
[28] S. Chen 등, "Conformer를 사용한 연속 음성 분리," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2021, pp. 5749-5753.
[29] J. Chen, Q. Mao 및 D. Liu, "단일 음성 분리를 위한 직접적인 문맥 인식 모델링을 위한 이중 경로 변압기 네트워크," Interspeech 논문집, 2020, pp. 2642-2646.
[30] H. Purwins 등, "오디오 신호 처리를 위한 심층 학습," IEEE 신호 처리 선정 주제 저널, vol. 13, no. 2, pp. 206-219, 2019.
[31] D. Michelsanti 등, "기반으로 한 오디오-시각 음성 개선 및 분리에 대한 개요," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, vol. 29, pp. 1368-1396, 2021.
[32] D. Wang 및 J. Lim, "음성 개선에서 위상의 중요성," IEEE 음향, 음성 및 신호 처리 트랜잭션, vol. 30, no. 4, pp. 679-681, 1982.
[33] K. Paliwal, K. Wójcicki 및 B. Shannon, "음성 개선에서 위상의 중요성," 음성 통신, vol. 53, no. 4, pp. 465-494, 2011.
[34] D. Rethage, J. Pons 및 X. Serra, "음성 소음 제거를 위한 Wavenet," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2018, pp. 5069-5073.
[35] S. W. Fu 등, "완전 합성곱 신경망에 의한 직접 평가 지표 최적화를 위한 파형 발화 개선," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, vol. 26, no. 9, pp. 1570-1584, 2018.
[36] A. Pandey 및 D. Wang, "시간 영역에서의 실시간 음성 개선을 위한 시간적 합성곱 신경망인 TCNN," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2019, pp. 6875-6879.
[37] H. Phan 등, "음성 개선을 위한 GAN 개선," IEEE 신호 처리 레터, vol. 27, pp. 1700-1704, 2020.
[38] S. Pascual, J. Serra 및 A. Bonafonte, "생성적 적대 신경망을 사용한 일반화된 음성 개선을 향하여," Interspeech 논문집, 2019, pp. 1791-1795.
[39] C. Donahue, B. Li 및 R. Prabhavalkar, "강건한 음성 인식을 위한 생성적 적대 신경망을 사용한 음성 개선 탐색," IEEE 음향, 음성 및 신호 처리 국제 회의 (ICASSP), 2018, pp. 5024-5028.
[40] D. Michelsanti 및 Z. H. Tan, "음성 개선 및 잡음 강인한 화자 검증을 위한 조건부 생성적 적대 신경망," Interspeech 논문집, 2017, pp. 2008-2012.
[41] Z. Meng 등, "음성 개선을 위한 적대적 특징 매핑," Interspeech 논문집, 2018, pp. 3259-3263.

[42] Y. Luo와 N. Mesgarani, "Conv-TasNet: 음성 분리를 위한 이상적인 시간-주파수 크기 마스킹을 능가하는 기술," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 1256-1266, 2019.
[43] S. Rickard와 O. Yilmaz, "음성의 근사 W-분리 직교성에 대하여," IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2002, pp. 529-532.
[44] A. S. Bregman, "Auditory scene analysis: 소리의 지각적 조직," MIT Press, 1994.
[45] O. Yilmaz와 S. Rickard, "시간-주파수 마스킹을 통한 음성 혼합물의 블라인드 분리," IEEE Transactions on Signal Processing, vol. 52, no. 7, pp. 1830-1847, 2004.
[46] Y. Wang와 D. Wang, "분류 기반 음성 분리의 확장을 향하여," IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7, pp. 1381-1390, 2013.
[47] Y. Wang와 D. Wang, "청각 장애인 청취자를 위한 잡음에서 음성 인식을 개선하기 위한 알고리즘," The Journal of the Acoustical Society of America, vol. 134, no. 4, pp. 3029-3038, 2013.
[48] C. Hummersone, T. Stokes, T. Brookes, "계산적 청각적 장면 분석의 목표로서 이상적인 비율 마스크에 대하여," Blind Source Separation: Advances in Theory, Algorithms and Applications, Springer, 2014, pp. 349-368.
[49] S. Srinivasan, N. Roman, D. Wang, "로버스트한 음성 인식을 위한 이진 및 비율 시간-주파수 마스크," Speech Communication, vol. 48, no. 11, pp. 1486-1501, 2006.
[50] A. Narayanan와 D. Wang, "로버스트한 음성 인식을 위한 심층 신경망을 이용한 이상적인 비율 마스크 추정," IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 7092-7096.
[51] Y. Wang, A. Narayanan, D. Wang, "지도된 음성 분리를 위한 훈련 목표에 대하여," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 12, pp. 1849-1858, 2014.
[52] K. Han 등, "음향의 스펙트럼 매핑을 학습하는 음성 감쇠 및 노이즈 제거," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 982-992, 2015.
[53] Z.Q. Wang, J. LeRoux, D. Wang, J.R. Hershey, "펼쳐진 반복적 위상 복원을 이용한 엔드 투 엔드 음성 분리," Proceedings of Interspeech, 2018, pp. 2708-2712.
[54] Z. Q. Wang, K. Tan, D. Wang, "스피커 분리를 위한 트리고네트릭 관점의 심층 학습 기반 위상 복원," IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 71-75.
[55] Y. Zhao, Z. Q. Wang, D. Wang, "잡음-울림이 있는 음성 개선을 위한 두 단계 심층 학습," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 1, pp. 53-62, 2019.
[56] H. Erdogan, J.R. Hershey, S. Watanabe, J. LeRoux, "깊은 순환 신경망을 이용한 위상 감지 및 인식 강화 음성 분리," IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 708-712.
[57] Y. Hu 등, "DCCRN: 위상에 대한 인식을 위한 깊은 복소 합성 재귀 신경망을 이용한 음성 개선," Proceedings of Interspeech, 2020, pp. 2472-2476.
[58] G. Yu 등, "DBT-Net: 단일 채널 음성 개선을 위한 이중 분기 연합 크기 및 위상 추정과 어텐션 인 어텐션 변환기," arXiv, vol. abs/2202.07931, 2022.
[59] A. Li 등, "두 개의 머리가 하나보다 낫다: 단일 채널 음성 개선을 위한 두 단계 복소 스펙트럼 매핑 접근 방식," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1829-1843, 2021.
[60] A. Li 등, "목표 분리를 위한 동시적인 노이즈 제거 및 울림 제거 프레임워크," Proceedings of Interspeech, 2021, pp. 2801-2805.
[61] K. Tan와 D. Wang, "단일 채널 음성 개선을 위한 게이트드 컨볼루션 재귀 신경망을 이용한 복소 스펙트럼 매핑 학습," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 380-390, 2020.
[62] H. Kuttruff, "Room Acoustics," CRC Press, 6th edition, 2016.
[63] J. Bradley, H. Sato, M. Picard, "방에서의 음성을 위한 초기 반사의 중요성에 대하여," The Journal of the Acoustical Society of America, vol. 113, no. 6, pp. 3233-3244, 2003.
[64] T. J. Schultz, "울림 방에서의 확산," Journal of Sound and Vibration, vol. 16, no. 1, pp. 17-28, 1971.
[65] D. Gelbart와 N. Morgan, "원거리 자동 음성 인식에서의 잡음과 울림 처리," 7th International Conference on Spoken Language Processing (ICSLP), 2002, pp. 2185-2188.
[66] Y. Luo와 N. Mesgarani, "시간 영역 오디오 분리 네트워크를 이용한 실시간 단일 채널 울림 제거 및 분리," Proceedings of Interspeech, 2018, pp. 342-346.
[67] N. Roman과 J. Woodruff, "이상적인 이진 마스킹을 통한 울림이 있는 잡음이 섞인 음성의 이해도," The Journal of the Acoustical Society of America, vol. 130, no. 4, pp. 2153-2161, 2011.
[68] T. May와 T. Gerkmann, "이진 마스크 추정을 위한 지도 학습의 일반화," 14th International Workshop on Acoustic Signal Enhancement (IWAENC), pp. 154-158, 2014.

[69] Z. Jin과 D. Wang, "회음이 있는 음성의 단일 선별에 대한 지도 학습 접근 방식," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP)에서, 2007, pp. 921-924.
[70] Y. Zhao, D. Wang, I. Merks 및 T. Zhang, "잡음과 회음이 있는 음성의 DNN 기반 개선," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP)에서, 2016, pp. 6525-6529.
[71] X. L. Zhang과 D. Wang, "단일 선별을 위한 심층 앙상블 학습 방법," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, 제 24권, 제 5호, pp. 967-977, 2016.
[72] X. Li, J. Li 및 Y. Yan, "잡음과 회음이 있는 조건에서의 단일 선별을 위한 심층 신경망을 사용한 이상적인 비율 마스크 추정," Interspeech 논문집에서, 2017, pp. 1203-1207.
[73] D. S. Williamson과 D. Wang, "복소 비율 마스크를 사용한 음성 회음 제거 및 노이즈 제거," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP)에서, 2017, pp. 5590-5594.
[74] D. S. Williamson과 D. Wang, "음성 회음 제거 및 노이즈 제거를 위한 복소 도메인에서의 시간-주파수 마스킹," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, 제 25권, 제 7호, pp. 1492-1501, 2017.
[75] V. Kothapally와 J. H. L. Hansen, "복소 시간-주파수 마스킹을 통한 생성적 적대 신경망을 사용한 단일 선별 음성 회음 제거," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, 제 30권, pp. 1600-1613, 2022.
[76] O. Ernst, S. E. Chazan, S. Gannot 및 J. Goldberger, "완전 합성곱 신경망을 사용한 음성 회음 제거," 26th European Signal Processing Conference (EUSIPCO)에서, 2018, pp. 390-394.
[77] V. Kothapally 등, "최적으로 평활화된 스펙트럼 매핑을 사용한 단일 선별 음성 회음 제거를 위한 스킵 컨볼루션 신경망," Interspeech 논문집에서, 2020, pp. 3935-3939.
[78] D. Ribas, J. Llombart, A. Miguel 및 L. Vicente, "와이드 잔여 신경망을 사용한 회음이 있는 신호를 위한 DeepSpeech 개선," arXiv, vol. abs/1901.00660, 2019.
[79] X. Xiao 등, "회음 도전 2014를 위한 NTU-ADSC 시스템," Reverberation Challenge Workshop 논문집에서, 2014.
[80] C. Dong, C. C. Loy, K. He 및 X. Tang, "깊은 합성곱 신경망을 사용한 이미지 초해상도," IEEE Pattern Analysis and Machine Intelligence 트랜잭션, 제 38권, 제 2호, pp. 295-307, 2016.
[81] J. Yu 등, "문맥적 주의를 사용한 생성적 이미지 보완," IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스에서, 2018, pp. 5505-5514.
[82] P. Ekstrand, "스펙트럼 밴드 복제에 의한 오디오 신호의 대역 확장," IEEE Benelux Workshop on Model Based Processing and Coding of Audio (MPCA) 논문집에서, 2002.
[83] V. Kuleshov, S. Z. Enam 및 S. Ermon, "신경망을 사용한 오디오 초해상도," ICLR (Workshop Track), 2017.
[84] T. Y. Lim 등, "오디오 초해상도를 위한 시간-주파수 네트워크," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP)에서, 2018, pp. 646-650.
[85] S. Birnbaum 등, "Feature-wise 변조를 사용하여 장거리 시퀀스 종속성을 포착하는 Temporal FiLM," Advances in Neural Information Processing Systems, vol. 32, 2019.
[86] N. C. Rakotonirina, "오디오 초해상도를 위한 Self-Attention," 31st IEEE International Workshop on Machine Learning for Signal Processing (MLSP)에서, 2021, pp. 1-6.
[87] H. Wang과 D. Wang, "강건한 음성 초해상도를 향해," IEEE/ACM 음향, 음성 및 언어 처리 트랜잭션, vol. 29, pp. 2058-2066, 2021.
[88] H. Liu 등, "음성 초해상도를 위해 필요한 것은 신경 보코더뿐," arXiv, vol. abs/2203.14941, 2022.
[89] S. Braun과 I. Tashev, "지도 학습 기반 음성 개선을 위한 손실 함수의 통합된 관점," 44th International Conference on Telecommunications and Signal Processing (TSP)에서, 2021, pp. 72-76.
[90] J. Lee, J. Skoglund, T. Shabestary 및 H. G. Kang, "시간 도메인에서의 실시간 음성 개선을 위한 위상 감지 합동 학습 알고리즘," IEEE Signal Processing Letters, vol. 25, no. 8, pp. 1276-1280, 2018.
[91] K. Wilson 등, "저지연 음성 개선을 위한 모델의 트레이드오프 탐색," 16th International Workshop on Acoustic Signal Enhancement (IWAENC)에서, 2018, pp. 366-370.
[92] A. Pandey와 D. Wang, "시간 도메인에서의 실시간 음성 개선을 위한 확장된 합성곱 신경망과 확장된 합성곱 신경망을 사용한 밀도 연결 신경망," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP)에서, 2020, pp. 6629-6633.
[93] D. Ulyanov, A. Vedaldi 및 V. Lempitsky, "스타일화를 위한 빠른 인스턴스 정규화," arXiv, vol. abs/1607.08022, 2016.
[94] K. He, X. Zhang, S. Ren 및 J. Sun, "렉티파이어에 대한 깊은 탐구: ImageNet 분류에서 인간 수준의 성능을 능가," IEEE International Conference on Computer Vision (ICCV)에서, 2015, pp. 1026-1034.
[95] W. Shi 등, "효율적인 하위 픽셀 합성곱 신경망을 사용한 실시간 단일 이미지 및 비디오 초해상도," IEEE Computer Vision and Pattern Recognition (CVPR)에서, 2016, pp. 1874-1883.
[96] A. Rix, J. Beerends, M. Hollier 및 A. Hekstra, "음성 품질의 지각적 평가 (PESQ)-전화망 및 코덱의 음성 품질 평가를 위한 새로운 방법," IEEE International Conference 14

[97] C. H. Taal, R. C. Hendriks, R. Heusdens and J. Jensen, "시간-주파수 가중 잡음이 있는 음성에 대한 단기 목표 명료도 측정," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 2010, pp. 4214-4217.
[98] X. Mao 등, "최소 제곱 생성적 적대 신경망," IEEE 컴퓨터 비전 국제 학회 (ICCV), 2017, pp. 2813-2821.
[99] C. Veaux, J. Yamagishi 및 S. King, "음성 은행 말뭉치: 대규모 지역 사투리 음성 데이터베이스의 설계, 수집 및 데이터 분석," 동양 국제 음성 데이터베이스 및 평가 COCOSDA, 2013, pp. 1-4.
[100] J. Thiemann, N. Ito 및 E. Vincent, "다양한 환경의 다채널 음향 잡음 데이터베이스 (DEMAND): 다채널 환경 잡음 녹음의 데이터베이스," 음향학회 회의록, vol. 19, no. 1, 미국 음향학회, 2013.
[101] T. Robinson 등, "WSJCAMO: 대용량 어휘 연속 음성 인식을 위한 영국 영어 음성 말뭉치," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 1995, pp. 81-84.
[102] M. Lincoln, I. McCowan, J. Vepa 및 H. K. Maganti, "다채널 월스트리트 저널 오디오 비주얼 말뭉치 (MC-WSJ-AV): 사양 및 초기 실험," IEEE 자동 음성 인식 및 이해 워크샵, 2005, pp. 357-362.
[103] J. Yamagishi, C. Veaux 및 K. MacDonald, "CSTR VCTK Corpus: CSTR 음성 복제 도구킷 (버전 0.92)을 위한 영어 다중 화자 말뭉치," 에든버러 대학교. 음성 기술 연구 센터 (CSTR), 2019.
[104] T. A. Hsieh 등, "음성 개선을 위한 폰 강화된 지각적 손실을 사용한 지각적 품질 개선," arXiv, vol. abs/2010.15174, 2020.
[105] S. W. Fu 등, "MetricGAN+ : 음성 개선을 위한 MetricGAN의 개선된 버전," Interspeech 논문집, 2021, pp. 201-205.
[106] I. Loshchilov 및 F. Hutter, "분리된 가중치 감쇠 정규화," arXiv, vol. abs/1711.05101, 2017.
[107] Y. Hu 및 P. C. Loizou, "음성 개선을 위한 목적적 품질 측정 평가," IEEE 오디오, 음성 및 언어 처리 트랜잭션, vol. 16, no. 1, pp. 229-238, 2008.
[108] P. Isola 등, "조건부 적대적 네트워크를 사용한 이미지 대 이미지 변환," IEEE 컴퓨터 비전 및 패턴 인식 (CVPR) 컨퍼런스, 2017, pp. 5967-5976.
[109] Z. Liu 등, "시프트된 윈도우를 사용한 계층적 비전 변환기인 Swin Transformer," IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 2021, pp. 10012-10022.
[110] N. Virag, "청각 시스템의 마스킹 특성을 기반으로 한 음성 개선," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 1995, pp. 796-799.
[111] H. Zheng 등, "소프트플러스 유닛을 사용한 심층 신경망 개선," 국제 신경망 학회 합동 컨퍼런스 (IJCNN), 2015.
[112] N. Kitawaki, H. Nagabuchi 및 K. Itoh, "저 비트율 음성 코딩 시스템을 위한 목적적 품질 평가," IEEE 통신 분야 선정 영역 저널, vol. 16, no. 2, pp. 242-248, 1988.
[113] J. Hansen 및 B. Pellom, "음성 개선 알고리즘을 위한 효과적인 품질 평가 프로토콜," 국제 음성 언어 처리 컨퍼런스 (ICSL), 1998, pp. 2819-2822.
[114] J. Tribolet, P. Noll, B. McDermott 및 R. Crochiere, "음성 파형 코더의 복잡성과 품질에 관한 연구," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 1978, pp. 586-590.
[115] T. H. Falk, C. Zheng 및 W. Chan, "감쇠된 및 감쇠되지 않은 음향의 비침입적 품질 및 명료도 측정," IEEE 오디오, 음성 및 언어 처리 트랜잭션, vol. 18, no. 7, pp. 1766-1774, 2010.
[116] R. C. Streijl, S. Winkler 및 D. S. Hands, "평균 의견 점수 (MOS) 재고: 방법과 응용, 한계와 대안," 멀티미디어 시스템, vol. 22, no. 2, pp. 213-227, 2016.
[117] J. H. Friedman, "다변량 적응형 회귀 스플라인," 통계학 연구소, vol. 19, no. 1, pp. 1-67, 1991.
[118] ITU-T 권고 P.835, "잡음 억제 알고리즘을 포함하는 음성 통신 시스템을 평가하기 위한 주관적 테스트 방법론," 국제 전기 통신 연합, 제네바, 2003.
[119] D. Klatt, "임계 대역 스펙트럼에서 인지적인 음소 거리 예측: 첫 번째 단계," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 1982, pp. 1278-1281.
[120] S. W. Fu, Y. Tsao, H. T. Hwang 및 H. M. Wang, "Quality-Net: BLSTM을 기반으로 한 비침입적 음성 품질 평가 모델," Interspeech 논문집, 2018, pp. 1873-1877.
[121] A. R. Avila 등, "신경망을 사용한 비침입적 음성 품질 평가," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 2019, pp. 631-635.
[122] A. A. Catellier 및 S. D. Voran, "Wawenets: 좁은 대역폭 및 넓은 대역폭 음성 품질 추정을 위한 비참조 컨볼루션 파형 기반 접근 방식," IEEE 음향, 음성 및 신호 처리 국제 학회 (ICASSP), 2020, pp. 331-335.

[123] J. Serrà, J. Pons and S. Pascual, "SESQA: 반지도 학습을 이용한 음질 평가," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), 2021, pp. 381-385.
[124] C. K. A. Reddy, V. Gopal and R. Cutler, "DNSMOS: 잡음 억제기 평가를 위한 비침입적 지각적 목표 음질 측정," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), 2021, pp. 6493-6497.
[125] C. K. A. Reddy, V. Gopal and R. Cutler, "DNSMOS P.835: 잡음 억제기 평가를 위한 비침입적 지각적 목표 음질 측정," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), 2022, pp. 886-890.
[126] J. F. Gemmeke et al., "Audio Set: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터셋," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), 2017, pp. 776-780.
[127] T. Ko et al., "강인한 음성 인식을 위한 간접 음향 데이터 증강에 관한 연구," IEEE 국제 음향, 음성 및 신호 처리 학회 (ICASSP), 2017, pp. 5220-5224.
[128] M. Krawczyk and T. Gerkmann, "단일 채널 음성 개선을 위한 음성 STFT 위상 복원," IEEE 음향, 음성 및 언어 처리 트랜잭션, 제 22권, 제 12호, pp. 1931-1940, 2014.

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

시끄러운

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

(b) 메트릭GAN+

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

(c) DEMUCS

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

(d) 페이즌

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

(e) PFPL
(e) PFPL

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

CMGAN

0.5 1 1.5
−101
시간 [초]
값
[
A .
U . ]

깨끗한

0.5 1   1.5
02468
시간 [초]
주
파
수
[
k
H z ]

시끄러운

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c
y
[
k
H z ]

(i) 메트릭GAN+

0.5 1   1.5
02468
시간 [초]
주
파
수
[
k
H z ]

(j) DEMUCS

0.5 1 1.5
02468
시간 [초]
주파수 [kHz]

(k) 페이즌

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c y
[
k H z ]

PFPL

0.5 1   1.5
02468
시간 [초]
F r e
q
u e
n c y
[
k H z ]

CMGAN

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c y
[
k H z ]

깨끗한

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c y
[ k H z ]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

시끄러운

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c y
[ k H z ]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

MetricGAN+

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c y
[ k H z ]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

(q) DEMUCS
(q) 디뮤크스

0.5 1 1.5
02468
시간 [초]
F r e
q
u e
n c y
[ k H z ]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

(r) 페이즌

0.5 1 1.5
02468
시간 [초]
주
파
수
[ k H z ]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

0.5      1        1.5
영점 오      일        일점 오

시간 [초]
주파수 [킬로헤르츠]

죄송합니다, 그 문장을 번역할 수 없습니다.

0.5 1   1.5
02468
시간 [초]
주파수 [kHz]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

CMGAN (시엠간)

0.5 1 1.5
02468
시간 [초]
주
파
수
[ k H z ]

0.5      1        1.5
영점 오      일        일점 오

시간 [초]
주파수 [킬로헤르츠]

0.5       1       1.5
영점 오       일       일점 오

시간 [초]
주파수 [킬로헤르츠]

깨끗한

그림 9: SNR = 0 dB에서 광대역 카페 잡음(DEMAND 데이터셋)에서 주관적 접근 방식의 시각화. (a-g)는 시간 영역 신호를 나타내고, (h-n)은 dB로 표시된 TF-크기 표현이며, (o-u)는 주어진 TF-위상 표현의 재구성된 BPD입니다. ( )와 ( )는 각각 시간 및 TF-크기 표현의 왜곡을 나타냅니다.

0.5 1 1.5 2 2.5 3 3.5
−101
시간 [초]
값
[
A .
U . ]

시끄러운

0.5 1 1.5 2 2.5 3 3.5
−101
시간 [초]
값
[
A .
U . ]

(b) 메트릭GAN+

0.5 1 1.5 2 2.5 3 3.5
−101
시간 [초]
값
[
A .
U . ]

(c) DEMUCS

0.5 1 1.5 2 2.5 3 3.5
마이너스 일 점 오
영 일
영 일

시간 [초]
값
[
A .
U . ]

(d) 페이즌

0.5 1 1.5 2 2.5 3 3.5
마이너스 일 점 오
영 일
영 일

시간 [초]
값
[
A .
U . ]

(e) PFPL
(e) PFPL

0.5 1 1.5 2 2.5 3 3.5
−101
시간 [초]
값
[
A .
U . ]

CMGAN

0.5 1 1.5 2 2.5 3 3.5
마이너스 일 점 오
영 일
영 일

시간 [초]
값
[
A .
U . ]

깨끗한

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

시끄러운

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

(i) 메트릭GAN+

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

(j) DEMUCS

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

(k) 페이즌

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

PFPL

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

CMGAN

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

깨끗한

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5 1 1.5 2 2.5 3 3.5
영점 오 일 오 십 오 이 삼 오 삼 십 오

02468
영 이 사 육 팔

시간 [초]
주파수 [kHz]

시끄러운

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5   1  1.5   2  2.5  3   3.5
영점 오   일점 오   일점 오   이점 오   이점 오   삼점 오   삼점 오
02468
영 이 사 육 팔

시간 [초]
주파수 [kHz]

MetricGAN+

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5  1   1.5  2  2.5   3  3.5
영점 오  일  일점 오  이  삼  삼점 오 

시간 [초]
주파수 [킬로헤르츠]

(q) DEMUCS
(q) 디뮤크스

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5   1  1.5   2  2.5   3  3.5
영점 오   일점 오   일점 오   이점 오   이점 오   삼점 오   삼점 오
영 이 사 육 팔

시간 [초]
주파수 [kHz]

(r) 페이즌

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5 1 1.5 2 2.5 3 3.5
영점 오 일점 오 일점 오 일점 오 일점 오 일점 오 일점
02468
영 이사륙팔

시간 [초]
주파수 [킬로헤르츠]

죄송합니다, 그 문장을 번역할 수 없습니다.

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5  1   1.5  2   2.5  3   3.5
영점 오  일  일점 오  이  삼  삼점 오

시간 [초]
주파수 [킬로헤르츠]

CMGAN (시엠간)

0.5 1 1.5 2 2.5 3 3.5
02468
시간 [초]
주파수 [kHz]

0.5  1   1.5  2   2.5  3   3.5
영점 오  일  일점 오  이  삼  삼점 오

시간 [초]
주파수 [kHz]

깨끗한

그림 10: SNR = 3 dB에서 좁은 대역 도어벨 소음 (Freesound 데이터셋)에 대한 주관적 접근 방식의 시각화. (a-g)는 시간 영역 신호를 나타내며, (h-n)은 dB로 표시된 TF-크기 표현이고, (o-u)는 주어진 TF-위상 표현의 재구성된 BPD입니다. ( )와 ( )는 각각 시간 및 TF-크기 표현에서의 왜곡을 나타냅니다.

