BART: 자연어 생성, 번역 및 이해를 위한 노이즈 제거 시퀀스 투 시퀀스 사전 훈련

마이크 루이스*, 이인한 리우*, 나만 고얼*, 마르잔 가즈비니네자드,
압델라만 모하메드, 오머 레비, 베스 스토야노프, 루크 제틀모이어

페이스북 인공지능
{마이크 루이스, 이인한, 나만}@fb.com

요약

우리는 BART라는 노이즈 제거 오토인코더를 제시합니다. 이는 시퀀스-투-시퀀스 모델의 사전 훈련을 위해 사용됩니다. BART는 (1) 임의의 노이징 함수를 사용하여 텍스트를 손상시키고, (2) 원래 텍스트를 재구성하는 모델을 학습함으로써 훈련됩니다. BART는 표준 Transformer 기반의 신경 기계 번역 아키텍처를 사용하며, 이는 BERT(양방향 인코더로 인해), GPT(왼쪽에서 오른쪽으로 디코더로 인해) 및 다른 최근 사전 훈련 방법들을 일반화한 것으로 볼 수 있습니다. 우리는 여러 가지 노이징 접근 방식을 평가하였고, 원래 문장의 순서를 임의로 섞거나 텍스트 일부를 단일 마스크 토큰으로 대체하는 새로운 인-필링 체계를 사용함으로써 최상의 성능을 얻었습니다. BART는 텍스트 생성을 위해 세밀하게 조정되었을 때 특히 효과적이지만, 이해 작업에도 잘 작동합니다. BART는 GLUE와 SQuAD에서 RoBERTa와 유사한 훈련 자원을 사용하여 성능을 일치시키며, 요약 대화, 질문 응답 및 요약 작업의 새로운 최고 성과를 달성하며, 최대 6 ROUGE의 이득을 얻습니다. BART는 대상 언어 사전 훈련만으로 기계 번역의 역 번역 시스템에 비해 1.1 BLEU의 향상을 제공합니다. 또한, BART 프레임워크 내에서 다른 사전 훈련 방법을 복제하는 실험을 보고하여 최종 작업 성능에 가장 영향을 미치는 요소를 더 정확하게 측정하였습니다.

1 소개

자기 지도 학습 방법은 다양한 NLP 작업에서 주목할만한 성과를 이뤄냈습니다 (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019). 가장 성공적인 접근 방식은 마스크된 언어 모델의 변형으로, 일부 단어가 마스크 처리된 텍스트를 재구성하는 데 훈련된 노이즈 제거 오토인코더입니다. 최근 연구에서는 마스크된 토큰의 분포를 개선함으로써 성과를 향상시키는 것이 입증되었습니다 (Joshi et al., 2019), 또한 단어의 순서를 개선함으로써도 성과를 향상시킬 수 있습니다.

마스크된 토큰은 예측됩니다 (Yang 등, 2019) 그리고 마스크된 토큰을 대체하기 위한 사용 가능한 문맥 (Dong 등, 2019). 그러나 이러한 방법들은 일반적으로 특정 유형의 최종 작업 (예: 구간 예측, 생성 등)에 초점을 맞추어 적용 범위가 제한됩니다.
본 논문에서는 양방향 및 자기회귀 변환기를 결합한 모델인 BART를 제안합니다. BART는 매우 다양한 종류의 최종 작업에 적용 가능한 시퀀스-투-시퀀스 모델로 구성된 노이즈 제거 오토인코더입니다. 사전 훈련은 두 단계로 이루어집니다. (1) 텍스트는 임의의 노이즈 함수로 손상되고, (2) 시퀀스-투-시퀀스 모델이 원래 텍스트를 재구성하기 위해 학습됩니다. BART는 표준 Transformer 기반의 신경 기계 번역 아키텍처를 사용합니다. 이 아키텍처는 BERT (양방향 인코더로 인해), GPT (왼쪽에서 오른쪽으로 디코더로 인해) 및 다른 최근 사전 훈련 방법들을 일반화할 수 있다고 볼 수 있습니다 (그림 1 참조).
이 설정의 주요 장점은 노이징의 유연성입니다. 원래 텍스트에 임의의 변형을 적용할 수 있으며, 길이를 변경하는 것도 가능합니다. 우리는 여러 가지 노이징 접근 방식을 평가하였고, 원래 문장의 순서를 임의로 섞거나 임의 길이의 텍스트 구간 (길이가 0인 경우 포함)을 단일 마스크 토큰으로 대체하는 새로운 인필링 체계를 사용함으로써 최상의 성능을 얻었습니다. 이 접근 방식은 BERT의 원래 단어 마스킹 및 다음 문장 예측 목표를 일반화시키며, 모델이 전체 문장 길이에 대해 더 많은 추론을 하고 입력에 대해 더 긴 범위의 변환을 수행하도록 합니다.
BART는 텍스트 생성을 위해 세밀하게 조정되었을 때 특히 효과적이지만 이해 작업에도 잘 작동합니다. GLUE (Wang 등, 2018) 및 SQuAD (Rajpurkar 등, 2016)에서 RoBERTa (Liu 등, 2019)와 유사한 훈련 자원을 사용하여 성능을 일치시키며, 요약 대화, 질문 응답 및 요약 작업의 새로운 최고 성능을 달성합니다. 예를 들어, XSum (Narayan 등, 2018)에서 이전 연구에 비해 6 ROUGE의 성능 향상을 보입니다.
BART는 세밀 조정에 대한 새로운 사고 방식을 열어줍니다. BART 모델 위에 몇 개의 추가 Transformer 레이어를 쌓은 기계 번역을 위한 새로운 체계를 제시합니다. 이러한 레이어는 외국어를 노이즈 처리된 언어로 번역하는 것을 학습합니다.

에이 _ 씨 _ 이
비   디

BERT: 무작위 토큰은 마스크로 대체되고 문서는 양방향으로 인코딩됩니다. 누락된 토큰은 독립적으로 예측되므로 BERT는 쉽게 생성에 사용될 수 없습니다.

자기회귀
디코더
에이 비 씨 디 이

<s> 에이 비 씨 디

GPT: 토큰은 자동 회귀적으로 예측되며, 이는 GPT가 생성에 사용될 수 있다는 것을 의미합니다. 그러나 단어는 왼쪽 문맥에만 의존할 수 있으므로 양방향 상호작용을 학습할 수 없습니다.

자기회귀
디코더
양방향
인코더

A B C D E

A _ B _ E      <s> A B C D
에이 _ 비 _ 이      <s> 에이 비 씨 디

BART: 인코더에 대한 입력은 디코더의 출력과 일치할 필요가 없으므로 임의의 노이즈 변환을 허용합니다. 여기에서는 텍스트 일부를 마스크 기호로 대체하여 문서를 손상시킵니다. 손상된 문서(왼쪽)는 양방향 모델로 인코딩되고, 원래 문서(오른쪽)의 가능성은 자기회귀 디코더로 계산됩니다. 세밀 조정을 위해 손상되지 않은 문서가 인코더와 디코더에 모두 입력되며, 디코더의 최종 은닉 상태에서 표현을 사용합니다.

그림 1: BART와 BERT (Devlin et al., 2019) 그리고 GPT (Radford et al., 2018)의 구조적 비교.

영어, BART를 통해 전파함으로써 BART를 사전 훈련된 대상 언어 모델로 사용합니다. 이 접근 방식은 WMT 루마니아어-영어 벤치마크에서 강력한 역번역 기계 번역 기준에 비해 1.1 BLEU로 성능을 향상시킵니다.
이러한 효과를 더 잘 이해하기 위해, 우리는 최근 제안된 다른 훈련 목표를 복제하는 소거 분석도 보고합니다. 이 연구는 데이터와 최적화 매개변수를 포함하여 전체적인 성능에 중요한 영향을 미치는 요소들을 신중하게 제어할 수 있도록 해줍니다 (Liu et al., 2019). 우리는 BART가 고려하는 모든 작업 범위에서 가장 일관되게 강력한 성능을 보여준다는 것을 발견했습니다.

2 모델

BART는 손상된 문서를 원래 문서로 매핑하는 데 사용되는 노이즈 제거 오토인코더입니다. 이는 손상된 텍스트에 대한 양방향 인코더와 왼쪽에서 오른쪽으로 자기 회귀 디코더로 구현되는 시퀀스-투-시퀀스 모델로 작동합니다. 사전 훈련을 위해 원래 문서의 음의 로그 우도를 최적화합니다.

2.1 건축

최근 연구에서 BART는 (Vaswani et al., 2017)의 표준 시퀀스-시퀀스 Transformer 아키텍처를 사용합니다. 다만, GPT를 따라 ReLU 활성화 함수를 GeLU(Hendrycks & Gimpel, 2016)로 수정하고 매개변수를 N(0,0.02)에서 초기화합니다. 기본 모델로는 인코더와 디코더에 각각 6개의 레이어를 사용합니다.

코더, 그리고 우리의 대형 모델에는 각각 12개의 레이어를 사용합니다. 이 아키텍처는 BERT에서 사용하는 것과 밀접한 관련이 있으며 다음과 같은 차이점이 있습니다: (1) 디코더의 각 레이어는 인코더의 최종 은닉 레이어에 대해 교차 어텐션을 수행합니다 (transformer sequence-to-sequence 모델과 동일); 그리고 (2) BERT는 단어 예측 전에 추가적인 피드포워드 네트워크를 사용하지만 BART는 그렇지 않습니다. 총적으로, BART는 동일한 크기의 BERT 모델보다 약 10% 더 많은 파라미터를 포함하고 있습니다.

2.2 BART 사전 훈련

BART는 문서를 손상시키고 그 후 재구성 손실 - 디코더의 출력과 원래 문서 사이의 교차 엔트로피 - 를 최적화함으로써 훈련됩니다. 특정 노이즈 체계에 맞추어진 기존의 노이즈 제거 오토인코더와 달리, BART는 어떤 유형의 문서 손상도 적용할 수 있습니다. 모든 소스 정보가 손실된 극단적인 경우에는 BART가 언어 모델과 동일합니다.
우리는 이전에 제안된 몇 가지 변형과 새로운 변형을 실험했지만, 다른 새로운 대안의 개발에 상당한 잠재력이 있다고 믿습니다. 우리가 사용한 변형은 아래에 요약되어 있으며, 예시는 그림 2에 나와 있습니다.

토큰 마스킹은 BERT (Devlin et al., 2019)를 따라 [MASK] 요소로 무작위 토큰을 샘플링하여 대체합니다.

토큰 삭제 무작위 토큰이 입력에서 삭제됩니다. 토큰 마스킹과는 달리, 모델은 어떤 위치가 누락된 입력인지 결정해야 합니다.
A B C . D E . A . C . E . A _ . D _ E .
A _C . _ E .                  C . D E . A B
문서 회전 토큰 마스킹

토큰 삭제                    텍스트 채움

D E . A B C .
문장 순열

그림 2: 우리가 실험하는 입력에 노이즈를 주기 위한 변환들입니다. 이러한 변환들은 조합될 수 있습니다.

텍스트 채움 작업에서는 여러 개의 텍스트 구간이 샘플링되며, 구간의 길이는 포아송 분포(λ = 3)에서 추출됩니다. 각 구간은 단일 [MASK] 토큰으로 대체됩니다. 길이가 0인 구간은 [MASK] 토큰의 삽입을 의미합니다. 텍스트 채움 작업은 SpanBERT (Joshi et al., 2019)에서 영감을 받았지만, SpanBERT는 다른 (클램프된 기하학적) 분포에서 구간 길이를 샘플링하고, 각 구간을 정확히 같은 길이의 [MASK] 토큰 시퀀스로 대체합니다. 텍스트 채움 작업은 모델이 구간에서 누락된 토큰의 수를 예측하는 방법을 학습시킵니다.

문장 순열 문서는 마침표를 기준으로 문장으로 나누어지며, 이러한 문장들은 무작위로 섞입니다.

문서 회전은 균일하게 선택된 토큰을 기준으로 문서를 회전시킵니다. 이 작업은 모델이 문서의 시작을 인식하는 것을 훈련시킵니다.

3. BART 세부 조정

BART가 생성한 표현은 하위 응용 프로그램에서 여러 가지 방식으로 사용될 수 있습니다.

3.1 시퀀스 분류 작업

시퀀스 분류 작업에서는 동일한 입력이 인코더와 디코더에 공급되며, 최종 디코더 토큰의 최종 은닉 상태가 새로운 다중 클래스 선형 분류기에 공급됩니다. 이 접근 방식은 BERT의 CLS 토큰과 관련이 있습니다. 그러나 우리는 추가 토큰을 끝에 추가하여 디코더의 토큰 표현이 완전한 입력의 디코더 상태에 참여할 수 있도록 합니다 (그림 3a).

3.2 토큰 분류 작업

토큰 분류 작업에 대해서는, 예를 들어 SQuAD의 답변 엔드포인트 분류와 같은 경우, 우리는 전체 문서를 인코더와 디코더에 입력하고, 디코더의 최상위 은닉 상태를 각 단어의 표현으로 사용합니다. 이 표현은 토큰을 분류하는 데 사용됩니다.

문서 회전 토큰 마스킹

토큰 삭제                    텍스트 채움

D E . A B C .
문장 순열

그림 2: 우리가 실험하는 입력에 노이즈를 주기 위한 변환들입니다. 이러한 변환들은 조합될 수 있습니다.

텍스트 채움 텍스트 스팬의 여러 개가 샘플링되며, 스팬 길이는 포아송 분포(λ = 3)에서 추출됩니다. 각 스팬은 단일 [MASK] 토큰으로 대체됩니다. 길이가 0인 스팬은 [MASK] 토큰의 삽입을 의미합니다. 텍스트 채움은 Span-BERT (Joshi et al., 2019)에서 영감을 받았지만, Span-BERT는 다른 (클램프된 기하학적) 분포에서 스팬 길이를 샘플링하고, 각 스팬을 정확히 같은 길이의 [MASK] 토큰 시퀀스로 대체합니다. 텍스트 채움은 모델이 스팬에서 누락된 토큰의 수를 예측하는 방법을 학습시킵니다.

문장 순열 문서는 마침표를 기준으로 문장으로 나누어지며, 이러한 문장들은 무작위로 섞입니다.

문서 회전은 균일하게 선택된 토큰을 기준으로 문서를 회전시킵니다. 이 작업은 모델이 문서의 시작을 인식하는 것을 훈련시킵니다.

3. BART 세부 조정

BART가 생성한 표현은 하위 응용 프로그램에서 여러 가지 방식으로 사용될 수 있습니다.

3.1 시퀀스 분류 작업

시퀀스 분류 작업에서는 동일한 입력이 인코더와 디코더에 공급되며, 최종 디코더 토큰의 최종 은닉 상태가 새로운 다중 클래스 선형 분류기에 공급됩니다. 이 접근 방식은 BERT의 CLS 토큰과 관련이 있습니다. 그러나 우리는 추가 토큰을 끝에 추가하여 디코더의 토큰 표현이 완전한 입력의 디코더 상태에 참여할 수 있도록 합니다 (그림 3a).

3.2 토큰 분류 작업

토큰 분류 작업에 대해서는, 예를 들어 SQuAD의 답변 엔드포인트 분류와 같은 경우, 우리는 전체 문서를 인코더와 디코더에 입력하고, 디코더의 최상위 은닉 상태를 각 단어의 표현으로 사용합니다. 이 표현은 토큰을 분류하는 데 사용됩니다.

3.3 시퀀스 생성 작업

original input sequence and rearranged to generate a
new sequence that captures the main ideas or answers
the question.

입력은 조작되지만, 이는 노이즈 제거 사전 훈련 목적과 밀접한 관련이 있습니다. 여기서 인코더 입력은 입력 시퀀스이며, 디코더는 자기회귀적으로 출력을 생성합니다.

3.4 기계 번역

우리는 또한 BART를 사용하여 영어로 번역하기 위한 기계 번역 디코더를 개선하는 것을 탐구합니다. 이전 연구인 Edunov et al. (2019)는 사전 훈련된 인코더를 통합함으로써 모델을 개선할 수 있다는 것을 보여주었지만, 사전 훈련된 언어 모델을 디코더에 사용하는 것으로부터의 이득은 제한되었습니다. 우리는 BART 모델 전체(인코더와 디코더 모두)를 기계 번역을 위한 단일 사전 훈련된 디코더로 사용할 수 있다는 것을 보여줍니다. 이를 위해 새로운 일련의 인코더 매개변수를 추가하여 이를 비트렉스트에서 학습합니다(그림 3b 참조).

더 정확히 말하면, 우리는 BART의 인코더 임베딩 레이어를 새로운 무작위로 초기화된 인코더로 대체합니다. 이 모델은 end-to-end로 훈련되며, 새로운 인코더를 외국어 단어를 BART가 영어로 변환할 수 있는 입력으로 매핑하도록 훈련합니다. 새로운 인코더는 원래 BART 모델과 별도의 어휘를 사용할 수 있습니다.

우리는 소스 인코더를 두 단계로 훈련합니다. 두 경우 모두 BART 모델의 출력에서 교차 엔트로피 손실을 역전파합니다. 첫 번째 단계에서는 대부분의 BART 매개변수를 고정하고, 무작위로 초기화된 소스 인코더, BART의 위치 임베딩, 그리고 BART의 인코더 첫 번째 레이어의 자기 어텐션 입력 투영 행렬만 업데이트합니다. 두 번째 단계에서는 일부 반복을 위해 모든 모델 매개변수를 훈련합니다.

4. 사전 훈련 목표 비교

BART는 이전 연구보다 훨씬 다양한 노이즈 스키마를 사전 훈련 중 지원합니다. 우리는 기본 크기 모델 (6개의 인코더 및 6개의 디코더 레이어, 숨겨진 크기 768)을 사용하여 다양한 옵션을 비교하고, §5에서 전체 대규모 실험을 고려할 대표적인 하위 작업 집합에서 평가합니다.

4.1 비교 목표

많은 사전 훈련 목표가 제안되었지만, 이들 사이의 공정한 비교는 어려웠습니다. 이는 훈련 데이터, 훈련 자원, 모델 간의 구조적 차이 및 세부 조정 절차의 차이 때문에 부분적으로 발생했습니다. 우리는 사전 훈련된 디코더와 사전 훈련된 인코더를 사용합니다.

레이블

에이 비 씨 디 이   <s> 에이 비 씨 디 이

BART를 분류 문제에 사용하기 위해, 동일한 입력이 인코더와 디코더에 공급되고, 최종 출력으로부터 얻은 표현이 사용됩니다.

임의로
초기화된 인코더

알파 베타 감마 델타 엡실론
미리 훈련된
디코더
미리 훈련된
인코더
에이 비 씨 디 이

<s> 에이 비 씨 디

(b) 기계 번역을 위해, 우리는 BART의 단어 임베딩을 대체하는 작은 추가 인코더를 학습합니다. 새로운 인코더는 겹치지 않는 어휘를 사용할 수 있습니다.

그림 3: 분류 및 번역을 위한 BART의 파인 튜닝.

최근에 제안된 강력한 사전 훈련 접근 방식을 다시 구현합니다. 식별 및 생성 작업에 대해 제안되었습니다. 가능한 한 사전 훈련 목표와 관련없는 차이를 제어하기 위해 노력합니다. 그러나 성능 향상을 위해 학습률과 레이어 정규화의 사용에 대해 작은 변경을 가합니다 (각 목표에 대해 별도로 조정). 참고로, 우리의 구현은 BERT의 게시된 숫자와 비교합니다. BERT는 책과 위키백과 데이터의 조합으로 1M 단계로 훈련되었습니다. 우리는 다음 접근 방식을 비교합니다.

언어 모델은 GPT (Radford et al., 2018)와 유사하게, 왼쪽에서 오른쪽으로 Transformer 언어 모델을 훈련시킵니다. 이 모델은 BART 디코더와 동일하며, 교차 어텐션은 없습니다.

XLNet를 기반으로 한 순열 언어 모델 (Yang et al., 2019)에서는 토큰의 1/6을 샘플링하여 자기회귀적으로 무작위 순서로 생성합니다. 다른 모델과 일관성을 유지하기 위해 XLNet의 상대적 위치 임베딩이나 세그먼트 간 어텐션은 구현하지 않습니다.

마스크된 언어 모델은 BERT를 따르며 (Devlin et al., 2019), 토큰의 15%를 [MASK] 기호로 대체하고 모델을 훈련시켜 원래의 토큰을 독립적으로 예측하도록 합니다.

멀티태스크 마스크드 언어 모델은 UniLM(Dong et al., 2019)과 같이 사용되며, 추가적인 셀프 어텐션 마스크로 언어 모델을 훈련합니다. 셀프 어텐션 마스크는 다음과 같은 비율로 무작위로 선택됩니다: 1/6은 왼쪽에서 오른쪽으로, 1/6은 오른쪽에서 왼쪽으로, 1/3은 언마스크 상태이며, 나머지 1/3은 처음 50%의 토큰이 언마스크 상태이고 나머지는 왼쪽에서 오른쪽으로 마스크가 적용됩니다.

마스크된 Seq-to-Seq는 MASS (Song et al., 2019)에서 영감을 받아 토큰의 50%를 포함하는 구간을 마스크하고, 시퀀스 대 시퀀스 모델을 훈련하여 마스크된 토큰을 예측합니다.

Permuted LM, Masked LM 및 Multitask에 대해
Masked LM의 경우, 우리는 두 개의 스트림 어텐션 (Yang et al., 2019)을 사용하여 시퀀스의 출력 부분의 가능도를 효율적으로 계산합니다 (왼쪽에서 오른쪽으로 단어를 예측하기 위해 출력에 대각선 자기 어텐션 마스크를 사용).

우리는 (1) 작업을 표준 시퀀스-시퀀스 문제로 처리하는 실험을 진행합니다. 여기서 인코더의 소스 입력과 디코더의 출력이 타겟입니다. 또는 (2) 디코더에서 소스를 타겟의 접두어로 추가하고 시퀀스의 타겟 부분에만 손실을 적용합니다. 우리는 BART 모델에 대해서는 전자가 더 잘 작동하고, 다른 모델에 대해서는 후자가 더 잘 작동하는 것을 발견했습니다. 우리의 모델들이 세부 조정 목표(인간 텍스트의 로그 우도)를 모델링하는 능력을 가장 직접적으로 비교하기 위해, 우리는 퍼플렉서티를 테이블 1에 보고합니다.

4.2 작업

SQuAD (Rajpurkar et al., 2016)는 Wikipedia 단락에 대한 추출형 질문 응답 과제입니다. 답변은 주어진 문서 맥락에서 추출된 텍스트 영역입니다. BERT (Devlin et al., 2019)와 유사하게, 우리는 BART의 인코더에 질문과 맥락을 연결하여 입력으로 사용하고, 또한 디코더에도 전달합니다. 이 모델에는 각 토큰의 시작과 끝 인덱스를 예측하는 분류기가 포함되어 있습니다.

MNLI (Williams et al., 2017)은 한 문장이 다른 문장을 함의하는지 예측하는 이진 분류 작업입니다.
미세 조정된 모델은 두 문장을 EOS 토큰이 추가된 상태로 연결하고, BART 인코더와 디코더에 전달합니다.
BERT와는 달리, EOS 토큰의 표현은 문장 간의 관계를 분류하는 데 사용됩니다.

ELI5 (Fanetal.,2019)은 장문 요약형 질문에 대한 답변 데이터셋입니다. 모델은 질문과 관련 문서를 연결한 것을 조건으로 답변을 생성합니다.

XSum (Narayan et al., 2018), 매우 추상적인 요약을 가진 뉴스 요약 데이터셋.

ConvAI2 (Dinan et al., 2019), 대화 응답 생성 작업은 맥락과 페르소나에 의존한다.

CNN/DM (Hermann et al., 2015)은 뉴스 요약 데이터셋입니다. 여기서의 요약은 일반적으로 원문과 밀접한 관련이 있습니다.

4.3 결과

결과는 표 1에 표시됩니다. 몇 가지 동향이 명확합니다:
모델                 SQuAD 1.1 MNLI ELI5 XSum ConvAI2 CNN/DM

F1    액  사람  사람   사람     사람

BERT 베이스 (Devlin et al., 2019) 88.5 84.3 - - - - -

마스크된 언어 모델 90.0 83.5 24.77 7.87 12.59 7.06
마스크된 Seq2seq 87.0 82.1 23.40 6.80 11.43 6.19
언어 모델 76.7 80.1 21.40 7.00 11.51 6.56
순열된 언어 모델 89.1 83.7 24.03 7.69 12.23 6.96
다중 작업 마스크된 언어 모델 89.2 82.4 23.73 7.50 12.39 6.74

BART 기반
토큰 마스킹 포함        90.4   84.1 25.05 7.08 11.73   6.10
토큰 삭제 포함        90.4   84.1 24.61 6.90 11.46   5.87
텍스트 채움 포함        90.8   84.0 24.26 6.61 11.05   5.83
문서 회전 포함        77.2   75.3 53.69 17.14 19.87 10.59
문장 섞기 포함        85.4   81.5 41.87 10.93 16.67  7.89
텍스트 채움 + 문장 섞기 포함 90.8 83.8 24.17 6.62 11.12 5.41

표 1: 사전 훈련 목표의 비교. 모든 모델은 비슷한 크기이며, 책과 위키백과 데이터의 조합으로 1백만 단계 동안 훈련되었습니다. 아래 두 블록의 항목들은 동일한 데이터와 코드 기반으로 훈련되었으며, 동일한 절차로 세밀 조정되었습니다. 두 번째 블록의 항목들은 이전 연구에서 제안된 사전 훈련 목표에 영감을 받았지만, 평가 목표에 초점을 맞추기 위해 단순화되었습니다 (§4.1 참조). 성능은 작업에 따라 상당히 다르지만, 텍스트 채움 기능을 갖춘 BART 모델들이 가장 일관되게 강력한 성능을 보여줍니다.

사전 훈련 방법의 성능은 작업에 따라 상당히 다릅니다. 사전 훈련 방법의 효과는 작업에 매우 의존적입니다. 예를 들어, 간단한 언어 모델은 최고의 ELI5 성능을 달성하지만, 최악의 SQUAD 결과를 보입니다.

토큰 마스킹은 사전 훈련 목표에서 중요합니다. 문서 회전 또는 문장 순열에 기반한 사전 훈련 방법은 독립적으로는 성능이 좋지 않습니다. 성공적인 방법은 토큰 삭제 또는 마스킹, 또는 자기-주의 마스크를 사용합니다. 생성 작업에서는 삭제가 마스킹보다 우수한 성능을 보입니다.

왼쪽에서 오른쪽으로의 사전 훈련은 생성을 개선합니다.
마스크된 언어 모델과 순열 언어 모델은 다른 모델들보다 생성에서 성능이 떨어지며, 우리가 고려하는 유일한 모델은 사전 훈련 중 왼쪽에서 오른쪽으로의 자기 회귀 언어 모델링을 포함하지 않습니다.

양방향 인코더는 SQuAD에서 중요하다. 이전 연구에서 언급된대로 (Devlin et al., 2019), 단순히 왼쪽에서 오른쪽으로 디코더만 사용하면 SQuAD에서 성능이 좋지 않다. 왜냐하면 분류 결정에 미래 문맥이 중요하기 때문이다. 그러나 BART는 양방향 레이어의 수를 절반으로 줄이면서도 유사한 성능을 달성한다.

사전 훈련 목적은 유일한 중요한 요소가 아닙니다. 우리의 Permuted Language Model은 XLNet (Yang et al., 2019)보다 성능이 떨어집니다. 이 차이 중 일부는 상대 위치 임베딩이나 세그먼트 수준의 재귀와 같은 다른 구조적 개선 사항을 포함하지 않았기 때문일 것입니다.

순수 언어 모델은 ELI5에서 가장 우수한 성능을 보입니다.
ELI5 데이터셋은 다른 작업들보다 훨씬 높은 혼란을 가지고 있으며, 다른 모델들이 BART보다 우수한 유일한 생성 작업입니다.
순수 언어 모델이 가장 우수한 성능을 보이며, 이는 BART가 입력에 느슨하게 제약을 받을 때 효과적이지 않다는 것을 시사합니다.

BART는 가장 일관되게 강력한 성능을 달성합니다. ELI5를 제외하고, 텍스트 채움을 사용하는 BART 모델은 모든 작업에서 잘 수행됩니다.

5 대규모 사전 훈련 실험

최근 연구에서는 사전 훈련이 대규모 배치 크기로 확장될 때 하류 성능이 크게 향상될 수 있다는 것을 보여주었다 (Yang et al., 2019; Liu et al., 2019) 그리고 말뭉치도 그렇다. BART의 성능을 이 조건에서 얼마나 잘 수행하는지 테스트하고 하류 작업에 유용한 모델을 만들기 위해, 우리는 RoBERTa 모델과 동일한 규모로 BART를 훈련시켰다.

5.1 실험 설정

우리는 각 인코더와 디코더에 12개의 레이어를 가진 큰 모델을 사전 훈련하며, 은닉 크기는 1024입니다. RoBERTa (Liu et al., 2019)를 따라서 배치 크기는 8000으로 설정하고, 모델을 500000 단계 동안 훈련합니다. 문서는 GPT-2 (Radford et al., 2019)와 동일한 바이트 페어 인코딩으로 토큰화됩니다. 섹션 §4의 결과를 기반으로 텍스트 채움과 문장 순열의 조합을 사용합니다. 각 문서에서 토큰의 30%를 마스킹하고 모든 문장을 순열합니다. 문장 순열은 SQuAD 1.1 SQuAD 2.0 MNLI SST QQP QNLI STS-B RTE MRPC CoLA에서 유의미한 가산 이득을 보여줍니다.
EM/F1    EM/F1  m/mm  Acc  Acc  Acc  Acc  Acc  Acc  Mcc

BERT    84.1/90.9 79.0/81.8 86.6/- 93.2 91.3 92.3 90.0 70.4 88.0 60.6
UniLM     -/-   80.5/83.4 87.0/85.9 94.5 - 92.7 - 70.9  -   61.1
XLNet   89.0/94.5 86.1/88.8 89.8/- 95.6 91.8 93.9 91.8 83.8 89.2 63.6
RoBERTa 88.9/94.6 86.5/89.4 90.2/90.2 96.4 92.2 94.7 92.4 86.6 90.9 68.0
BART    88.8/94.6 86.1/89.2 89.9/90.1 96.6 92.5 94.9 91.2 87.0 90.4 62.8

표 2: SQuAD 및 GLUE 작업에 대한 대형 모델 결과. BART는 RoBERTa와 XLNet과 비슷한 성능을 보여주며, BART의 단방향 디코더 레이어는 구별적인 작업에서 성능을 감소시키지 않는 것으로 나타납니다.

CNN/DailyMail    XSum
R1   R2   RL  R1   R2   RL

CNN/DailyMail    XSum
R1   R2   RL  R1   R2   RL

리드-3                   40.42 17.62 36.67 16.30 1.60 11.95
PTGEN (See et al., 2017) 36.44 15.66 33.42 29.70 9.21 23.24
PTGEN+COV (See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72
UniLM                    43.33 20.21 40.51 -  -    -
BERTSUMABS (Liu & Lapata, 2019) 41.72 19.39 38.76 38.76 16.33 31.15
BERTSUMEXTABS (Liu & Lapata, 2019) 42.13 19.60 39.18 38.81 16.50 31.27

BART                     44.16 21.28 40.90 45.14 22.27 37.25

표 3: 두 가지 표준 요약 데이터셋의 결과. BART는 두 가지 작업과 모든 메트릭에서 이전 작업을 능가하며, 더 추상적인 데이터셋에서 약 6 포인트의 이득을 얻습니다.

CNN/DM 요약 데이터셋에서 우리는 더 큰 사전 훈련 모델이 이 작업에서 더 잘 배울 수 있을 것이라고 가설을 세웠습니다. 모델이 데이터에 더 잘 맞도록 마지막 10%의 훈련 단계에서 드롭아웃을 비활성화했습니다. 우리는 Liu et al. (2019)의 사전 훈련 데이터와 동일한 데이터를 사용했으며, 이는 뉴스, 책, 이야기 및 웹 텍스트로 구성된 160Gb입니다.

5.2 차별적인 작업

표 2는 잘 연구된 SQuAD와 GLUE 작업(Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011)에서 BART의 성능을 몇 가지 최근 접근법과 비교합니다. 가장 직접적으로 비교 가능한 기준은 RoBERTa입니다. RoBERTa는 동일한 자원으로 사전 훈련되었지만 다른 목적을 가지고 있습니다. 전반적으로, BART는 대부분의 작업에서 모델 간에 작은 차이만 있어서 유사하게 수행됩니다. 이는 BART의 생성 작업에 대한 개선이 분류 성능을 희생하지 않고 이루어진다는 것을 시사합니다.

5.3 세대 과제

우리는 또한 여러 텍스트 생성 작업을 실험합니다.
BART는 입력 텍스트에서 출력 텍스트로의 표준 시퀀스-투-시퀀스 모델로 세밀하게 조정됩니다. 세밀하게 조정하는 동안 우리는 레이블 스무딩 교차 엔트로피 손실 (Pereyra et al., 2017)을 사용하며, 스무딩 매개변수는 0.1로 설정됩니다. 생성 중에는 빔 크기를 5로 설정하고, 빔 탐색에서 중복된 삼항식을 제거하며, 최소 길이, 최대 길이, 길이 패널티를 검증 세트에서 조정하여 모델을 튜닝합니다 (Fan et al., 2017).

ConvAI2
유효한 F1 유효한 PPL

Seq2Seq + 어텐션 16.02 35.07
최고 시스템 19.09 17.51
BART 20.72 11.85

표 4: BART는 대화 응답 생성에서 이전 연구를 능가합니다. Perplexities는 ConvAI2의 공식 토크나이저를 기준으로 재조정되었습니다.

요약
요약에 대한 최신 기술과 비교하기 위해, 우리는 두 가지 요약 데이터셋인 CNN/DailyMail과 XSum에 대한 결과를 제시합니다. 이 두 데이터셋은 각각 독특한 특성을 가지고 있습니다.
CNN/DailyMail의 요약은 원문 문장과 유사한 경향이 있습니다. 추출 모델은 여기에서 잘 작동하며, 첫 세 개의 원문 문장만 사용한 기준선도 매우 경쟁력이 있습니다. 그럼에도 불구하고, BART는 모든 기존 작업을 능가합니다.
반면, XSum은 매우 추상적이며, 추출 모델은 성능이 좋지 않습니다. BART는 BERT를 활용한 최고의 이전 작업보다 ROUGE 메트릭스 전체에서 약 6.0 포인트 더 높은 성능을 보여줍니다. 이는 이 문제에서의 중요한 발전을 나타냅니다. 질적으로, 샘플 품질은 높습니다 (§6 참조).

대화 응답 생성을 평가합니다.
CONVAI2 (Dinan et al., 2019)에서는 에이전트들이 이전 문맥과 텍스트로 지정된 페르소나에 기반하여 응답을 생성해야 합니다.
BART는 자동화된 두 가지 측정 지표에서 이전 연구보다 우수한 성과를 보입니다.
ELI5
R1  R2  RL

최상의 추출 23.5 3.1 17.5
언어 모델 27.8 4.7 23.1
Seq2Seq 28.3 5.1 22.8
Seq2Seq 멀티태스크 28.9 5.4 23.1
BART 30.6 6.2 24.3

표 5: BART는 도전적인 ELI5 추상적 질문 응답 데이터셋에서 최첨단 결과를 달성합니다. 비교 모델은 Fan et al. (2019)에서 가져온 것입니다.

RO: Te rog să traduci propozițiile în coreeană și să nu le scrii în afara traducerii.

EN: Please translate the sentences into Korean below and do not write down except the translation.

기준선 36.80
고정 BART 36.29
조정된 BART 37.96

표 6: WMT'16 RO-EN에서의 기준선과 BART의 성능(BLEU)을 보여줍니다. BART는 단일 언어 영어 사전 훈련을 사용하여 강력한 역번역(BT) 기준선을 개선합니다.

추상적 QA 우리는 최근에 제안된 ELI5 데이터셋을 사용하여 모델이 긴 자유 형식의 답변을 생성하는 능력을 테스트합니다. 우리는 BART가 최고의 이전 연구보다 1.2 ROUGE-L로 우수한 성능을 보여준다는 것을 발견했지만, 데이터셋은 질문에 의해 약하게 지정된 답변 때문에 여전히 도전적입니다.

5.4 번역

우리는 또한 Sennrich et al. (2016)의 역번역 데이터로 보강된 WMT16 루마니아어-영어의 성능을 평가했습니다. 우리는 6층의 트랜스포머 소스 인코더를 사용하여 루마니아어를 BART가 영어로 변환할 수 있는 표현으로 매핑합니다. 이는 §3.4에서 소개된 접근 방식을 따릅니다. 실험 결과는 표 6에 제시되었습니다. 우리는 기준선인 Transformer 아키텍처 (Vaswani et al., 2017)와 Transformer-large 설정 (기준선 행)과의 결과를 비교합니다. 우리 모델의 두 단계 성능을 고정된 BART 및 조정된 BART 행에서 보여줍니다. 각 행에 대해 우리는 역번역 데이터로 보강된 원래의 WMT16 루마니아어-영어에서 실험을 진행합니다. 우리는 빔 폭을 5로, 길이 패널티를 α = 1로 사용합니다. 예비 결과는 역번역 데이터 없이 우리의 접근 방식이 효과적이지 않았으며, 과적합에 취약함을 시사했습니다. 향후 연구에서는 추가적인 정규화 기술을 탐구해야 합니다.

6 질적 분석

BART는 요약 지표에서 큰 개선을 보여줍니다. 이전 최첨단 기술에 비해 최대 6점까지 개선되었습니다. 자동화된 지표 이상으로 BART의 성능을 이해하기 위해 우리는 질적으로 그 세대를 분석합니다.

표 7은 BART에 의해 생성된 예시 요약을 보여줍니다. 예시는 사전 훈련 말뭉치 생성 이후에 발행된 WikiNews 기사에서 가져온 것으로, 모델의 훈련 데이터에 기술된 사건의 가능성을 제거하기 위해 선택되었습니다. Narayan et al. (2018)을 따라, 우리는 요약하기 전에 기사의 첫 문장을 제거하여 문서의 쉬운 추출 요약이 없도록 합니다.
놀랍게도, 모델의 출력은 유창하고 문법적으로 영어입니다. 그러나 모델의 출력은 입력에서 복사된 구문이 거의 없는 추상적입니다. 출력은 일반적으로 사실적이며, 입력 문서와 배경 지식에서 지원하는 증거를 통합합니다 (예: 이름을 올바르게 완성하거나 PG&E가 캘리포니아에서 운영되는 것을 추론). 첫 번째 예시에서는 물고기가 지구 온난화로부터 암초를 보호한다는 것을 추론하는 것이 텍스트에서 비자명한 추론을 필요로 합니다. 그러나 이 작업이 Science에 발표되었다는 주장은 출처에서 지원되지 않습니다.
이 샘플들은 BART 사전 훈련이 자연어 이해와 생성의 강력한 조합을 배웠음을 보여줍니다.

7 관련 연구

언어 모델에 기반한 초기 사전 훈련 방법들이 있었습니다. GPT (Radford et al., 2018)는 오직 왼쪽 문맥만을 모델링하여 일부 작업에 문제가 있습니다. ELMo (Peters et al., 2018)는 왼쪽만 또는 오른쪽만을 표현하는 것을 연결하지만, 이러한 특징들 간의 상호작용을 사전 훈련하지 않습니다. Radford et al. (2019)는 매우 큰 언어 모델이 비지도 다중 작업 모델로 작용할 수 있다는 것을 입증했습니다.
BERT (Devlin et al., 2019)는 마스크된 언어 모델링을 도입하여 왼쪽과 오른쪽 문맥 단어들 간의 상호작용을 사전 훈련할 수 있게 했습니다. 최근의 연구는 더 긴 시간 동안 훈련하는 것(Liu et al., 2019), 계층 간 매개 변수를 연결하는 것(Lan et al., 2019), 그리고 단어 대신 구간을 마스킹하는 것(Joshi et al., 2019)으로 매우 강력한 성능을 얻을 수 있다는 것을 보여주었습니다. BERT의 경우 예측은 자기 회귀적으로 이루어지지 않아 생성 작업에 효과적이지 않습니다.
UniLM (Dong et al., 2019)은 BERT를 왼쪽 문맥만을 허용하는 마스크의 앙상블로 세밀하게 조정합니다. BART와 마찬가지로 UniLM은 생성 및 판별 작업에 모두 사용할 수 있습니다. 차이점은 UniLM의 예측이 조건부로 독립적이고, BART의 예측은 자기 회귀적이라는 것입니다. BART는 디코더가 항상 손상되지 않은 문맥으로 훈련되기 때문에 사전 훈련과 생성 작업 간의 불일치를 줄입니다.
MASS (Song et al., 2019)는 BART에 가장 유사한 모델일 것입니다. 연속된 토큰의 일부가 마스킹된 입력 시퀀스는 누락된 토큰으로 이루어진 시퀀스로 매핑됩니다. MASS는 판별 작업에는 효과적이지 않습니다. 왜냐하면 인코더와 디코더에는 겹치지 않는 토큰 집합이 주입되기 때문입니다.
XL-Net (Yang et al., 2019)은 BERT를 확장하여 사전 훈련과정에서 양방향 문맥을 고려합니다.

연구자들은 피지 해안의 산호초에서 세 가지 종류의 산호를 조사했습니다. 연구자들은 물고기가 풍부할 때, 그들이 산호에서 해조류와 해초를 먹는 것을 발견했으며, 이는 그들을 더 저항성 있는 산호병균인 Vibrio coralliilyticus로부터 보호하는 것으로 보였습니다. 연구자들은 해조류가 온도 상승과 마찬가지로 산호의 화학 방어 기전을 덜 효과적으로 만들 수 있다고 제안했으며, 물고기들은 해조류를 제거함으로써 산호를 보호하고 있다고 말했습니다.

피지 해안의 어업은 과학 저널에 실린 연구에 따르면 지구 온난화의 영향으로부터 산호초를 보호하고 있습니다.

사쿨러스는 외교관의 아내로서 면책권을 가지고 있었으며, 교통사고에 관여했습니다. ... 존슨 총리는 왓포드의 한 병원에서 언론과의 대화 중에 이 사건에 대해 질문을 받았습니다. 그는 "앤 사쿨러스가 돌아오기를 바랍니다... 만약 우리가 해결할 수 없다면, 저는 개인적으로 백악관에 이 문제를 제기할 것입니다."라고 말했습니다.

보리스 존슨은 백악관과 미국 외교관 앤 사쿨러스의 외교 면책권 문제를 제기할 것이라고 말했습니다.

시리아 국영 언론에 따르면, 정부군은 어제부터 이전에 SDF가 통제하던 영토로 배치를 시작했습니다. ...
10월 6일, 미국 대통령 도널드 트럼프와 터키 대통령 레프테이프 에르도안이 전화 통화를 나눴습니다. 그 후 양국은 북동부 시리아로의 임박한 침공에 대해 발표했습니다. ... 수요일에는 터키가 공습으로 시작하여 지상 침공을 진행했습니다.

시리아 정부군은 터키의 지역 침입에 대응하여 미국 지원 시리아 민주군(SDF)이 점유한 영토로 진입했습니다.

이는 누구도 이 기록을 세운 적이 없는 첫 번째로, 42.195 킬로미터(약 26마일)의 풀 마라톤을 이 시간 내에 완주한 것입니다. 그러나 이는 IAAF의 "오픈 레이스"가 아니기 때문에 공식적으로 인정된 세계 기록은 아닙니다. 그의 기록은 1시간 59분 40.2초입니다. 킵초게는 오스트리아 비엔나에서 달렸습니다. 이는 킵초게가 2시간 장벽을 깨는 데 도움을 주기 위해 특별히 설계된 이벤트였습니다.

케냐 선수 엘리우드 킵초게는 2시간 이내에 마라톤을 완주했습니다.

PG&E는 건조한 상황 속에서 예상되는 강한 바람에 대응하여 정전을 예정했다고 밝혔다. 목표는 산불의 위험을 줄이는 것이다. 약 80만 명의 고객들이 정전에 영향을 받을 예정이었으며, 이는 내일 중순까지 지속될 것으로 예상되었다.

전력이 차단되어 수백만 명의 고객에게 전기가 공급되지 않았습니다. 이는 전력 차단 계획의 일환입니다.

테이블 7: XSum-tuned BART 모델의 예시 요약문 (위키뉴스 기사 기반). 명확성을 위해, 원문의 관련 부분만 표시되었습니다. 요약문은 기사 전체와 이전 지식을 결합하여 정보를 제공합니다.

순열로 자동 회귀적으로 가리킨 마스크 토큰을 예측합니다. 이 목적은 예측이 왼쪽과 오른쪽 문맥에 의존할 수 있도록 합니다. 반면, BART 디코더는 사전 훈련 중에 왼쪽에서 오른쪽으로 작동하여 생성 중인 설정과 일치합니다.

여러 논문들은 사전 훈련된 표현을 사용하여 기계 번역을 개선하는 방법을 탐구했습니다. 가장 큰 개선은 소스 언어와 대상 언어 모두에 대해 사전 훈련을 진행한 경우에서 나타났습니다 (Song et al., 2019; Lample & Conneau, 2019), 하지만 이는 관심 언어들에 대해 사전 훈련을 필요로 합니다. 다른 연구들은 인코더를 사전 훈련된 표현을 사용하여 개선할 수 있다는 것을 보여주었습니다 (Edunov et al., 2019), 하지만 디코더의 개선은 제한적입니다. 우리는 BART를 사용하여 기계 번역 디코더를 개선하는 방법을 보여줍니다.

8 결론

우리는 BART를 소개했습니다. BART는 손상된 문서를 원본으로 매핑하는 학습 방법입니다. BART는 구별적인 작업에서 RoBERTa와 유사한 성능을 달성하면서, 텍스트 생성 작업에서 새로운 최고 성과를 달성했습니다. 미래의 연구는 사전 훈련을 위해 문서를 손상시키는 새로운 방법을 탐구해야 할 것입니다. 아마도 특정 최종 작업에 맞게 그들을 맞추는 방법을 고려해야 합니다.
참고문헌

에네코 아기레, 루이스 마르케스, 그리고 리처드 위센토스키 (편집). 제4회 국제 의미 평가 워크샵 (SemEval-2007) 논문집. 계산언어학 협회, 체코 프라하, 2007년 6월.

이도 다간, 오렌 글리크만, 그리고 베르나르도 마그니니.
PASCAL 텍스트 함의 인식 챌린지.
기계 학습 도전 과제에서 예측 불확실성, 시각적 객체 분류, 그리고 텍스트 함의 인식을 평가하는 것.
스프링거, 2006년.

제이콥 데블린, 민위 창, 켄튼 리, 그리고 크리스티나 투타노바. BERT: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. 2019년 북미 협회 컴퓨터 언어학 회의 논문집: 인간 언어 기술, 1권 (장문과 단문 논문), 4171-4186쪽, 미네소타 주 미니애폴리스, 2019년 6월. 컴퓨터 언어학 협회. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.

Emily Dinan, Varvara Logacheva, Valentin Malykh,
Alexander Miller, Kurt Shuster, Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
Lowe, 등. 두 번째 대화 지능 챌린지 (convai2). arXiv 사전 인쇄
arXiv:1902.00098, 2019.

윌리엄 B 돌란과 크리스 브로켓. 문장의 동의어 말뭉치를 자동으로 구축하는 방법. 2005년 국제 패러프레이징 워크샵 논문집에서.

리 동, 난 양, 웬희 왕, 후루 웨이, 시아오동 리우, 유 왕, 지안펑 가오, 밍 조우, 그리고 샤오-웬 혼. 자연어 이해와 생성을 위한 통합 언어 모델 사전 훈련. arXiv 사전 인쇄 arXiv:1905.03197, 2019.

세르게이 에두노프, 알렉세이 바예프스키, 그리고 마이클 아울리.
언어 생성을 위한 사전 훈련된 언어 모델 표현. 
2019년 북미 협회 컴퓨터 언어학 회의: 인간 언어 기술, 1권 (장문 및 단문 논문), 2019년.

안젤라 팬, 데이비드 그랑지에, 그리고 마이클 아울리. 조절 가능한 추상적 요약. arXiv 사전 인쇄 arXiv:1711.05217, 2017.

안젤라 팬, 야신 제르니트, 이단 페레즈, 데이비드 그랑지에, 제이슨 웨스트론, 그리고 마이클 아울리. Eli5: 긴 형식의 질문에 대한 답변. arXiv 사전 인쇄 arXiv:1907.09190, 2019.

Dan Hendrycks와 Kevin Gimpel. 가우시안 오차 선형 유닛 (gelus). arXiv 사전 인쇄 arXiv:1606.08415, 2016.

칼 모리츠 헤르만, 토마스 코치스키, 에드워드 그레펜스테트, 라세 에스페홀트, 윌 케이, 무스타파 술레이만, 필 블런솜. 기계에게 읽고 이해하는 법 가르치기. 신경 정보 처리 시스템 발전에서, 1693-1701쪽, 2015년.

MandarJoshi, DanqiChen, YinhanLiu, DanielSWeld,
Luke Zettlemoyer, 그리고 Omer Levy. Spanbert: 표현과 예측을 통해 사전 훈련을 개선하는 방법. arXiv 사전 인쇄 arXiv:1907.10529, 2019.

Guillaume Lample과 Alexis Conneau. 교차 언어 모델 사전 훈련. arXiv 사전 인쇄 arXiv:1901.07291, 2019.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: 언어 표현의 자기 지도 학습을 위한 라이트 버트. arXiv 사전 인쇄 arXiv:1909.11942, 2019.

헥터 J 르베스크, 어니스트 데이비스, 그리고 레오라 모르겐. 유인오그라드 스키마 도전. AAAI 봄 심포지엄: 상식 추론의 논리적 형식화, 46권, 47쪽, 2011년.

양 리우와 미렐라 라파타. 사전 훈련된 인코더를 사용한 텍스트 요약. arXiv 사전 인쇄 arXiv:1908.08345, 2019.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
강력하게 최적화된 bert 사전 훈련 접근 방식.
arXiv 사전 인쇄 arXiv:1907.11692, 2019.

토마스 미콜로프, 카이 첸, 그렉 코라도, 제프리 딘. 벡터 공간에서 단어 표현의 효율적인 추정. arXiv 사전 인쇄 arXiv:1301.3781, 2013.

샤시 나라얀, 셰이 B 코헨, 그리고 미렐라 라파타.
세부 사항은 주지 마시고, 요약만 주세요! 주제- 
극단적 요약을 위한 주제 인식 합성곱 신경망. arXiv 사전 인쇄 arXiv:1808.08745, 2018.

가브리엘 페레이라, 조지 터커, 얀 코로프스키, 우카시 카이저, 그리고 제프리 힌튼. 자신감 있는 출력 분포를 벌점화하여 신경망을 규제하는 방법. arXiv 사전 인쇄 arXiv:1701.06548, 2017.

매튜 E 피터스, 마크 뉴만, 모히트 이예르, 매트 가드너, 크리스토퍼 클락, 켄튼 리, 그리고 루크 제틀모이어. 깊은 문맥화된 단어 표현. arXiv 논문 arXiv:1802.05365, 2018.

Alec Radford, Karthik Narasimhan, Tim Salimans, 그리고 Ilya Sutskever. 생성적 사전 훈련에 의한 언어 이해력 향상. URL https://s3-us-west-2. amazonaws. com/openai- assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 언어 모델은 비지도 다중 작업 학습자입니다. OpenAI 블로그, 1(8), 2019.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
and Percy Liang. Squad: 텍스트 이해를 위한 100,000개 이상의 질문. arXiv 사전 인쇄 arXiv:1606.05250, 2016.

아비게일 시, 피터 J 리우, 그리고 크리스토퍼 D 매닝. 핵심을 짚어라: 포인터-생성자 네트워크를 이용한 요약. arXiv 사전 인쇄 arXiv:1704.04368, 2017.

리코 센릭, 배리 하도우, 알렉산드라 버치.
에딘버러 기계 번역 시스템 WMT 16.
기계 번역에 관한 첫 번째 컨퍼런스 논문: 제2권, 공유 작업 논문, 2016년.

리처드 소처, 알렉스 페렐리진, 전우, 재슨 차앙, 크리스토퍼 D 매닝, 앤드류 엔, 그리고 크리스토퍼 포츠. 감성 트리뱅크를 통한 의미 합성에 대한 재귀적인 딥 모델. EMNLP 논문집, 1631-1642쪽, 2013년.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to sequence pre-training for language generation. In International Conference on Machine Learning, 2019. 

케이타오 송, 쉬 탄, 타오 친, 지안펑 루, 그리고 티얀 리우. Mass: 언어 생성을 위한 마스크된 시퀀스 대 시퀀스 사전 훈련. 2019년 국제 기계 학습 컨퍼런스에서 발표.

아시쉬 바스와니, 노암 샤지어, 니키 파마르, 야코브 우스코레이트, 리온 존스, 에이단 엔 고메즈, 우카시 카이저, 그리고 일리아 폴로수킨. 주의는 당신이 필요한 모든 것입니다. 신경 정보 처리 시스템에서의 진보, 2017년, 5998-6008쪽.

알렉스 왕, 아만프리트 싱, 줄리안 마이클, 펠릭스 힐, 오머 레비, 그리고 사무엘 S 보우먼. Glue: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. arXiv 사전 인쇄 arXiv:1804.07461, 2018.

알렉스 워스타트, 아만프리트 싱, 그리고 사무엘 R. 보우먼. 신경망 수용성 판단. arXiv 사전 인쇄물 1805.12471, 2018.

아디나 윌리엄스, 니키타 낭기아, 그리고 사무엘 R 보우먼. 추론을 통한 문장 이해를 위한 광범위한 커버리지의 챌린지 코퍼스. arXiv 사전 인쇄 arXiv:1704.05426, 2017.

아디나 윌리엄스, 니키타 낭기아, 그리고 사무엘 R. 보우먼. 추론을 통한 문장 이해를 위한 광범위한 챌린지 말뭉치. NAACL-HLT 학회 논문집, 2018.

지린 양, 지항 다이, 이밍 양, 하임 카보넬, 루슬란 살라후트디노프, 그리고 쿼크 V 레. Xlnet: 언어 이해를 위한 일반화된 자기회귀 사전훈련. arXiv 사전인쇄 arXiv:1906.08237, 2019.

