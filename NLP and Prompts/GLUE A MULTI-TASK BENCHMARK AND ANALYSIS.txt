ICLR 2019에서 학회 논문으로 발표되었습니다.

GLUE: 자연어 이해를 위한 다목적 벤치마크 및 분석 플랫폼

알렉스 왕1, 아만프리트 싱1, 줄리안 마이클2, 펠릭스 힐3,
오머 레비2 & 사무엘 R. 보우먼1
1뉴욕 대학교 쿠란트 수학 연구소
2워싱턴 대학교 폴 G. 앨런 컴퓨터 과학 및 공학 학부
3딥마인드
{alexwang,amanpreet,bowman}@nyu.edu
{julianjm,omerlevy}@cs.washington.edu
felixhill@google.com

요약

자연어 이해(NLU) 기술이 최대한 유용하려면, 단일 작업, 장르 또는 데이터셋에 독점적으로 처리하는 방식으로 언어를 처리할 수 있어야 합니다. 이 목표를 달성하기 위해, 우리는 일반 언어 이해 평가(GLUE) 벤치마크를 소개합니다. 이는 다양한 기존 NLU 작업의 성능을 평가하기 위한 도구 모음입니다. GLUE는 훈련 데이터가 제한된 작업을 포함하여, 작업 간에 일반적인 언어 지식을 공유하고 장려하는 모델을 선호합니다. GLUE는 또한 모델의 상세한 언어 분석을 가능하게 하는 수작업 진단 테스트 스위트도 포함하고 있습니다. 우리는 전이 및 표현 학습에 대한 현재 방법을 기반으로 한 기준선을 평가하고, 모든 작업에 대해 다중 작업 훈련이 개별 작업별 모델 훈련보다 더 나은 성능을 발휘한다는 것을 발견했습니다. 그러나 최고 모델의 절대적인 성능이 낮으므로 개선된 일반 NLU 시스템의 필요성이 있습니다.

1. 소개

인간의 언어 이해 능력은 일반적이고 유연하며 견고합니다. 그에 반해, 대부분의 NLU 모델은 단어 수준 이상의 작업을 위해 특정한 목적으로 설계되었으며, 도메인 외 데이터에 대해 어려움을 겪습니다. 만약 우리가 입력과 출력 간의 표면적인 대응을 감지하는 것 이상의 이해를 갖춘 모델을 개발하고자 한다면, 다양한 언어적 작업을 다른 도메인에서 수행할 수 있는 보다 통합된 모델을 개발하는 것이 중요합니다.

이 방향의 연구를 용이하게 하기 위해, 우리는 일반 언어 이해 평가(GLUE) 벤치마크를 제시합니다. 이는 질문에 대답하는, 감정 분석, 텍스트 함의 등의 NLU 작업과 모델 평가, 비교, 분석을 위한 연관된 온라인 플랫폼을 포함한 모음입니다. GLUE는 단일 문장 및 문장 쌍 입력을 처리하고 해당 예측을 수행할 수 있는 능력을 제외한 모델 아키텍처에 대한 제약을 두지 않습니다. 일부 GLUE 작업에는 풍부한 훈련 데이터가 있지만, 다른 작업에는 제한적이거나 테스트 세트의 장르와 일치하지 않는 데이터가 있습니다. 따라서 GLUE는 샘플 효율적인 학습과 작업 간 효과적인 지식 전이를 용이하게 하는 언어 지식 표현을 학습할 수 있는 모델을 선호합니다. GLUE의 데이터셋은 벤치마크를 위해 처음부터 생성된 것이 아니며, NLP 커뮤니티에서 도전적이고 흥미로운 것으로 암묵적으로 합의된 기존 데이터셋에 의존합니다. 데이터셋 중 4개는 비공개 테스트 데이터를 포함하고 있으며, 이는 벤치마크가 공정하게 사용되도록 보장하기 위해 사용될 것입니다.

모델이 학습한 지식의 유형을 이해하고 언어적으로 의미 있는 해결 전략을 장려하기 위해 GLUE는 훈련된 모델을 조사하기 위한 수작업 분석 예제 세트도 포함하고 있습니다. 이 데이터셋은 모델이 과제를 견고하게 해결하기 위해 처리해야 할 세계 지식과 논리 연산자 사용과 같은 일반적인 도전 과제를 강조하기 위해 설계되었습니다.

1. 사적인 테스트 데이터를 평가하기 위해 벤치마크 사용자들은 gluebenchmark.com에 제출해야 합니다.

ICLR 2019에서 학회 논문으로 발표되었습니다.

코퍼스 | 훈련 | | 테스트 | 작업 메트릭스         도메인

단일 문장 작업

CoLA 8.5k 1k 수용 가능성 Matthews corr. 잡음.
SST-2 67k 1.8k 감정 정확도 영화 리뷰.

유사성 및 패러프레이즈 작업

MRPC 3.7k 1.7k 패러프레이즈 정확도/F1        뉴스
STS-B 7k 1.4k 문장 유사도 Pearson/Spearman 상관관계 오류
QQP 364k 391k 패러프레이즈 정확도/F1         사회적 QA 질문

추론 작업

MNLI  393k  20k NLI       일치 정확도/불일치 정확도 기타
QNLI  105k 5.4k QA/NLI    정확도             위키피디아
RTE    2.5k 3k NLI        정확도             뉴스, 위키피디아
WNLI   634  146 coreference/NLI 정확도       소설 책

표 1: 작업 설명과 통계. 모든 작업은 단일 문장 또는 문장 쌍 분류 작업입니다. 단, STS-B는 회귀 작업입니다. MNLI는 세 개의 클래스를 가지고 있으며, 다른 모든 분류 작업은 두 개의 클래스를 가지고 있습니다. 굵게 표시된 테스트 세트는 어떤 형태로도 공개된 적이 없는 레이블을 사용합니다.

GLUE가 제기하는 문제를 더 잘 이해하기 위해, 우리는 간단한 기준선과 최신 문장 표현 모델과 함께 실험을 진행합니다. 우리는 통합된 다중 작업 훈련 모델이 각 작업을 개별적으로 훈련시킨 유사한 모델보다 약간 우수한 성능을 보인다는 것을 발견했습니다. 우리의 최고의 다중 작업 모델은 최근 제안된 사전 훈련 기술인 ELMo (Peters et al., 2018)를 사용합니다. 그러나 이 모델은 여전히 상당히 낮은 절대 점수를 달성합니다. 진단 데이터셋을 사용한 분석 결과, 우리의 기준선 모델은 강력한 어휘 신호에는 잘 대응하지만 더 깊은 논리 구조에는 어려움을 겪습니다.

요약하자면, 우리는 다음을 제공합니다: (i) 기존의 주석이 달린 데이터셋을 기반으로 구축된 아홉 개의 문장 또는 문장 쌍 NLU 작업 모음. 이 작업 모음은 다양한 텍스트 장르, 데이터셋 크기 및 난이도를 다루기 위해 선택되었습니다. (ii) 주로 비공개 테스트 데이터를 기반으로 한 온라인 평가 플랫폼과 리더보드. 이 플랫폼은 모델에 구애받지 않으며, 아홉 가지 작업의 결과를 생성할 수 있는 모든 방법을 평가할 수 있습니다. (iii) 전문가가 구축한 진단 평가 데이터셋. (iv) 문장 표현 학습에 대한 주요 기존 접근 방식의 기준 결과.

2 관련 연구

Collobert et al. (2011)은 POS 태깅, 청킹, 개체명 인식 및 의미 역할 라벨링을 공유 문장 이해 구성 요소와 함께 다중 작업 모델을 사용하여 공동으로 학습했습니다. 더 최근의 연구에서는 핵심 NLP 작업의 라벨을 사용하여 딥 뉴럴 네트워크의 하위 수준 훈련을 지도하는 것(Søgaard & Goldberg, 2016; Hashimoto et al., 2017)과 다중 작업 학습을 위한 자동으로 학습되는 교차 작업 공유 메커니즘(Ruder et al., 2017)을 탐구했습니다.

다중 작업 학습을 넘어서서, 일반 NLU 시스템 개발에 대한 많은 연구는 문장-벡터 인코더에 초점을 맞추고 있습니다 (Le & Mikolov, 2014; Kiros et al., 2015 등). 이를 위해 미분류 데이터 (Hill et al., 2016; Peters et al., 2018), 분류된 데이터 (Conneau & Kiela, 2018; McCann et al., 2017) 및 이들의 조합 (Collobert et al., 2011; Subramanian et al., 2018)을 활용하는 연구가 많이 이루어졌습니다. 이 분야에서는 최근에 SentEval (Conneau et al., 2017; Conneau & Kiela, 2018)로 정립된 표준 평가 방법이 등장했습니다. GLUE와 마찬가지로, SentEval은 하나 또는 두 개의 문장을 입력으로 사용하는 일련의 기존 분류 작업에 의존합니다. 그러나 GLUE와 달리, SentEval은 문장-벡터 인코더만을 평가하며, 이는 문장을 고립적으로 다루는 작업에 모델을 평가하기에 적합합니다. 그러나 문장 간의 문맥화와 정렬은 기계 번역 (Bahdanau et al., 2015; Vaswani et al., 2017), 질문 응답 (Seo et al., 2017) 및 자연어 추론 (Rockt¨ aschel et al., 2016)과 같은 작업에서 최첨단 성능을 달성하는 데 중요합니다. GLUE는 이러한 방법의 개발을 용이하게하기 위해 설계되었습니다. 모델에 대한 제한이 없으며, 문장에 대한 명시적인 벡터 또는 기호적 표현을 사용하지 않는 모델을 포함하여 어떤 종류의 표현 또는 문맥화도 허용합니다.

GLUE는 SentEval과는 다르게 평가 과제의 선택에서도 차이를 보입니다.
SentEval의 많은 과제들은 감성 분석과 밀접한 관련이 있습니다. 예를 들어 MR (Pang & Lee)과 같은 과제가 있습니다.

2019년 ICLR에서 학회 논문으로 발표되었습니다.

2005년), SST (Socher et al., 2013), CR (Hu & Liu, 2004) 및 SUBJ (Pang & Lee, 2004)입니다. 다른 작업들은 거의 해결되어 평가가 상대적으로 정보를 제공하지 않습니다. 예를 들어 MPQA (Wiebe et al., 2005)와 TREC 질문 분류 (Voorhees et al., 1999)입니다. GLUE에서는 다양하고 어려운 벤치마크를 구축하려고 노력합니다.

McCann et al. (2018)은 decaNLP를 소개하며, 이는 NLP 시스템의 성능을 여러 데이터셋에 기반하여 평가하는 방식을 제시한다. 그들의 벤치마크는 요약 및 텍스트-SQL 의미 파싱과 같은 작업을 자동 변환을 통해 질문 응답으로 재구성한다. 이 벤치마크는 GLUE의 리더보드와 오류 분석 도구가 부족하지만, 더 중요한 것은 우리가 이를 더 포괄적이지만 즉각적으로 실용적인 목표로 보기 때문이다: GLUE는 현재 해당 작업에 사용되는 방법과 유사한 방법을 사용하여 특정 작업 집합에서 좋은 성능을 내는 방법을 보상하지만, 그들의 벤치마크는 모든 NLU를 질문 응답의 틀 아래 통합하는 목표에 대한 진전을 이루는 시스템을 보상한다.

3 과제

GLUE는 네 가지 영어 문장 이해 작업을 중심으로 하며, 다양한 도메인, 데이터 양 및 난이도를 포함합니다. GLUE의 목표는 일반화 가능한 NLU 시스템의 개발을 촉진하기 위한 것이므로, 우리는 벤치마크를 설계할 때 모델이 모든 작업에 걸쳐 상당한 지식(예: 훈련된 매개변수)을 공유해야 하지만 일부 작업 특정 구성 요소를 유지해야 하는 성능이 좋은 것이 요구되도록 하였습니다. 각 작업에 대해 사전 훈련이나 다른 외부 지식 소스 없이 단일 모델을 훈련하고 이 벤치마크에서 결과 모델 세트를 평가하는 것이 가능하지만, 데이터 부족한 작업을 포함한 우리의 포함으로 인해 이 접근 방식은 경쟁력이 없어질 것으로 예상됩니다. 아래와 표 1에서 작업에 대해 설명하고 있습니다. 부록 A에는 추가적인 세부 정보가 포함되어 있습니다. 그렇지 않은 경우, 작업은 정확도로 평가되며 클래스 간에 균형을 유지합니다.

3.1 단일문장 작업

CoLA 언어 수용성 코퍼스 (Warstadt et al., 2018)는 언어 이론에 관한 책과 저널 기사에서 추출된 영어 수용성 판단으로 구성되어 있습니다. 각 예시는 문법적으로 올바른 영어 문장인지에 대한 주석이 달린 단어 시퀀스입니다. 저자들을 따라, 우리는 Matthews 상관 계수 (Matthews, 1975)를 평가 지표로 사용합니다. 이는 불균형한 이진 분류의 성능을 평가하며 -1부터 1까지의 범위를 가지며, 0은 정보 없는 추측의 성능을 나타냅니다. 우리는 저자들로부터 개인적인 라벨을 얻은 표준 테스트 세트를 사용합니다. 우리는 테스트 세트의 도메인 내 및 도메인 외 섹션의 조합에 대한 단일 성능 숫자를 보고합니다.

SST-2 스탠포드 감성 트리뱅크(Socher et al., 2013)는 영화 리뷰에서 추출된 문장과 그들의 감성에 대한 인간 주석으로 구성되어 있습니다. 이 과제는 주어진 문장의 감성을 예측하는 것입니다. 우리는 긍정/부정 두 가지로 분류하고, 문장 수준의 레이블만 사용합니다.

3.2 유사성과 다른 표현 작업

MRPC는 Microsoft Research Paraphrase Corpus의 약자로, Dolan & Brockett (2005)에 의해 구축된 코퍼스입니다. 이 코퍼스는 온라인 뉴스 소스에서 자동으로 추출된 문장 쌍으로 구성되어 있으며, 문장 쌍이 의미적으로 동등한지에 대한 인간 주석이 포함되어 있습니다. 클래스가 불균형하므로 (긍정 68%), 일반적인 관행을 따라 정확도와 F1 점수를 함께 보고합니다.

QQP Quora Question Pairs2 데이터셋은 커뮤니티 질문-답변 웹사이트 Quora에서 가져온 질문 쌍의 모음입니다. 이 작업은 두 개의 질문 쌍이 의미적으로 동등한지를 판단하는 것입니다. MRPC와 마찬가지로, QQP의 클래스 분포는 균형이 맞지 않습니다(부정적인 경우가 63%입니다). 따라서 정확도와 F1 점수를 함께 보고합니다. 저희는 저자로부터 개인적인 라벨을 얻은 표준 테스트 세트를 사용합니다. 테스트 세트는 훈련 세트와 다른 라벨 분포를 가지고 있음을 관찰하였습니다.

STS-B 시맨틱 텍스트 유사성 벤치마크(Cer et al., 2017)는 뉴스 헤드라인, 비디오 및 이미지 캡션, 자연어 추론 데이터에서 추출된 문장 쌍의 모음입니다.

2 데이터.quora.com/퍼스트-쿠오라-데이터셋-릴리즈-질문-쌍

2019년 ICLR에서 학회 논문으로 발표되었습니다.

거친 분류 세부 분류

어휘 의미론

어휘 함의, 형태론적 부정, 사실성,
대칭성/집합성, 중복, 명명된 개체, 양화사

서술어-인자 구조

핵심 주장, 전치사구, 생략/암시,
어휘적 연결/동일시 능동/수동, 명사화,
소유격/부분, 대격, 관계절,
조정 범위, 교차성, 제한성

논리

부정, 이중 부정, 간격/숫자, 연결, 배타, 조건부, 보편적, 존재적, 시간적, 상향 단조,
하향 단조, 비단조

지식, 상식, 세계 지식

표 2: 진단 데이터셋에 주석이 달린 언어 현상의 유형은 네 가지 주요 범주로 구성되어 있습니다. 각 현상에 대한 설명은 부록 E를 참조하십시오.

각 쌍은 1부터 5까지의 유사도 점수로 사람이 주석을 달았습니다. 이 작업은 이러한 점수를 예측하는 것입니다.
일반적인 관행을 따라, 우리는 피어슨 상관계수와 스피어만 상관계수를 사용하여 평가합니다.

3.3 추론 작업

MNLI 다중 장르 자연어 추론 코퍼스 (Williams et al., 2018)는 텍스트 함의 주석이 달린 문장 쌍의 크라우드소싱된 모음집입니다. 전제 문장과 가설 문장이 주어지면, 전제가 가설을 함의하는지 (함의), 가설과 모순되는지 (모순), 또는 둘 다 아닌지 (중립)를 예측하는 작업입니다. 전제 문장은 전사된 말, 소설 및 정부 보고서를 포함한 열 가지 다른 출처에서 수집되었습니다. 저희는 저자로부터 개인 라벨을 얻은 표준 테스트 세트를 사용하여 일치하는 (도메인 내) 및 불일치하는 (크로스 도메인) 섹션에서 평가합니다. 또한 보조 훈련 데이터로 55만 개의 예제로 SNLI 코퍼스 (Bowman et al., 2015)를 사용하고 권장합니다.

QNLI 스탠포드 질문 응답 데이터셋(Rajpurkar et al. 2016)은 질문-문단 쌍으로 구성된 질문 응답 데이터셋입니다. 여기서 문단은 위키피디아에서 가져온 것이며, 해당 질문에 대한 답변이 문단의 한 문장에 포함되어 있습니다(주석 작성자가 작성). 우리는 각 질문과 해당 문맥의 각 문장 사이에 쌍을 형성하여 문장 쌍 분류 작업으로 변환하고, 질문과 문맥 문장 사이의 어휘적 중복이 낮은 쌍을 필터링합니다. 이 작업은 문맥 문장이 질문에 대한 답변을 포함하는지 여부를 결정하는 것입니다. 이 원래 작업의 수정된 버전은 모델이 정확한 답변을 선택해야 하는 요구 사항을 제거하지만, 입력에 답변이 항상 존재하고 어휘적 중복이 신뢰할 수 있는 단서라는 단순화된 가정도 제거합니다. 기존 데이터셋을 NLI로 재구성하는 이 과정은 White et al. (2017)에서 소개된 방법과 Demszky et al. (2018)에서 확장된 방법과 유사합니다. 우리는 이 변환된 데이터셋을 QNLI (Question-answering NLI)라고 부릅니다.

RTE Recognizing Textual Entailment (RTE) 데이터셋은 연례적인 텍스트 함의 도전에서 나온 것입니다. RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), 그리고 RTE5 (Bentivogli et al., 2009)의 데이터를 결합합니다. 예시는 뉴스와 위키피디아 텍스트를 기반으로 구성되었습니다. 우리는 모든 데이터셋을 두 개의 클래스로 분할하며, 세 개의 클래스 데이터셋의 경우 중립과 모순을 함의되지 않음으로 통합하여 일관성을 유지합니다.

pronoun and the sentence with the possible referent have a matching relationship.

3. QNLI의 이전 버전은 과제가 여기에서 설명하는 것보다 더 쉬운 과제로 모델링되고 해결될 수 있는 문제가 있었습니다. 우리는 이 가능성을 제거한 QNLI의 업데이트 버전을 이후에 공개했습니다.
4. RTE4는 공개적으로 이용할 수 없으며, RTE6과 RTE7은 표준 NLI 과제에 적합하지 않습니다.

2019년 ICLR에서 학회 논문으로 발표되었습니다.

태그           문장 1          문장 2         앞 뒤

어휘 함의 (어휘 의미론), 하향 단조 (논리)

회의 시간은 아직 정해지지 않았다고 스타벅스 대변인이 말했다.

미팅의 시기는 고려되지 않았다고 스타벅스 대변인이 말했다.

N E

보편적 양화사
(논리학)

우리의 가장 깊은 위로를 표합니다.
이 사고로 영향을 받은 모든 분들에게.

이 사고로 영향을 받은 피해자에게 가장 깊은 위로를 전합니다.

E: Can you help me with my homework?
N: 숙제를 도와줄 수 있을까요?

양화사 (어휘 의미론), 이중 부정 (논리학)

나는 날지 않는 벌새를 본 적이 없다.

나는 벌새를 본 적이 없어요.

N E

테이블 3: 진단 세트에서의 예시입니다. Fwd (resp. Bwd)는 문장 1 (resp. 문장 2)이 전제일 때의 라벨을 나타냅니다. 라벨은 함의 (E), 중립 (N) 또는 모순 (C)입니다. 예시는 그들이 보여주는 현상으로 태그가 되어 있으며, 각 현상은 네 가지 큰 범주 중 하나에 속합니다 (괄호 안).

원래 문장에는 대명사 대신 대체된 것이 포함되어 있습니다. 우리는 원래 말뭉치의 저자들에 의해 개인적으로 공유된 소설에서 파생된 새로운 예시들로 구성된 작은 평가 세트를 사용합니다. 포함된 훈련 세트는 두 개의 클래스 사이에 균형을 이루고 있지만, 테스트 세트는 그들 사이에 불균형이 있습니다 (65%는 entailment가 아님). 또한, 데이터의 특이점으로 인해 개발 세트는 적대적입니다: 가설들은 때로는 훈련 및 개발 예시 사이에서 공유되므로, 모델이 훈련 예시를 기억한다면 해당 개발 세트 예시에 대해 잘못된 레이블을 예측할 것입니다. QNLI와 마찬가지로, 각 예시는 개별적으로 평가되므로 이 작업에서 모델의 점수와 원래 작업에서의 점수 사이에 체계적인 대응이 없습니다. 우리는 변환된 데이터셋을 WNLI (Winograd NLI)라고 부릅니다.

3.4 평가

GLUE 벤치마크는 SemEval과 Kaggle과 동일한 평가 모델을 따릅니다. 벤치마크에서 시스템을 평가하기 위해서는 주어진 과제의 테스트 데이터에서 시스템을 실행한 다음 결과를 gluebenchmark.com 웹사이트에 업로드해야 합니다. 벤치마크 사이트는 과제별 점수와 그 점수들의 매크로 평균을 보여주어 시스템의 리더보드 상 위치를 결정합니다. 정확도와 F1과 같은 여러 지표가 있는 과제의 경우, 전체 매크로 평균을 계산할 때 지표들의 가중 평균을 사용합니다. 웹사이트는 진단 데이터셋에 대한 세부 결과도 제공합니다. 자세한 내용은 부록 D를 참조하세요.

4 진단 데이터셋

FraCaS suite (Cooper et al., 1996)와 최근의 Build-It-Break-It 대회 (Ettinger et al., 2017)에서 영감을 얻어, 우리는 시스템 성능 분석을 위한 작은 수동으로 선별된 테스트 세트를 포함시켰습니다. 주요 벤치마크는 대부분 응용 프로그램 중심의 예제 분포를 반영하고 있지만, 우리의 진단 데이터셋은 모델이 포착하기에 흥미롭고 중요한 사전 정의된 현상 집합을 강조합니다. 우리는 표 2에서 전체 현상 집합을 보여줍니다.

각 진단 예시는 현상을 보여주는 태그가 있는 NLI 문장 쌍입니다. NLI 작업은 (비근거화된) 문장 이해에 관련된 기술의 전체 집합을 쉽게 평가할 수 있기 때문에 이러한 분석에 적합합니다. 이는 구문적 모호성 해결부터 세계 지식과 함께한 실용적 추론까지 포함합니다. 우리는 다양한 언어 현상에 대한 예시를 생성하고 뉴스, Reddit, Wikipedia, 학술 논문 등에서 자연스러운 문장을 기반으로 예시를 만들어 데이터가 다양하게 유지되도록 합니다. 이 접근 방식은 최소한의 균일한 예시 집합으로 언어학 이론을 테스트하기 위해 설계된 FraCaS와는 다릅니다. 우리 데이터셋의 샘플은 표 3에 나와 있습니다.

cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html에서 유사한 예제를 확인하세요.

2019년 ICLR에서 학회 논문으로 발표되었습니다.

주석 처리 과정은 우리가 대략적으로 FraCaS suite (Cooper et al., 1996)에서 사용된 것과 유사한 현상을 기반으로 시작합니다. 우리는 각 예제를 생성하기 위해 대상 현상을 증명할 수 있는 문장을 찾아서 두 가지 방법으로 편집합니다. 각 문장 쌍 내에서 높은 어휘 및 구조적 중복을 유지하고 표면적인 단서를 제한하기 위해 최소한의 수정을 가합니다. 그런 다음 문장 간의 추론 관계를 레이블링하여 각 문장을 전제로 번갈아가며 두 개의 레이블이 지정된 예제를 생성합니다 (총 1100개). 가능한 경우, 단일 소스 문장에 대해 다른 레이블을 가진 여러 개의 문장 쌍을 생성하여 어휘적 및 구조적으로 매우 유사하지만 서로 다른 함의 관계에 해당하는 최소한의 문장 쌍 세트를 보유합니다. 결과적으로 얻어진 레이블은 42% 함의, 35% 중립, 23% 모순입니다.

평가
진단 세트의 클래스 분포가 균형을 이루지 않기 때문에, 우리는 평가를 위해 Matthews 상관 계수의 세 클래스 일반화인 R3 (Gorodkin, 2004)을 사용합니다.

최근 연구에서는 크라우드소싱 데이터에 종종 의도된 작업을 해결하지 않고도 잘 수행할 수 있는 아티팩트가 포함되어 있다는 것을 보여주었습니다(Schwartz et al., 2017; Poliak et al., 2018; Tsuchiya, 2018, i.a.). 따라서 우리는 이러한 아티팩트를 확인하기 위해 데이터를 감사합니다. 우리는 Guru-rangan et al. (2018)의 방법론을 재현하여 SNLI와 MNLI에서 가설만을 입력으로 사용하여 entailment 레이블을 예측하기 위해 두 개의 fastText 분류기(Joulin et al., 2016)를 훈련시켰습니다. 우리의 진단 데이터에서 각각 32.7%와 36.4%의 거의 무작위 정확도를 얻었으며, 이는 데이터가 이러한 아티팩트로 인해 영향을 받지 않음을 보여줍니다.

진단 세트에서 인간의 기준 성능을 확립하기 위해, 우리는 6명의 NLP 연구원이 진단 세트에서 임의로 추출한 50개의 문장 쌍 (100개의 함의 예제)을 주석으로 달았습니다. 주석자 간의 일치도는 높으며, Fleiss의 κ 값은 0.73입니다. 주석자들 사이의 평균 R3 점수는 0.80으로, 제5절에서 설명된 기준 시스템 중 어떤 것보다 훨씬 높습니다.

사용 목적 진단 예시는 특정 현상을 해결하기 위해 선별된 것이며, NLI는 자연어 입력 분포가 없는 작업이므로 진단 세트에서의 성능은 전체 성능이나 하류 응용 프로그램에서의 일반화를 반영하지 않을 것으로 예상하지 않습니다. 모델 간의 분석 세트 성능은 비교되어야 하지만 범주 간의 비교는 되어서는 안 됩니다. 이 세트는 벤치마크가 아닌 오류 분석, 질적 모델 비교 및 적대적 예제 개발을 위한 분석 도구로 제공됩니다.

5 베이스라인

기준선으로, GLUE 작업에 훈련된 멀티태스크 학습 모델과 최근 사전 훈련 방법을 기반으로 한 여러 가지 변형 모델을 평가합니다. 이에 대해 간단히 설명합니다. 자세한 내용은 부록 B를 참조하십시오. 우리는 AllenNLP 라이브러리(Gardner et al., 2017)에서 모델을 구현합니다. 기준선의 원본 코드는 https://github.com/nyu-mll/GLUE-baselines에서 제공되며, 더 최신 버전은 https://github.com/jsalt18-sentence-repl/jiant에서 이용할 수 있습니다.

건축 우리 가장 간단한 기준선 아키텍처는 문장-벡터 인코더를 기반으로 하며, GLUE의 복잡한 구조를 평가하는 능력을 제외합니다. Conneau et al. (2017)의 영감을 받아, 이 모델은 두 개의 레이어로 구성된 1500D (방향당) BiLSTM과 max pooling, 그리고 300D GloVe 단어 임베딩 (840B Common Crawl 버전; Pennington et al., 2014)을 사용합니다. 단일 문장 작업의 경우, 문장을 인코딩하고 결과 벡터를 분류기에 전달합니다. 문장 쌍 작업의 경우, 문장을 독립적으로 인코딩하여 벡터 u, v를 생성하고 [u;v;|u−v|;u ∗ v]를 분류기에 전달합니다. 분류기는 512D 은닉층을 가진 MLP입니다.

우리는 문장 쌍 작업을 위해 Seo et al. (2017)의 영감을 받은 어텐션 메커니즘을 사용하는 모델의 변형도 고려합니다. 이 모델은 모든 단어 쌍 사이에 어텐션 메커니즘을 적용한 후, 최대 풀링을 하는 두 번째 BiLSTM을 사용합니다. 문장 간 상호작용을 명시적으로 모델링함으로써, 이러한 모델들은 문장-벡터 패러다임을 벗어납니다.

사전 훈련 단계에서 우리는 ELMo와 CoVe라는 최근 방법 두 가지를 기본 모델에 추가합니다. 우리는 둘 다 기존에 훈련된 모델을 사용합니다.

pair of two-layer neural language models trained on the Billion Word Benchmark (Chelba et al., 2013). Each word is represented by a contextual embedding, produced by taking a

2019년 ICLR에서 학회 논문으로 발표되었습니다.

단일 문장 유사도 및 패러프레이즈 자연어 추론 모델 평균 CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI

단일 작업 훈련

BiLSTM 63.9 15.7 85.9 69.3/79.4 81.7/61.4 66.0/62.8 70.3/70.8 75.7 52.8 65.1
+ELMo 66.4 35.0 90.2 69.0/80.8 85.7/65.6 64.0/60.2 72.9/73.4 71.7 50.1 65.1
+CoVe 64.0 14.5 88.5 73.4/81.4 83.3/59.4 67.2/64.1 64.5/64.8 75.4 53.5 65.1
+Attn 63.9 15.7 85.9 68.5/80.3 83.5/62.9 59.3/55.8 74.2/73.8 77.2 51.9 65.1
+Attn, ELMo 66.5 35.0 90.2 68.8/80.2 86.5/66.1 55.5/52.5 76.9/76.7 76.7 50.4 65.1
+Attn, CoVe 63.2 14.5 88.5 68.6/79.7 84.1/60.1 57.2/53.6 71.6/71.5 74.5 52.7 65.1

멀티태스크 훈련

BiLSTM  64.2 11.6 82.8 74.3/81.8 84.2/62.5 70.3/67.8 65.4/66.1 74.6 57.4 65.1
+ELMo   67.7 32.1 89.3 78.0/84.7 82.6/61.1 67.2/67.9 70.3/67.8 75.5 57.4 65.1
+CoVe   62.9 18.5 81.9 71.5/78.7 84.9/60.6 64.4/62.7 65.4/65.7 70.8 52.7 65.1
+Attn   65.6 18.6 83.0 76.2/83.9 82.4/60.1 72.8/70.5 67.6/68.3 74.3 58.4 65.1
+Attn, ELMo 70.0 33.6 90.4 78.0/84.4 84.3/63.1 74.2/72.3 74.1/74.5 79.8 58.9 65.1
+Attn, CoVe 63.1 8.3 80.7 71.8/80.0 83.4/60.5 69.8/68.4 68.1/68.6 72.9 56.0 65.1

사전 훈련된 문장 표현 모델

CBoW    58.9 0.0 80.0 73.4/81.5 79.1/51.4 61.2/58.7 56.0/56.4 72.1 54.1 65.1
CBoW    58.9 0.0 80.0 73.4/81.5 79.1/51.4 61.2/58.7 56.0/56.4 72.1 54.1 65.1

Skip-Thought 61.3 0.0 81.8 71.7/80.8 82.2/56.4 71.8/69.7 62.9/62.8 72.9 53.1 65.1
Skip-Thought 61.3 0.0 81.8 71.7/80.8 82.2/56.4 71.8/69.7 62.9/62.8 72.9 53.1 65.1

InferSent 63.9 4.5 85.1 74.1/81.2 81.7/59.1 75.9/75.3 66.1/65.7 72.7 58.0 65.1
InferSent 63.9 4.5 85.1 74.1/81.2 81.7/59.1 75.9/75.3 66.1/65.7 72.7 58.0 65.1

DisSent 62.0 4.9 83.7 74.1/81.7 82.6/59.5 66.1/64.8 58.7/59.1 73.9 56.4 65.1
DisSent 62.0 4.9 83.7 74.1/81.7 82.6/59.5 66.1/64.8 58.7/59.1 73.9 56.4 65.1

GenSen  66.2 7.7 83.1 76.6/83.0 82.9/59.8 79.3/79.2 71.4/71.3 78.6 59.2 65.1
GenSen  66.2 7.7 83.1 76.6/83.0 82.9/59.8 79.3/79.2 71.4/71.3 78.6 59.2 65.1

표 4: GLUE 작업 테스트 세트의 기준 성능입니다. MNLI의 경우, 일치 및 불일치 테스트 세트의 정확도를 보고합니다. MRPC와 Quora의 경우, 정확도와 F1을 보고합니다. STS-B의 경우, Pearson 및 Spearman 상관 관계를 보고합니다. CoLA의 경우, Matthews 상관 관계를 보고합니다. 다른 모든 작업에 대해서는 정확도를 보고합니다. 모든 값은 100으로 스케일링되었습니다. 비슷한 표는 온라인 플랫폼에서 제공됩니다.

두 모델의 각 레이어의 해당 숨겨진 상태들의 선형 조합입니다. 우리는 저자들의 권장사항을 따라 다른 임베딩 대신 ELMo 임베딩을 사용합니다.

CoVe (McCann et al., 2017)은 원래 영어-독일어 번역을 위해 훈련된 두 개의 층으로 구성된 BiLSTM 인코더를 사용합니다. 단어의 CoVe 벡터는 최상위 LSTM의 해당 숨겨진 상태입니다. 원래 작업과 마찬가지로, 우리는 CoVe 벡터를 GloVe 단어 임베딩에 연결합니다.

훈련 우리는 BiLSTM 문장 인코더와 공유된 후처리 어텐션 BiLSTM을 사용하여 모델을 훈련합니다. 각 작업에 대해 별도로 훈련된 분류기를 사용합니다. 각 훈련 업데이트마다, 각 작업의 훈련 예제 수에 비례하는 확률로 훈련할 작업을 샘플링합니다. 우리는 Adam (Kingma & Ba, 2015)을 사용하여 모델을 훈련하며, 초기 학습률은 10^-4이고 배치 크기는 128입니다. 우리는 매크로 평균 점수를 검증 지표로 사용하며, 학습률이 10^-5보다 낮아지거나 성능이 5번의 검증 체크 후에도 향상되지 않을 때 훈련을 중지합니다.

우리는 또한 구성 및 훈련은 동일하지만 매개변수를 공유하지 않는 단일 작업 모델 세트를 훈련시킵니다. 다중 작업 모델과 공정한 비교를 위해 각 작업에 대해 매개변수나 훈련 설정을 조정하지 않으므로 이러한 단일 작업 모델은 일반적으로 각 작업의 최신 기술을 대표하지 않습니다.

문장 표현 모델 마지막으로, 우리는 다음과 같은 훈련된 문장-벡터 인코더 모델들을 우리의 벤치마크를 사용하여 평가합니다: GloVe 임베딩을 사용한 평균 단어 가방 모델 (CBoW), Skip-Thought (Kiros et al., 2015), InferSent (Conneau et al., 2017), DisSent (Nie et al., 2017), 그리고 GenSen (Subramanian et al., 2018). 이러한 모델들에 대해서는, 우리는 그들이 생성하는 표현에 대해 과제별 분류기만을 훈련시킵니다.

6github.com/allenai/allennlp/blob/master/tutorials/how to/elmo.md
6깃허브 닷컴 슬래시 알렌에이아이 슬래시 알렌엔엘피 슬래시 블롭 슬래시 마스터 슬래시 튜토리얼스 슬래시 하우 투 슬래시 엘모 닷엠디

ICLR 2019에서 학회 논문으로 발표되었습니다.

거친-계조      세밀한-계조
모델   모두  LS PAS L K  UQuant MNeg 2Neg Coref Restr Down

단일 작업 훈련

BiLSTM 21 25 24 16 16 70 53 4 21 -15 12
+ELMo 20 20 21 14 17 70 20 42 33 -26 -3
+CoVe 21 19 23 20 18 71 47 -1 33 -15 8
+Attn 25 24 30 20 14 50 47 21 38 -8 -3
+Attn, ELMo 28 30 35 23 14 85 20 42 33 -26 -3
+Attn, CoVe 24 29 29 18 12 77 50 1 18 -1 12

멀티태스크 훈련

BiLSTM 20 13 24 14 22 71 17 -8 31 -15 8
+ELMo 21 20 21 19 21 71 60 2 22 0 12
+CoVe 18 15 11 18 27 71 40 7 40 0 8
+Attn 18 13 24 11 16 71 1 -12 31 -15 8
+Attn, ELMo 22 18 26 13 19 70 27 5 31 -26 -3
+Attn, CoVe 18 16 25 16 13 71 26 -8 33 9 8

사전 훈련된 문장 표현 모델

CBoW 9 6 13 5 10 3 0 13 28 -15 -11
CBoW 9 6 13 5 10 3 0 13 28 -15 -11
Skip-Thought 12 2 23 11 9 61 6 -2 30 -15 0
InferSent 18 20 20 15 14 77 50 -20 15 -15 -9
DisSent 16 16 19 13 15 70 43 -11 20 -36 -9
GenSen 20 28 26 14 12 78 57 2 21 -15 12

표 5: 진단 세트 결과. 우리는 금과 예측된 라벨 간의 R3 계수를 100으로 스케일링하여 보고합니다. 대분류 카테고리는 어휘 의미론 (LS), 술어-인자 구조 (PAS), 논리 (L) 및 지식과 상식 (K)입니다. 우리의 예시 세분화된 카테고리는 범용 양화 (UQuant), 형태론적 부정 (MNeg), 이중 부정 (2Neg), 은유/공조 (Coref), 제한성 (Restr) 및 하향 단조 (Down)입니다.

6 벤치마크 결과

우리는 각 모델을 세 번 훈련시키고, 최고의 매크로 평균 개발 세트 성능으로 실행을 평가합니다 (부록 C의 테이블 6 참조). 단일 작업 및 문장 표현 모델의 경우, 각 개별 작업에 대해 최상의 실행을 평가합니다. 주요 벤치마크 작업의 성능을 테이블 4에 제시합니다.

우리는 다중 작업 훈련이 주의력이나 ELMo를 사용하는 모델들에게 단일 작업 훈련보다 전반적으로 더 좋은 점수를 제공한다는 것을 발견했습니다. 주의력은 일반적으로 단일 작업 훈련에서는 무시할 수 있거나 부정적인 종합 효과를 가지지만, 다중 작업 훈련에서는 도움이 됩니다. 우리는 GloVe나 CoVe 임베딩 대신 ELMo 임베딩을 사용하는 것이 일반적으로 단일 문장 작업에 특히 일관된 개선을 보인다는 것을 알 수 있습니다. CoVe를 사용하는 것은 오로지 GloVe만 사용하는 것보다 혼합된 효과를 가지고 있습니다.

사전 훈련된 문장 표현 모델들 중에서, 우리는 CBoW에서 Skip-Thought로, 그리고 Infersent와 GenSen으로 이동함에 따라 상당히 일관된 향상을 관찰합니다. GLUE 작업에 직접 훈련된 모델들과 비교했을 때, InferSent는 경쟁력이 있으며 GenSen은 두 가장 우수한 모델을 제외하고 모두 성능을 능가합니다.

각 작업별 결과를 살펴보면, 문장 표현 모델은 CoLA에서 직접 작업에 훈련된 모델과 비교하여 성능이 상당히 떨어집니다. 반면에 STS-B에서는, 직접 작업에 훈련된 모델은 최고의 문장 표현 모델의 성능을 상당히 뒤쳐지는 것으로 나타납니다. 마지막으로, 특정 작업에서는 어떤 모델도 특히 잘 수행하지 못합니다. WNLI에서는, 어떤 모델도 가장 빈도가 높은 클래스를 추측하는 것(65.1%)을 넘지 못하며, 우리는 모델 예측을 가장 빈도가 높은 기준으로 대체합니다. RTE와 종합적으로, 우리의 최상의 기준선조차도 개선의 여지가 있습니다. 이러한 초기 결과는 현재 모델과 방법으로 GLUE를 해결하는 것이 불가능하다는 것을 나타냅니다.

2019년 ICLR에서 학회 논문으로 발표되었습니다.

7 분석

우리는 각 모델의 MNLI 분류기를 진단 세트에서 평가하여 기준선을 분석하여 언어 능력에 대한 더 나은 감각을 얻습니다. 결과는 표 5에 제시됩니다.

전반적으로 대분류 성능은 모든 모델에 대해 낮습니다: 28점이라는 가장 높은 총점도 절대적인 성능이 낮음을 나타냅니다. 주어-동사 구조에서는 성능이 높고 논리에서는 낮은 경향이 있지만, 카테고리 간에는 숫자가 밀접하게 비교되지 않습니다. 주요 벤치마크와 달리, 다중 작업 모델은 거의 항상 단일 작업 모델보다 성능이 떨어집니다. 이는 아마도 우리의 간단한 다중 작업 훈련 방식으로 인해 MNLI와 다른 작업 사이에 파괴적인 간섭이 있을 것으로 예상됩니다. GLUE 작업에 훈련된 모델은 대부분 사전 훈련된 문장 표현 모델보다 우수한 성능을 보입니다. GenSen을 제외하고는요. 어텐션을 사용하는 것이 ELMo나 CoVe를 사용하는 것보다 진단 점수에 더 큰 영향을 미칩니다. 이는 어텐션이 NLI에서 일반화에 특히 중요하다는 것을 나타냅니다.

세분화된 하위 카테고리 대부분의 모델들은 보편적 양화를 상대적으로 잘 다룹니다. 관련 예시를 살펴보면, "모두"와 같은 어휘적 단서에 의존하는 것이 좋은 성능을 제공하는 경우가 많습니다. 마찬가지로, 어휘적 단서는 형태론적 부정 예시에서도 좋은 신호를 제공합니다.

우리는 모델들 사이에 다양한 약점을 관찰합니다. 이중 부정은 특히 GloVe 임베딩만 사용하는 GLUE 훈련된 모델들에게 어려움을 줍니다. 이는 ELMo로 개선되었으며, 어느 정도로는 CoVe로도 개선되었습니다. 또한, 어텐션은 전체 결과에 혼합된 효과를 가지며, 어텐션을 사용하는 모델들은 하향적 단조성에 어려움을 겪는 경향이 있습니다. 우리는 그들의 예측을 조사해보니, 모델들은 상위어/하위어 대체와 단어 삭제를 함의의 신호로 감지하지만, 그것을 잘못된 방향으로 예측합니다 (대체/삭제된 단어가 상향적 단조 문맥에 있는 것처럼). 이는 McCoy & Linzen (2019)의 최근 연구 결과와 일치하는데, 이 시스템들은 전제와 가설 사이의 부분순서 관계를 휴리스틱 단축키로 사용한다는 것입니다. 양자 범위의 미묘한 차이에 따라 종종 의존하는 제한성 예제는 거의 모든 모델에게 특히 어려움을 줍니다.

전반적으로, 문장-벡터 표현을 넘어서는 어텐션 메커니즘과 같은 것들이 도메인 밖의 데이터에서 성능 향상을 도울 수 있는 증거가 있으며, ELMo와 CoVe와 같은 전이 방법들은 그들의 감독 신호에 특정한 언어 정보를 인코딩한다. 그러나, 표현 능력의 증가는 오버피팅을 초래할 수 있으며, 어텐션 모델의 하향 단조 문맥에서의 실패와 같은 것들이 발생할 수 있다. 우리는 우리의 플랫폼과 진단 데이터셋이 비슷한 분석에 유용하게 사용될 것이라 기대하며, 이를 통해 모델 디자이너들이 모델의 일반화 동작과 암묵적인 지식을 더 잘 이해할 수 있기를 바란다.

8 결론

우리는 GLUE를 소개합니다. 이는 자연어 이해 시스템을 평가하고 분석하기 위한 플랫폼과 자원의 모음입니다. 우리는 우리의 과제에 공동으로 훈련된 모델들이 개별 과제에 대해 훈련된 모델들의 결합 성능보다 더 좋은 성능을 보인다는 것을 발견했습니다. 우리는 GLUE 벤치마크에서 최고의 문장 표현 모델들을 능가하는 ELMo와 같은 어텐션 메커니즘과 전이 학습 방법의 유용성을 확인했지만, 아직 개선할 여지가 있다는 것을 알 수 있었습니다. 우리의 진단 데이터셋에서 이러한 모델들을 평가할 때, 많은 언어 현상에서 실패하는 것을 발견했습니다. 이는 향후 연구 가능성을 제시하며, 일반적인 목적의 NLU 모델을 설계하는 방법에 대한 질문은 아직 답이 없다고 생각하며, GLUE가 이 도전에 대한 풍부한 연구 가능성을 제공할 수 있다고 믿습니다.

감사의 말씀

우리는 Ellie Pavlick, Tal Linzen, Kyunghyun Cho, 그리고 Nikita Nangia에게 이 작업에 대한 초기 단계에서의 의견에 감사드립니다. 또한, Ernie Davis, Alex Warstadt, 그리고 Quora의 Nikhil Dandekar와 Kornel Csernai에게는 비공개 평가 데이터에 대한 접근을 제공해 준 것에 감사드립니다. 이 프로젝트는 Google, Tencent Holdings, 그리고 Samsung Research로부터 SB에게 재정적인 지원을 받았으며, AW는 AdeptMind와 NSF 대학원 연구 장학금을 통해 혜택을 받았습니다.

ICLR 2019에서 학회 논문으로 발표되었습니다.

참고문헌

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations, 2015.

드미트리 바다나우, 경현 조, 그리고 요슈아 벤지오. 정렬과 번역을 동시에 학습하는 신경 기계 번역. 학습 표현 국제 학회 논문집, 2015년.

로이 바르 하임, 이도 다간, 빌 돌란, 리사 페로, 다닐로 지암피콜로, 베르나르도 마그니니, 그리고 이단 슈펙터. 두 번째 PASCAL 텍스트 함의 인식 챌린지. 2006년.

루이사 벤티보글리, 이도 다간, 호아 트랑 당, 다닐로 지암피콜로, 그리고 베르나르도 마그니니. 2009년 제5회 PASCAL 텍스트 함의 인식 챌린지.

사무엘 R. 보우먼, 가보르 안젤리, 크리스토퍼 포츠, 그리고 크리스토퍼 D. 매닝. 자연어 추론 학습을 위한 대규모 주석 말뭉치. Empirical Methods in Natural Language Processing 학회 논문집, 632-642쪽. Association for Computational Linguistics, 2015.

다니엘 세르, 모나 디압, 에네코 아기레, 이니고 로페스-가스피오, 그리고 루시아 스페시아. Semeval-2017 과제
1: 다국어 및 교차언어 의미적 텍스트 유사성에 초점을 맞춘 평가. 2017년 제11회 국제 의미 평가 워크샵에서 발표.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 통계적 언어 모델링의 진전을 측정하기 위한 10억 단어 벤치마크. arXiv 사전 인쇄물 1312.3005, 2013.

Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 자연어 처리 (거의) 처음부터. 기계 학습 연구 저널, 12(Aug):2493–2537, 2011.

알렉시스 코너와 도우 키엘라. SentEval: 범용 문장 표현을 위한 평가 도구. 2018년 제11회 언어 자원 및 평가 국제 학회 논문집.

알렉시스 코너, 도우 키엘라, 홀거 슈벵크, 로이크 바로, 안토완 보르드. 자연어 추론 데이터로부터의 범용 문장 표현의 지도 학습. 자연어 처리에 대한 경험적 방법 컨퍼런스 논문집, 2017년 9월 9일-11일, 코펜하겐, 덴마크, 681-691쪽, 2017년.

RobinCooper, DickCrouch, JanVanEijck, ChrisFox, JosefVanGenabith, JanJaspars, HansKamp,
David Milward, Manfred Pinkal, Massimo Poesio, Steve Pulman, Ted Briscoe, Holger Maier, and
Karsten Konrad. 프레임워크를 사용하여 기술 보고서, The FraCaS Consortium, 1996.

이도 다간, 오렌 글리크만, 그리고 베르나르도 마그니니. PASCAL 텍스트 함의 인식 도전. 기계 학습 도전과제. 예측 불확실성 평가, 시각적 객체 분류, 그리고 텍스트 함의 인식, 177-190쪽. 스프링거, 2006년.

도로티아 뎀스키, 켈빈 구, 퍼시 리앙. 질문 응답 데이터셋을 자연어 추론 데이터셋으로 변환하기. arXiv 사전 인쇄물 1809.02922, 2018.

윌리엄 B 돌란과 크리스 브로켓. 문장의 동의어 말뭉치를 자동으로 구축하기. 2005년 국제 동의어 워크샵 논문집에서.

앨리슨 에팅거, 수다 라오, 할 다움 III, 그리고 에밀리 M 벤더. 언어적으로 일반화 가능한 NLP 시스템을 향해: 워크샵과 공유 작업. 2017년 제1회 언어적으로 일반화 가능한 NLP 시스템 구축 워크샵에서 발표.

매트 가드너, 조엘 그루스, 마크 노이만, 오이빈드 타피오르드, 프라딥 다시기, 넬슨 F. 리우, 매튜 피터스, 마이클 슈미츠, 루크 S. 제틀모이어. AllenNLP: 깊은 의미론적 자연어 처리 플랫폼. 2017.

다닐로 지암피콜로, 베르나르도 마그니니, 이도 다간, 그리고 빌 돌란. 세 번째 PASCAL 텍스트 함의 인식 챌린지. ACL-PASCAL 텍스트 함의 및 파라프레이징 워크샵 논문집, 1-9쪽. 컴퓨터 언어학 협회, 2007년.

10
ICLR 2019에서 학회 논문으로 발표되었습니다.

Jan Gorodkin. k개 범주 할당을 k개 범주 상관 계수로 비교하기. Com- put. Biol. Chem., 28(5-6):367–374, 2004년 12월. ISSN 1476-9271.

수친 구루랑간, 스와바 스와야믹디타, 오머 레비, 로이 슈왈츠, 사무엘 R. 보우먼, 그리고 노아 A. 스미스. 자연어 추론 데이터에서의 주석 오류. 2018년 미국 컴퓨터언어학회 북미 지부 인간언어기술 학회 논문집.

카즈마 하시모토, 카이밍 씨옹, 요시마사 쓰루오카, 그리고 리처드 소처. 공동 다중 작업 모델: 다중 NLP 작업을 위한 신경망 성장. 2017년 자연어 처리에 대한 경험적인 방법 회의 논문집에서.

펠릭스 힐, 경현 조, 안나 코르호넨. 라벨이 없는 데이터로부터 문장의 분산 표현 학습. 2016년 북미 협회 컴퓨터 언어학 연구 대회: 인간 언어 기술.

민칭 후와 빙 리우. 고객 리뷰의 채굴과 요약. 지식 발견과 데이터 마이닝에 관한 제10회 ACM SIGKDD 국제 컨퍼런스 논문집, 168-177쪽. ACM, 2004년.

아르망 주랭, 에두아르 그라브, 피오트르 보야노프스키, 토마스 미코로프. 효율적인 텍스트 분류를 위한 기교 모음. 2016년 arXiv 사전 인쇄물 1607.01759.

디에더릭 P 킹마와 지미 바. Adam: 확률적 최적화를 위한 방법. 학습 표현 국제 학회 논문집, 2015.

라이언 키로스, 유쿤 주, 루슬란 R 살라후딘노프, 리처드 제멜, 라퀼 우르타순, 안토니오 토럴바, 그리고 산자 피들러. 스킵-씨오트 벡터. 신경 정보 처리 시스템 발전에서, 3294-3302쪽, 2015년.

Quoc Le와 Tomas Mikolov. 문장과 문서의 분산 표현. Eric P. Xing과 Tony Jebara(편집), 기계 학습 제 31 회 국제 회의 논문집, Machine Learning Research 논문집 32권, 1188-1196쪽, 중국 베이징, 2014년 6월 22-24일. PMLR.

헥터 J 르베스크, 어니스트 데이비스, 그리고 레오라 모르겐스턴. 윈로그라드 스키마 도전. AAAI 봄 심포지엄: 상식적 추론의 논리적 형식화, 46권, 47쪽, 2011년.

브라이언 W 매튜스. T4 파지 리소자임의 예측된 보조 구조와 관찰된 보조 구조의 비교. Biochimica et Biophysica Acta (BBA)-단백질 구조, 405(2):442–451, 1975.

브라이언 맥캔, 제임스 브래드버리, 카이밍 셩, 그리고 리처드 소처. 번역에서 배운 것: 맥락화된 단어 벡터. 신경 정보 처리 시스템 발전에서, 6297-6308쪽, 2017년.

브라이언 맥캔, 니티시 시리시 케스카르, 카이밍 셩, 그리고 리처드 소처. 자연어 디카토론: 질문 답변을 위한 멀티태스크 학습. arXiv 사전 인쇄물 1806.08730, 2018.

R. Thomas McCoy와 Tal Linzen. 자연어 추론을 위한 비-추론적인 하위 시퀀스. 언어 계산 학회 논문집, 2권, 357-360쪽, 2019년.

알렌 니, 에린 D 베넷, 노아 D 굿맨. 반대: 명시적 담화 관계로부터의 문장 표현 학습. arXiv 사전 인쇄물 1710.04334, 2017.

보 팡과 릴리안 리. 감성 교육: 최소 절단을 기반으로 한 주관성 요약을 사용한 감성 분석. 연례 회의 42회에 대한 논문집, 271쪽. 연산 언어학 협회, 2004년.

보 팡과 릴리안 리. 별을 보다: 등급 척도에 대한 감성 분류를 위한 클래스 관계의 활용. 연례 회의 논문집, 제43회 연례 회의 논문집, 115-124쪽. 연례 회의 논문집, 2005년.

11
ICLR 2019에서 학회 논문으로 발표되었습니다.

제프리 페닝턴, 리처드 소처, 그리고 크리스토퍼 매닝. GloVe: 단어 표현을 위한 글로벌 벡터. 자연어 처리에 대한 경험적 방법 회의 논문집, 1532-1543쪽, 2014년.

매튜 E 피터스, 마크 뉴만, 모히트 이예르, 매트 가드너, 크리스토퍼 클락, 켄튼 리, 그리고 루크 제틀모이어. 깊은 문맥화된 단어 표현. 2018년 북미 협회 컴퓨터 언어학 연구 대회: 인간 언어 기술.

아담 폴리악, 제이슨 나라도우스키, 아파라지타 할다르, 레이첼 루딩거, 벤자민 반 더뮤.
자연어 추론에서의 가설 기반 베이스라인에 대한 연구. *SEM@NAACL-HLT, 2018.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 그리고 Percy Liang. SQuAD: 텍스트 이해를 위한 100,000개 이상의 질문. Empirical Methods in Natural Language Processing Conference 논문집, 2383-2392쪽. Association for Computational Linguistics, 2016.

팀 록트아셀, 에드워드 그레펜스테트, 모리츠 헤르만, 칼, 토마스 코치스키, 그리고 필 블런솜.
신경망 어텐션을 이용한 함의 추론에 대한 추론. 국제 학습 표현 대회 논문집, 2016.

세바스찬 루더, 요아킴 빙겔, 이자벨 오그엔슈타인, 안데르스 쇼가드. 슬루스 네트워크: 느슨하게 관련된 작업 사이에서 공유할 내용을 학습하는 방법. 2017년 arXiv 사전 인쇄물 1705.08142.

로이 슈왈츠, 마르텐 샙, 이오아니스 콘스타스, 리 질레스, 예진 최, 그리고 노아 A. 스미스. 서로 다른 작문 과제가 언어 스타일에 미치는 영향: ROC 스토리 클로즈 과제의 사례 연구. CoNLL 2017 논문집에서 발표된 내용입니다.

민준 서, 아니루다 켐바비, 알리 파르하디, 한나네 하지시르지. 기계 이해를 위한 양방향 주의 흐름. 학습 표현 국제 학회 논문집, 2017.

리처드 소처, 알렉스 페렐리진, 전 우, 제이슨 차앙, 크리스토퍼 D 매닝, 앤드류 엔, 그리고 크리스토퍼 포츠. 감성 트리뱅크를 위한 재귀적인 깊은 모델들에 대한 의미론적 합성. 2013년 Empirical Methods in Natural Language Processing Conference 논문집, 1631-1642쪽.

안데르스 쇼가드와 요아브 골드버그. 하위 레이어에서 감독되는 저수준 작업을 사용한 깊은 다중 작업 학습. 2016년 협회 연구 발표논문집 (2권: 단문), 2권, 231-235쪽.

산딥 수브라마니안, 아담 트리슐러, 요슈아 벤지오, 그리고 크리스토퍼 J. 팔. 대규모 다중 작업 학습을 통한 일반적인 분산 문장 표현 학습. 2018년 학습 표현 국제 학회 논문집.

마사토시 츠치야. 텍스트 추론을 위한 훈련 데이터의 숨겨진 편향에 의한 성능 영향. 제11회 언어 자원 및 평가 국제 학회 (LREC 2018) 논문집, 일본 미야자키, 2018년 5월 7-12일. 유럽 언어 자원 협회 (ELRA).

아시쉬 바스와니, 노암 샤지어, 니키 파마르, 야코브 우스코레이트, 리온 존스, 에이단 엔 고메즈, 우카시 카이저, 그리고 일리아 폴로수킨. 주의는 당신이 필요한 모든 것입니다. 신경 정보 처리 시스템에서의 진보, 6000-6010쪽, 2017년.

엘렌 M 보어히스 외. TREC-8 질문 응답 트랙 보고서. TREC, 99권, 77-82쪽, 1999년.

알렉스 워스타트, 아만프리트 싱, 그리고 사무엘 R 보우먼. 신경망 수용 가능성 판단. 
arXiv 사전 인쇄물 1805.12471, 2018.

아론 스티븐 화이트, 푸슈펜드레 라스토기, 케빈 두, 벤자민 반 더뮤. 추론은 모든 것이다: 의미 리소스를 통합된 평가 프레임워크로 재구성하기. 제8회 국제 공동 언어 처리 학회 논문집 (1권: 장문), 1권, 996-1005쪽, 2017년.

12
ICLR 2019에서 학회 논문으로 발표되었습니다.

단일 문장 유사도 및 다양한 자연어 추론 모델의 평균 CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI

단일 작업 훈련

BiLSTM 66.7 17.6 87.5 77.9/85.1 85.3/82.0 71.6/72.0 66.7 77.0 58.5 56.3
+ELMo 68.7 44.1 91.5 70.8/82.3 88.0/84.3 70.3/70.5 68.6 71.2 53.4 56.3
+CoVe 66.8 25.1 89.2 76.5/83.4 86.2/81.8 70.7/70.8 62.4 74.4 59.6 54.9
+Attn 66.9 17.6 87.5 72.8/82.9 87.7/83.9 66.6/66.7 70.0 77.2 58.5 60.6
+Attn, ELMo 67.9 44.1 91.5 71.1/82.1 87.8/83.6 57.9/56.1 72.4 75.2 52.7 56.3
+Attn, CoVe 65.6 25.1 89.2 72.8/82.4 86.1/81.3 59.4/58.0 67.9 72.5 58.1 57.7

멀티태스크 훈련

BiLSTM 60.0 18.6 82.3 75.0/82.7 84.4/79.3 69.0/66.9 65.6 74.9 59.9 9.9
+ELMo 63.1 26.4 90.9 80.2/86.7 84.2/79.7 72.9/71.5 67.4 76.0 55.6 14.1
+CoVe 59.3 9.8 82.0 73.8/81.0 83.4/76.6 64.5/61.9 65.5 70.4 52.7 32.4
+Attn 60.5 15.2 83.1 77.5/85.1 82.6/77.2 72.4/70.5 68.0 73.7 61.7 9.9
+Attn, ELMo 67.3 36.7 91.1 80.6/86.6 84.6/79.6 74.4/72.9 74.6 80.4 61.4 22.5
+Attn, CoVe 61.4 17.4 82.1 71.3/80.1 83.4/77.7 68.6/66.7 68.2 73.2 58.5 29.6

사전 훈련된 문장 표현 모델

CBoW 61.4 4.6 79.5 75.0/83.7 75.0/65.5 70.6/71.1 57.1 62.5 71.9 56.3
CBoW 61.4 4.6 79.5 75.0/83.7 75.0/65.5 70.6/71.1 57.1 62.5 71.9 56.3

Skip-Thought 61.8 0.0 82.0 76.2/84.3 78.9/70.7 74.8/74.8 63.4 58.5 73.4 49.3
Skip-Thought 61.8 0.0 82.0 76.2/84.3 78.9/70.7 74.8/74.8 63.4 58.5 73.4 49.3

InferSent 65.7 8.6 83.9 76.5/84.1 81.7/75.9 80.2/80.4 67.8 63.5 71.5 56.3
InferSent 65.7 8.6 83.9 76.5/84.1 81.7/75.9 80.2/80.4 67.8 63.5 71.5 56.3

DisSent 63.8 11.7 82.5 77.0/84.4 81.8/75.6 68.9/69.0 61.2 59.9 73.9 56.3
DisSent 63.8 11.7 82.5 77.0/84.4 81.8/75.6 68.9/69.0 61.2 59.9 73.9 56.3

GenSen 67.8 10.3 87.2 80.4/86.2 82.6/76.6 81.3/81.8 71.4 62.5 78.4 56.3
GenSen 67.8 10.3 87.2 80.4/86.2 82.6/76.6 81.3/81.8 71.4 62.5 78.4 56.3

표6: GLUE 작업의 개발 세트에서의 기준 성능입니다. MNLI의 경우, 일치 및 불일치 테스트 세트에 대한 정확도를 보고합니다. MRPC 및 QQP의 경우, 정확도와 F1을 보고합니다. STS-B의 경우, Pearson 및 Spearman 상관 관계를 보고합니다. CoLA의 경우, Matthews 상관 관계를 보고합니다. 다른 모든 작업에 대해서는 정확도를 보고합니다. 모든 값은 100으로 스케일링되었습니다.

Janyce Wiebe, Theresa Wilson, and Claire Cardie. 언어에서 의견과 감정을 주석으로 표시하는 것. 언어 자원 및 평가 국제 학회 논문집, 39권, 165-210쪽. Springer, 2005.

아디나 윌리엄스, 니키타 낭기아, 그리고 사무엘 R. 보우먼. 추론을 통한 문장 이해를 위한 광범위한 챌린지 말뭉치. 2018년 북미 지부 연합 컴퓨터 언어학 협회: 인간 언어 기술 대회 논문집.

유쿤 주, 라이언 키로스, 리치 제멜, 루슬란 살라후트디노프, 라켈 우르타순, 안토니오 토랄바, 그리고 산자 피들러. 책과 영화를 조합하여 이야기 같은 시각적 설명을 위한 정렬: 영화 시청과 책 읽기를 통해. 컴퓨터 비전 국제 학회 논문집, 19-27쪽, 2015년.

추가적인 기준 세부사항

QNLI 균형 잡힌 데이터셋을 구축하기 위해, 우리는 질문에 가장 유사한 문장이 답변 문장이 아닌 모든 쌍을 선택하고, 또한 질문에 가장 유사한 문장이 올바른 문장이지만 다른 혼동을 주는 문장이 가까운 두 번째인 경우와 동일한 양의 경우를 선택합니다. 우리의 유사성 측정은 사전 훈련된 GloVe 임베딩을 사용한 CBoW 표현에 기반합니다. 기존 데이터셋을 NLI 형식으로 변환하는 이 접근 방식은 White et al. (2017)의 최근 연구와 Dagan et al. (2006)의 텍스트 추론에 대한 원래 동기와 밀접한 관련이 있습니다. 두 연구 모두 많은 NLP 작업을 텍스트 추론으로 생산적으로 축소할 수 있다고 주장합니다.

13
ICLR 2019에서 학회 논문으로 발표되었습니다.

B 추가 기준 세부 사항

주의 메커니즘

우리는 주의 메커니즘을 다음과 같이 구현합니다: 숨겨진 상태의 두 시퀀스 u1,u2,...,uM과 v1,v2,...,vN이 주어지면, 우리는 먼저 행렬 H를 계산합니다. 여기서 Hij = ui · vj입니다. 각 ui에 대해, H의 i번째 행을 softmax를 적용하여 주의 가중치 αi를 얻고, 해당하는 문맥 벡터 ˜ vi = (cid:80)을 얻습니다.

j
αijvj는 vj의 주목 가중 합을 취함으로써 계산됩니다. 우리는 [u1; ˜ v1],...[uM; ˜ vM] 시퀀스에 대해 max pooling을 사용하여 두 번째 BiLSTM을 통과시켜 u(cid:48)를 생성합니다. 우리는 vj 벡터를 유사하게 처리하여 v(cid:48)를 얻습니다. 마지막으로, [u(cid:48);v(cid:48);|u(cid:48) − v(cid:48)|;u(cid:48) ∗ v(cid:48)]를 분류기에 입력합니다.

B.2 훈련

우리는 BiLSTM 문장 인코더와 공유된 후처리 어텐션 BiLSTM을 사용하여 모델을 훈련시킵니다. 각 작업별로 별도로 훈련된 분류기를 사용합니다. 각 훈련 업데이트마다, 각 작업별 훈련 예제 수에 비례하는 확률로 작업을 샘플링하여 훈련합니다. 각 작업의 손실을 해당 작업의 예제 수에 반비례하게 스케일링하여 전체적인 성능을 향상시키는 것을 확인했습니다. 우리는 초기 학습률이 10^-3이고 배치 크기가 128이며 그래디언트 클리핑을 사용하여 Adam (Kingma & Ba, 2015)으로 모델을 훈련시킵니다. 우리는 모든 작업에 대한 매크로 평균 점수를 검증 지표로 사용하며, 매 10,000번의 업데이트마다 검증을 수행합니다. 검증 성능이 향상되지 않을 때마다 학습률을 5로 나눕니다. 학습률이 10^-5보다 작아지거나 5번의 검증 체크 후에도 성능이 향상되지 않을 때 훈련을 중지합니다.

B.3 문장 표현 모델

우리는 다음과 같은 문장 표현 모델을 평가합니다:

1. CBoW, 문장의 토큰들의 GloVe 임베딩의 평균입니다.

2. Skip-Thought (Kiros et al., 2015)은 중간 문장을 주어진 경우 이전 문장과 다음 문장을 생성하기 위해 훈련된 시퀀스-투-시퀀스(s) 모델입니다. 우리는 Toronto Book Corpus (Zhu et al. 2015, TBC)의 문장 시퀀스로 훈련된 원래의 사전 훈련 모델7을 사용합니다.

3. InferSent (Conneau et al., 2017), MNLI와 SNLI에서 훈련된 BiLSTM과 max-pooling을 사용합니다.

4. DisSent (Nie et al., 2017)은 TBC에서 유도된 데이터를 사용하여 두 문장을 관련시키는 담화 표지 (because, so 등)를 예측하기 위해 훈련된 max-pooling을 사용하는 BiLSTM입니다. 우리는 8가지 분류를 위해 훈련된 변형을 사용합니다.

5. GenSen (Subramanian et al., 2018)은 다양한 지도 및 비지도 학습 목적으로 훈련된 시퀀스-투-시퀀스 모델입니다. 우리는 MNLI와 SNLI에서 훈련된 모델의 변형을 사용하며, TBC에서 Skip-Thought 목적과 Billion Word Benchmark에서 구성 구문 분석 목적을 사용합니다.

우리는 기본 매개변수를 사용하여 동결된 문장 인코더 위에 과제별 분류기를 훈련시킵니다. 자세한 내용과 코드는 https://github.com/nyu-mll/SentEval를 참조하세요.

C 개발 세트 결과

GLUE 웹사이트는 사용자가 하루에 두 번의 제출만 할 수 있도록 제한합니다. 이는 개인 테스트 데이터에 과적합을 피하기 위한 것입니다. GLUE에 대한 향후 작업에 대한 참고 자료로, 우리의 기준선이 달성한 최상의 개발 세트 결과를 표 6에 제시합니다.

D  벤치마크 웹사이트 세부사항

GLUE의 온라인 플랫폼은 React, Redux 및 TypeScript를 사용하여 구축되었습니다. 데이터 저장을 위해 Google Firebase를 사용하고, 제출이 이루어질 때 평가 스크립트를 호스팅하고 실행하기 위해 Google Cloud Functions를 사용합니다. 그림 1은 리더보드에서 우리의 기준선의 시각적인 표현을 보여줍니다.

7github.com/ryankiros/skip-thoughts
7깃허브 닷컴 슬래시 알 와이 알 케이 아이 알 오 에스 슬래시 라이언 키로스 슬래시 스킵-스로츠

14
ICLR 2019에서 학회 논문으로 발표되었습니다.

그림 1: 벤치마크 웹사이트 리더보드. 확장된 보기에서는 각 제출물에 대한 추가적인 세부사항을 포함하여 간단한 산문 설명과 매개변수 개수가 표시됩니다.

카테고리        개수 % 중립 % 모순 % 함축

어휘 의미론 368 31.0 27.2 41.8
서술-인자 구조 424 37.0 13.7 49.3
논리 364 37.6 26.9 35.4
지식 284 26.4 31.7 41.9

테이블 7: 대분류 카테고리별 진단 데이터셋 통계. 일부 예시는 여러 카테고리에 속하는 현상으로 태그될 수 있음.

추가 진단 데이터 세부 사항

데이터셋은 단어 의미와 문장 구조부터 고수준 추론과 세계 지식의 적용까지 자연어 이해의 여러 수준을 분석할 수 있도록 설계되었습니다.
이러한 종류의 분석을 가능하게 하기 위해, 우리는 먼저 네 가지의 넓은 범주의 현상을 식별합니다: 어휘 의미, 술어-인자 구조, 논리, 그리고 지식. 그러나 이러한 범주들은 모호하기 때문에, 우리는 각각을 더 세분화된 하위 범주들로 나눕니다. 모든 세분화된 범주들에 대한 설명은 이 섹션의 나머지 부분에서 제공됩니다. 이러한 범주들은 언어 현상과 함의를 이해하기 위해 사용될 수 있는 하나의 관점에 불과하며, 예시들이 어떻게 범주화되어야 하는지, 범주들이 어떻게 되어야 하는지 등에 대해 논의할 여지가 분명히 있습니다. 이러한 범주들은 특정한 언어 이론에 기반하지 않고, 구문과 의미 연구에서 언어학자들이 종종 식별하고 모델링한 문제들을 넓게 기반으로 합니다.

데이터셋은 벤치마크로 제공되는 것이 아니라, 모델이 포착할 수 있는 현상의 유형을 큰 그림으로 그리기 위한 분석 도구로 제공되며, 오류 분석, 질적 모델 비교 및 모델의 약점을 드러내는 적대적 예제 개발을 위한 예제 세트를 제공합니다. 언어의 분포가 다소 임의적이므로 동일한 모델의 성능을 다른 범주에서 비교하는 것은 도움이 되지 않을 것입니다. 대신, 동일한 범주에서 다른 모델의 성능을 비교하거나 보고된 점수를 오류 분석 가이드로 사용하는 것을 권장합니다.

표 7에서 진단 세트의 거친 카테고리 수와 레이블 분포를 보여줍니다.

15
ICLR 2019에서 학회 논문으로 발표되었습니다.

어휘 의미론

이러한 현상들은 단어의 의미 측면에 집중되어 있습니다.

어휘적 함의는 문장 수준뿐만 아니라 단어 수준에서도 적용될 수 있습니다. 예를 들어, 우리는 "개"가 "동물"을 어휘적으로 함의한다고 말합니다. 왜냐하면 개인 것은 모두 동물이기 때문입니다. 또한 "개"는 "고양이"와 어휘적으로 모순됩니다. 왜냐하면 둘 다 동시에 존재할 수 없기 때문입니다. 이러한 관계는 많은 유형의 단어 (명사, 형용사, 동사, 많은 전치사 등)에 적용되며 어휘적 함의와 문장적 함의 사이의 관계는 철저히 탐구되었습니다. 예를 들어, 자연 언어 시스템에서는 이 관계가 언어의 단조성에 의존하기 때문에 많은 어휘적 함의 예시는 단조성 범주 중 하나로 태그가 지정됩니다. 그러나 모든 경우에는 이를 수행하지는 않습니다 (논리학에서 단조성을 참조하십시오).

형태적 부정은 한 단어가 다른 단어에서 파생된 특수한 경우로, "저렴한"에서 "비싼", "동의하다"에서 "반대하다" 등이 있습니다. 또한 "언제나"와 "결코"와 같은 예시도 포함합니다. 이러한 예시들은 부정이나 이중 부정으로 레이블을 지정하며, 단어 수준의 논리적 부정을 포함한다고 볼 수 있습니다.

문장에 나타나는 사실성 명제는 문장 전체와 관련하여 어떤 함의 관계에 있을 수 있습니다. 이는 나타나는 맥락에 따라 결정됩니다. 많은 경우, 이는 문장 내의 어휘적인 신호(일반적으로 동사나 부사)에 의해 결정됩니다. 예를 들어,

나는 "나는 X를 인정한다"는 "X"를 함축한다고 인식한다.

"나는 그 X를 인식하지 못했다"는 "X"를 함축한다.

"나는 X를 믿는다"는 "X"를 함의하지 않는다.

"나는 X를 거부하고 있다"는 "나는 X를 하고 있다"와 모순된다.

"나는 X를 거부하지 않고 있다"는 "나는 X를 하고 있다"와 모순되지 않는다.

"나는 X를 거의 끝냈어"는 "나는 X를 끝냈어"와 반대이다.

• "나는 X를 거의 마쳤다"는 "나는 X를 마쳤다"를 의미한다.

"나는 X를 인정한다"와 같은 구문은 종종 사실적이라고 불리며, 부정에도 불구하고 X의 함의(전제로 간주되는 것)가 지속됩니다. "나는 X를 거부하고 있다"와 같은 구문은 종종 함의적이라고 불리며, 부정에 민감합니다. 또한 문장이 언급된 개체의 존재를 (비)함의하는 경우도 있습니다. 예를 들어, "나는 유니콘을 찾았다"는 "유니콘이 존재한다"를 함의하지만, "나는 유니콘을 찾고 있다"는 반드시 "유니콘이 존재한다"를 함의하지는 않습니다. 개체가 반드시 존재하지 않아도 되는 해석은 종종 의미론적 해석이라고 불리며, 이는 설명으로 표시된 속성(의도)을 다루는 것처럼 보입니다. 설명과 일치하는 개체 집합으로 귀결되지 않는 것(존재하지 않는 경우 비어있을 것임)으로 축소되지 않습니다.

우리는 이러한 현상과 관련된 모든 예시를 Factivity 라벨 아래에 배치합니다. 중첩된 명제나 개체의 존재가 전체적인 주장에 함축되는지 여부를 결정하는 데는 맥락에 따라 달려 있지만, 매우 자주 어휘적인 트리거에 크게 의존하기 때문에 이 범주를 어휘 의미론 아래에 배치합니다.

대칭성/집합성 일부 명제는 대칭적인 관계를 나타내지만, 다른 명제는 그렇지 않습니다. 예를 들어, "John married Gary"는 "Gary married John"을 함축하지만 "John likes Gary"는 "Gary likes John"을 함축하지 않습니다. 대칭적인 관계는 종종 두 인자를 주어로 모아서 다시 표현할 수 있습니다. 예를 들어, "John met Gary"는 "John and Gary met"을 함축합니다. 관계가 대칭적인지, 또는 인자를 주어로 모을 수 있는지는 종종 그 헤드 워드 (예: "like", "marry" 또는 "meet")에 의해 결정되므로 우리는 이를 어휘 의미론에 분류합니다.

중복성 단어가 문장에서 제거되어도 의미가 변하지 않는다면, 그것은 단어의 의미가 문장에 의해 충분히 표현되었다는 것을 의미합니다. 따라서 이러한 경우를 식별하는 것은 어휘 및 문장 의미론에 대한 이해를 반영합니다.

16
ICLR 2019에서 학회 논문으로 발표되었습니다.

명명된 개체 단어는 종종 세상에 존재하는 개체를 지칭합니다. 이러한 이름에 대해 이해하고자 하는 다양한 종류의 이해가 있을 수 있습니다. 이해하고자 하는 것에는 구성 구조(예: "BaltimorePolice"는 "PoliceoftheCityofBaltimore"와 동일함)나 실제 세계에서의 참조 및 약어 확장(예: "SNL"은 "Saturday Night Live"의 약어)이 포함됩니다. 이 범주는 세계 지식과 밀접한 관련이 있지만, 지칭된 개체에 대한 배경 지식이 아닌 어휘 항목으로서의 이름의 의미에 초점을 맞춥니다.

양자화 언어에서 논리적 양자화는 종종 "모든", "대부분", "일부" 및 "없음"과 같은 어휘적 트리거를 통해 표현됩니다. 양자화 및 단조성 범주는 이러한 양자화자와 그들의 인자에 대한 작업을 포함하는 함의에 대한 것으로 보존되지만, 우리는 양자화자의 교환 가능성 (예: 대부분은 많은 것을 함의한다)을 어휘 의미론의 문제로 간주하기로 선택합니다.

E.2 서술어-인자 구조

문장의 의미를 이해하는 중요한 구성 요소는 그 부분들이 어떻게 하나로 구성되는지 이해하는 것입니다. 이 범주에서는, 우리는 구문적 모호성부터 의미적 역할과 상호 참조에 이르는 다양한 문제들을 다룹니다.

구문적 모호성: 관계절, 연결 범위 이 두 가지 범주는 순전히 구문적 모호성을 해결하는 데 관여합니다. 관계절과 연결 범위는 영어에서 많은 모호성의 원천입니다.

전치사구 전치사구 부착은 NLP 시스템의 구문 분석기가 여전히 고민하는 특히 어려운 문제입니다. 우리는 이것을 구문론과 의미론의 문제로 보며, 전치사구는 다양한 의미 역할을 표현할 수 있으며 종종 직접적인 구문적 부착을 넘어 의미적으로 적용될 수 있다고 생각합니다.

핵심 주어 동사는 특정한 주어와 목적어를 선택하는데, 이는 맥락이나 표면 형태에 따라 교환 가능할 수 있습니다. 한 예로, 타동사 변칙이 있습니다: "제이크가 꽃병을 깼다"는 "꽃병이 깨졌다"를 함축하지만, "제이크가 꽃병을 깼다"는 "제이크가 깼다"를 함축하지 않습니다. 대칭성/집합성에서 볼 수 있는 다른 핵심 주어의 재배치도 핵심 주어 라벨에 속합니다.

대체: 능동/수동, 소유격/부분격, 명사화, 대격 모든 네 가지 범주는 영어에서 특정 패턴을 따르는 문법적 대체를 나타냅니다.

능동/수동: "나는 그를 봤다"는 "그는 나에게서 보였다"와 동등하며 "그는 보였다"를 함축한다.

• 소유격/부분격: "코끼리의 발"은 "코끼리의 발"과 같은 의미입니다.

명사화: "나는 그에게 사직서를 제출하도록 만들었다"는 "그의 사직서 제출을 일으켰다"를 의미한다.

• 대접격: "나는 그에게 케이크를 굽혔다"는 "나는 그를 위해 케이크를 굽혔다"와 "나는 케이크를 굽혔다"를 의미하지만 "나는 그를 굽혔다"는 의미하지 않는다.

생략/암시적 의미는 종종 동사나 다른 술어의 주어가 텍스트에서 생략되어 있으며, 독자가 빈칸을 채워넣습니다. 우리는 올바른 또는 잘못된 지시자로 빈칸을 명시적으로 채워넣음으로써 함의 예를 구성할 수 있습니다. 예를 들어, 전제 "푸틴은 러시아의 지배 체제 내에서 너무 굳건하게 자리 잡았기 때문에 그 멤버들 중 많은 사람들은 다른 지도자를 상상할 수 없다"는 "푸틴은 러시아의 지배 체제 내에서 너무 굳건하게 자리 잡았기 때문에 그 멤버들 중 많은 사람들은 푸틴 외에 다른 지도자를 상상할 수 없다"를 함의하며 "푸틴은 러시아의 지배 체제 내에서 너무 굳건하게 자리 잡았기 때문에 그 멤버들 중 많은 사람들은 자기 자신 외에 다른 지도자를 상상할 수 없다"와 모순됩니다.

이것은 종종 은유의 특수한 경우로 간주되지만, 우리는 명시적인 은유와 구분하기로 결정했습니다. 명시적인 은유는 종종 공지된 참조의 경우로 간주되며 (그리고 현대적인 참조 해결 시스템에서 일부로 시도되기도 합니다).

17
ICLR 2019에서 학회 논문으로 발표되었습니다.

Anaphora/Coreference는 여러 표현이 동일한 개체나 사건을 가리킬 때를 가리킵니다. 이는 표현의 의미가 문맥에서 다른 표현(선행 표현)에 의존하는 Anaphora와 밀접한 관련이 있습니다. 이 두 현상은 상당한 중첩을 가지고 있습니다. 예를 들어, 대명사("그녀", "우리", "그것")는 선행 표현과 공동 참조를 가지고 있습니다. 그러나 이들은 독립적으로 발생할 수도 있습니다. 예를 들어, "Theresa May"와 "British Prime Minister"라는 두 개의 명사구가 동일한 개체를 가리키는 공동 참조가 될 수 있습니다. 또는 "other"와 같은 단어로부터의 Anaphora는 선행 표현을 통해 무언가를 구별해야 합니다. 이 범주에서는 선행 표현이나 다른 구문과 공동 참조를 가지는 명시적인 구문(Anaphoric 또는 그렇지 않은)만 포함합니다. 이러한 예시는 Ellipsis/Implicits와 마찬가지로 구성합니다.

교차성 많은 수정자들, 특히 형용사들은 비교적 교차하지 않는 사용을 허용하며, 이는 그들의 함의 행동에 영향을 미친다. 예를 들어:

교차적: "그는 바이올리니스트이자 나이 든 외과의사입니다"는 "그는 나이 든 바이올리니스트이다"와 "그는 외과의사이다"를 함축합니다.

비교 불가능: "그는 바이올리니스트이자 숙련된 외과의사입니다"는 "그는 숙련된 바이올리니스트입니다"를 함축하지 않습니다.

• 비교적으로: "그는 가짜 외과의사이다"는 "그는 외과의사이다"를 함축하지 않는다.

일반적으로 한 수정자의 교차 사용은 "늙은"이 "늙은 남자들"에서처럼 해석될 수 있는 것으로, 이는 둘 다 속성을 가진 개체 집합(그들은 늙고 남자이다)을 가리킬 수 있다. 언어학자들은 종종 이를 집합 교집합을 사용하여 형식화하며, 따라서 그 이름이 붙여졌다.

교차성은 사실성과 관련이 있습니다. 예를 들어, "가짜"는 반대의 함의를 가진 수정어로 간주될 수 있으며, 이러한 예시들은 그렇게 레이블링될 것입니다. 그러나 우리는 일반적으로 같은 단어가 교차적이고 교차적이지 않은 용법을 모두 허용하기 때문에, 이를 인자 구조의 모호성으로 간주하여 교차성을 술어-인자 구조에 분류하기로 선택합니다.

제한성은 주로 명사 수식어의 사용 속성을 가리키는 데 사용됩니다. 특히, 한 수식어의 제한적인 사용은 기술되는 개체 또는 개체를 식별하는 데 사용되며, 비제한적인 사용은 식별된 개체에 추가적인 세부 정보를 추가합니다. 이 구별은 종종 함의를 통해 강조될 수 있습니다.

제한적 : "오늘 마감인 숙제를 모두 끝냈어요"는 "모든 숙제를 끝냈어요"를 의미하지 않습니다.

비제한적 : "나는 모든 성가신 베드버그를 없앴다"는 "나는 모든 베드버그를 없앴다"를 의미한다.

일반적으로 사용되는 비제한적 한정사는 보충적인 한정사, "which" 또는 "who"로 시작하는 관계절, 그리고 불필요한 단어 (예: "pesky")입니다. 비제한적 사용은 여러 형태로 나타날 수 있습니다.

E.3 논리

문장의 구조를 이해하면, 논리 연산자를 사용하여 얻을 수 있는 얕은 결론의 기준이 종종 있으며, 종종 수학적 도구를 사용하여 모델링할 수도 있습니다. 실제로, 수학적 논리의 발전은 처음에는 아리스토텔레스의 살리기즘에서 프레게의 기호론에 이르기까지 자연 언어 의미에 대한 질문에 의해 이끌렸습니다. 함의의 개념도 수학적 논리에서 빌려왔습니다.

명제 구조: 부정, 이중 부정, 연결, 배타적 논리합, 조건문
명제 논리의 모든 기본 연산은 자연어에서 나타나며, 우리의 예시와 관련이 있는 곳에 태그를 달아줍니다.

부정: "고양이가 매트 위에 앉았다"는 "고양이가 매트 위에 앉지 않았다"와 반대된다.

• 이 시장은 탐색하기 어렵지 않다는 것은 이 시장을 탐색할 수 있다는 것을 의미한다.

18
ICLR 2019에서 학회 논문으로 발표되었습니다.

• 접속사: "온도와 눈의 일관성이 딱 맞아야 한다"는 "온도가 딱 맞아야 한다"를 함축한다.
• 배타적 논리합: "인생은 대담한 모험이거나 아무것도 아니다"는 "인생은 대담한 모험이다"를 함축하지 않지만, 함축된다.
• 조건문: "둘 다 적용되면, 그것들은 본질적으로 불가능하다"는 "그것들은 본질적으로 불가능하다"를 함축하지 않는다.

조건문은 언어에서의 사용이 논리적 의미와 항상 일치하지 않기 때문에 더 복잡합니다. 예를 들어, 그들은 주장보다 더 높은 수준에서 사용될 수 있습니다: "생각해 보면, 그것은 완벽한 역심리 전술이다"는 "그것은 완벽한 역심리 전술이다"를 함축합니다.

양자화: 보편적, 실존적 양자화는 종종 "모든", "일부", "많은", "없는"과 같은 단어로 유발됩니다. 일반화된 양자화로 수학 논리에서 그 의미를 모델링하는 풍부한 연구가 있습니다. 이 두 가지 범주에서는 보편적 양자화와 실존적 양자화의 자연어 유사체로부터의 직관에 초점을 맞춥니다.

• 보편적: "모든 앵무새는 두 개의 날개를 가지고 있다"는 "내 앵무새는 두 개의 날개를 가지고 있다"를 함축하지만, 그 반대는 아니다.
• 존재적: "일부 앵무새는 두 개의 날개를 가지고 있다"는 "내 앵무새는 두 개의 날개를 가지고 있다"를 함축하지 않지만, 그 반대는 함축한다.

단조성: 상향 단조, 하향 단조, 비단조 단조성은 특정 논리 체계에서 논증 위치의 특성입니다. 일반적으로, 이는 하나의 하위 표현식만 다른 표현식 사이의 함의 관계를 유도하는 방법을 제공합니다. 언어에서는 논리 연산자와 양화자를 통해 어떤 함의가 전파되는 방식을 설명할 수 있습니다.

예를 들어, "애완동물"은 "애완 다람쥐"를 의미하며, 이는 더욱 구체적으로 "행복한 애완 다람쥐"를 의미합니다. 우리는 양화사 "하나의", "없는" 및 "정확히 하나의"가 단조성 측면에서 어떻게 다른지를 보여줄 수 있습니다.

• "나는 다람쥐 애완동물을 가지고 있어."는 "나는 애완동물을 가지고 있어."를 함축하지만 "나는 행복한 다람쥐 애완동물을 가지고 있어."를 함축하지는 않는다.
• "나는 다람쥐 애완동물을 가지고 있지 않아."는 "나는 애완동물을 가지고 있지 않아."를 함축하지 않지만 "나는 행복한 다람쥐 애완동물을 가지고 있지 않아."를 함축한다.
• "나는 정확히 한 마리의 다람쥐 애완동물을 가지고 있어."는 "나는 정확히 한 마리의 애완동물을 가지고 있어."나 "나는 정확히 한 마리의 행복한 다람쥐 애완동물을 가지고 있어."를 함축하지 않는다.

모든 예시에서 "petsquirrel"은 우리가 한정사의 한정자 위치라고 부르는 곳에 나타납니다. 우리는 다음과 같이 말합니다:

• "a"는 제한자에서 상향적으로 단조성을 가집니다: 제한자에서의 함의는 전체 문장의 함의를 유발합니다.
• "no"는 제한자에서 하향적으로 단조성을 가집니다: 제한자에서의 함의는 반대 방향으로 전체 문장의 함의를 유발합니다.
• "정확히 하나"는 제한자에서 비단조성을 가집니다: 제한자에서의 함의는 전체 문장의 함의를 유발하지 않습니다.

이렇게 하면, 하위 구문의 함의에 기반을 둔 문장들 간의 함의는 거의 항상 단조성 판단에 의존합니다. 예를 들어, 어휘적 함의를 참조하세요. 그러나, 이는 매우 일반적인 문장 쌍의 클래스이기 때문에, 논리 카테고리를 의미 있게 유지하기 위해 항상 이러한 예제들에 단조성 태그를 달지는 않습니다.

더 풍부한 논리 구조: 간격/숫자, 시간적 추론의 일부 더 높은 수준의 이성의 측면은 전통적으로 논리를 사용하여 모델링되어 왔습니다. 실제 수학적 추론(숫자에 기반한 함의)과 시간적 추론(수학적 타임라인에 대한 추론으로 종종 모델링됨)과 같은 것들이 그 예입니다.

• 간격/숫자: "오늘 밤에는 2잔 이상 마셨어요"는 "오늘 밤에는 1잔 이상 마셨어요"를 의미합니다.
• 시간적: "메리가 떠나기 전에 존이 들어왔어요"는 "존이 메리가 떠난 후에 들어왔어요"를 의미합니다.

19
ICLR 2019에서 학회 논문으로 발표되었습니다.

지식

엄밀히 말하면, 단어 의미, 구문 구조, 대용어 등을 명확히 하기 위해서는 언어 이해의 모든 수준에서 세계 지식과 상식이 필요합니다. 그래서 우리의 전체 스위트 (그리고 함의 테스트)는 어느 정도로 이러한 기능을 테스트합니다. 그러나 이러한 범주에서는 문장의 옳은 의미 구분뿐만 아니라 세계 사건에 대한 구체적인 지식이나 단어 의미나 사회적 또는 물리적 역학에 대한 상식의 적용에 따라 함의가 달려 있는 예제를 수집합니다.

세계 지식 이 카테고리에서는 사실로 명확하게 표현될 수 있는 지식에 초점을 맞추며, 더 넓고 흔하지 않은 지리, 법률, 정치, 기술 또는 문화적인 지식을 다룹니다.
예시:

"이 기사는 인터넷 전체에서 가장 양파 맛이 강한 기사입니다"는 "이 기사는 풍자같이 읽힙니다"를 의미합니다.

반응은 강하게 발열했다. 이는 반응 매체가 매우 뜨거워졌음을 의미한다.

"지금 후지산 주변에는 놀라운 하이킹 코스들이 있습니다"는 "일본에는 놀라운 하이킹 코스들이 있지만, 네팔에는 놀라운 하이킹 코스들이 없습니다"를 의미합니다.

이 범주에서는 사실로 표현하기 어려운 지식에 초점을 맞추며, 문화나 교육적 배경과는 독립적으로 대부분의 사람들이 가지고 있다고 기대하는 지식을 다룹니다. 이에는 물리적이고 사회적인 역학에 대한 기본적인 이해와 어휘적 의미(단순한 어휘적 함의나 논리적 관계 이상)가 포함됩니다. 예시:

틸러슨의 퇴임 발표는 전 세계에 충격을 주었다.
전 세계 사람들은 틸러슨의 퇴임에 대비하고 있었다.

마크 심스는 몇 년 동안 매주 한 번씩 이발사를 만나고 있습니다.

허밍버드는 밝은 주황색과 빨간색에 매우 끌리기 때문에 (따라서 먹이통은 일반적으로 이러한 색상으로 되어 있습니다) 라는 것은 "먹이통은 허밍버드를 유혹하기 위해 일반적으로 색칠되어 있습니다"를 의미합니다.

20

