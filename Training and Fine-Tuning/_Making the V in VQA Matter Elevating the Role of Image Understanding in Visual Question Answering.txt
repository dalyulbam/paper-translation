VQA의 V를 중요하게 만들기:
시각적 질문 응답에서 이미지 이해의 역할 높이기

야쉬 고얼∗1 테자스 코트∗1 더글라스 서머스-스테이2 드루브 바트라3 데비 파리크3
1버지니아 공과대학교 2육군 연구소 3조지아 공과대학교
1{ygoyal, tjskhot}@vt.edu 2douglas.a.summers-stay.civ@mail.mil 3{dbatra, parikh}@gatech.edu

요약

시각과 언어의 교차점에서의 문제는 도전적인 연구 질문과 다양한 응용 프로그램을 가능하게 하는 중요성을 가지고 있습니다. 그러나 우리의 세계에 내재된 구조와 언어의 편향은 시각적인 형태보다는 학습에 더 간단한 신호가 되기 때문에 시각 정보를 무시하는 모델을 만들어 내며, 이는 그들의 능력에 대한 과대 평가로 이어집니다. 우리는 Visual Question Answering (VQA) 작업을 위해 이러한 언어 우선순위를 극복하고 시각적인 요소를 중요시하는 것을 제안합니다. 구체적으로, 우리는 인기있는 VQA 데이터셋을 균형있게 조정하기 위해 보완적인 이미지를 수집합니다. 이렇게 하면 균형잡힌 데이터셋의 각 질문은 단순히 하나의 이미지가 아닌 질문에 대한 두 가지 다른 답변을 얻을 수 있는 두 개의 유사한 이미지와 관련이 있습니다. 우리의 데이터셋은 원래 VQA 데이터셋보다 더 균형잡힌 구조를 가지며, 이미지-질문 쌍의 수는 대략 두 배입니다. 우리의 완전한 균형잡힌 데이터셋은 Visual Question Answering Dataset and Challenge (VQA v2.0)의 2차 버전의 일부로 http://visualqa.org/에서 공개적으로 이용 가능합니다.
우리는 또한 균형잡힌 데이터셋에서 여러 최첨단 VQA 모델을 벤치마킹합니다. 모든 모델은 균형잡힌 데이터셋에서 현저하게 성능이 떨어지며, 이는 이러한 모델들이 실제로 언어 우선순위를 이용하는 것을 학습했음을 시사합니다. 이 결과는 실무자들 사이에서 질적인 감각으로 알려져 있는 것에 대한 첫 번째 구체적인 경험적 증거를 제공합니다.
마지막으로, 우리의 보완적인 이미지를 식별하기 위한 데이터 수집 프로토콜은 우리가 새로운 해석 가능한 모델을 개발할 수 있도록 합니다. 이 모델은 주어진 (이미지, 질문) 쌍에 대한 답변을 제공하는 것 외에도 반례 기반의 설명을 제공합니다. 구체적으로, 이 모델은 원본 이미지와 유사한 이미지를 식별하고, 동일한 질문에 대해 다른 답변을 가지고 있다고 믿습니다. 이는 기계에 대한 사용자들 사이에서 신뢰를 구축하는 데 도움이 될 수 있습니다.

첫 두 명의 저자가 동등하게 기여했습니다.

누가 안경을 쓰고 있나요? 어린이는 어디에 앉아 있나요?

우산이 거꾸로 되어 있나요? 침대에는 몇 명의 아이가 있나요?
여자 남자 팔 냉장고

아니요 예           1 2

그림 1: 균형 잡힌 VQA 데이터셋에서의 예시.

1. 소개

언어와 시각 문제는 이미지 캡션 [8, 4, 7, 19, 40, 21, 28] 및 시각 질문 응답 (VQA) [3, 26, 27, 10, 31]과 같은 것들이 최근 몇 년 동안 인기를 얻었습니다. 컴퓨터 비전 연구 커뮤니티는 "버킷" 인식을 넘어서 다중 모달 문제를 해결하기 위해 진전하고 있기 때문입니다.
언어의 복잡한 구성 구조는 시각과 언어의 교차점에서의 문제를 도전적으로 만듭니다. 그러나 최근의 연구들 [6, 47, 49, 16, 18, 1]은 언어가 실제로 시각적 콘텐츠를 이해하지 않고도 얕은 성능을 낼 수 있는 강력한 사전을 제공한다는 것을 지적했습니다.
이 현상은 이미지 캡션 [6] 및 시각 질문 응답 [47, 49, 16, 18, 1]에서도 관찰되었습니다. 예를 들어, VQA [3] 데이터셋에서 "어떤 스포츠인가요?"로 시작하는 질문의 41%에 대해 가장 일반적인 답은 "테니스"이며, "몇 개인가요?"로 시작하는 질문의 39%에 대해 "2"가 올바른 답입니다. 또한, Zhang et al. [47]은 VQA 데이터셋에서 특정 '시각적 선입견 편향'을 지적합니다 - 구체적으로, 사람들은 그림에 대한 질문을 할 때 이미지를 보았습니다. 따라서 사람들은 실제로 시계 탑이 있는 이미지에서만 "사진에 시계 탑이 있나요?"라는 질문을 합니다. 특히, 한 가지 특이한 예로 - 질문에 대한 답이 이미지에 따라 달라지는 경우도 있습니다.

1
VQA 데이터셋에서 "Do you see a ..."로 시작하는 n-gram을 가진 질문에 대해 "yes"로 대답하면 질문의 나머지를 읽지 않고 연관된 이미지를 보지 않아도 VQA 정확도가 87%가 됩니다!
이러한 언어 우선순위는 기계가 이미지를 올바르게 이해하는 목표에 대한 진전을 만들고 있다는 잘못된 인상을 줄 수 있습니다. 실제로는 언어 우선순위를 이용하여 높은 정확도를 달성하고 있을 뿐입니다. 이는 컴퓨터 비전의 최첨단 기술 발전을 방해할 수 있습니다 [39, 47].
이 연구에서는 이러한 언어 편향을 극복하고 VQA에서 이미지 이해의 역할을 높이기 위한 제안을 합니다. 이 목표를 달성하기 위해 언어 편향이 크게 줄어든 균형 잡힌 VQA 데이터셋을 수집합니다. 구체적으로, VQA 데이터셋에서 (이미지, 질문, 답변) 쌍 (I,Q,A)이 주어지면, 우리는 인간 주체에게 질문 Q에 대한 답변이 A가 되도록 하는 이미지 I'(I와 다른 이미지)를 식별하도록 요청합니다. 균형 잡힌 데이터셋의 예시는 그림 1에 나와 있습니다. 추가적인 무작위 예시는 그림 2와 프로젝트 웹사이트에서 확인할 수 있습니다.
우리의 가설은 이 균형 잡힌 데이터셋이 VQA 모델이 시각 정보에 집중하도록 강제할 것이라는 것입니다. 결국, 질문 Q에 대해 두 가지 다른 답변 (A와 A')을 가진 두 가지 다른 이미지 (I와 I')가 있을 때, 올바른 답을 알기 위한 유일한 방법은 이미지를 보는 것입니다. 언어 모델은 이 두 경우 (Q,I)와 (Q,I')를 구별할 근거가 없으며, 구성상 하나를 틀리게 해야 합니다. 우리는 이 구성이 언어+비전 모델이 언어 우선순위를 이용하여 높은 정확도를 달성하는 것을 방지할 것이라고 믿습니다. 이를 통해 VQA 평가 프로토콜이 이미지 이해의 진전을 더 정확하게 반영할 수 있게 됩니다.
또한, 우리의 균형 잡힌 VQA 데이터셋은 VGGNet [37]의 시맨틱 (fc7) 공간에서 원본 이미지 I와 유사한 픽된 보완 이미지 I'를 가지기 때문에 특히 어렵습니다. 따라서 VQA 모델은 두 이미지 사이의 미묘한 차이를 이해하여 두 이미지에 대한 답을 정확하게 예측해야 합니다.
단순히 데이터셋 전체에서 답변 분포 P(A)가 균일하게 되도록 하는 것만으로는 위에서 언급한 언어 편향을 완화하는 목표를 달성할 수 없습니다. 이는 언어 모델이 질문 n-gram과 답변 사이의 상관관계를 이용하기 때문입니다. 예를 들어, "Is there a clock"로 시작하는 질문은 98%의 확률로 "yes"라는 답을 가지고 있고, "Is the man standing"으로 시작하는 질문은 69%의 확률로 "no"라는 답을 가지고 있습니다. 우리가 필요한 것은 데이터셋 전체에서 P(A)의 엔트로피를 높이는 것뿐만 아니라 P(A|Q)의 엔트로피를 높이는 것입니다. 이를 통해 이미지 I가 A를 결정하는 데 역할을 해야 합니다. 이것이 우리가 각 질문 수준에서 균형을 맞추는 동기입니다.

1. http://visualqa.org/

우리의 완전한 균형 잡힌 데이터셋은 대략 110만 개의 (이미지, 질문) 쌍을 포함하고 있습니다. 이는 VQA [3] 데이터셋의 거의 두 배 크기이며, COCO [23]의 약 20만 개 이미지에 대해 약 1300만 개의 관련된 답변이 있습니다. 우리는 이 균형 잡힌 VQA 데이터셋이 VQA 접근 방식을 평가하기에 더 나은 데이터셋이라고 믿으며, 이는 프로젝트 웹사이트에서 공개적으로 다운로드할 수 있습니다.
마지막으로, 우리의 데이터 수집 프로토콜은 반례 기반 설명 모드를 개발할 수 있도록 합니다. 우리는 이미지에 대한 질문에 답변하는 것뿐만 아니라, 이미지-질문 쌍에 대한 답변을 "설명"함으로써 "어려운 부정적 예"를 제공함으로써 설명 모드를 제안합니다. 이는 현재의 이미지와 유사한 이미지로 생각되지만 질문에 대한 다른 답변을 가지고 있다고 믿는 이미지의 예를 제공합니다. 이러한 설명 모드는 VQA 모델의 사용자가 모델에 대한 신뢰를 더욱 갖게 하고, 모델의 실패를 식별하는 데 도움이 될 것입니다.
우리의 주요 기여는 다음과 같습니다: (1) 우리는 기존의 VQA 데이터셋 [3]을 균형 잡힌 데이터셋으로 만들기 위해 보완적인 이미지를 수집함으로써 거의 모든 질문이 단일 이미지가 아닌 두 개의 유사한 이미지와 관련되도록 했습니다. 결과적으로 더 균형 잡힌 VQA 데이터셋이 되었으며, 원래 VQA 데이터셋의 약 두 배 크기입니다. (2) 우리는 공개적으로 사용 가능한 코드로 기존의 '균형 잡히지 않은' VQA 데이터셋에서 훈련된 최신 VQA 모델을 우리의 균형 잡힌 데이터셋에서 평가하고, 이러한 모델이 우리의 새로운 균형 잡힌 데이터셋에서 성능이 좋지 않다는 것을 보여줍니다. 이 결과는 이러한 모델들이 더 높은 정확도를 달성하기 위해 기존의 VQA 데이터셋에서 언어 우선순위를 이용하고 있다는 우리의 가설을 확인합니다. (3) 마지막으로, 우리의 보완적인 장면을 식별하기 위한 데이터 수집 프로토콜은 기계에 대한 신뢰를 구축하는 데 도움이 되는 반례 기반 설명을 제공하는 새로운 해석 가능한 모델을 개발할 수 있게 합니다. 이 모델은 이미지에 대한 질문에 답변하는 것뿐만 아니라, 원본 이미지와 유사한 이미지를 검색하여 질문에 대한 다른 답변을 가지고 있다고 믿습니다. 이러한 설명은 사용자들 사이에서 기계에 대한 신뢰를 구축하는 데 도움이 될 수 있습니다.

관련 연구

시각 질문 응답. 최근에는 시각 질문 응답 데이터셋 [3, 22, 26, 31, 10, 46, 38, 36]과 모델 [9, 25, 2, 43, 24, 27, 47, 45, 44, 41, 35, 20, 29, 15, 42, 33, 17]이 제안되었습니다. 저희 연구는 Antol et al. [3]의 VQA 데이터셋을 기반으로 하며, 이는 가장 널리 사용되는 VQA 데이터셋 중 하나입니다. 우리는 이 인기 있는 데이터셋에 존재하는 언어적 편향을 줄여서, 더 균형잡힌 데이터셋을 만들었으며, VQA 데이터셋의 약 두 배 크기입니다. 우리는 우리의 균형 잡힌 VQA 데이터셋에서 '기준' VQA 모델 [24], 어텐션 기반 VQA 모델 [25], 그리고 VQA Real Open Ended Challenge 2016 [9]에서 우승한 모델을 벤치마킹하고, 언어만을 다루는 모델과 비교합니다.

2
그림 2: 우리가 제안한 균형 잡힌 VQA 데이터셋의 무작위 예시입니다. 각 질문에는 답이 다른 두 개의 비슷한 이미지가 있습니다.

데이터 균형 및 증강. 고수준에서 볼 때, 
우리의 작업은 '어려운 부정적인'을 수집하여 더 엄격한 평가 프로토콜을 구축하는 것으로 볼 수 있습니다. 이러한 의미에서, 이는 Hodosh 등의 작업 [14]과 유사합니다. 그들은 이진 강제 선택 이미지 캡션 작업을 만들었습니다.

기계는 두 가지 유사한 자막 중 하나를 선택하여 이미지에 자막을 달아야 합니다. 비교를 위해 Hodosh et al. [14]는 이미지에 두 가지 유사한 자막을 만들기 위해 수동으로 설계된 규칙을 구현했으며, 우리는 VQA의 질문에 대한 두 가지 유사한 이미지를 수집하기 위해 새로운 주석 인터페이스를 만들었습니다.

3
그림 3: 보완적인 이미지를 수집하기 위한 우리의 아마존 메카니컬 터크 (AMT) 인터페이스의 스냅샷.

아마도 우리의 작업과 가장 관련이 있는 것은 Zhang et al. [47]의 것입니다. 그들은 이 작업을 상당히 제한된 설정에서 수행하며, 클립아트로 만든 추상적인 장면에서 이진 (예/아니오) 질문을 연구합니다 (VQA 추상 장면 데이터셋 [3]의 일부입니다). 클립아트를 사용하면 Zhang et al.은 인간 주석가에게 "질문에 대한 답이 변경되도록 클립아트 장면을 변경하라"고 요청할 수 있습니다. 불행히도, 실제 이미지에서는 이미지 콘텐츠의 이러한 세밀한 편집은 불가능합니다. 우리의 작업은 Zhang et al.보다는 제안된 보완적인 이미지 데이터 수집 인터페이스, 실제 이미지에의 적용, 모든 질문에 대한 확장 (이진 질문뿐만 아니라), 최신 VQA 모델의 균형 잡힌 데이터셋에 대한 벤치마킹, 그리고 마지막으로 카운터 예제 기반 설명을 가진 새로운 VQA 모델의 독창성에 있습니다.
설명을 포함한 모델. 최근 몇 가지 작업에서는 일반적으로 '블랙 박스'이고 해석하기 어려운 딥러닝 모델의 예측에 대한 '설명'을 생성하는 메커니즘을 제안했습니다 [13, 34, 48, 11, 32]. [13]은 이미지 카테고리에 대한 자연어 설명 (문장)을 생성합니다. [34, 48, 11, 32]은 모델이 예측을 수행하는 동안 모델이 초점을 맞춘 영역을 강조하기 위해 이미지 위에 겹쳐진 '시각적 설명' 또는 공간 맵을 제공합니다. 이 작업에서는 세 번째 설명 모드인 카운터 예제를 소개합니다. 모델은 해당 카테고리에 속하지 않지만 모델이 가까운 것으로 믿는 인스턴스입니다.

3. 데이터셋

우리는 An-
tol et al. [3]에 의해 소개된 VQA 데이터셋을 기반으로 구축합니다. VQA 실제 이미지 데이터셋은 COCO [23]에서 약 204,000개의 이미지, 자유 형식의 자연 언어 질문 614,000개 (이미지 당 3개의 질문) 및 6백만 개 이상의 자유 형식 (간결한) 답변 (질문 당 10개의 답변)을 포함합니다. 이 데이터셋은 VQA 도메인에서 상당한 진전을 이끌어 냈지만, 앞에서 언급한대로 강한 언어 편향이 있습니다. 이 언어 편향에 대항하기 위한 우리의 주요 아이디어는 다음과 같습니다 - VQA 데이터셋의 모든 (이미지, 질문, 답변) 쌍 (I,Q,A)에 대해, 우리의 목표는 이미지 I(cid:48)를 식별하는 것입니다.

나와 비슷하지만 질문 Q에 대한 답변으로 A를 얻게 됩니다. A와 다른 A(cid:48)가 되기 위해. 우리는 이러한 보완적인 이미지를 수집하기 위해 주석 인터페이스(그림 3에 표시됨)를 아마존 메카니컬 터크(AMT)에서 구축했습니다. AMT 작업자들은 I의 24개의 가장 가까운 이웃 이미지, 질문 Q, 그리고 답변 A를 보여주고, Q에 대한 답변이 A가 아닌 이미지 I(cid:48)를 24개의 이미지 목록에서 선택하도록 요청합니다. 

"질문이 의미가 있다"는 것을 포착하기 위해, 우리는 작업자들에게 (그리고 그들이 이해했는지 확인하기 위해 자격 테스트를 실시했습니다) 질문에서 가정된 어떤 전제가 선택한 이미지에 대해 참이어야 한다고 설명했습니다. 예를 들어, "여자가 무엇을 하고 있나요?"라는 질문은 여자가 존재하고 이미지에서 볼 수 있다고 가정합니다. 여자가 보이지 않는 이미지에서 이 질문을 하면 의미가 없습니다. 

우리는 24개의 가장 가까운 이웃을 계산하기 위해 먼저 각 이미지를 깊은 합성곱 신경망(CNN)의 펜얼티메이트('fc7') 레이어의 활성화로 표현한 다음 (cid:96)2-거리를 사용하여 이웃을 계산합니다. 

보완적인 이미지가 수집되면, 우리는 이러한 새로운 이미지에 대한 답변을 수집하기 위해 두 번째 라운드의 데이터 주석을 수행합니다. 구체적으로, 우리는 선택한 이미지 I(cid:48)와 질문 Q를 10명의 새로운 AMT 작업자에게 보여주고 10개의 실제 답변을 수집합니다(3과 유사). 10개 중 가장 일반적인 답변은 새로운 답변 A(cid:48)입니다. 

이 두 단계의 데이터 수집 과정은 결국 동일한 질문 Q에 대해 의미론적으로 유사하지만 서로 다른 답변 A와 A(cid:48)를 가진 보완적인 이미지 I와 I(cid:48)의 쌍을 얻게 됩니다. I와 I(cid:48)가 의미론적으로 유사하기 때문에 VQA 모델은 I와 I(cid:48) 사이의 미묘한 차이를 이해하여 두 이미지에 대해 올바른 답변을 제공해야 합니다. 보완적인 이미지의 예시는 그림 1, 그림 2 및 프로젝트 웹사이트에서 확인할 수 있습니다. 

때로는 24개의 이웃 중 하나를 보완적인 이미지로 선택하는 것이 불가능할 수도 있습니다. 이는 (1) 질문이 24개의 이미지 중 어느 것에도 의미가 없는 경우(예: 질문이 '여자가 무엇을 하고 있나요?'이고 이웃 이미지에 여자가 없는 경우) 또는 (2) 질문이 일부 이웃 이미지에 적용되지만 질문에 대한 답변이 여전히 A(원래 이미지 I와 동일)인 경우입니다. 이러한 경우에는 데이터 수집 인터페이스에서 AMT 작업자가 "불가능"을 선택할 수 있도록 허용했습니다. 

"불가능"으로 주석이 달린 데이터를 분석한 결과, 이는 일반적으로 (1) 질문에서 이야기하는 객체가 원본 이미지에서 너무 작아서 가장 가까운 이웃 이미지는 전역적으로 유사하지만 객체를 포함하지 않을 수 있어 질문이 의미를 갖지 않는 경우이거나, (2) 질문에서의 개념이 드물 때 발생합니다(예: 작업자들에게 "바나나의 색깔은 무엇인가요?"라는 질문에 대한 답변이 "노란색"이 아닌 이미지를 선택하도록 요청한 경우).

불균형 데이터셋에서의 답변

균형 잡힌 데이터셋에서의 답변

그림 4: 원본 (불균형) VQA 데이터셋 [3]에서 무작위로 추출한 60,000개의 질문에 대한 질문 유형별 답변 분포 (상단) 및 우리가 제안한 균형 데이터셋에서의 분포 (하단).

총으로, 이러한 "불가능한" 선택은 VQA 데이터셋의 모든 질문 중 22%를 차지합니다. 우리는 더 정교한 인터페이스를 통해 작업자가 24개 이상의 인접 이미지를 스크롤할 수 있도록 한다면 이 비율을 줄일 수 있을 것으로 믿습니다. 그러나 (1) 여전히 0이 되지는 않을 것입니다 (COCO에 "여자가 날고 있습니까?"라는 질문에 "아니오"라는 답변이 없을 수도 있습니다), 그리고 (2) 작업자에게 작업이 훨씬 더 번거로울 것이므로 데이터 수집 비용도 상당히 증가할 것입니다.

우리는 VQA 데이터셋의 모든 train, val 및 test 분할에 대해 보완적인 이미지와 해당하는 새로운 답변을 수집했습니다. AMT 작업자들은 약 135,000개의 질문에 대해 "불가능"을 선택했습니다. 총으로, 우리는 train을 위해 약 195,000개의 보완적인 이미지, val을 위해 93,000개의 보완적인 이미지, 그리고 test 세트를 위해 191,000개의 보완적인 이미지를 수집했습니다. 또한, 우리는 ∼18,000개의 추가적인 (질문, 이미지) 쌍을 포함하여 test 세트를 보강했습니다.

추가적인 수단은 테스트 데이터에서 이상한 동향을 감지하는 것입니다.
따라서, 우리의 완전한 균형 잡힌 데이터셋은 443K개 이상의 훈련, 214K개의 검증 및 453K개의 테스트 (질문, 이미지) 쌍을 포함합니다.
원래 VQA 데이터셋 [3]을 따라, 우리는 테스트 세트를 test-dev, test-standard, test-challenge 및 test-reserve로 나눕니다.
자세한 내용은 [3]을 참조하십시오. 우리의 완전한 균형 잡힌 데이터셋은 공개적으로 다운로드할 수 있습니다.

우리는 실험에서 공개된 VQA 평가 스크립트를 사용합니다. 평가 지표는 각 질문에 대해 10개의 실제 정답을 사용하여 VQA 정확도를 계산합니다. 위에서 설명한대로, 우리는 VQA 데이터셋과 일관성을 유지하기 위해 보충 이미지와 해당 질문에 대해 10개의 답변을 수집했습니다 [3]. 10개의 새로운 답변의 대다수 투표가 이미지를 선택한 사람의 의도한 답변과 일치하지 않을 수도 있습니다. 이는 인간 간의 의견 차이나 보충 이미지를 선택한 작업자의 선택에 따라 발생할 수 있습니다.

5
실수를 했습니다. 우리는 이것이 사실임을 발견했습니다 - 즉, A가 A와 동일한 경우에 대해 우리는 우리의 질문 중 약 9%에 해당합니다.
그림 4는 우리의 새로운 균형 잡힌 VQA 데이터셋과 원래(불균형) VQA 데이터셋 [3]의 질문 유형별 답변 분포를 비교합니다. 우리는 몇 가지 흥미로운 동향을 알아냈습니다. 첫째, 이진 질문 (예: "이것은", "이것은", "있는가", "있는가", "하는가")은 균형 잡힌 데이터셋에서 "예"와 "아니오" 답변에 대해 더 균형 잡힌 분포를 가지고 있습니다. "야구"는 이제 "테니스"보다 "어떤 스포츠"에서 약간 더 인기가 있으며, 더 중요한 것은 전반적으로 "야구"와 "테니스"가 답변 분포에서 덜 지배적이다는 것입니다. "프리스비", "스키", "축구", "스케이트보드", "스노우보드" 및 "서핑"과 같은 여러 다른 스포츠가 균형 잡힌 데이터셋의 답변 분포에서 더 잘 보입니다. 이는 균형 잡힌 데이터셋에 더 무거운 꼬리가 포함되어 있다는 것을 시사합니다. 색상, 동물, 숫자 등을 포함한 다른 질문 유형에서도 유사한 동향을 볼 수 있습니다. 양적으로는, 질문 유형의 빈도에 따라 가중치를 부여하여 평균화된 답변 분포의 엔트로피가 균형화 후 56% 증가한다는 것을 발견했습니다.
통계가 보여주듯이, 우리의 균형 잡힌 데이터셋은 완벽하게 균형 잡힌 것은 아니지만, 원래 VQA 데이터셋보다 훨씬 균형 잡힌 것입니다. 이 균형화의 결과가 최첨단 VQA 모델의 성능에 미치는 영향은 다음 섹션에서 논의됩니다.

4. 기존 VQA 모델의 벤치마킹

우리가 VQA 모델을 훈련시키는 첫 번째 접근 방식은 언어 우선 사항 대신 시각 정보를 강조하는 것입니다. 우리의 새로운 균형 잡힌 VQA 데이터셋에서 기존 최첨단 VQA 모델을 재훈련하는 것입니다. 우리의 가설은 균형 잡힌 데이터셋에서 질문에 대해 정확하게 답변하는 모델을 훈련하는 것만으로도 언어 신호만으로는 빈약해진 상황에서 시각 신호에 더 집중하도록 모델을 유도할 것이라는 것입니다. 우리는 다음과 같은 모델들로 실험을 진행합니다:
Deeper LSTM Question + norm Image (d-LSTM+n-I) [24]: 이 모델은 [3]에서 소개된 VQA 모델과 데이터셋입니다. 이미지의 CNN 임베딩과 질문의 Long-Short Term Memory (LSTM) 임베딩을 사용하며, 이 두 임베딩을 점별 곱셈으로 결합한 후, 다중 레이어 퍼셉트론 분류기를 사용하여 훈련 데이터셋에서 가장 빈도가 높은 1000개의 답변에 대한 확률 분포를 예측합니다.
Hierarchical Co-attention (HieCoAtt) [25]: 이는 최근에 소개된 어텐션 기반 VQA 모델로, 이미지와 질문 모두에 '공동 주의'를 기반으로 답변을 예측합니다. 구체적으로, 이 모델은 질문을 계층적으로 모델링하여 (따라서 공동 주의 메커니즘을 통해 이미지도 모델링됨) 단어 수준, 구문 수준 및 전체 질문 수준에서 결합합니다. 이러한 수준은 재귀적으로 결합되어 훈련 데이터셋에서 가장 빈도가 높은 1000개의 답변에 대한 분포를 생성합니다.

멀티모달 컴팩트 바이리너 풀링 (MCB) [9]:
이 모델은 VQA Challenge 2016의 실제 이미지 트랙에서 우승한 항목입니다. 이 모델은 멀티모달 컴팩트 바이리너 풀링 메커니즘을 사용하여 이미지 특징에 주목하고 주목된 이미지 특징을 언어 특징과 결합합니다. 이러한 결합된 특징은 전체 연결 레이어를 통해 3000개의 가장 빈도가 높은 답변에 대한 확률 분포를 예측하기 위해 전달됩니다. MCB는 이전 두 모델이 VGGNet [37]의 이미지 특징을 사용하는 반면에 더 강력한 CNN 아키텍처 ResNet [12]의 이미지 특징을 사용합니다.

기준선: 이러한 모델의 정확도를 비교하기 위해 다음 기준선과 비교합니다. Prior: 훈련 세트에서 가장 일반적인 답변을 모든 테스트 질문에 대해 예측합니다. 가장 일반적인 답변은 균형 잡힌 세트와 균형 잡히지 않은 세트 모두에서 "예"입니다. 언어만: 이 언어만 기준선은 Deeper LSTM Question + norm Image [24]와 유사한 아키텍처를 가지고 있지만 질문만을 입력으로 받고 시각적 정보를 사용하지 않습니다. VQA 모델을 언어만 기준선과 비교함으로써 VQA 모델이 질문에 대답하기 위해 이미지를 얼마나 활용했는지를 정량화할 수 있습니다. 결과는 표 1에 나와 있습니다. 원래 (균형 잡히지 않은) 데이터셋과 정확도를 공정하게 비교하기 위해 원래 데이터셋과 유사한 크기의 균형 잡힌 훈련 세트를 생성합니다 (표에서 Bhalf로 참조됨). 벤치마킹을 위해 전체 균형 잡힌 훈련 세트를 사용한 결과도 보고합니다.

접근하다    UU    UB   BhalfB BB

이전       27.38 24.04 24.04 24.04
언어만 48.21 41.40 41.47 43.01
d-LSTM+n-I [24] 54.40 47.56 49.23 51.62
HieCoAtt [25] 57.09 50.31 51.88 54.57
MCB [9]     60.36 54.22 56.08 59.14

표 1: 균형 잡히지 않은/균형 잡힌 VQA 데이터셋에서 훈련/테스트한 VQA 모델의 성능. UB는 균형 잡히지 않은 훈련 데이터에서 훈련하고 균형 잡힌 검증 데이터에서 테스트한 것을 의미합니다. UU, BhalfB 및 BB는 유사하게 정의됩니다.

VQA 데이터셋에서 훈련된 현재 최첨단 VQA 모델들은 우리의 균형 잡힌 데이터셋을 평가할 때 (즉, 테이블에서 UU와 UB를 비교함) 원래의 균형 잡히지 않은 VQA 데이터셋을 평가할 때에 비해 현저히 성능이 떨어지는 것을 확인할 수 있습니다. 이 결과는 기존 모델들이 데이터셋에 존재하는 심각한 언어 편향을 학습했기 때문에 동일한 질문에 대해 서로 다른 이미지에서 다른 답변이 있는 경우에도 올바른 답변을 제공하는 능력이 감소되었음을 확인해줍니다. 이러한 모델들이 우리의 균형 잡힌 데이터셋으로 훈련되면 성능이 향상됩니다 (테이블에서 UB와 BhalfB를 비교함). 더 나아가, 모델들이 완전히 균형 잡힌 데이터셋으로 훈련될 때

6
데이터셋(원래 데이터셋의 약 두 배 크기)의 경우, 정확도는 2-3% 향상됩니다 (BhalfB와 BB를 비교). 이 정확도의 증가는 현재 VQA 모델이 데이터에 굶주려 있으며, 더 큰 VQA 데이터셋에서 혜택을 받을 수 있다는 것을 시사합니다. 표의 절대 숫자가 나타내는 대로, 이미지에서 자세한 정보를 추출하고 이 정보를 사용하여 이미지에 대한 자유 형식의 자연어 질문에 정확하게 답변할 수 있는 시각적 이해 모델을 구축하는 데는 큰 개선 여지가 있습니다. 이 균형 잡힌 데이터셋의 구성에서 예상한 대로, 균형 잡힌 데이터셋에서는 질문만으로 접근하는 방식이 균형 잡히지 않은 데이터셋에 비해 상당히 나쁜 성능을 보입니다. 이는 원래 VQA 데이터셋의 언어 편향을 확인하고, 우리가 제안한 균형 잡힌 데이터셋에서 이를 성공적으로 완화했음을 다시 확인합니다.
언어 편향의 부재 외에도, 균형 잡힌 데이터셋에서 시각적 추론도 어렵습니다. 왜냐하면 CNN이 학습한 이미지 표현에서 서로 매우 유사한 이미지 쌍이 있지만 동일한 질문에 대해 다른 답변을 가지기 때문입니다. 성공적으로 VQA 모델을 운영하기 위해서는 이러한 이미지의 미묘한 차이를 이해해야 합니다. 우리 데이터셋의 쌍 구성은 VQA 모델의 성능을 독특한 방식으로 분석할 수 있게 해줍니다. VQA 모델의 예측을 고려할 때, 해당 질문 Q에 대해 보완적인 이미지(I,I(cid:48))가 모두 올바른 답변 예측을 받은 질문의 수, 또는 모두 동일한 (올바른 또는 잘못된) 답변 예측을 받은 질문의 수, 또는 모두 다른 답변 예측을 받은 질문의 수를 세어볼 수 있습니다. HieCoAtt [25] 모델의 경우, 균형 잡히지 않은 데이터셋에서 훈련한 경우, 쌍의 13.5%가 올바르게 답변되었으며, 59.9%의 쌍이 동일한 예측을 가졌으며, 40.1%의 쌍이 다른 예측을 가졌습니다. 반면, 균형 잡힌 데이터셋에서 훈련한 경우, 동일한 모델은 쌍의 17.7%를 올바르게 답변하였으며, 성능이 4.2% 향상되었습니다! 또한, 10.5% 더 적은 쌍 (49.4%)에 대해 동일한 답변을 예측합니다. 이는 균형 잡힌 데이터셋에서 훈련함으로써 이 VQA 모델이 그렇지 않으면 유사한 두 이미지 사이의 차이를 구별할 수 있도록 학습했음을 보여줍니다. 그러나 여전히 개선할 여지가 많이 남아 있습니다. VQA 모델은 여전히 인간이 제기한 동일한 질문에 대해 서로 다른 정답을 가지는 두 이미지의 차이를 구별할 수 없습니다.
VQA v2.0 데이터셋에서 모델을 벤치마크하기 위해 이러한 모델을 VQA v2.0 train+val에서 훈련하고 VQA v2.0 test-standard에서 결과를 보고합니다. VQA v2.0 데이터셋에 대한 결과를 보고하는 논문은 test-standard 정확도를 보고하고, Table 2에 보고된 정확도와 자신의 방법의 정확도를 비교하는 것이 좋습니다.
다른 답변 유형에 대한 정확도 분석: Multimodal Compact Bilinear Pooling (MCB) [9] 및 Hierarchical Co-attention (HieCoAtt) [25] 모델에 대한 답변 유형별 정확도 분석을 추가로 수행합니다.

접근 방식    모두    예/아니오    숫자    기타

이전 25.98 61.20 00.36 01.17
언어만 44.26 67.01 31.55 27.37
d-LSTM+n-I [24] 54.22 73.46 35.18 41.83
MCB [9] 62.27 78.82 38.28 53.36

표 2: VQA v2.0 train+val로 훈련된 VQA 모델의 성능, VQA v2.0 test-standard 데이터셋에서 테스트.

접근 방식 UU UB BhalfB BB

MCB [9]
예/아니오 81.20 70.40 74.89 77.37
숫자 34.80 31.61 34.69 36.66
기타 51.19 47.90 47.43 51.23
전체 60.36 54.22 56.08 59.14

안녕하세요. HieCoAtt [25]입니다.
예/아니오 79.99 67.62 70.93 71.80
숫자 34.83 32.12 34.07 36.53
기타 45.55 41.96 42.11 46.25
전체 57.09 50.31 51.88 54.57

표 3: MCB [9] 및 HieCoAtt [25] 모델에 의해 달성된 답변 유형별 정확도 분석 결과, 불균형/균형 VQA 데이터셋에서 훈련/테스트한 경우. UB는 불균형 훈련 데이터셋에서 훈련하고 균형 검증 데이터셋에서 테스트하는 것을 의미합니다. UU, BhalfB 및 BB는 유사하게 정의됩니다.

표 3에 결과가 표시됩니다. 먼저, 우리는 UU에서 UB로 가는 동안 "예/아니오" 유형의 정확도가 상당히 감소한다는 것을 즉시 알 수 있습니다. (MCB의 경우 약 10.8% 감소, HieCoAtt의 경우 약 12.4% 감소). 이는 VQA 모델이 "예/아니오" 유형의 질문에 대해 언어적 편향을 실제로 이용하고 있으며, 이로 인해 균형이 잡히지 않은 검증 세트에서 높은 정확도를 보이는 것을 시사합니다. 왜냐하면 균형이 잡히지 않은 검증 세트에도 이러한 편향이 포함되어 있기 때문입니다. 그러나 편향이 크게 줄어든 균형 잡힌 검증 세트에서 테스트할 때 성능이 크게 감소합니다. 둘째로, 최첨단 VQA 모델의 경우 UB에서 BhalfB로의 개선의 가장 큰 원인은 "예/아니오" 답 유형 (MCB의 경우 약 4.5% 개선, HieCoAtt의 경우 약 3% 개선)과 "숫자" 답 유형 (MCB의 경우 약 3% 개선, HieCoAtt의 경우 약 2% 개선)입니다. 이 추세는 특히 흥미로운데, "예/아니오"와 "숫자" 답 유형은 기존 접근 방식에서는 최소한의 개선만이 보여진 유형입니다. 예를 들어, 20162년 VQA Real Open Ended Challenge에서 발표된 결과에서는 "예/아니오" 답 유형 범주에서 상위 4개 접근 방식 간의 정확도 차이가 미미한 0.15%이며 (상위 10개 접근 방식 간의 차이는 3.48%입니다), "숫자" 답 유형의 정확도는 거의 차이가 없습니다.

2http://visualqa.org/challenge.html
2http://visualqa.org/challenge.html

1.51%와 2.64% 각각입니다. 최신 세대의 최첨단 접근법들 간의 주요 차이점은 "기타" 응답 유형에서 나타납니다. 상위 4개 및 상위 10개 항목 중 정확도가 각각 7.03%와 10.58%로 다양합니다. 이 결과는 불균형한 VQA 데이터셋에 존재하는 언어 선호도(특히 "예/아니오" 및 "숫자" 응답 유형 질문에서)가 모든 최첨단 VQA 모델에 대해 유사한 정확도를 제공하며, 서로 크게 구별할 수 없게 만든다는 것을 시사합니다. 우리의 균형 잡힌 데이터셋에서 이러한 다른 VQA 모델들을 벤치마킹함으로써 (언어 선호도를 줄인 상태에서) 우리는 이 작업에 대한 '좋은' 모델(주의 기반 또는 구성 모델과 같이 '올바른' 귀납적 편향을 인코딩하는 모델)과 데이터셋의 편향에 맞춰 자기 조정하는 단순히 고용량 모델과 구별할 수 있을 것입니다.

5. 반례 설명

우리는 새로운 설명 방식을 제안합니다: 반례입니다. 우리는 이미지에 대한 질문을 받았을 때, 답변뿐만 아니라 입력 이미지와 유사하지만 모델이 입력 질문과 다른 답변을 가지고 있다고 믿는 예시 이미지도 제공하는 모델을 제안합니다. 이는 사용자에게 모델이 질문하는 개념을 실제로 '이해'한다는 신뢰감을 심어줄 것입니다. 예를 들어, "소화전은 어떤 색인가요?"라는 질문에 대해 VQA 모델이 "빨간색"이라고 말하는 것 외에도 "이와는 다르게"라고 추가하고 빨간색이 아닌 소화전을 포함한 예시 이미지를 보여준다면, 이 모델은 더 신뢰할 수 있다고 인식될 수 있습니다.

5.1. 모델

구체적으로, 테스트 시간에 우리의 "부정적인 설명" 또는 "반례 설명" 모델은 두 단계로 작동합니다. 첫 번째 단계에서는 일반적인 VQA 모델과 유사하게 (이미지, 질문) 쌍 (Q, I)을 입력으로 받아서 예측된 답변 Apred를 예측합니다. 두 번째 단계에서는 이 예측된 답변 Apred와 질문 Q를 사용하여 I와 유사하지만 질문 Q에 대한 Apred와 다른 답변을 가진 이미지를 검색합니다. 유사성을 보장하기 위해 모델은 I의 K개의 최근접 이웃 이미지 중 하나인 INN = {I1, I2, ..., IK}를 반례로 선택합니다. 

이러한 "부정적인 설명"을 어떻게 찾을 수 있을까요? INN에서 반례를 선택하는 한 가지 방법은 컴퓨터 비전에서 인기있는 고전적인 "하드 부정적인 마이닝" 전략을 따르는 것입니다. 구체적으로, 단순히 P(Apred|Q,Ii)가 가장 낮은 이미지를 선택합니다. 여기서 i는 1, 2, ..., K에 속하는 값입니다. 우리는 이 강력한 기준과 비교합니다. 이렇게 하면 P(Apred|Q,Ii)가 Ii에 대해 낮아지지만, Q가 Ii에 대해 "의미가 있다"는 것을 보장하지는 않습니다. 따라서 "Q: 여자가 무엇을 하고 있나요? A: 놀고 있어요."에 대한 부정적인 설명을 찾을 때는 주의해야 합니다.

3. 그것은 대조적인 예시에서 소화전의 색깔을 어떻게 생각하는지 쉽게 전달할 수도 있습니다. 우리는 이를 향후 연구에서 탐구할 것입니다.

“테니스”, 이 “하드 네거티브 마이닝” 전략은 여성이 없는 이미지를 선택할 수 있으며, 이는 모델이 질문을 이해했다고 사용자에게 설명하기에 혼란스럽고 의미 없는 설명이 될 수 있습니다. 더 나은 반례를 식별하기 위해 질문의 관련성 [30] 구성 요소를 추가할 수 있습니다.
대신, 균형 잡힌 데이터 수집 메커니즘을 활용하여 좋은 반례를 식별하기 위해 직접 훈련합니다. 인간에 의해 선택된 I(cid:48)는 정의상 좋은 반례입니다. Q는 I(cid:48)와 관련이 있습니다 (작업자들이 그렇게 하도록 요청되었기 때문에), I(cid:48)는 원래 답변 A와 다른 답변 A(cid:48)를 가지고 있으며, I(cid:48)는 I와 유사합니다. 따라서, 우리는 I(cid:48)가 질문 Q와 답변 A에 대한 INN (K = 24)의 반례인 지도 학습 데이터를 가지고 있습니다. 이 지도 학습 데이터를 기반으로 모델을 훈련하여 부정적인 또는 반례 설명을 제공하는 모델을 만듭니다.
요약하면, 테스트 시간 동안 우리의 모델은 두 가지 작업을 수행합니다. 첫째로, 질문에 대답합니다 (일반적인 VQA 모델과 유사). 둘째로, 반례를 통해 답변을 설명합니다. 첫 번째 단계에서는 이미지 I와 질문 Q를 입력으로 받고, 예측된 답변 Apred를 출력합니다. 두 번째 (설명) 단계에서는 질문 Q, 설명해야 할 답변 A4, 모델이 반례를 식별해야 하는 INN 세트를 입력으로 받습니다. 훈련 시간에는 이미지 I, 질문 Q, 해당하는 정답 A를 입력으로 받아 질문에 대답하는 방법을 학습합니다. 또한, Q, A, I(cid:48) (인간이 선택한), INN (I(cid:48) ∈ INN)을 입력으로 받아 설명하는 방법을 학습합니다.
우리의 모델 구조는 공유 기본 '트렁크' 위에 두 개의 헤드를 포함하고 있습니다 - 질문에 대답하기 위한 헤드와 설명을 제공하기 위한 다른 헤드. 구체적으로, 우리의 모델은 세 가지 주요 구성 요소로 구성됩니다.

공유 기반: 우리 모델의 첫 번째 구성 요소는 이미지와 질문의 표현을 학습하는 것입니다. 이는 2채널 네트워크로, 한 쪽 분기에서 이미지 CNN 임베딩을 입력으로 받고, 다른 쪽 분기에서 질문 LSTM 임베딩을 입력으로 받아 두 임베딩을 점별 곱셈으로 결합합니다. 이렇게 하면 우리에게 [24]의 모델과 유사한 공동 QI 임베딩이 제공됩니다. 두 번째와 세 번째 구성 요소인 응답 모델과 설명 모델은 이 공동 QI 임베딩을 입력으로 받으므로, 이 첫 번째 공유 구성 요소 위에 두 개의 헤드로 간주될 수 있습니다. 총 25개의 이미지 - 원본 이미지 I와 24개의 후보 이미지 {I1, I2, ..., I24} - 가 이 네트워크의 공유 구성 요소를 통과합니다.

답변하는 부분: 두 번째 구성 요소는 질문에 대답하는 방법을 배우는 것입니다. [24]와 유사하게, 완전히 연결된 레이어로 구성되어 있으며 소프트맥스를 통해 확률을 예측합니다.

실제로는, 이 설명을 해석하기 위한 답은 첫 번째 단계 Apred로 예측된 답일 것입니다. 그러나 우리는 질문에 대한 정답 A에 대한 부정적인 설명 주석에만 접근할 수 있습니다. 설명 모듈에 A를 제공하는 것은 답변과 설명을 따로히 평가하는 데에도 도움이 됩니다.

8
답변에 대한 능력 분포는 QI 임베딩을 기반으로 합니다.
원본 이미지I에 해당하는 QI 임베딩만이 이 구성 요소를 통과하고 교차 엔트로피 손실을 결과로 얻습니다.

3. 설명하는 헤드: 세 번째 구성 요소는 대조 예시 이미지를 통해 답변 A를 설명하는 것을 배우는 것입니다. 이는 2채널 네트워크로, 공통 임베딩 공간으로 선형 변환하는 공동 QI 임베딩(첫 번째 구성 요소의 출력)과 설명해야 할 답변 A(입력으로 제공)를 결합합니다. 이는 이러한 2개의 임베딩의 내적을 계산하여 INN의 각 이미지에 대해 스칼라 숫자를 생성합니다(입력으로 제공되며, 여기서 대조 예시를 선택합니다). K개의 후보 이미지에 대한 K개의 내적 값은 완전히 연결된 레이어를 통해 전달되어 K개의 점수 S(Ii)를 생성합니다. 여기서 i ∈ {1,2,...,K}입니다. 그런 다음 K개의 후보 이미지 {I1, I2, ..., IK}는 이러한 점수 S(Ii)에 따라 가장 가능성이 높은 순서로 정렬됩니다. 이 구성 요소는 S(I(cid:48)) − S(Ii) > M − (cid:15), Ii ∈ {I1,I2,...,IK} \ {I(cid:48)}를 장려하는 순서 쌍 힌지 랭킹 손실로 훈련됩니다. 즉, 인간이 선택한 이미지 I(cid:48)의 점수가 M(하이퍼파라미터)의 원하는 마진과 (cid:15)의 여유를 가진 다른 모든 후보 이미지보다 높아지도록 장려합니다. 이것은 물론 순서 쌍 힌지 랭킹 손실의 전통적인 '제약 형식'이며, 우리는 표준 표현식 max(cid:16) 0,M − (cid:0) S(I(cid:48)) − S(Ii)(cid:1)(cid:17)를 최소화합니다. 공유 구성 요소에 대한 결합 손실 함수는 다음과 같습니다.

L = −logP(A|I,Q) + λΣi max(0,M − ΣS(Ii)) (1)

첫 번째 항은 (I, Q)에 대한 교차 엔트로피 손실(답변 모듈 훈련용)이고, 두 번째 항은 설명 모델이 INN에서 다른 I보다 이미지 I(cid:48)에 높은 점수를 부여하도록 장려하는 쌍별 힌지 손실의 합입니다. λ는 두 손실 사이의 균형 가중치 매개변수입니다.

5.2. 결과

model, and the explanation E generated by our model.

이론적으로는 훈련 중에 입력으로 Apred를 제공할 수도 있다는 점에 유의하십시오. 결국 이는 테스트 시나리오에서 예상되는 사용 사례와 일치합니다. 그러나 이러한 대체 설정(A 대신 Apred를 입력으로 제공하는 경우)은 특이하고 자연스럽지 않은 설명 훈련 목표로 이어집니다. 구체적으로, 설명 헤드는 여전히 A를 설명하는 방법을 학습하게 될 것이며, 이는 우리가 부정적인 설명 인간 주석을 수집한 답변입니다. Apred로 질문에 대답하는 모델을 구축하지만 다른 답변 A를 설명하는 방법을 학습하는 것은 단순히 자연스럽지 않습니다! 현재 "end-to-end" 훈련을 위한 현재 추세가 실패하는 흥미로운 시나리오임을 유의하십시오.

그림 5: 우리 모델에 의해 생성된 세 가지 반례 또는 부정적인 설명(오른쪽 세 열)과 함께 입력 이미지(왼쪽), 입력 질문 Q 및 예측된 답변 A.

우리 모델에서 VQA 헤드와 설명 헤드가 생성한 상위 세 개의 부정적인 설명을 볼 수 있습니다. 이러한 설명 대부분은 합리적이고 이해하기 쉽습니다 - 이미지는 I와 유사하지만 I에 대해 예측된 답변과 다른 답변을 가지고 있습니다.
정량적 평가를 위해 우리 모델을 여러 가지 기준과 비교합니다. 무작위: INN의 후보 이미지를 무작위로 정렬합니다. 즉, INN에서 무작위로 이미지를 선택하여 가장 가능성이 높은 반례로 선택합니다. 거리: 원본 이미지 I와의 거리가 증가하는 순서로 후보 이미지를 정렬합니다. 즉, I와 가장 유사한 INN 이미지를 가장 가능성이 높은 반례로 선택합니다. VQA 모델: VQA 모델의 예측 답변에 대한 확률을 사용하여 후보 이미지를 P(A|Q,Ii)의 오름차순으로 정렬합니다. 즉, Q에 대한 답변으로 A를 가장 적게 가질 가능성이 있는 INN 이미지를 가장 가능성이 높은 반례로 선택합니다.
I(cid:48) - 인간이 선택한 이미지 -는 좋은 반례이지만, 반드시 유일한 (또는 "최상의") 반례는 아닙니다. 인간에게는 Q가 의미가 있고 답변이 A가 아닌 이미지를 선택하라는 명확한 기준이 없었습니다. "최상의" 반례가 무엇을 의미하는지 명확하지 않습니다. 인간이 선택한 반례에 대한 이러한 잠재적인 모호성에 대한 견고성을 제공하기 위해 ImageNet [5]의 상위 5개 평가 지표와 유사한 방식으로 우리의 접근 방식을 Recall@5 지표를 사용하여 평가합니다. 이는 인간이 선택한 I(cid:48)가 우리 모델이 생성한 I의 정렬된 목록의 상위 5개 중에 얼마나 자주 나타나는지를 측정합니다.

임의의 거리 VQA [3] 우리 것

리콜@5 20.79 42.84 21.65 43.39

표 4: 강력한 기준 모델과 비교하여 우리 모델의 부정적 또는 반례 설명 성능.

표 4에서 우리의 설명 모델이 무작위 기준선뿐만 아니라 VQA [3] 모델보다 유의하게 우월함을 확인할 수 있습니다. 흥미롭게도, 가장 강력한 기준선은 거리입니다. 우리의 접근법이 이를 능가하지만, 식별하는 것은 분명합니다.

9
I의 가장 가까운 이웃들 중에서 I의 반례가 되는 이미지를 찾는 것은 어려운 작업입니다. 다시 말해, 이미지에서 의미 있는 세부 사항을 추출할 수 있는 시각적 이해 모델은 아직도 찾기 어렵다는 것을 시사합니다.

결론

요약하자면, 이 논문에서는 시각적 질문 응답 작업에 대한 강한 언어 선호도를 다루고 이 작업에서 성공하기 위해 필요한 이미지 이해의 역할을 강조합니다. 우리는 '보완적인' 이미지를 수집하여 인기있는 VQA 데이터셋 [3]을 '균형'있게 만들기 위한 새로운 데이터 수집 인터페이스를 개발합니다. 데이터셋의 각 질문에 대해, 질문에 대한 서로 다른 답변을 가진 비슷한 모습의 두 보완적인 이미지가 있습니다.
이 노력은 원래 VQA 데이터셋보다 구성상 더 균형 잡힌 데이터셋을 만들어내며, 크기도 약 두 배로 증가합니다. 우리는 이 균형 잡힌 데이터셋에서 '답변 분포의 꼬리'가 더 무거워진다는 점을 질적으로와 양적으로 확인하였으며, 이는 모델이 이용할 수 있는 강한 언어 선호도를 줄입니다. 우리의 완전한 균형 잡힌 데이터셋은 Visual Question Answering Dataset and Challenge (VQA v2.0)의 2차 반복의 일부로 http://visualqa.org/에서 공개되어 있습니다.
우리는 균형 잡힌 데이터셋에서 여러 (거의) 최신 VQA 모델을 벤치마킹하고, 이 균형 잡힌 데이터셋에서 테스트하면 성능이 상당히 하락한다는 것을 확인하여, 이 모델들이 실제로 언어 편향을 이용했다는 가설을 확인합니다.
마지막으로, 우리의 보완적인 이미지를 중심으로 한 프레임워크는 우리에게 설명 가능한 새로운 모델을 개발할 수 있게 합니다 - 이미지에 대한 질문을 받으면, 우리의 모델은 답변뿐만 아니라 '반례'로 간주되는 비슷한 이미지 목록도 반환합니다. 즉, 답변이 예측된 응답과 동일하지 않은 경우입니다. 이러한 설명을 생성함으로써 사용자는 시스템이 응답을 의미하는 것으로 간주하는 것에 대한 더 나은 정신적 모델을 구축하고, 궁극적으로 신뢰를 구축할 수 있을 수 있습니다.

감사의 말씀을 드립니다. Anitha Kannan과 Aishwarya Agrawal에게 유익한 토론에 감사드립니다. 이 연구는 DP와 DB에게 NSF CAREER 수상금, DP에게 ONR YIP 수상금, DB에게 ONR Grant N00014-14-1-0679, DP에게 Sloan Fellowship, DB와 DP에게 ARO YIP 수상금, DP에게 Paul G. Allen 가족 재단으로부터의 Allen Distinguished Investigator 수상금, DB와 DP에게 ICTAS Junior Faculty 수상금, DP와 DB에게 Google Faculty Research Awards, DP와 DB에게 Amazon Academic Research Awards, DB에게 AWS in Education Research grant, DB에게 NVIDIA GPU 기부금으로 일부가 지원되었습니다. 본문의 견해와 결론은 저자들의 것으로, 미국 정부나 후원기관의 공식 정책이나 승인을 반드시 대표하는 것으로 해석되어서는 안 됩니다.

참고문헌

[1] A.Agrawal,D.Batra,andD.Parikh. 시각적 질문 응답 모델의 동작 분석. EMNLP, 2016. 1
[2] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. 신경 모듈 네트워크를 사용한 깊은 구성적 질문 응답. CVPR, 2016. 2
[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. VQA: 시각적 질문 응답. ICCV, 2015. 1, 2, 4, 5, 6, 9, 10
[4] X. Chen and C. L. Zitnick. Mind's Eye: 이미지 캡션 생성을 위한 재귀적 시각적 표현. CVPR, 2015. 1
[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: 대규모 계층적 이미지 데이터베이스. CVPR, 2009. 9
[6] J. Devlin, S. Gupta, R. B. Girshick, M. Mitchell, and C. L. Zitnick. 이미지 캡션 생성을 위한 최근접 이웃 접근 방식 탐색. CoRR, abs/1505.04467, 2015. 1
[7] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. 장기 재귀 합성곱 신경망을 사용한 시각 인식 및 설명. CVPR, 2015. 1
[8] H. Fang, S. Gupta, F. N. Iandola, R. Srivastava, L. Deng, P. Doll´ ar, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zitnick, and G. Zweig. 캡션에서 시각적 개념 및 다시. CVPR, 2015. 1
[9] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach. 시각적 질문 응답 및 시각적 그라운딩을 위한 다중 모달 컴팩트 이중 풀링. EMNLP, 2016. 2, 6, 7
[10] H. Gao, J. Mao, J. Zhou, Z. Huang, and A. Yuille. 기계에게 말하고 있나요? 다국어 이미지 질문 응답을 위한 데이터셋 및 방법. NIPS, 2015. 1, 2
[11] Y. Goyal, A. Mohapatra, D. Parikh, and D. Batra. 투명한 AI 시스템을 향하여: 시각적 질문 응답 모델 해석. ICML Workshop on Visualization for Deep Learning, 2016. 4
[12] K. He, X. Zhang, S. Ren, and J. Sun. 이미지 인식을 위한 깊은 잔여 학습. CVPR, 2016. 6
[13] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, and T. Darrell. 시각적 설명 생성. ECCV, 2016. 4
[14] M. Hodosh and J. Hockenmaier. 이진 강제 선택 작업을 사용한 이미지 설명에 대한 집중적 평가. Vision and Language Workshop, Association for Computational Linguistics 연례 회의, 2016. 3
[15] I. Ilievski, S. Yan, and J. Feng. 시각적 질문 응답을 위한 집중적 동적 주의 모델. CoRR, abs/1604.01485, 2016. 2
[16] A. Jabri, A. Joulin, and L. van der Maaten. 시각적 질문 응답 기준선 재방문. ECCV, 2016. 1
[17] K. Kafle and C. Kanan. 시각적 질문 응답을 위한 응답 유형 예측. CVPR, 2016. 2
[18] K. Kafle and C. Kanan. 시각적 질문 응답: 데이터셋, 알고리즘 및 미래의 도전 과제. CoRR, abs/1610.01465, 2016. 1

10
[19] A. Karpathy and L. Fei-Fei. Deep Visual-Semantic Alignments for Generating Image Descriptions. In CVPR, 2015.
1
[20] J.-H. Kim, S.-W. Lee, D.-H. Kwak, M.-O. Heo, J. Kim, J.-W. Ha, and B.-T. Zhang. Multimodal Residual Learning for Visual QA. In NIPS, 2016.
2
[21] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models. TACL, 2015.
1
[22] R.Krishna,Y.Zhu,O.Groth,J.Johnson,K.Hata,J.Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv preprint arXiv:1602.07332, 2016.
2
[23] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ ar, and C. L. Zitnick. Microsoft COCO: Common Objects in Context. In ECCV, 2014.
2, 4
[24] J. Lu, X. Lin, D. Batra, and D. Parikh. Deeper LSTM and normalized CNN Visual Question Answering model. https://github.com/VT-vision-lab/ VQA_LSTM_CNN, 2015.
2, 6, 7, 8
[25] J. Lu, J. Yang, D. Batra, and D. Parikh. Hierarchical Question-Image Co-Attention for Visual Question Answering. In NIPS, 2016.
2, 6, 7
[26] M. Malinowski and M. Fritz. A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input. In NIPS, 2014.
1, 2
[27] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. In ICCV, 2015.
1, 2
[28] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain Images with Multimodal Recurrent Neural Networks. In NIPS, 2014.
1
[29] H. Noh and B. Han. Training recurrent answering units with joint loss minimization for vqa. CoRR, abs/1606.03647, 2016.
2
[30] A. Ray, G. Christie, M. Bansal, D. Batra, and D. Parikh. Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions. In EMNLP, 2016.
8
[31] M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. In NIPS, 2015.
1, 2
[32] M. T. Ribeiro, S. Singh, and C. Guestrin. ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier. In Knowledge Discovery and Data Mining (KDD), 2016.
4
[33] K. Saito, A. Shin, Y. Ushiku, and T. Harada. Dualnet: Domain-invariant network for visual question answering. CoRR, abs/1606.06108, 2016.
2
[34] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra. Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization. arXiv preprint arXiv:1610.02391, 2016.
4
[35] K. J. Shih, S. Singh, and D. Hoiem. Where to look: Focus regions for visual question answering. In CVPR, 2016.
2
[36] A. Shin, Y. Ushiku, and T. Harada. The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA). arXiv preprint arXiv:1609.06657, 2016.
2

[37] K. Simonyan과 A. Zisserman. 대규모 이미지 인식을 위한 매우 깊은 합성곱 신경망. ICLR, 2015. 2, 4, 6
[38] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Ur-
tasun, and S. Fidler. MovieQA: 영화를 통한 이야기 이해를 위한 질문-답변. CVPR, 2016. 2
[39] A. Torralba와 A. Efros. 데이터셋 편향에 대한 편견 없는 관찰. CVPR, 2011. 2
[40] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 보여주고 말하기: 신경망 이미지 캡션 생성기. CVPR, 2015. 1
[41] P.Wang,Q.Wu,C.Shen,A.vandenHengel,andA.R.Dick. 시각적 질문 응답을 위한 명시적 지식 기반 추론. CoRR, abs/1511.02570, 2015. 2
[42] Q. Wu, P. Wang, C. Shen, A. van den Hengel, and A. R.
Dick. 외부 소스의 지식에 기반한 자유 형식 시각적 질문 응답. CVPR, 2016. 2
[43] C. Xiong, S. Merity, and R. Socher. 시각적 및 텍스트 질문 응답을 위한 동적 메모리 네트워크. ICML, 2016. 2
[44] H. Xu and K. Saenko. 질문-지도된 공간적 주의를 탐색하는 질문에 대한 답변: 시각적 질문 응답을 위한 ECCV, 2016. 2
[45] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. 이미지 질문 응답을 위한 쌓인 주의 네트워크. CVPR, 2016. 2
[46] L. Yu, E. Park, A. C. Berg, and T. L. Berg. 시각적 매드립스: 빈칸 채우기 설명 생성 및 질문 응답. ICCV, 2015. 2
[47] P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and
D. Parikh. Yin and Yang: 이진 시각적 질문의 균형과 응답. CVPR, 2016. 1, 2, 4
[48] B.Zhou,A.Khosla,A.Lapedriza,A.Oliva,andA.Torralba.
판별적 위치 지정을 위한 깊은 특징 학습. CVPR, 2015. 4
[49] B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus. 시각적 질문 응답을 위한 간단한 기준선. CoRR, abs/1512.02167, 2015. 1

11. 11번째

