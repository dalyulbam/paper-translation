자기 교육과 사전 교육은 보완적이다.

음성 인식을 위해

Qiantong Xu∗ Alexei Baevski∗ Tatiana Likhomanenko Paden Tomasello
키안통 쉬∗ 알렉세이 바에프스키∗ 타티아나 리코만코 파덴 토마셀로

알렉시스 코노 르로낭 콜로베르 가브리엘 신나에브 마이클 오리

페이스북 인공지능 연구

요약

자기 교육과 비감독 사전 훈련은 미분류 데이터를 사용하여 음성 인식 시스템을 개선하는 효과적인 접근 방법으로 나타났다. 그러나 그들이 유사한 패턴을 학습하는지 또는 효과적으로 결합될 수 있는지는 명확하지 않다. 본 논문에서는 의사 레이블링과 wav2vec 2.0으로 사전 훈련하는 것이 다양한 레이블 데이터 설정에서 보완적임을 보여준다. Libri-light의 레이블이 지정된 데이터로부터 단지 10분, 그리고 LibriVox의 미분류 데이터로부터 53,000시간을 사용하면 Librispeech의 깨끗한 테스트 세트와 다른 테스트 세트에서 3.0%/5.2%의 WER을 달성할 수 있다. 이는 1년 전에 레이블이 지정된 데이터로 훈련된 최고의 게시된 시스템과 경쟁한다. Librispeech의 모든 레이블 데이터로 훈련하면 1.5%/3.1%의 WER을 달성할 수 있다.

1 소개

라벨이 달린 음성 데이터로 훈련된 음성 인식 모델은 최근에 상당한 발전을 이루었습니다 [1, 2, 3, 4]. 이러한 모델의 단점은 잘 동작하기 위해 많은 라벨이 달린 데이터가 필요하다는 것인데, 이는 일반적으로 영어와 몇몇 다른 언어에만 제공됩니다. 따라서, 7,000개 이상의 세계 언어 중 대다수에 대해서는 순수한 지도 학습은 현실적으로 불가능합니다 [5]. 이로 인해 라벨이 달리지 않은 음성 데이터를 어떻게 더 잘 활용할 수 있는지에 대한 많은 관심이 있었습니다 [6, 7, 8].

이에는 강력한 결과를 보여준 고전적인 자가 훈련 [9, 10, 11]도 포함되어 있으며, 이는 주석이 없는 오디오 데이터에 가짜 레이블을 부여한 다음 추가된 레이블 데이터로 최종 시스템을 재훈련하는 것입니다. 다른 연구 방향은 주석이 없는 음성 데이터에 대한 사전 훈련 표현을 수행한 후 레이블이 지정된 데이터로 세밀하게 조정하는 것입니다 [16, 17, 18, 19, 20, 21, 22, 23, 24].

이 논문에서는 자기 학습(self-training)과 비지도 사전 훈련(unsupervised pre-training)을 결합하여 레이블이 없는 데이터를 활용하는 다른 접근 방식을 조합합니다. 둘 다 경쟁력 있는 벤치마크에서 우수한 결과를 얻었으며, 우리가 탐구하는 중심 질문은 두 가지 방법이 서로 보완적인지 여부입니다. 구체적으로, 최근에 소개된 wav2vec 2.0 모델 [24]과 Kahn et al. (2020; [13]) 및 Xu et al. (2020; [14])의 자기 학습 접근 방식을 기반으로 합니다. 우리는 가짜 레이블이 지정된 데이터를 처음부터 훈련하거나 사전 훈련된 모델을 세밀하게 조정하여 모델을 훈련합니다. 두 가지 방법이 얼마나 보완적인지 더 잘 이해하기 위해, 우리는 동일한 레이블이 없는 데이터를 두 가지 방법에 모두 사용합니다.

결과적으로, Librispeech 전체 말뭉치와 Libri-light의 저자원 레이블 데이터 설정에 대한 실험 결과, 자기 학습과 비지도 사전 훈련은 실제로 보완적인 관계임을 보여주었으며, 이는 최근 자연어 이해 분야의 연구 결과와 일치합니다 [25]. 레이블 데이터로 10분만 사용 가능한 매우 저자원 설정에서 wav2vec 2.0과 자기 학습의 조합은 Librispeech의 깨끗한 테스트 세트와 다른 테스트 세트에서 WER 3.0%/5.2%를 달성합니다.

동등한 기여.

프리프린트. 심사 중.
a
r
X
i
v
:
2
0
1
0
.
1
1
4
3
0
v
1
[
c
s .
L
G
]
2
2
O
c
t
2
0
2
0
최근의 사전 훈련만으로 비교하여 상대적인 WER 감소율은 25%와 40%입니다 [24]. 언어 모델 없이 음향 모델만 사용하면 WER이 3.7%/6.5%가 됩니다 - 이는 의사 레이블링에 사용되는 언어 모델이 최종 모델로 압축되는 가설을 지원합니다. 960시간의 레이블된 훈련 데이터를 모두 사용하면 Librispeech에서 1.5%/3.1%의 WER을 달성합니다.

2 배경

2.1 비지도 사전 훈련 모델

우리는 Baevski et al. (2020; [24])의 최근 소개된 wav2vec 2.0 모델을 실험합니다.
이 모델은 원시 오디오 X를 잠재 음성 표현 z1,...,zT로 매핑하는 합성곱 특징 인코더 f : X (cid:55)→ Z를 포함하고 있습니다. 이 표현은 컨텍스트 표현 c1,...,cT를 출력하기 위해 Transformer g : Z (cid:55)→ C에 입력됩니다 [26, 18, 27]. 각 zt는 약 25ms의 오디오를 20ms 간격으로 표현하며, Transformer 아키텍처는 BERT [28, 26]를 따릅니다. 훈련 중에는 특징 인코더 표현이 양자화 모듈 Z (cid:55)→ Q를 사용하여 목표를 나타내는 q1,...,qT로 이산화됩니다. 양자화 모듈은 G = 2 코드북과 각각 320개의 항목을 가진 V = 320 항목에서 항목을 선택하기 위해 Gumbel softmax를 사용하며, 선택된 항목은 q를 얻기 위해 연결됩니다 [29, 30, 18].

모델은 마스크된 특징 인코더 출력을 사용하여 대조적인 작업을 통해 훈련됩니다. 훈련 시에는 무작위로 시작하는 인덱스를 가진 10개의 시간 단계가 마스크 처리됩니다. 목표는 K = 100 개의 다른 마스크된 시간 단계에서 샘플링된 Qt와 함께 마스크된 시간 단계의 실제 양자화된 잠재 변수 qt를 식별하는 것을 요구합니다: -log exp(sim(ct,qt)) (cid:80) ˜ q∼Qt exp(sim(ct,˜ q)) 여기서 ct는 Transformer의 출력이고, sim(a,b)는 코사인 유사도를 나타냅니다. 목표는 코드북 다양성 벌칙과 함께 보강되어 모델이 모든 코드북 항목을 사용하도록 장려합니다 [31].

2.2 자기 훈련 방법

우리는 Kahn et al. (2020; [13])와 Synnaeve et al. (2020; [2])의 가짜 라벨링 전략을 채택합니다. 이는 먼저 사용 가능한 라벨이 달린 데이터로 초기 음향 모델을 훈련시킨 다음, 가짜 라벨링 단계에서 초기 모델과 언어 모델을 사용하여 라벨이 없는 데이터에 라벨을 부여합니다. 마지막으로, 가짜 라벨이 달린 데이터와 원래의 라벨이 달린 데이터를 사용하여 새로운 음향 모델을 훈련시킵니다.

이전 작업에서는 라벨링 단계를 반복하여 각 새로운 모델로 다른 모델을 훈련시키기 위해 여러 라운드의 가짜 라벨링을 고려했습니다 [14]. 반복적인 가짜 라벨링은 더 정확하지만, 계산적으로 부담이 적은 단일 반복을 선택하고, 여전히 비지도 사전 훈련과 가짜 라벨링이 보완적인지에 대해 추론할 수 있도록 합니다. 다른 연구 방향은 결과로 얻은 가짜 라벨 데이터를 원래 라벨 데이터의 분포와 일치시키는 필터링을 조사했습니다 [15]. 두 가지 방법 모두 결과를 개선할 수 있으며, 이를 향후 연구에 맡기겠습니다.

2.3 두 가지 접근법을 결합하기

접근 방식을 결합하기 위해, 우리는 의사 라벨링을 위한 초기 모델을 사전 훈련된 모델로 대체합니다.
결과적인 훈련 파이프라인은 다음과 같습니다: 먼저 라벨이 없는 데이터에 대해 wav2vec 2.0 모델을 사전 훈련하고, 사용 가능한 라벨이 있는 데이터로 세밀 조정한 후, 모델을 사용하여 라벨이 없는 데이터에 라벨을 지정하고, 마지막으로 의사 라벨링된 데이터를 사용하여 최종 모델을 훈련합니다. 우리의 실험에서는 의사 라벨링된 데이터로 원래 wav2vec 2.0 모델을 세밀 조정하는 변형도 고려합니다.

3 실험 설정

3.1 데이터셋

사전 훈련과 자체 훈련을 위한 미분류 데이터로는 Librispeech 코퍼스 (LS-960; [32])의 음성 오디오를 고려합니다. 이 데이터에는 960시간의 오디오가 포함되어 있으며, LibriVox (LV-60k)의 오디오 데이터도 포함됩니다. 후자의 경우 Kahn et al. (2020; [33])의 전처리를 따라 53.2k 시간의 오디오가 생성됩니다. 우리는 다섯 가지 레이블이 지정된 데이터 설정을 고려합니다: 전체 960시간의 텍스트가 있는 Librispeech, 100시간으로 구성된 train-clean-100 하위 집합, 그리고 train-10h (10시간), train-1h (1시간) 및 train-10min (10분)의 Libri-light 제한된 자원 훈련 하위 집합. 우리는 표준 Librispeech dev-other/clean 및 test-clean/other 세트에서 평가합니다.

2
3.2 사전 훈련된 모델

미리 훈련된 모델은 fairseq [34]에서 구현되었으며, 우리는 공개된 fairseq github 저장소에서 이를 얻었습니다.2 이 저장소는 우리가 고려하는 다섯 가지 레이블이 지정된 데이터 설정에 대해 세밀하게 조정된 모델을 제공합니다 (§ 3.1). 우리는 24개의 트랜스포머 블록으로 구성된 LARGE 구성으로 실험을 진행하였으며, 모델 차원은 1,024이고 내부 차원은 4,096이며, 16개의 어텐션 헤드로 구성되어 약 300M의 매개변수를 포함합니다. 특징 인코더는 일곱 개의 블록을 포함하고 각 블록의 시간 합성곱은 512개의 채널을 가지며 (5,2,2,2,2,2,2)의 스트라이드와 (10,3,3,3,3,2,2)의 커널 폭을 가지며, 약 25ms의 수용 영역과 약 20ms의 스트라이드를 가지게 됩니다. 레이블이 지정된 데이터로 사전 훈련 후, 이 모델은 Connectionist Temporal Classification (CTC; [35])와 문자 기반의 출력 어휘를 사용하여 세밀하게 조정됩니다.

3.3 자기 교육

우리는 LS-960 또는 LV-60k의 오디오 데이터를 wav2vec 2.0 LARGE로 가공하여 가짜 레이블을 붙입니다. 레이블링에는 Synnaeve et al. (2020; [2])의 이중 패스 재점수화 절차를 따릅니다. 먼저, wav2vec 2.0과 표준 Librispeech 4-gram 언어 모델을 결합하여 빔 서치 중에 후보 전사 목록을 생성합니다. 빔 크기는 800입니다. 그 다음, n-최상위 목록은 상위 50개의 점수가 가장 높은 항목으로 제한되고, Librispeech 언어 말뭉치 [36, 2]에서 훈련된 Transformer LM으로 재점수화됩니다. Transformer LM은 모델 차원이 1,280이고 내부 차원이 6,144이며 16개의 어텐션 헤드를 가지는 20개의 블록으로 구성됩니다. n-gram 모델은 개발 세트에서 150.3의 퍼플렉서티를 얻으며, Transformer 언어 모델은 49.2입니다. 우리는 이것이 정확도 손실이 거의 없으면서 빔 서치에 Transformer LM을 직접 통합하는 것보다 효율적이라고 판단했습니다. 디코딩 및 재점수화 하이퍼파라미터는 무작위 매개변수 탐색을 사용하여 각 실험에 대해 Librispeech의 dev-other에서 조정되었습니다. LM 가중치와 단어 삽입 패널티 [2]는 128회 실험에서 [0, 5] 및 [-5, 5] 범위에서 무작위로 샘플링하여 조정되었습니다.

3.4 최종 모델

우리는 Synnaeve et al. (2020; [2])를 따르고, wav2letter++ [37]을 사용하여 의사 레이블을 사용한 후 log-Mel 필터뱅크 입력을 사용하여 Transformer 기반의 시퀀스 대 시퀀스 모델을 훈련합니다. 인코더는 커널 폭이 3인 4개의 시간 합성곱 레이어를 포함한 컨볼루션 프론트엔드를 사용하며, 이후에는 모델 차원이 768이고, 어텐션 헤드가 4개이며, 피드포워드 네트워크(FFN) 차원이 3072인 36개의 Transformer 블록을 사용합니다 [28, 2]. 이 모델은 약 300M 개의 파라미터를 포함하고 있습니다.

전체 Librispeech 훈련 세트가 레이블 데이터로 사용되는 경우, 훈련 전사에서 계산된 10k 단어 조각 출력 어휘를 사용합니다 [38]. 그렇지 않은 경우, train-clean-100 전사에서 추정된 5k WP로 전환합니다 [14, 8]. 언어 모델은 § 3.3과 유사하게 통합됩니다. 4-gram 언어 모델을 사용하고, Transformer LM으로 재점수화합니다. 디코딩 및 재점수화에 사용되는 빔 크기는 50입니다.

4 결과

4.1 저자원 레이블 데이터

사전 훈련은 고자원 및 저자원 레이블 훈련 데이터 설정에서 매우 효과적임이 입증되었으며, 자체 훈련은 적어도 중간 정도의 레이블 데이터가 있는 경우 가장 효과적이었습니다 (≥ 100시간; [14, 15]). 두 방법을 결합한 경우 더 효과적일 수 있는지 알아보기 위해, 우리는 10분, 1시간 및 10시간의 레이블 데이터를 가진 Libri-light 설정에서 실험을 시작합니다. 사전 훈련 및 의사 레이블링에는 Librispeech의 트랜스크립션 없는 960시간 또는 LibriVox의 53.2k 시간을 사용합니다 (§ 3.1). 기준선으로는 Librispeech에서 사전 훈련된 wav2vec 2.0을 고려하고, 레이블 데이터 분할 중 하나에서 세밀 조정합니다.

우리는 공개적으로 사용 가능한 wav2vec 2.0 모델을 사용하여 미분류된 데이터에 의사 레이블 (ST)을 부여하고, 그 결과 레이블로 최종 모델을 훈련시키기 위해 두 가지 옵션을 평가합니다. 첫 번째 옵션은 단어 조각 어휘 (s2s scratch)를 사용하여 무작위 초기화로부터 새로운 시퀀스 모델을 훈련시키는 것이며, 이는 Synnaeve et al. (2019; [2]; § 3.4)를 따릅니다. 다른 옵션은 CTC와 문자 기반 어휘 (ctc ft)를 사용하여 의사 레이블 데이터에서 wav2vec 2.0을 세밀 조정하는 것입니다.

2https://github.com/pytorch/fairseq/tree/master/examples/wav2vec

2https://github.com/pytorch/fairseq/tree/master/examples/wav2vec

3
표 1: Libri-light 저자원 레이블 데이터 설정에서 Librispeech 개발 및 테스트 세트의 WER
10분, 1시간 및 10시간에 대한 결과입니다. 미분류 데이터로는 Librispeech (LS-960) 또는
더 큰 LibriVox (LV-60k)의 오디오를 사용합니다. ST (s2s scratch)는 무작위 초기화에서
의사 레이블 데이터로 시퀀스 투 시퀀스 모델을 훈련시키며, ST (ctc ft)는 CTC와 문자 기반 어휘를 사용하여
의사 레이블을 사용하여 wav2vec 2.0을 세밀 조정합니다. 모든 결과는 추론 시점에서 언어 모델을 사용합니다.

모델

정리되지 않은 데이터 정리 다른 정리 다른

10분 레이블이 지정된
Discr. BERT [27] LS-960 15.7 24.1 16.3 25.2
wav2vec 2.0 [24] LS-960 6.6 10.6 6.8 10.8
+ ST (s2s scratch) LS-960 4.1 7.0 5.0 8.1
+ ST (ctc ft) LS-960 3.6 6.6 4.0 7.2

wav2vec 2.0 [24] LV-60k 5.0 8.4 5.2 8.6
+ ST (s2s scratch) LV-60k 2.6 4.7 3.1 5.4
+ ST (ctc ft) LV-60k 2.8 4.6 3.0 5.2

wav2vec 2.0 [24] LV-60k 5.0 8.4 5.2 8.6
+ ST (s2s scratch) LV-60k 2.6 4.7 3.1 5.4
+ ST (ctc ft) LV-60k 2.8 4.6 3.0 5.2

1시간 레이블링
Discr. BERT [27] LS-960 8.5 16.4 9.0 17.6
wav2vec 2.0 [24] LS-960 3.8 7.1 3.9 7.6
+ ST (s2s scratch) LS-960 2.9 5.6 3.4 6.6
+ ST (ctc ft) LS-960 2.8 5.5 3.1 6.3

10시 레이블
Discr. BERT [27] LS-960 5.3 13.2 5.9 14.1
IPL [14] LS-960 23.5 25.5 24.4 26.0
wav2vec 2.0 [24] LS-960 2.9 5.7 3.2 6.1
+ ST (s2s scratch) LS-960 2.5 5.1 3.5 5.9
+ ST (ctc ft) LS-960 2.6 5.2 2.9 5.7

표 1은 사전 훈련과 자체 훈련의 조합(wav2vec 2.0 + ST)이 모든 저자원 설정에서 사전 훈련만(wav2vec 2.0)보다 우수한 성능을 보여준다는 것을 보여줍니다. 또한, 10시간 레이블링된 설정에서 반복적인 의사 레이블링 [14]보다 매우 큰 개선을 이루어냅니다. 이는 사전 훈련으로 인해 초기 모델이 훨씬 강력하기 때문이며, 단지 10시간의 레이블링된 데이터로만 좋은 지도 학습 모델을 훈련시키는 것은 매우 어렵기 때문입니다.

라벨이 지정된 데이터로 단 10분만 사용하여, LibriVox와 사전 훈련 및 가짜 라벨링의 조합은 test-other에서 5.2%의 WER을 달성합니다. LS-960을 라벨이 지정되지 않은 데이터로 사용하고 라벨이 지정된 데이터로 10분 사용할 경우, wav2vec 2.0 + ST는 test-clean/other에서 4.0%/7.2%의 WER을 달성합니다. 이는 가장 잘 알려진 가짜 라벨링 접근법 [15]의 4.2%/8.6%와 비교하여 더 좋은 성능을 보입니다. 이 접근법은 100시간의 라벨이 지정된 데이터를 사용합니다. 추가적인 라벨이 지정되지 않은 데이터는 큰 개선을 이끌어내며, LS-960의 4.0%/7.2%에서 LV-60k의 3.0%/5.2%로 WER을 25-28% 줄일 수 있습니다. 그러나 추가적인 라벨이 지정된 데이터를 늘리는 것은 더 이상의 효과를 가져오지 않습니다. 이 문제에 대해서는 § 5에서 다시 다루겠습니다. Fine-tuning (ctc ft)은 일반적으로 WP 어휘를 사용한 시퀀스 모델의 처음부터 훈련하는 것보다 우수한 성능을 보입니다. 이는 모델이 사전 훈련된 표현을 활용할 수 있기 때문입니다.

4.2 고자원 레이블 데이터

다음으로, 우리는 더 많은 레이블이 달린 데이터로 성능을 평가합니다. 우리는 Librispeech의 100시간 깨끗한 하위 집합과 Librispeech의 모든 960시간의 레이블이 달린 데이터를 고려합니다. 표 2는 LS-960이 레이블이 달린 100시간 데이터가 있는 경우 기준선을 능가하기에 충분하지 않음을 보여줍니다. 그러나 LV-60k를 사용하면 성능이 향상되어 wav2vec 2.0에 비해 test-other에서 상대적인 WER 감소율이 10% 달성됩니다.

모든 Librispeech 벤치마크를 레이블된 데이터로 사용할 때, wav2vec 2.0과 가짜 레이블링을 결합하면 WER 1.5%/3.1%를 달성할 수 있습니다. 이 결과는 강력한 시퀀스 대 시퀀스로 달성되었습니다.

4
표 2: 라벨링된 데이터로 청정 100시간 하위집합 또는 Librispeech의 모든 960시간을 사용할 때의 WER (표 1 참조). 이전 연구에서는 라벨이 없는 860시간 (LS-860)을 사용했지만, 라벨링된 데이터를 포함한 총 시간은 960시간으로 우리의 설정과 비교 가능하다.

모델

정리되지 않은 개발 테스트 데이터를 정리하고 다른 데이터를 정리합니다.

100시간 레이블링된
Discr. BERT [27] LS-960 4.0 10.9 4.5 12.1
ST [13] LS-860 5.4 19.0 5.8 20.1
IPL [14] LS-860 5.0 8.0 5.6 9.0
Noisy student [15] LS-860 3.9 8.8 4.2 8.6
wav2vec 2.0 [24] LS-960 2.1 4.8 2.3 5.0
+ ST (s2s scratch) LS-960 2.3 4.6 2.7 5.4
+ ST (ctc ft) LS-960 2.2 4.6 2.4 5.0

IPL [14]     LV-60k 3.19 6.14 3.72 7.11
wav2vec 2.0 [24] LV-60k 1.9 4.0 2.0 4.0
+ ST (s2s scratch) LV-60k 1.4 2.8 1.9 3.8
+ ST (ctc ft) LV-60k 1.7 3.2 1.9 3.6

IPL [14]     LV-60k 3.19 6.14 3.72 7.11
wav2vec 2.0 [24] LV-60k 1.9 4.0 2.0 4.0
+ ST (s2s scratch) LV-60k 1.4 2.8 1.9 3.8
+ ST (ctc ft) LV-60k 1.7 3.2 1.9 3.6

960h 라벨이 지정된
감독된
SpecAugment [1] -    -   -   2.5 5.8
ContextNet [3] -    1.9 3.9  1.9 4.1
Conformer [4] -     2.1 4.3  1.9 3.9

반지도 학습
IPL [14]     LV-60k 1.85 3.26 2.10 4.01
노이즈 스튜던트 [15] LV-60k 1.6 3.4 1.7 3.4
wav2vec 2.0 [24] LV-60k 1.6 3.0 1.8 3.3
+ ST (s2s 스크래치) LV-60k 1.1 2.7 1.5 3.1
+ ST (ctc 파인튠) LV-60k 1.6 2.9 1.8 3.3

모델은 처음부터 훈련되었습니다. 작은 설정에서 CTC와 함께 세밀 조정보다는 효과가 적지만, 강력한 시퀀스 대 시퀀스 모델은 이러한 큰 설정에서 뛰어납니다. 모델의 디코더 부분은 언어 모델과 같은 역할을 하는데, 이는 과적합되지 않습니다. 세밀 조정의 성능이 낮은 이유는 CTC가 많은 가짜 레이블 데이터가 있는 경우에는 더 복잡한 시퀀스 대 시퀀스 모델보다 경쟁력이 떨어지기 때문입니다 [2].

4.3 추론 시간에 언어 모델 없이 얻은 결과

표 3은 언어 모델 없이도 결합 훈련 모델이 매우 좋은 성능을 보여준다는 것을 보여줍니다. 이는 의사 라벨링 중에 사용된 언어 모델이 의사 라벨링된 데이터로 일부 증류되었기 때문입니다 [2]. 이 효과는 특히 LM이 없는 10분 라벨 설정에서 뚜렷하게 나타납니다. 여기서 wav2vec 2.0 + ST (s2s scratch)는 기준선인 wav2vec 2.0 - LM의 WER을 테스트-기타에서 상대적으로 83% 감소시킵니다. 더 많은 라벨링된 데이터가 사용 가능해지면 언어 모델 없는 음향 모델의 성능이 향상되지만, 자기 훈련된 모델이 언어 모델을 증류한 것에는 여전히 분명한 영향이 있습니다. 일반적으로, (s2s scratch) 설정에서의 시퀀스 투 시퀀스 모델은 세밀 조정에 사용되는 CTC 모델과 비교하여 의사 라벨링 시 사용된 언어 모델을 더 잘 증류할 수 있습니다.

5 분석

우리는 이전에 더 많은 레이블된 데이터로 인해 개선이 감소하는 것을 보았다 (§ 4.1). 이를 더 잘 이해하기 위해, 우리는 Librispeech에서 실험을 수행하였는데, 이 실험에서는 레이블되지 않은 데이터와 레이블된 데이터 사이의 고정된 비율을 고려하였다. 표 4는 상대적인 개선이 레이블된 데이터만큼이 아닌 레이블되지 않은 데이터의 양에 따라 결정된다는 것을 보여준다. 표 1은 고정된 비율로 10분 레이블 분할에 대해 훨씬 큰 개선을 보여줬다.

5
테이블 3: 10분 동안의 Librispeech에서 언어 모델 (LM)을 사용한 WER 및 라벨이 지정된 960시간의 데이터와 라이브리복스의 라벨이 지정되지 않은 데이터.

모델

개발      테스트
청소 다른 청소 다른

10분 레이블이 지정된
wav2vec 2.0 [24]    5.0 8.4  5.2 8.6
- 언어 모델(LM)               38.3 41.0 40.2 38.7

wav2vec 2.0 + ST (s2s scratch) 2.6 4.7 3.1 5.4
- LM                3.3 5.9  3.7 6.5
wav2vec 2.0 + ST (ctc ft) 2.8 4.6 3.0 5.2
- LM                4.2 6.9  4.3 7.2

wav2vec 2.0 + ST (s2s scratch) 2.6 4.7 3.1 5.4
- 언어 모델 (LM)                3.3 5.9  3.7 6.5
wav2vec 2.0 + ST (ctc ft) 2.8 4.6 3.0 5.2
- 언어 모델 (LM)                4.2 6.9  4.3 7.2

960h 라벨이 지정되었습니다.
wav2vec 2.0 [24]    1.6 3.0  1.8 3.3
- LM                2.1 4.5  2.2 4.5

wav2vec 2.0 + ST (s2s scratch) 1.1 2.7 1.5 3.1
- LM                1.3 3.1  1.7 3.5
wav2vec 2.0 + ST (ctc ft) 1.6 2.9 1.8 3.3
- LM                1.7 3.6  1.9 3.9

wav2vec 2.0 + ST (s2s scratch) 1.1 2.7 1.5 3.1
- 언어 모델 (LM)                1.3 3.1  1.7 3.5
wav2vec 2.0 + ST (ctc ft) 1.6 2.9 1.8 3.3
- 언어 모델 (LM)                1.7 3.6  1.9 3.9

표 4: 성능의 주요 동력은 레이블 및 레이블되지 않은 데이터의 비율입니다. 우리는 각 레이블 설정에 8.6배 더 많은 레이블되지 않은 데이터를 추가합니다. 결과는 n-gram 모델과 LS-960의 하위 집합을 사용하여 dev-other에 기록되었습니다.

라벨이 지정되지 않은 개발-기타 % 변화

wav2vec 2.0 10분 86분 12.9
+ ST     10분 86분  12.0   7%

wav2vec 2.0 1시간 8.6시간    8.5
+ ST     1시간   8.6시간     7.6    11%

wav2vec 2.0 10h 86h    6.9
+ ST     10h  86h      6.5    6%

wav2vec 2.0 100시간 860시간 5.7
+ ST     100시간 860시간     5.3    7%

라벨이 지정된 데이터와 라벨이 지정되지 않은 데이터의 상대적인 개선은 100시간 라벨이 지정된 설정과 비교할 만합니다 (표 2).

6 결론

비감독 사전 훈련과 가짜 라벨링은 음성 인식에 상호 보완적입니다. 이를 통해 10분의 전사된 음성만으로도 단어 오류율이 1년 전에는 960시간의 라벨링된 데이터로 훈련된 최고 시스템에만 해당되던 수준을 달성할 수 있습니다.

참고문헌

[1] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le. Specaugment: 자동 음성 인식을 위한 간단한 데이터 증강 방법. Interspeech 논문집, 2019.

[2] G. Synnaeve 등. End-to-end ASR: 지도 학습에서 준지도 학습으로의 전환과 현대 아키텍처. arXiv, abs/1911.08460, 2019.

6
[3] W. Han 등. Contextnet: 전역 컨텍스트를 활용하여 자동 음성 인식을 위한 합성곱 신경망 개선. arXiv, 2020.

[4] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, 그 외. Conformer: 음성 인식을 위한 컨볼루션 보강 트랜스포머. arXiv, 2020.

[5] M. P. Lewis, G. F. Simon, and C. D. Fennig. Ethnologue: 세계의 언어, 19판. 온라인 버전: http://www.ethnologue.com, 2016.

[6] A. H. Liu, H.-Y. Lee, and L.-S. Lee. 비판적인 언어 모델을 사용한 end-to-end 음성 인식의 적대적 훈련. arXiv, 2018.

[7] M. K. Baskar, S. Watanabe, R. Astudillo, T. Hori, L. Burget, and J. ˇ Cernocký. 비대칭 음성과 텍스트를 사용한 준지도 시퀀스-투-시퀀스 음성인식. arXiv, 2019.

[8] W.-N. Hsu, A. Lee, G. Synnaeve, and A. Hannun. 지역 우선 일치를 통한 준지도 학습 음성 인식. arXiv, 2020.

[9] H. Scudder. 어떤 적응형 패턴인식 기계의 오류 확률. IEEE Trans. on Inform. Theory, 1965.

[10] D. Yarowsky. 지도 학습 방법과 견줄만한 비지도 단어 의미 분석. ACL 학회 논문집, 1995년.

[11] E. Riloff. 태그가 없는 텍스트에서 추출 패턴을 자동으로 생성하는 방법. AAAI 학회 논문집, 1996년.

[12] S. H. K. Parthasarathi와 N. Strom. 100만 시간의 음성으로 음향 모델을 구축하는 과정에서 얻은 교훈. arXiv, 2019.

[13] J. Kahn, A. Lee, and A. Hannun. 자기 학습을 통한 end-to-end 음성 인식. ICASSP, 2020.

[14] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert. 음성 인식을 위한 반복적인 의사 라벨링. arXiv, 2020.

[15] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, 그리고 기타. 자동 음성 인식을 위한 개선된 노이즈 학생 훈련. arXiv, 2020.

[16] A. v. d. Oord, Y. Li, and O. Vinyals. 대조적 예측 코딩을 이용한 표현 학습. arXiv, abs/1807.03748, 2018.

[17] S. Schneider, A. Baevski, R. Collobert, and M. Auli. wav2vec: 음성 인식을 위한 비지도 사전 훈련. Interspeech 논문집, 2019.

[18] A. Baevski, S. Schneider, and M. Auli. vq-wav2vec: 이산 음성 표현의 자기 지도 학습. ICLR 논문집, 2020.

[19] 정영아, 허위난, 탕희, 그리고 글라스 J.R. 음성 표현 학습을 위한 비지도 자기회귀 모델. arXiv, abs/1904.03240, 2019.

[20] D. Jiang, X. Lei, W. Li, N. Luo, Y. Hu, 그 외. 비지도 사전 훈련을 사용하여 transformer 기반 음성 인식 개선하기. arXiv, abs/1910.09932, 2019.

[21] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. v. d. Oord. 강건하고 다국어로 학습하는 음성 표현 학습. arXiv, 2020.

[22] M. Rivière, A. Joulin, P.-E. Mazaré, and E. Dupoux. 비지도 사전 훈련은 언어 간에 잘 전이됩니다. arXiv, abs/2002.02848, 2020.

[23] W. Wang, Q. Tang, and K. Livescu. 양방향 음성 인코더의 비지도 사전 훈련을 위한 가려진 복원을 통한 사전 훈련. arXiv, 2020.

[24] A. Baevski, H. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: 음성 표현의 자기 지도 학습을 위한 프레임워크. NeurIPS 논문집, 2020.

[25] J. Du, E. Grave, B. Gunel, V. Chaudhary, O. Celebi, 그리고 기타. 자기 학습은 자연어 이해를 위한 사전 훈련을 개선시킵니다. arXiv, 2020.

[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: 언어 이해를 위한 깊은 양방향 트랜스포머의 사전 훈련. arXiv, abs/1810.04805, 2018.

[27] 바에브스키, M. 아울리, 그리고 A. 모하메드. 음성 인식을 위한 자기 지도 사전 훈련의 효과성. arXiv, abs/1911.03912, 2019.

7
[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, 그리고 기타. 주의는 당신이 필요한 모든 것이다.
NIPS 학회 논문집, 2017.

[29] H. Jegou, M. Douze, and C. Schmid. 가장 가까운 이웃 검색을 위한 제품 양자화. IEEE Trans. Pattern Anal. Mach. Intell., 2011.

[30] 에릭 장, 시샤앙 구, 그리고 벤 풀. Gumbel-softmax를 이용한 범주형 재매개화. arXiv, abs/1611.01144, 2016.

[31] S. Dieleman, A. v. d. Oord, and K. Simonyan. 현실적인 음악 생성의 도전: 규모에 맞춘 원시 오디오 모델링. arXiv, 2018.

[32] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: 공공 도메인 오디오북을 기반으로 한 ASR 말뭉치. ICASSP 논문집, 페이지 5206-5210. IEEE, 2015.

[33] J. Kahn 및 동료. Libri-light: 제한된 또는 없는 감독 하에서의 asr을 위한 벤치마크. ICASSP, 2020.의 절차에서.

[34] M. Ott et al. fairseq: 시퀀스 모델링을 위한 빠르고 확장 가능한 도구킷. NAACL Sys. Demo. 논문집, 2019.

[35] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber. 연결주의 시간 분류: 순환 신경망을 사용하여 세분화되지 않은 순서 데이터에 레이블 지정. ICML, 2006.

[36] A. Baevski와 M. Auli. 신경망 언어 모델링을 위한 적응형 입력 표현. ICLR, 2018.

[37] V. Pratap 등. Wav2letter++: 빠른 오픈소스 음성 인식 시스템. ICASSP, 2019.

[38] T. Kudo와 J. Richardson. Sentencepiece: 신경망 텍스트 처리를 위한 간단하고 언어 독립적인 서브워드 토크나이저 및 디토크나이저. EMNLP Sys. Demo. 논문집, 2018.

8

