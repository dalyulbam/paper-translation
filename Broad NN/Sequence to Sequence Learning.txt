신경망을 이용한 시퀀스 대 시퀀스 학습

일리야 스츠케버
Google
ilyasu@google.com

오리올 비냘스
Google
vinyals@google.com

쿽 V. 레
Google
qvl@google.com

요약

깊은 신경망(DNNs)은 어려운 학습 과제에서 우수한 성능을 달성한 강력한 모델입니다. DNN이 큰 레이블이 지정된 훈련 세트가 있을 때 잘 작동하지만, 시퀀스를 시퀀스로 매핑하는 데는 사용할 수 없습니다. 이 논문에서는 시퀀스 구조에 대해 최소한의 가정만 하는 일반적인 종단간 시퀀스 학습 접근법을 제시합니다. 우리의 방법은 다층 장단기 기억(LSTM)을 사용하여 입력 시퀀스를 고정된 차원의 벡터로 매핑하고, 다른 깊은 LSTM을 사용하여 그 벡터로부터 목표 시퀀스를 디코딩합니다. 우리의 주요 결과는 WMT’14 데이터셋의 영어에서 프랑스어 번역 작업에서, LSTM에 의해 생성된 번역이 전체 테스트 세트에서 34.8의 BLEU 점수를 달성했으며, 여기서 LSTM의 BLEU 점수는 어휘 밖 단어에 대해 감점되었습니다. 또한, LSTM은 긴 문장에서 어려움을 겪지 않았습니다. 비교를 위해, 구문 기반 SMT 시스템은 같은 데이터셋에서 33.3의 BLEU 점수를 달성합니다. 앞서 언급한 SMT 시스템에 의해 생성된 1000개의 가설을 LSTM으로 재정렬했을 때, 그것의 BLEU 점수는 36.5로 증가하는데, 이는 이 작업에서 이전 최고 결과에 근접합니다. LSTM은 또한 단어 순서에 민감하고 능동태와 수동태에 상대적으로 불변인 합리적인 구문 및 문장 표현을 학습했습니다. 마지막으로, 우리는 모든 원본 문장의 단어 순서를 뒤집음으로써(목표 문장은 제외하고) LSTM의 성능이 현저히 향상되었다는 것을 발견했습니다. 이는 원본과 목표 문장 사이에 많은 단기 의존성을 도입했기 때문으로, 이는 최적화 문제를 더 쉽게 만들었습니다.

1 장 서론

깊은 신경망(DNNs)은 음성 인식 [13, 7]과 시각적 객체 인식 [19, 6, 21, 20]과 같은 어려운 문제에서 우수한 성능을 달성하는 매우 강력한 기계 학습 모델입니다. DNNs는 적당한 수의 단계에 대해 임의의 병렬 연산을 수행할 수 있기 때문에 강력합니다. DNNs의 능력을 보여주는 놀라운 예는 오직 2개의 은닉층을 사용하여 N N-비트 숫자를 정렬할 수 있다는 것입니다 [27]. 그러므로, 신경망은 전통적인 통계 모델과 관련이 있지만, 복잡한 계산을 학습합니다. 더욱이, 큰 DNN은 레이블이 지정된 훈련 세트가 네트워크의 매개변수를 지정할 충분한 정보를 가지고 있다면 지도 역전파로 훈련될 수 있습니다. 따라서, 큰 DNN의 매개변수 설정이 좋은 결과를 달성한다면(예를 들어, 사람들이 매우 빠르게 작업을 해결할 수 있다면), 지도 역전파는 이 매개변수를 찾아 문제를 해결할 것입니다.

그들의 유연성과 강력함에도 불구하고, DNN은 입력과 목표가 고정된 차원의 벡터로 합리적으로 인코딩될 수 있는 문제에만 적용될 수 있습니다. 이것은 상당한 제한입니다, 왜냐하면 많은 중요한 문제들은 미리 알려지지 않은 길이를 가진 시퀀스로 가장 잘 표현되기 때문입니다. 예를 들어, 음성 인식과 기계 번역은 순차적인 문제입니다. 마찬가지로, 질문에 대한 대답도 질문을 나타내는 단어의 시퀀스를 매핑하는 것으로 볼 수 있습니다.


이 내용은 도메인 독립적인 방식으로 시퀀스를 시퀀스로 매핑하는 학습 방법의 유용성을 설명합니다. DNN(Deep Neural Networks)에 있어 시퀀스는 입력 및 출력의 차원이 고정되어야 한다는 요구 때문에 도전적입니다. 이 논문에서는 LSTM(Long Short-Term Memory) 아키텍처를 사용하여 일반적인 시퀀스 대 시퀀스 문제를 해결할 수 있음을 보여줍니다. LSTM은 입력 시퀀스를 한 타임스텝씩 읽어 고정 차원 벡터 표현을 얻은 다음, 또 다른 LSTM을 사용하여 그 벡터에서 출력 시퀀스를 추출합니다. LSTM의 장기 시간 의존성 데이터 학습 능력은 입력과 해당 출력 사이의 상당한 시간 지연을 고려할 때 이러한 응용에 적합합니다. 이 분야에는 신경망을 사용한 여러 관련 시도가 있었으며, 이 접근 방식은 전체 입력 문장을 벡터로 매핑하는 최초의 연구자들인 Kalchbrenner와 Blunsom, 그리고 Cho 등과 밀접한 관련이 있습니다. Graves는 신경망이 입력의 다른 부분에 집중할 수 있는 새로운 차별화 가능한 주의 메커니즘을 도입했고, 이 아이디어의 변형은 Bahdanau 등에 의해 기계 번역에 성공적으로 적용되었습니다. Connectionist Sequence Classification은 시퀀스를 시퀀스로 매핑하는 또 다른 인기 있는 기법입니다.


Figure 1: 이 모델은 입력된 문장 "ABC"를 읽고 "WXYZ"라는 출력 문장을 생성합니다. 문장의 끝 토큰을 출력한 후에는 예측을 멈춥니다. LSTM이 입력 문장을 역순으로 읽는 이유는 이 방식이 데이터에 단기 의존성을 도입하여 최적화 문제를 더 쉽게 만들기 때문입니다.

WMT’14 영어-프랑스어 번역 작업에서, 이 연구는 5개의 깊은 LSTM(총 384M 파라미터, 각각 8,000 차원 상태)을 사용하여 직접 번역을 통해 BLEU 점수 34.81을 달성했습니다. 이는 대규모 신경망을 사용한 직접 번역으로는 지금까지 가장 높은 결과입니다. 이 BLEU 점수는 80,000 단어의 어휘를 가진 LSTM으로 달성되었으며, 참조 번역에 80,000 단어에 포함되지 않은 단어가 있을 때마다 점수가 감점되었습니다. 이 결과는 상대적으로 최적화되지 않은 소규모 어휘 신경망 아키텍처가 구문 기반 SMT 시스템보다 우수함을 보여줍니다.

마지막으로, LSTM을 사용하여 동일한 작업의 SMT 기준선에 대한 1000개의 최고 리스트를 재평가했습니다. 이를 통해 BLEU 점수 36.5를 얻었으며, 이는 기준선보다 3.2 포인트 향상된 것이며 이전에 발표된 최고 결과(37.0)에 근접합니다. LSTM은 매우 긴 문장에서도 문제가 없었는데, 이는 원본 문장의 단어 순서를 역전시켜 단기 의존성을 많이 도입함으로써 최적화 문제를 단순화시켰기 때문입니다. 결과적으로, SGD는 긴 문장에서도 문제 없이 LSTM을 학습할 수 있었습니다. 원본 문장의 단어 순서를 역전시키는 간단한 방법은 이 연구의 주요 기술적 기여 중 하나입니다.

LSTM의 유용한 특성 중 하나는 가변 길이의 입력 문장을 고정 차원 벡터 표현으로 매핑하는 능력입니다. 번역이 원문 문장의 의미를 포착하는 패러프레이즈 경향이 있기 때문에, 번역 목적은 LSTM이 그 의미를 포착하는 문장 표현을 찾도록 장려합니다. 문장의 의미는 멀리 있을 것입니다. 질적 평가는 이 주장을 지원하며, 우리 모델은 단어 순서를 인식하고 능동태와 수동태에 상당히 불변합니다.

2 모델

순환 신경망(RNN) [31, 28]은 시퀀스에 대한 피드포워드 신경망의 자연스러운 일반화입니다. 입력 시퀀스 (x1,...,xT)가 주어지면, 표준 RNN은 다음 방정식을 반복하여 출력 시퀀스 (y1,...,yT)를 계산합니다.

ht = 시그마(Whxxt + Whhht−1)
yt = Wyhht

RNN은 입력과 출력 간의 정렬이 미리 알려진 경우에는 시퀀스를 쉽게 매핑할 수 있습니다. 그러나 입력과 출력 시퀀스의 길이가 복잡하고 단조롭지 않은 관계를 가지며 서로 다른 경우에는 RNN을 어떻게 적용해야 할지 명확하지 않습니다.

일반적인 시퀀스 학습을 위한 가장 간단한 전략은 입력 시퀀스를 고정 크기의 벡터로 매핑하는 하나의 RNN을 사용한 다음, 벡터를 다른 RNN을 사용하여 대상 시퀀스로 매핑하는 것입니다 (이 접근 방식은 Cho et al. [5]에 의해도 채택되었습니다). RNN은 모든 관련 정보를 제공받기 때문에 원칙적으로 작동할 수 있지만, 결과적으로 장기 의존성으로 인해 RNN을 훈련하기 어려울 수 있습니다 (그림 1) [14, 4, 16, 15]. 그러나 Long Short-Term Memory (LSTM) [16]은 장기적인 시간 의존성을 가진 문제를 학습하는 데 성공적으로 알려져 있으므로, 이러한 설정에서 LSTM이 성공할 수 있습니다.

LSTM의 목표는 조건부 확률 p(y1,...,yT′|x1,...,xT)를 추정하는 것입니다. 여기서 (x1,...,xT)는 입력 시퀀스이고 y1,...,yT′는 그에 해당하는 출력 시퀀스입니다. 이때 T′의 길이는 T와 다를 수 있습니다. LSTM은 이 조건부 확률을 계산하기 위해 먼저 입력 시퀀스 (x1,...,xT)의 마지막 숨겨진 상태에 의해 주어지는 고정 차원 표현 v를 얻은 다음, x1,...,xT의 표현 v로 초기 숨겨진 상태를 설정한 표준 LSTM-LM 공식을 사용하여 y1,...,yT′의 확률을 계산합니다.

p(y1,...,yT′|x1,...,xT) = x1,...,xT에 대한 y1,...,yT′의 조건부 확률

T′
Y t=1p(yt|v,y1,...,yt−1) (1)
이 식에서 각 p(yt|v,y1,...,yt−1) 분포는 어휘 사전의 모든 단어에 대한 소프트맥스로 표현됩니다. 우리는 Graves의 LSTM 공식을 사용합니다 [10]. 각 문장이 특별한 문장의 끝을 나타내는 "<EOS>" 기호로 끝나야 한다는 점에 유의하세요. 이를 통해 모델은 가능한 모든 길이의 시퀀스에 대한 분포를 정의할 수 있습니다. 전체 구조는 그림 1에 개요가 제시되어 있으며, 표시된 LSTM은 "A", "B", "C", "<EOS>"의 표현을 계산한 다음 이 표현을 사용하여 "W", "X", "Y", "Z", "<EOS>"의 확률을 계산합니다.

우리의 실제 모델은 위의 설명과 세 가지 중요한 점에서 다릅니다. 첫째로, 우리는 입력 시퀀스에 대한 하나의 LSTM과 출력 시퀀스에 대한 다른 LSTM을 사용했습니다. 이렇게 함으로써 모델 파라미터의 수를 증가시키고 무시할 만한 계산 비용으로 LSTM을 여러 언어 쌍에 동시에 훈련시키기에 자연스럽게 만들 수 있습니다[18]. 둘째로, 우리는 깊은 LSTM이 얕은 LSTM보다 훨씬 우수한 성능을 보였으므로 네 개의 레이어를 가진 LSTM을 선택했습니다. 셋째로, 우리는 입력 문장의 단어 순서를 반전시키는 것이 매우 유용하다는 것을 발견했습니다. 예를 들어, 문장 a,b,c를 문장 α,β,γ로 매핑하는 대신 LSTM에게 c,b,a를 α,β,γ로 매핑하도록 요청합니다. 여기서 α,β,γ는 a,b,c의 번역입니다. 이렇게 하면 a는 α와 가까워지고, b는 β와 꽤 가까워지고, 이와 같은 식으로 입력과 출력 사이에서 SGD가 "소통"하기 쉬운 사실이 됩니다. 우리는 이 간단한 데이터 변환으로 LSTM의 성능을 크게 향상시킬 수 있다는 것을 발견했습니다.

3 실험

우리는 WMT'14 영어에서 프랑스어 기계 번역 작업에 우리의 방법을 두 가지 방식으로 적용했습니다. 우리는 참조 SMT 시스템을 사용하지 않고 입력 문장을 직접 번역하는 데 사용했으며, SMT 기준선의 n-최상의 목록을 재점수화하는 데 사용했습니다. 우리는 이러한 번역 방법의 정확도를 보고하고, 샘플 번역을 제시하며, 결과적인 문장 표현을 시각화합니다.

3
3.1 데이터셋 세부사항

우리는 WMT'14 영어에서 프랑스어 데이터셋을 사용했습니다. 우리는 12백만 개의 문장으로 구성된 3억 48백만 개의 프랑스어 단어와 3억 4백만 개의 영어 단어로 이루어진 모델을 훈련시켰습니다. 이는 [29]에서 선택된 깨끗한 "선택된" 하위 집합입니다. 우리는 이번 번역 작업과 이 특정 훈련 세트 하위 집합을 선택한 이유는 기준 SMT [29]에서 토큰화된 훈련 및 테스트 세트와 1000개의 최상의 목록이 공개적으로 제공되기 때문입니다.

전형적인 신경 언어 모델은 각 단어에 대한 벡터 표현에 의존하기 때문에, 우리는 두 언어 모두에 대해 고정된 어휘를 사용했습니다. 원본 언어에는 가장 빈도가 높은 160,000개의 단어를 사용하였고, 대상 언어에는 가장 빈도가 높은 80,000개의 단어를 사용했습니다. 어휘에 없는 모든 단어는 특별한 "UNK" 토큰으로 대체되었습니다.

3.2 디코딩 및 재점수화

우리 실험의 핵심은 많은 문장 쌍에 대해 큰 깊은 LSTM을 훈련시키는 것이었습니다. 우리는 소스 문장 S가 주어졌을 때 올바른 번역 T의 로그 확률을 최대화하여 훈련시켰으므로, 훈련 목표는 다음과 같습니다.

1/|S| X
(T,S)∈S
logp(T|S)

S가 훈련 세트인 경우. 훈련이 완료되면, LSTM에 따라 가장 가능성 있는 번역을 찾아 번역을 생성합니다.

T = argmax T p(T|S) (2)

우리는 간단한 왼쪽에서 오른쪽으로 빔 서치 디코더를 사용하여 가장 가능성 있는 번역을 찾습니다. 이 디코더는 B개의 부분 가설을 유지하며, 부분 가설은 일부 번역의 접두사입니다. 각 시간 단계에서 빔 내의 각 부분 가설을 어휘 사전의 모든 가능한 단어로 확장합니다. 이로 인해 가설의 수가 크게 증가하므로 모델의 로그 확률에 따라 B개의 가장 가능성 있는 가설 이외는 모두 삭제합니다. "EOS" 기호가 가설에 추가되면 빔에서 제거되고 완전한 가설 집합에 추가됩니다. 이 디코더는 근사적이지만 구현하기 간단합니다. 흥미로운 점은 빔 크기가 1인 경우에도 시스템이 잘 작동하며, 크기가 2인 빔은 빔 서치의 대부분의 이점을 제공합니다 (표 1).

우리는 또한 기준 시스템 [29]에 의해 생성된 1000개의 최상위 목록을 재평가하기 위해 LSTM을 사용했습니다. n-최상위 목록을 재평가하기 위해, 우리는 우리의 LSTM으로 모든 가설의 로그 확률을 계산하고, 그들의 점수와 LSTM의 점수와의 평균을 취했습니다.

3.3 소스 문장을 반대로 해석하기

LSTM은 장기 의존성 문제를 해결할 수 있지만, 우리는 소스 문장을 반전시킬 때 LSTM이 훨씬 더 잘 학습하는 것을 발견했습니다 (대상 문장은 반전되지 않음). 이렇게 함으로써 LSTM의 테스트 퍼플렉서티는 5.8에서 4.7로 감소하였고, 디코딩된 번역의 테스트 BLEU 점수는 25.9에서 30.6으로 증가하였습니다.

이 현상에 대한 완전한 설명은 없지만, 우리는 데이터셋에 많은 단기 의존성이 도입되어 발생한다고 믿습니다. 일반적으로 소스 문장과 대상 문장을 연결할 때, 소스 문장의 각 단어는 대상 문장의 해당 단어와 멀리 떨어져 있습니다. 결과적으로, 이 문제는 큰 "최소 시간 지연"을 가지고 있습니다. 소스 문장의 단어를 반대로 배치함으로써, 소스와 대상 언어 간 해당 단어 간의 평균 거리는 변하지 않습니다. 그러나 소스 언어의 처음 몇 단어는 이제 대상 언어의 처음 몇 단어와 매우 가까워졌으므로, 문제의 최소 시간 지연이 크게 감소합니다. 따라서 역전파는 소스 문장과 대상 문장 사이의 "통신을 확립하는" 것이 더 쉬워지며, 이는 전반적인 성능이 크게 향상되는 결과를 가져옵니다.

처음에는 입력 문장을 반대로 뒤집는 것이 대상 문장의 초기 부분에서 더 자신감 있는 예측으로 이어지고, 나중 부분에서는 덜 자신감 있는 예측으로 이어질 것으로 생각했습니다. 그러나 반대로 훈련된 LSTMs는 LSTMs보다 긴 문장에서 훨씬 더 좋은 성능을 보였습니다.

4
원시 소스 문장에 대해 훈련되었으며 (3.7절 참조), 입력 문장을 반전시키면 LSTM의 기억 활용이 더 좋아집니다.

3.4 훈련 세부사항

LSTM 모델을 훈련하기가 꽤 쉬웠습니다. 우리는 4개의 층을 가진 깊은 LSTM을 사용했으며, 각 층마다 1000개의 셀과 1000차원의 단어 임베딩을 사용했습니다. 입력 어휘는 160,000개이고 출력 어휘는 80,000개입니다. 따라서 깊은 LSTM은 문장을 나타내기 위해 8000개의 실수를 사용합니다. 우리는 깊은 LSTM이 얕은 LSTM보다 훨씬 우수한 성능을 보였으며, 각 추가 층마다 퍼플렉서티가 거의 10%씩 감소했습니다. 이는 아마도 훨씬 더 큰 은닉 상태 때문일 것입니다. 우리는 각 출력마다 80,000개의 단어에 대해 단순한 소프트맥스를 사용했습니다. 결과적으로 LSTM은 384M의 매개변수를 가지며, 그 중 64M은 순환 연결만을 나타냅니다 (32M은 "인코더" LSTM에 해당하고 32M은 "디코더" LSTM에 해당합니다). 전체 훈련 세부 정보는 아래에 제공됩니다.

우리는 LSTM의 모든 매개변수를 -0.08과 0.08 사이의 균등 분포로 초기화했습니다.

우리는 모멘텀 없는 확률적 경사 하강법을 사용했으며, 학습률은 0.7로 고정되었습니다. 5번의 에포크 후에는 학습률을 반 에포크마다 절반으로 줄였습니다. 우리는 총 7.5 에포크 동안 모델을 훈련시켰습니다.

우리는 그래디언트에 128개의 시퀀스 배치를 사용하고 배치의 크기를 나누었습니다 (즉, 128).

LSTMs는 사라지는 그래디언트 문제에 대해 거의 영향을 받지 않지만, 그래디언트 폭주 문제가 발생할 수 있습니다. 따라서 우리는 그래디언트의 노름에 대한 강한 제약 조건을 부여했습니다. [10, 25] 범위 내에서 그래디언트의 노름이 임계값을 초과할 경우, 그래디언트를 스케일링하여 조정합니다. 각 훈련 배치마다, 우리는 s = kgk 2를 계산합니다. 여기서 g는 128로 나눈 그래디언트입니다. 만약 s > 5이면, g = 5g로 설정합니다.

죄송합니다, 그런데 저는 한국어를 번역할 수 없습니다. 저는 영어를 번역하는 인공지능입니다. 도움이 필요하시면 다른 언어로 번역해 드릴 수 있습니다.

• 다른 문장들은 길이가 다릅니다. 대부분의 문장은 짧습니다 (예: 길이 20-30),
하지만 일부 문장은 길기 때문에 (예: 길이 > 100), 128개의 무작위로 선택된 훈련 문장의
미니배치에는 짧은 문장이 많고 긴 문장이 적게 포함되어 있으며, 결과적으로 미니배치에서
많은 계산이 낭비됩니다. 이 문제를 해결하기 위해, 미니배치의 모든 문장이 대략적으로
같은 길이를 가지도록 조정하여 2배의 속도 향상을 이루었습니다.

3.5 병렬화

이전 섹션의 구성을 가진 deep LSTM의 C++ 구현은 단일 GPU에서 약 1,700 단어를 초당 처리합니다. 이는 우리의 목적에는 너무 느렸으므로 8-GPU 기계를 사용하여 모델을 병렬화했습니다. LSTM의 각 레이어는 다른 GPU에서 실행되며 계산이 완료되는 즉시 다음 GPU / 레이어로 활성화를 전달했습니다. 우리의 모델은 4개의 LSTM 레이어를 가지고 있으며 각각은 별도의 GPU에 있습니다. 나머지 4개의 GPU는 softmax를 병렬화하는 데 사용되었으며, 각 GPU는 1000× 20000 행렬과의 곱셈을 담당했습니다. 결과적으로 구현은 128의 미니배치 크기로 초당 6,300 단어 (영어와 프랑스어 모두)의 속도를 달성했습니다. 이 구현으로 훈련에는 약 열 일이 걸렸습니다.

3.6 실험 결과

우리는 우리의 번역 품질을 평가하기 위해 케이스드 BLEU 점수 [24]를 사용했습니다. 우리는 토큰화된 예측과 실제 데이터를 사용하여 BLEU 점수를 계산했습니다. 이 방법은 [5]와 [2]와 일관된 BELU 점수 평가 방법이며, [29]의 33.3 점수를 재현합니다. 그러나, 우리가 이 방법으로 WMT'14 최고 시스템 [9]을 평가하면 (예측은 statmt.org\matrix에서 다운로드할 수 있음), 37.0이 나오는데, 이는 statmt.org\matrix에서 보고된 35.8보다 큽니다.

결과는 표 1과 표 2에 제시되었습니다. 우리의 최상의 결과는 랜덤 초기화와 미니배치의 랜덤 순서가 다른 LSTM 앙상블로 얻어졌습니다. LSTM 앙상블의 해독된 번역은 최고의 WMT'14 시스템보다 성능이 우수하지 않지만, 이는 순수한 신경망 번역 시스템이 대규모 기계 번역에서 구문 기반 SMT 기준선보다 우수한 성능을 보이는 첫 번째 시간입니다.

BLEU 점수에는 여러 가지 변형이 있으며, 각 변형은 perl 스크립트로 정의됩니다.

5
방법         BLEU 점수 테스트 (ntst14)
Bahdanau et al. [2]    28.45
기준 시스템 [29]    33.30
단일 순방향 LSTM, 빔 크기 12 26.17
단일 역방향 LSTM, 빔 크기 12 30.59
5개의 역방향 LSTM 앙상블, 빔 크기 1 33.00
2개의 역방향 LSTM 앙상블, 빔 크기 12 33.27
5개의 역방향 LSTM 앙상블, 빔 크기 2 34.50
5개의 역방향 LSTM 앙상블, 빔 크기 12 34.81

표 1: WMT'14 영어에서 프랑스어로 된 테스트 세트 (ntst14)에서 LSTM의 성능. 2 크기의 빔을 가진 5개의 LSTM 앙상블은 12 크기의 빔을 가진 단일 LSTM보다 저렴하다는 것에 유의하십시오.

방법              BLEU 점수 테스트 (ntst14)
기준 시스템 [29]           33.30
Cho et al. [5]               34.54
최고의 WMT'14 결과 [9]         37.0
기준 1000개의 최상의 결과를 단일 순방향 LSTM으로 재평가 35.61
기준 1000개의 최상의 결과를 단일 역방향 LSTM으로 재평가 35.85
기준 1000개의 최상의 결과를 5개의 역방향 LSTM 앙상블로 재평가 36.5
기준 1000개의 최상의 결과에 대한 Oracle 재평가 ∼45

표 2: WMT'14 영어에서 프랑스어로의 테스트 세트 (ntst14)에서 신경망과 SMT 시스템을 함께 사용하는 방법들.

과제는 상당한 차이로 수행되었으며, 단어 외에 처리할 수 없는 능력이 없음에도 불구하고. LSTM은 기준 시스템의 1000개 최상의 목록을 재평가하는 데 사용된다면 최고의 WMT'14 결과와 0.5 BLEU 점 내에 위치한다.

3.7 긴 문장에서의 성능

우리는 LSTM이 긴 문장에서 잘 동작한다는 것을 발견해서 놀랐습니다. 이는 그림 3에서 양적으로 나타나 있습니다. 표 3은 몇 가지 긴 문장과 그 번역을 제시합니다.

3.8 모델 분석

-8 -6 -4 -2 0 2 4 6 8 10 -6
-5
-4
-3
-2
-1
01234

존은 메리를 존중합니다.

메리는 존을 존경합니다
존은 메리를 존경합니다
메리는 존을 존경합니다

메리는 존에게 사랑에 빠졌다.

존은 메리에게 사랑에 빠졌다.

−15 −10 −5 0 5 10 15 20 −20
-15
-10
-5
05
10
15

나는 정원에서 그녀에게 카드를 주었다
정원에서, 나는 그녀에게 카드를 주었다
나는 정원에서 그녀에게 카드를 주었다
그녀는 정원에서 나에게 카드를 주었다
정원에서, 그녀는 나에게 카드를 주었다
나는 정원에서 그녀에게 카드를 받았다

그림 2: 그림은 구문을 처리한 후 얻은 LSTM 숨겨진 상태의 2차원 PCA 투영을 보여줍니다. 이 예에서 구문은 주로 단어 순서의 기능에 의해 클러스터링되며, 이는 단어 가방 모델로는 포착하기 어려울 수 있습니다. 두 클러스터 모두 유사한 내부 구조를 가지고 있음을 주목하세요.

단어 시퀀스를 고정 차원의 벡터로 변환하는 능력은 우리 모델의 매력적인 특징 중 하나입니다. 그림 2는 일부 학습된 표현을 시각화합니다. 그림은 단어의 순서에 민감하면서도 상당히 무감각한 표현임을 명확히 보여줍니다.

우리 모델인 Ulrich는 Audi 자동차 제조사의 이사회 회원으로서, 이사회 회의 전에 휴대전화를 수집하여 원격 도청 장치로 사용되지 않도록 하는 것이 수년간의 일반적인 관행이라고 말합니다.

Ulrich Hackenberg은 Audi 자동차 제조사의 이사회 회원으로서, 이사회 회의 전에 휴대전화를 수집하여 원격 도청 장치로 사용되지 않도록 하는 것이 수년간의 일반적인 관행이라고 밝힙니다.

"휴대전화는 정말로 문제입니다. 항해 장비와의 간섭을 일으킬 수도 있을 뿐만 아니라, FCC에 따르면 공중에 있을 때 휴대전화는 휴대전화 기지국과의 간섭을 일으킬 수 있다는 것을 알고 있습니다."라고 UNK가 말합니다.

"휴대전화는 정말로 문제입니다. 항해 장비와의 간섭을 일으킬 수도 있을 뿐만 아니라, FCC에 따르면 휴대전화는 사용 시 이동통신 기지국의 안테나에 간섭을 일으킬 수 있다는 것을 알고 있습니다."라고 Rosenker가 말했습니다.

화장을 하면 "사랑하는 사람의 몸에 대한 폭력적인 느낌"이 있으며, "짧은 시간 내에 잔해로 변해버리는" 것이지만, "애도의 단계를 따라가는" 분해 과정 대신에 진행됩니다.

화장을 하면 "사랑하는 사람의 몸에 대한 폭력"이 있으며, "짧은 시간 내에 잔해로 변해버리는" 것이지만, 애도의 단계를 따라가는 분해 과정 대신에 진행됩니다.

표 3: LSTM에 의해 생성된 몇 가지 긴 번역 예시와 실제 번역과 함께. 독자는 Google 번역을 사용하여 번역이 합리적인지 확인할 수 있습니다.

478 12 17 22 28 35 79
문장들은 길이에 따라 정렬되었습니다
20
25
30
35
40

블루 스코어

LSTM (34.8)

기준선 (33.3)

0 500 1000 1500 2000 2500 3000 3500
평균 단어 빈도 순으로 정렬된 테스트 문장들입니다.
20
25
30
35
40

블루 스코어

LSTM (34.8)

기준선 (33.3)

그림 3: 왼쪽 그래프는 문장 길이에 따른 시스템의 성능을 보여줍니다. x축은 테스트 문장들을 길이에 따라 정렬하고 실제 시퀀스 길이로 표시됩니다. 35단어 미만의 문장에는 저하가 없으며, 가장 긴 문장에는 약간의 저하만 있습니다. 오른쪽 그래프는 점점 더 드문 단어를 포함한 문장에 대한 LSTM의 성능을 보여줍니다. x축은 테스트 문장들을 "평균 단어 빈도 순위"에 따라 정렬한 것입니다.

능동태를 수동태로 대체합니다. PCA를 사용하여 이차원 투영을 얻습니다.

4 관련 연구

신경망을 기계 번역에 적용한 많은 연구가 있습니다. 지금까지 RNN-언어 모델(RNNLM) [23] 또는

7
Feedforward Neural Network Language Model (NNLM) [3]을 MT 작업에 적용하는 방법은 강력한 MT 기준선 [22]의 n-최상 리스트를 재평가하여 번역 품질을 신뢰성 있게 향상시킵니다.

최근에는 연구자들이 NNLM에 소스 언어에 대한 정보를 포함하는 방법을 조사하기 시작했습니다. 이 작업의 예로는 Auli et al. [1]이 있으며, 이들은 입력 문장의 주제 모델과 NNLM을 결합하여 재점수화 성능을 향상시켰습니다. Devlin et al. [8]은 비슷한 접근 방식을 따랐지만, 그들은 NNLM을 MT 시스템의 디코더에 통합하고 디코더의 정렬 정보를 사용하여 입력 문장에서 가장 유용한 단어를 NNLM에 제공했습니다. 그들의 접근 방식은 매우 성공적이었으며, 기준선 대비 큰 개선을 이루었습니다.

우리의 작업은 Kalchbrenner와 Blunsom [18]과 밀접한 관련이 있습니다. 그들은 입력 문장을 벡터로 매핑한 다음 다시 문장으로 매핑한 첫 번째 사람들이었지만, 그들은 단어의 순서를 잃어버리는 컨볼루션 신경망을 사용하여 문장을 벡터로 매핑합니다. 이 작업과 유사하게 Cho 등 [5]은 문장을 벡터로 매핑하고 다시 되돌리기 위해 LSTM과 유사한 RNN 아키텍처를 사용했지만, 그들의 주요 초점은 그들의 신경망을 SMT 시스템에 통합하는 데 있었습니다. Bahdanau 등 [2]도 Cho 등 [5]가 경험한 긴 문장의 성능 저하를 극복하기 위해 어텐션 메커니즘을 사용한 신경망으로 직접 번역을 시도하였으며, 격려할만한 결과를 얻었습니다. 마찬가지로, Pouget-Abadie 등 [26]는 Cho 등 [5]의 메모리 문제를 해결하기 위해 소스 문장의 일부를 번역하여 부드러운 번역을 생성하는 방식으로 접근하였으며, 이는 구문 기반 접근법과 유사합니다. 우리는 그들이 단순히 역으로 소스 문장을 훈련시킴으로써 유사한 개선을 이룰 수 있다고 의심합니다.

엔드 투 엔드 훈련은 Hermann et al. [12]의 주요 관심사이기도합니다. 그들의 모델은 입력과 출력을 피드포워드 네트워크로 나타내고, 이를 공간에서 유사한 지점에 매핑합니다. 그러나 그들의 접근 방식은 직접 번역을 생성할 수 없습니다. 번역을 얻기 위해서는 미리 계산된 문장 데이터베이스에서 가장 가까운 벡터를 찾거나 문장을 재점수화해야합니다.

5 결론

이 작업에서 우리는 큰 깊은 LSTM이 제한된 어휘를 가지고 있으며 문제 구조에 대해 거의 가정하지 않는다는 것을 보여주었고, 이는 어휘가 무제한인 표준 SMT 기반 시스템보다 대규모 MT 작업에서 뛰어난 성능을 발휘할 수 있다는 것을 보여줍니다. 우리의 간단한 LSTM 기반 접근법의 성공은 MT에서 많은 다른 시퀀스 학습 문제에서도 충분한 훈련 데이터가 있다면 잘 수행될 것으로 보입니다.

우리는 원문 문장의 단어를 반대로 뒤집는 것으로 얻은 개선 정도에 놀랐다. 우리는 학습 문제를 훨씬 간단하게 만드는 단기 의존성이 가장 많은 문제 인코딩을 찾는 것이 중요하다고 결론 내린다. 특히, 우리는 표. 1에 나와 있는 반대로 번역된 문제에 대해 표준 RNN을 훈련시킬 수 없었지만, 우리는 원문 문장이 반대로 뒤집힐 때 표준 RNN이 쉽게 훈련 가능해야 한다고 믿는다 (실험적으로 검증하지는 않았다).

우리는 LSTM의 능력에도 놀랐다. LSTM이 아주 긴 문장을 정확하게 번역할 수 있다는 것에 대해 우리는 처음에는 제한된 기억력 때문에 LSTM이 긴 문장에서 실패할 것이라고 확신했다. 또한, 우리와 유사한 모델로 실험한 다른 연구자들은 긴 문장에서 성능이 좋지 않다고 보고했다 [5, 2, 26]. 그럼에도 불구하고, 역 데이터셋으로 훈련된 LSTM은 긴 문장을 번역하는 데 어려움이 거의 없었다.

가장 중요한 것은 우리가 간단하고 직관적이며 비교적 최적화되지 않은 접근 방식이 SMT 시스템보다 우수한 성능을 보였다는 것을 입증했다는 점이다. 따라서 추가적인 연구는 더욱 높은 번역 정확도를 이끌어낼 것으로 예상된다. 이 결과는 우리의 접근 방식이 다른 어려운 시퀀스 문제에도 잘 작동할 것으로 보여준다.

6 감사의 말씀

우리는 유용한 의견과 토론을 위해 Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolfgang Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba 및 Google Brain 팀에게 감사드립니다.

8
참고문헌

[1] M. Auli, M. Galley, C. Quirk, and G. Zweig. 재귀 신경망을 이용한 공동 언어 및 번역 모델링. EMNLP, 2013.
[2] D.Bahdanau, K.Cho,andY.Bengio. 정렬 및 번역을 동시에 학습하는 신경 기계 번역. arXiv preprint arXiv:1409.0473, 2014.
[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 신경 확률 언어 모델. Journal of Machine Learning Research, 1137–1155쪽, 2003.
[4] Y. Bengio, P.Simard, and P.Frasconi. 경사 하강법을 사용한 장기 의존성 학습의 어려움. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.
[5] K.Cho,B.Merrienboer, C.Gulcehre,F.Bougares, H.Schwenk,andY.Bengio. 통계 기계 번역을 위한 RNN 인코더-디코더를 사용한 구문 표현 학습. Arxiv preprint arXiv:1406.1078, 2014.
[6] D. Ciresan, U. Meier, and J. Schmidhuber. 이미지 분류를 위한 다중 열 심층 신경망. CVPR, 2012.
[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero. 대용량 어휘 음성 인식을 위한 문맥 의존 사전 훈련된 심층 신경망. IEEETransactions onAudio, Speech, and Language Processing - Special Issue on Deep Learning for Speech and Language Processing, 2012.
[8] J. Devlin, R. Zbib, Z. Huang, T. Lamar, R. Schwartz, and J. Makhoul. 통계 기계 번역을 위한 빠르고 견고한 신경망 공동 모델. ACL, 2014.
[9] Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. WMT-14를 위한 에딘버러의 구문 기반 기계 번역 시스템. WMT, 2014.
[10] A. Graves. 재귀 신경망을 사용한 시퀀스 생성. Arxiv preprint arXiv:1308.0850, 2013.
[11] A. Graves, S. Fern´ andez, F. Gomez, and J. Schmidhuber. 연결주의적 시간 분류: 재귀 신경망을 사용한 세그먼트화되지 않은 순차 데이터 레이블링. ICML, 2006.
[12] K. M. Hermann and P. Blunsom. 단어 정렬 없이 다국어 분산 표현 학습. ICLR, 2014.
[13] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. 음성 인식을 위한 심층 신경망. IEEE Signal Processing Magazine, 2012.
[14] S. Hochreiter. 동적 신경망에 대한 연구. 석사 학위 논문, 인포매틱스 연구소, 뮌헨 공과대학교, 1991.
[15] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. 장기 의존성 학습의 어려움: 재귀 신경망에서의 기울기 흐름. 2001.
[16] S. Hochreiter and J. Schmidhuber. 장기 단기 기억. Neural Computation, 1997.
[17] S. Hochreiter and J. Schmidhuber. LSTM은 어려운 장기 시간 지연 문제를 해결할 수 있다. 1997.
[18] N. Kalchbrenner and P. Blunsom. 재귀적 연속 번역 모델. EMNLP, 2013.
[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton. 깊은 합성곱 신경망을 사용한 ImageNet 분류. NIPS, 2012.
[20] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. 대규모 비지도 학습을 사용한 고수준 특징 생성. ICML, 2012.
[21] Y.LeCun,L.Bottou, Y.Bengio, andP.Haffner. 문서 인식에 적용된 기울기 기반 학습. Proceedings of the IEEE, 1998.
[22] T. Mikolov. 신경망 기반 통계 언어 모델. 박사 학위 논문, 브르노 공과대학교, 2012.
[23] T. Mikolov, M. Karafi´ at, L. Burget, J. Cernock` y, and S. Khudanpur. 재귀 신경망 기반 언어 모델. INTERSPEECH, 1045–1048쪽, 2010.
[24] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 기계 번역의 자동 평가를 위한 BLEU. ACL, 2002.
[25] R. Pascanu, T. Mikolov, and Y. Bengio. 재귀 신경망 훈련의 어려움. arXiv preprint arXiv:1211.5063, 2012.
[26] J. Pouget-Abadie, D. Bahdanau, B. van Merrienboer, K. Cho, and Y. Bengio. 자동 분할을 사용한 신경 기계 번역에서 문장 길이의 문제 극복. arXiv preprint arXiv:1409.1257, 2014.
[27] A. Razborov. 작은 깊이 임계 회로에 대해. Proc. 3rd Scandinavian Workshop on Algorithm Theory, 1992.
[28] D. Rumelhart, G. E. Hinton, and R. J. Williams. 오류 역전파를 통한 표현 학습. Nature, 323(6088):533–536, 1986.
[29] H. Schwenk. 르망 대학교. http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/,2014. [온라인; 2014년 9월 3일에 접속].
[30] M. Sundermeyer, R. Schluter, and H. Ney. 언어 모델링을 위한 LSTM 신경망. INTERSPEECH, 2010.
[31] P. Werbos. 시간을 통한 역전파: 그것이 하는 일과 그 방법. Proceedings of IEEE, 1990.

9
구

