TTS 보코더
분석
잡음이 있는 파형

... ... ... ...

TTS 음향
모델 훈련

STFT
분석
잡음이 섞인 파형

추출 ... ... ... ... STFT

합성
| 스펙트럼 |
계산    TTS 보코더

분석
TTS 음향
모델 훈련

단계
크기 스펙트럼                향상된 파형

MCEP + BAP
+ V/UV + F0

RNN (순환 신경망)

RNN (순환 신경망)

MCEP + BAP
+ V/UV + F0
소음이 있는          향상된

시끄러운 MCEP-DFT   향상된 MCEP-DFT

MCEP + BAP = MCEP + BAP
+ V/UV + F0 = V/UV + F0
enhanced = 향상된
RNN-DFT = RNN-DFT
RNN-V = RNN-V

그림 1: 직접 보코딩된 매개변수를 향상시키는 RNN 기반 음성 개선 방법을 사용하여 TTS 음향 모델을 훈련시키는 것 (상단)과 크기 스펙트럼의 매개화 (하단).

두 개의 RNN 기반 방법, 하나는 [12]에서 제안된 TTS 스타일의 보코더 매개 변수 도메인에서 작동하는 것이고, 다른 하나는 크기 스펙트럼을 설명하는 매개 변수 집합을 향상시킵니다. 비교를 간단히하기 위해 이 작업에서는 텍스트 기반 특징을 사용하지 않습니다.
이 논문은 다음과 같이 구성되어 있습니다: 섹션 2에서는 RNN에 대한 간단한 요약을 제시하고, 섹션 3에서는 제안된 음성 개선 시스템을 소개하며, 섹션 4에서는 실험을 진행합니다. 토론과 결론이 이어집니다.

깊은 순환 신경망

RNN은 적어도 하나의 피드백 연결을 가진 네트워크로, 순차적인 데이터를 모델링할 수 있는 잠재력을 가질 수 있습니다. 그러나 사라지는 그래디언트 문제로 인해 훈련하기 어렵습니다. LSTM은 장단기 메모리 네트워크로, 특정 구조를 가진 단위로 구성되어 사라지는 그래디언트 문제를 겪지 않으므로 훈련하기 쉬울 수 있습니다. LSTM 단위는 임의의 길이 동안 값을 기억하고, 입력이 어떻게 영향을 미치는지, 그 값을 출력에 어떻게 전달하고 이전 값을 잊고 기억할지를 제어할 수 있습니다. LSTM은 음성 문제를 포함한 다양한 문제에 적용되었으며, 실제 노이즈 데이터로 훈련할 때 특히 흥미로울 수 있습니다. 즉, 노이즈가 있는 환경에서 음성이 생성되고 그에 따라 변화하는 녹음 데이터로 훈련하는 경우입니다.

3. RNN을 사용한 음성 개선

Fig.1은 본 논문에서 조사한 두 가지 RNN 기반 방법을 보여줍니다. 상단의 다이어그램은 [12]에서 제안된 개선 방법을 나타냅니다. 이 방법을 RNN-V라고 합니다. 이 방법에서는 일반적으로 SPSS에 사용되는 보코더의 합성 모듈을 사용하여 깨끗하고 노이즈가 있는 음향 특성의 병렬 데이터베이스로 RNN을 훈련시킵니다. 이 보코더를 사용하여 추출된 음향 특성은 부드러운 크기 스펙트럼의 Mel 켑스트럴(MCEP) 계수, 밴드 에피리오딕(BAP) 값, 음성/무음(V/UV) 결정 및 F0입니다. 이 음향 특성은 겹치는 F0 적응 창을 사용하여 프레임 수준에서 추출됩니다. RNN이 훈련되면 노이즈가 있는 음향 특성을 개선하기 위해 사용할 수 있습니다.

위의 그림 1의 상단 다이어그램에 표시된 것처럼, 이러한 향상된 특징들은 TTS 음향 모델을 훈련시키는 데 사용됩니다. 그림 1의 하단에는 본 논문에서 제안하는 대체 구조인 RNN-DFT가 나와 있습니다. 이 방법에서는 단기 푸리에 변환(STFT)을 사용하여 음성 파형을 분석하여 각 시간 프레임의 이산 푸리에 변환(DFT)을 얻습니다. 이 복소 신호의 크기 값을 계산하고, 이를 단순히 크기 스펙트럼이라고 합니다. 또한 이 크기 스펙트럼의 위상을 계산합니다. 크기 스펙트럼의 차원을 줄이기 위해 N 길이의 크기 스펙트럼에서 M Melcepstral 계수를 추출하고, M<N 이 되도록 계수의 수를 줄입니다. 이러한 계수를 MCEP-DFT 계수라고 합니다. 깨끗하고 노이즈가 있는 음성 신호에서 추출한 MCEP-DFT 계수의 병렬 데이터베이스로 RNN을 훈련시킵니다. 모델이 훈련되면 노이즈가 있는 MCEP-DFT를 향상시킨 것을 생성하는 데 사용할 수 있습니다. 음성 신호를 재구성하기 위해 이러한 계수는 왜곡된 이산 코사인 변환을 통해 크기 스펙트럼으로 변환됩니다. 노이즈 파형에서 추출한 DFT로부터 얻은 향상된 크기 스펙트럼과 원래 위상을 결합하여 역 이산 푸리에 변환을 사용하여 파형 신호를 얻습니다. 이 신호는 다시 TTS 스타일 보코더를 사용하여 분석되고, 추출된 특징은 TTS 음향 모델을 훈련시키는 데 사용됩니다.

4. 실험

이 섹션에서는 이러한 방법을 훈련하고 테스트하는 데 사용된 데이터베이스와 보코딩 및 합성 음성을 사용한 실험에 대해 자세히 설명합니다.

4.1. 데이터베이스

보이스 뱅크 코퍼스 [24]에서 28명의 화자를 선택했습니다 - 14명은 남성이고 14명은 여성으로, 모두 동일한 사투리 지역 (잉글랜드)의 화자입니다. 또한 다른 56명의 화자 - 28명은 남성이고 28명은 여성으로, 다른 사투리 지역 (스코틀랜드와 미국)의 화자입니다. 각 화자마다 약 400개의 문장이 있습니다. 모든 데이터는 48kHz로 샘플링되었으며, 철자 변환도 가능합니다.
훈련에 사용된 잡음 데이터베이스를 생성하기 위해 10가지 다른 잡음 유형을 사용했습니다: 두 가지 인공적으로 생성된 잡음 (음성 모양의 잡음과 수다)과 Demand 데이터베이스 [25]에서 가져온 여덟 가지 실제 잡음 녹음입니다. 음성 모양의 잡음은 주파수 응답을 가진 필터로 흰 잡음을 필터링하여 생성되었습니다.
아키텍처 훈련 데이터 MCEP (dB) BAP (dB) V/UV (%) F0 (Hz)
잡음 - 9.86 / 10.68 2.62 / 2.41 9.55 / 7.88 40.27 / 4.38
DNN 14명의 여성 + 14명의 남성 5.69 / 6.10 1.96 / 1.82 4.00 / 4.25 27.09 / 10.90
RNN 14명의 여성 + 14명의 남성 4.63 / 5.06 1.83 / 1.74 2.50 / 2.30 24.52 / 8.34
RNN 14명의 여성 4.70 / 5.89 1.85 / 1.97 2.63 / 5.01 24.08 / 39.68
RNN 14명의 남성 6.18 / 5.23 2.04 / 1.73 5.36 / 2.32 37.87 / 6.45
RNN 28명의 여성 + 28명의 남성 4.59 / 5.05 1.86 / 1.72 2.46 / 2.15 24.90 / 8.43

표 1: 여성 / 남성 목소리의 보코딩 매개변수로부터 계산된 왜곡 지표.

장기간 연설 수준의 남성 화자와 일치했습니다.
배블 소음은 훈련이나 테스트에 사용되지 않은 Voice Bank 코퍼스의 6명 화자의 음성을 추가하여 생성되었습니다. 다른 8개의 소음은 Demand 데이터베이스의 48kHz 버전의 첫 번째 채널에서 선택되었습니다. 선택된 소음은 다음과 같습니다: 가정 소음 (부엌 내부), 사무실 소음 (회의실), 세 개의 공공 공간 소음 (카페테리아, 레스토랑, 지하철 역), 두 개의 교통 소음 (자동차 및 지하철) 및 거리 소음 (번화한 교차로). 훈련에 사용된 신호 대 잡음 (SNR) 값은 15dB, 10dB, 5dB 및 0dB였습니다. 따라서 스피커당 약 10개의 다른 문장이 각 조건에서 있었습니다. 소음은 ITU-T P.56 방법 [26]을 사용하여 활성 음성 수준을 계산하기 위해 [13]에서 제공된 코드를 사용하여 깨끗한 파형에 추가되었습니다. 깨끗한 파형은 정규화된 후에 소음에 추가되었으며, 각 문장의 시작과 끝에서 200ms 이상의 정적 세그먼트가 제거되었습니다.
테스트 세트를 만들기 위해 같은 코퍼스의 영국의 남성과 여성 화자 두 명과 Demand 데이터베이스에서 다섯 가지 다른 소음을 선택했습니다. 선택된 소음은 다음과 같습니다: 가정 소음 (거실), 사무실 소음 (사무 공간), 교통 소음 (버스) 및 두 개의 거리 소음 (오픈 에어 카페테리아 및 공공 광장). 약간 더 높은 SNR 값인 17.5dB, 12.5dB, 7.5dB 및 2.5dB를 사용했습니다. 이로써 20가지 다른 소음 조건 (다섯 가지 소음 x 네 가지 SNR)이 만들어졌으며, 스피커당 약 20개의 다른 문장이 각 조건에서 있었습니다. 소음은 이전에 설명한 동일한 절차를 따라 추가되었습니다. 소음이 섞인 음성 데이터베이스는 영구적으로 다음 주소에서 이용 가능합니다: http://dx.doi.org/10.7488/ds/1356

4.2. 음향적 특징

STRAIGHT [27]을 사용하여 60개의 MCEP 계수와 25개의 BAP 구성 요소를 추출하였으며, SPTK [28]을 사용하여 RAPT F0 추출 방법 [29]으로 F0 및 V/UV 정보를 추출하였습니다.

MCEP (dB) BAP (dB) V/UV (%) F0 (Hz)
소음 9.86 / 10.68 2.62 / 2.41 9.55 / 7.88 40.27 / 4.38
깨끗한* 1.84 / 1.61 1.24 / 1.10 0.58 / 0.62 17.14 / 1.84
소음* 9.41 / 10.13 2.75 / 2.50 10.39 / 8.49 41.17 / 4.70
OMLSA 8.19 / 8.36 3.15 / 2.77 8.73 / 8.28 34.03 / 6.31
RNN-V 4.59 / 5.05 1.86 / 1.72 2.46 / 2.15 24.90 / 8.43
RNN-DFT 4.90 / 5.22 2.44 / 2.32 2.06 / 2.44 22.59 / 3.31

표 2: 여성/남성 목소리의 보코딩된 파라미터로 계산된 왜곡 측정치입니다. CLEAN* 및 NOISY*는 재합성된 깨끗한 신호와 노이즈 신호에서 추출된 파라미터를 사용하여 계산된 왜곡을 나타냅니다.

모든 이러한 기능들은 5ms의 슬라이딩 윈도우를 사용하여 추출되었습니다. 보코더 기능의 결과 차원은 87입니다. 16ms의 해밍 윈도우와 4ms의 쉬프트를 사용하여 1024 크기의 DFT를 추출했습니다. 그것의 크기 값에서 87개의 Mel 켑스트럴 계수를 추출했습니다. 이 숫자는 STRAIGHT 보코더를 사용하여 추출된 매개변수의 수와 일치하도록 선택되었으며, 방법 간의 비교를 공정하게 만들었습니다.

4.3. 음성 개선 방법

우리는 다양한 유형의 신경망을 훈련시켜, 노이즈가 있는 자연어 음성에서 추출된 음향적 특징을 깨끗한 자연어 음성에서 추출된 특징으로 매핑했습니다. 사용한 비용 함수는 모든 음향적 차원을 대상으로 한 제곱 오차의 합이었습니다. [8]과 유사하게 학습률을 2.0e-5로 설정하고, 가우시안 분포를 따르는 평균이 0이고 분산이 0.1인 임의로 초기화된 가중치로 모델을 훈련시키기 위해 확률적 경사 하강법을 사용했습니다. 운동량은 0으로 설정되었습니다. 우리는 TESLA K40 GPU 보드를 사용하여 CURRENNT 도구 [30]를 사용하여 모델을 훈련시켰습니다.
전통적인 음성 개선 방법으로는 [31]에서 설명한 통계 모델 기반 방법을 선택했습니다. 이 방법은 최적으로 수정된 로그-스펙트럼 크기 음성 추정기(OMLSA)와 [32]에서 제안된 최소 제어 재귀 평균 잡음 추정기의 개선된 버전을 사용합니다. 해당 코드는 저자의 웹사이트에서 제공되며, 다른 DNN 기반 음성 개선을 위한 비교 기준으로 사용되었습니다. [6, 12].

4.4. 목표적인 측정

이 섹션에서는 TTS 보코더에 의해 추출된 음향 파라미터를 사용하여 계산된 왜곡 측정치를 제시합니다. 왜곡 측정치는 MCEP 왜곡(dB), BAP 왜곡(dB), 유성 프레임에서 계산된 F0 왜곡(Hz), 전체 발화에서 계산된 VUV 왜곡으로 구성됩니다. 이러한 측정치는 각 테스트 화자(여성/남성)의 모든 발화에 대해 프레임 수준에서 계산되고 프레임별로 평균화됩니다. 왜곡은 항상 깨끗한 음성에서 추출된 보코더 파라미터를 참조로 사용하여 계산됩니다. 다음 섹션에서는 이러한 왜곡 측정치를 평가 지표로 사용하여 네트워크 아키텍처, 훈련 데이터 양, 향상된 특징 및 잡음 유형의 영향을 분석합니다.

4.4.1. 네트워크 구조와 훈련 데이터

표 1은 노이즈가 있는 테스트 데이터(NOISY)와 신경망 기반 개선 방법 다섯 가지의 왜곡 측정치를 제시한다. 이 방법들은 네트워크 구조와 훈련 데이터 양 측면에서 차이가 있다. 이 네트워크들은 RNN-V 방법을 따라 TTS 보코더에서 유도된 음향 특징을 사용하여 훈련되었다.
SNR (dB)
5     10   15
M
C E P 왜곡 (dB) 2468 10 12 14
16
18
NOISY - 여성

버스
카페 생활
사무실
피스퀘어

SNR (dB)
5    10   15
M
C E P 왜곡 (dB)

3
3.5
4
4.5 5 5.5 6 6.5
7
7.5
8
RNN - 여성

버스
카페 생활
사무실
피스퀘어

SNR (dB)
5    10   15
M
C E P 왜곡 (dB)

3
3.5
4
4.5 5 5.5 6 6.5
7
7.5
8
RNN-FFT - 여성

버스
카페 생활
사무실
피스퀘어

SNR (dB)
5     10   15
M C E
P d i s t o r t i o n ( d B )
468
10 12 14
16
18
소음 - 남성

버스
카페 생활
사무실
피스퀘어

SNR (dB)
5    10   15
M C E
P d i s t o r t i o n ( d B )

SNR (dB)
5    10   15
M C E
P 왜곡 (dB)

3
3.5
4
4.5
5 5.5 6 6.5
7
7.5
8
RNN - 남성

버스
카페 생활
사무실
피스퀘어

SNR (dB)
5    10   15
M C E
P d i s t o r t i o n ( d B )

SNR (dB)
5    10   15
M C E
P 왜곡 (dB)

3
3.5
4
4.5
5 5.5 6 6.5
7
7.5
8
RNN-FFT - 남성

버스
카페 생활
사무실
피스퀘어

그림 2: 여성 (위)과 남성 (아래)의 잡음 및 SNR 조건별 멜 켑스트럼 왜곡.

DNN은 512개의 로지스틱 유닛으로 이루어진 4개의 피드포워드 레이어로 구성된 딥 신경망을 의미합니다. RNN은 입력에 가장 가까운 512개의 로지스틱 유닛으로 이루어진 2개의 피드포워드 레이어와 출력에 가장 가까운 256개의 유닛으로 이루어진 양방향 LSTM (BLSTM) 레이어 2개로 구성된 네트워크를 의미합니다. 이는 [12]에서 제안된 것입니다.
대부분의 모델 기반 음성 개선 방법은 남성과 여성 화자의 데이터를 모두 사용하여 모델을 훈련시킵니다. 그러나 여기에서 제안된 방법은 F0를 직접 개선하기 때문에 단일 성별의 데이터만 사용하여 두 개의 별도 모델을 훈련시켰습니다. 훈련에 사용된 데이터는 표 1의 Training data 열에 표시되어 있습니다.
이 표에서 RNN의 성능이 DNN보다 우수하며 특히 V/UV 및 MCEP 왜곡에 대해 더 좋은 결과를 보입니다. 그러나 남성 화자 데이터의 F0 왜곡은 두 성별의 데이터를 사용하여 훈련할 때 증가하는 것으로 보입니다. 여성과 남성 데이터로 훈련된 모델을 사용한 결과는 F0 왜곡 측면에서 약간 더 나은 결과를 보이지만 MCEP 왜곡 측면에서는 더 나쁜 결과를 보입니다. 이는 혼성 모델이 데이터 양이 두 배로 많기 때문일 수 있습니다. 28명에서 56명의 화자 데이터로 데이터 양을 더 늘리면 MCEP 및 V/UV 왜곡이 감소하지만 BAP 및 F0 왜곡은 개선되지 않습니다.

4.4.2. 향상된 기능과 잡음 유형

이 섹션에서는 가장 많은 양의 데이터로 훈련된 모델에 초점을 맞춥니다. 즉, 혼합 성별의 56명의 화자입니다. 표 2는 잡음이 있는 음성(NOISY), 재합성된 깨끗한 음성(CLEAN*) 및 잡음이 있는 음성(NOISY*)의 왜곡과 OMLSA, RNN-V 및 RNN-DFT라는 개선 방법을 보여줍니다. RNN-V는 표 1의 마지막 행에 나열된 동일한 시스템입니다.
재합성된 데이터는 이전에 설명한 STFT 설정을 사용하여 분석 및 합성된 데이터를 의미합니다. CLEAN* 결과에서 관찰된 왜곡은 이 과정에서 도입된 오류이며, NOISY*에서 관찰된 왜곡은 재합성 및 존재로 인해 발생합니다.

첨가 잡음에 대한. 표에서 볼 수 있듯이, 깨끗한 웨이브폼(CLEAN*)을 재합성할 때 BAP, VUV 및 F0 왜곡이 약간 증가했습니다. 잡음이 섞인 음성(NOISY*)을 재합성하는 것은 MCEP 왜곡을 증가시키지 않는 것으로 보입니다 (NOISY*와 NOISY 값을 비교). 그리고 다른 유형의 왜곡을 약간 증가시킵니다. 이 결과는 재구성 과정이 TTS 음향 특성 추출에 크게 영향을 미치지 않는 것으로 보입니다.
개선 방법에 관해서는, 표 2에서 OMLSA는 모든 음향 특성에 대해 RNN 기반 방법과 비교했을 때 더 많은 오류를 보입니다. RNN-V는 남성과 여성 음성 모두에 대해 낮은 MCEP 및 BAP 왜곡을 얻었고, RNN-DFT는 낮은 VU/V 및 F0 오류를 얻었습니다. 사실, 이 방법만이 남성 데이터의 F0 오류를 감소시킬 수 있었습니다.
비교를 위해 STFT 분석을 통해 얻은 크기 스펙트럼에서 계산된 MCEP-DFT의 멜 켑스트럴 왜곡을 계산했습니다. 깨끗한 음성에서 추출된 계수를 참조로 사용했습니다. 여성/남성 잡음이 섞인 음성 데이터의 MCEP-DFT 왜곡은 각각 9.87/10.48dB로 나타났습니다. 이 값은 MCEP 왜곡 (표 2의 NOISY 행)과 유사합니다. MCEP-DFT 왜곡은 RNN을 사용하여 개선할 경우 4.9359/5.3829dB로 감소합니다. 왜곡은 감소했지만 표 2에서 볼 수 있는 RNN-V의 MCEP 왜곡보다 여전히 큽니다.
Fig.2에서 RNN 기반 방법의 성능이 잡음 유형과 SNR에 어떻게 의존하는지 확인하기 위해 각 잡음 유형과 SNR에 대한 왜곡을 분석한 그림을 제시합니다. 이 그림들에서 카페테리아(cafe)와 거실(living) 잡음이 가장 도전적인 것으로 나타납니다. MCEP 왜곡은 개선 후에도 상당히 높습니다. 이는 이러한 잡음의 녹음에는 종종 경쟁하는 스피커, 음악 및 기타 비정상적인 잡음이 포함되기 때문일 것입니다. 버스와 사무실 잡음은 주로 정지 상태이므로 신호를 왜곡시키지 않는 것으로 보입니다. 서로 다른 잡음에 의해 가져온 왜곡 사이의 차이가 있습니다.

여성 - 보코딩

자연적인 청결 시끄러운 OMLSA RNN-V RNN-DFT
654321

여성 - 합성

자연스러운 청결한 시끄러운 OMLSA RNN-V RNN-DFT
654321       남성 - 보코더

자연스러운 청결한 시끄러운 OMLSA RNN-V RNN-DFT
654321 남성 - 합성
그림 3: 여성 (위)과 남성 (아래)의 보코딩 (왼쪽) 및 합성 (오른쪽) 음성을 사용한 청취 실험의 순위 결과.

유형은 향상을 통해 작아지지만 여전히 남아 있습니다. 향상 후 왜곡 감소는 낮은 SNR에서 더 높아 보입니다. RNN 및 RNN-DFT 경우 모두에 대해.

4.5. 텍스트 음성 변환

우리는 이전에 훈련된 영어 여성 화자의 깨끗한 데이터 모델을 적용하여 여성과 남성 테스트 데이터를 위한 숨겨진 마르코프 모델(HMM) 기반의 합성 음성을 구축했습니다. MCEP 계수, BAP 및 Mel 스케일 F0 정적 값 및 델타 및 델타-델타를 사용하여 모델을 훈련시켰으며, 이를 다섯 개의 스트림으로 구성했습니다. 이러한 모델에서 생성하기 위해 전역 분산을 고려한 최대 우도 매개변수 생성 알고리즘을 사용했습니다.

4.6. 주관적 평가

우리는 다섯 가지 다른 유형의 보코딩 및 합성 음성을 평가했습니다: 깨끗한 음성 (CLEAN), 잡음이 섞인 음성 (NOISY) 및 세 가지 방법으로 개선된 음성 (OMLSA, RNN-V, RNN-DFT). 보코딩된 음성은 합성 음성의 품질과 개선된 보코딩 샘플의 품질이 관련되어 있는지 확인하기 위해 이 평가에 포함되었습니다. OMLSA 및 RNN-DFT 방법은 개선된 파형을 생성하고, RNN-DFT는 개선된 보코딩 매개변수의 시퀀스를 생성합니다. OMLSA 및 RNN-DFT의 보코딩된 음성을 생성하기 위해 우리는 TTS 보코더를 사용하여 파형을 분석하고 재합성했습니다. RNN-V의 보코딩된 음성을 생성하기 위해 우리는 단순히 개선된 매개변수를 합성했습니다.

4.6.1. 청취 실험 설계

샘플을 평가하기 위해 우리는 MUSHRA 스타일 [36] 청취 테스트를 만들었습니다. 이 테스트는 30개의 화면으로 구성되어 있으며, 각각 15개의 화면으로 구성된 두 개의 블록으로 구성되어 있습니다. 첫 번째 블록은 남성 목소리이고 두 번째 블록은 여성 목소리입니다. 각 블록의 첫 번째 절반은 보코딩된 음성 샘플의 화면으로 이루어져 있으며, 두 번째 절반은 합성 음성의 화면입니다. 각 화면에서 참가자들은 각 방법의 동일한 문장 샘플의 전체 품질을 0부터 100까지의 척도로 평가하도록 요청되었습니다. 우리는 특히 보코딩된 샘플 중 일부가 배경 소음을 포함하고 있기 때문에, 청취자들에게 음성과 배경을 모두 고려한 전체 품질을 평가하도록 요청했습니다. 이는 [11]에서 제안된 방법론과 일치합니다. 각 화면마다 다른 문장이 사용됩니다. 각각의 음성 유형(보코딩 및 합성)에 대해 42개의 다른 문장이 6명의 청취자에게 사용되었습니다. 보코딩된 음성에 사용된 문장은 Voice bank corpus에서 녹음된 문장의 일부였고, 합성에 사용된 문장은 Harvard 문장 [37]이었습니다. 훈련 화면은 항상 동일한 문장으로 구성되었으며, 보코딩된 음성의 샘플로 이루어져 있었습니다. 자연스러운 깨끗한 음성도 테스트에 포함되어 참가자들이 좋은 품질에 대한 참조를 가지고 있을 뿐만 아니라 지시에 따라 자료를 확인하고 100으로 점수를 매겼는지 확인할 수 있도록 했습니다. 우리는 이 평가에 24명의 영어 원어민을 모집했습니다.
4.6.2. 결과

Figure 3는 여성(위)과 남성(아래)의 보코딩(왼쪽)과 합성(오른쪽) 음성에 대한 청취자 응답의 순위 순서에 대한 상자그림을 보여줍니다. 순위 순서는 각 음성에 대해 화면별 및 청취자별로 점수에 따라 얻어졌습니다. 실선과 점선은 중앙값과 평균값을 나타냅니다. 유의한 차이를 검정하기 위해 Mann-Whitney U 검정을 사용하였으며, Bonferroni 보정을 적용하여 p값을 0.01로 설정하였습니다. 서로 유의한 차이가 없는 쌍은 각 상자그림 상단에 나타나는 수평선으로 연결되어 있습니다.

모든 경우에 자연음성은 예상대로 가장 높은 순위를 차지하였고, 소음은 가장 낮은 순위를 차지하였습니다. 모든 경우에 RNN-DFT가 모든 개선 전략 중에서 더 높은 순위를 차지하였습니다. 합성 음성에서는 깨끗한 음성과 RNN-DFT로 개선된 음성 간의 차이가 보코딩 음성보다 작습니다. 실제로 양 성별 모두 RNN-DFT로 개선된 음성으로 훈련된 합성 음성은 깨끗한 음성으로 구축된 음성과 유의한 차이가 없었습니다. 보코딩과 합성 음성에 대한 선호 순서는 같아 보입니다: OMLSA, 그 다음에 RNN-V와 RNN-DFT입니다. RNN 기반 방법의 이점은 보코딩과 합성 음성 모두에서 확인할 수 있으며, OMLSA 방법의 개선은 TTS 음향 모델 훈련 이후에 감소하는 것으로 보입니다.

5. 토론

우리는 RNN-DFT 방법에서 필요한 재구성 과정이 노이즈 데이터로부터 TTS 음향 특징을 추출하는 데 부정적인 영향을 미치지 않는 것으로 발견했습니다. 그러나 RNN-DFT 방법은 MCEP와 BAP 왜곡을 RNN-V 방법보다 더 증가시킨다는 것을 관찰했습니다. 노이즈 음성 데이터로부터 위상을 직접 재구성할 수 있다는 가정은 왜곡을 증가시킬 수도 있습니다. RNN-DFT는 RNN-V와 비교했을 때 V/UV 및 F0 오류를 감소시키는 것으로 보입니다. 이는 RNN-V 접근 방식이 F0 데이터를 직접 향상시키기 때문에 예상치 못한 결과입니다. 두 방법 모두 모든 테스트된 노이즈에 대해 MCEP 왜곡을 감소시켰으며, 비정상적인 노이즈와 정상적인 노이즈 간의 차이를 줄였습니다.
우리는 [12]에서 TTS 모델 훈련에 사용되는 음향 매개변수를 향상시키면 더 높은 품질의 합성 음성을 생성할 것이라 주장했습니다. 그러나 주관적인 점수는 RNN-DFT가 더 높은 품질의 보코딩 및 합성 음성을 생성한다는 것을 보여줍니다. 사실, RNN-DFT로 향상된 합성 음성은 깨끗한 데이터를 사용하여 구축된 음성과 동등한 순위로 평가되었습니다. F0 궤적을 직접 향상시키는 것은 상당히 어려운 작업이기 때문에 RNN-V가 잘 작동하지 않았다고 생각합니다. F0 추출 오류는 일부 프레임에서 상당히 크지만 다른 프레임에서는 작을 수 있습니다.

결론

이 논문에서는 두 가지 다른 음성 개선 방법을 제시했습니다. 이 방법들은 잡음이 섞인 음성 데이터로 훈련된 TTS 음성의 품질을 향상시키기 위해 재귀 신경망을 사용합니다. 한 방법에서는 기본 주파수와 Melcepstral 계수를 포함한 음향 특성으로 훈련된 TTS 모델을 훈련하는 데 사용되는 음향 특성과 함께 RNN을 훈련시킵니다. 다른 방법에서는 RNN을 전형적인 음성 개선 방법에서 일반적으로 수행되는 대로 크기 스펙트럼에서 추출된 매개 변수로 훈련시킵니다. 파형 재구성을 위해 위상 정보를 직접 얻습니다.

원본 잡음 신호로부터 크기 스펙트럼을 얻으며, RNN의 출력을 사용하여 크기 스펙트럼을 얻습니다. 우리는 멜 켑스트럴 왜곡이 더 높지만 두 번째 방법이 보코딩된 음성과 합성 음성, 여성 데이터와 남성 데이터 모두에 대해 더 높은 품질로 평가되었다는 것을 발견했습니다. 이 방법으로 개선된 데이터로 훈련된 합성 음성은 깨끗한 음성으로 훈련된 음성과 유사하게 평가되었습니다. 앞으로 우리는 DNN을 사용하여 훈련된 음성에도 유사한 개선이 적용될 수 있는지, 그리고 크기 스펙트럼을 직접 사용하여 RNN을 훈련하는 것이 결과를 더 개선시킬 수 있는지 조사하고자 합니다.

감사의 말씀 드립니다. 이 작업은 EPSRC의 프로그램 그랜트 EP/I031022/1 (NST) 및 EP/J002526/1 (CAF)에 의해 부분적으로 지원되었으며, 일본 과학기술국의 CREST (uDialogue 프로젝트)로부터도 지원을 받았습니다. 전체 NST 연구 데이터 컬렉션은 http://hdl.handle.net/10283/786에서 액세스할 수 있습니다.

7. 참고문헌
[1] H. Zen, K. Tokuda, and A. W. Black, "통계적 매개변수 음성 합성," 음성 통신, 제51권, 제11호, 1039-1064쪽, 2009년.

[2] J. Yamagishi, Z. Ling, and S. King, "HMM 기반 음성 합성의 견고성," Interspeech Proceedings, Brisbane, Australia, Sep. 2008, pp. 581-584.

[3] J. Yamagishi, C. Veaux, S. King, and S. Renals, "음성 장애를 가진 개인을 위한 음성 합성 기술: 보이스뱅킹과 재구성," 음향과 기술 연구, 제33권, 제1호, pp. 1-5, 2012.

[4] Y. Hu와 P. C. Loizou, "음성 개선 알고리즘의 주관적 비교," ICASSP 논문집, 제1권, 2006년 5월, pp. I-I.

[5] Y. Wang과 D. Wang, "시간 영역 신호 재구성을 위한 심층 신경망," ICASSP 논문집, 2015년 4월, pp. 4390-4394.

[6] Y.Xu,J.Du,L.-R.Dai,andC.-H.Lee,“깊은 신경망을 기반으로 한 음성 개선에 대한 회귀 접근," IEEETrans. onAudio,SpeechandLanguageProcessing,vol.23,no.1,pp.7–19,Jan2015.

[7] K. Kinoshita, M. Delcroix, A. Ogawa, and T. Nakatani, "깊은 신경망을 이용한 텍스트 기반 음성 개선," Interspeech 학회 논문집, 2015년 9월, 1760-1764쪽.

[8] F. Weninger, J. Hershey, J. LeRoux, and B. Schuller, "단일 채널 음성 분리를 위한 차별적으로 훈련된 순환 신경망," Proc. GlobalSIP, 2014년 12월, pp. 577-581.

[9] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Roux, J. R.
Hershey, and B. Schuller, Proc. Int. Conf. Latent Variable Anal-
ysis and Signal Separation. Springer International Publishing,
2015, ch. Speech Enhancement with LSTM Recurrent Neural
NetworksanditsApplicationtoNoise-RobustASR,pp.91–99.

[9] F. Weninger, H. Erdogan, S. Watanabe, E. Vincent, J. Roux, J. R.
Hershey, 그리고 B. Schuller, Proc. Int. Conf. Latent Variable Anal-
ysis and Signal Separation. Springer International Publishing,
2015, ch. Speech Enhancement with LSTM Recurrent Neural
NetworksanditsApplicationtoNoise-RobustASR,pp.91–99.

[10] T.TodaandK.Tokuda, "HMM 기반 음성 합성을 위한 전역 분산을 고려한 음성 매개 변수 생성 알고리즘," IEICETrans.Inf.Syst.,vol.E90-D,no.5,pp.816–824,2007.

[11] R. Karhila, U. Remes, and M. Kurimo, "HMM 기반 음성 합성 적응에서의 잡음: 분석, 평가 방법 및 실험," J. Sel. Topics in Sig. Proc., vol. 8, no. 2, pp. 285–295, 2014년 4월.

[12] C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi,
"Speech enhancement for a noise-robust text-to-speech synthesis system using deep recurrent neural networks," in Proc. Inter-
speech,(submitted)2016.

[12] C. Valentini-Botinhao, X. Wang, S. Takaki, and J. Yamagishi,
"깊은 순환 신경망을 사용한 잡음 강건한 텍스트 음성 합성 시스템을 위한 음성 개선," in Proc. Inter-
speech,(제출됨)2016.

[13] P. C. Loizou, 음성 개선: 이론과 실제, 1판. 
Boca Raton, FL, 미국: CRC Press, Inc., 2007.

[14] Y. Bengio, P. Simard, 그리고 P. Frasconi, "경사 하강법으로 장기 의존성을 학습하는 것은 어렵다," IEEE 신경망 트랜잭션, 제5권, 제2호, 157-166쪽, 1994년.
[15] S. Hochreiter 그리고 J. Schmidhuber, "장기 단기 기억," J. Neuralcomputation, 제9권, 제8호, 1735-1780쪽, 1997년.

[16] F.A.Gers, J.Schmidhuber, 그리고 F.Cummins, "LSTM을 이용한 지속적인 예측을 위한 학습: LSTM," J.Neuralcomputation, 제12권, 제10호, 2451-2471쪽, 2000년.

[17] A. Graves, A.-r. Mohamed, and G. Hinton, "Speech recognition with deep recurrent neural networks," in Proc. ICASSP, 2013, pp. 6645-6649. 
[17] A. 그레이브스, A.-r. 모하메드, 그리고 G. 힌튼, "깊은 순환 신경망을 이용한 음성 인식," Proc. ICASSP, 2013, pp. 6645-6649.

[18] H.Sak, A.W.Senior, 그리고 F.Beaufays, "대용량 어휘 음성인식을 위한 장단기 기억 기반 순환 신경망 구조," CoRR, vol.abs/1402.1128, 2014.

[19] S.-H. Chen, S.-H. Hwang, and Y.-R. Wang, "마단어 텍스트 음성 변환을 위한 RNN 기반 운율 정보 합성기," Proc.ICASSP, vol.6, no.3, pp.226–239, 1998.

[20] Y. Fan, Y. Qian, F.-L. Xie, and F. K. Soong, "양방향 LSTM 기반 재귀 신경망을 사용한 TTS 합성." Interspeech 논문집, 2014, pp. 1964-1968.

[21] R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory,
"장기 단기 기억, 양방향, 깊은 순환 신경망을 사용한 운율 윤곽 예측." in Proc. Interspeech,
2014, pp. 2268-2272.

[22] H. Zen and H. Sak, "저지연 음성 합성을 위한 단방향 장기 기억력 순환 신경망과 재귀 출력층," ICASSP. IEEE, 2015, pp. 4470-4474.

[23] S. Achanta, T. Godambe, and S.V. Gangashetty, "통계적 매개 음성 합성을 위한 재귀 신경망 구조에 대한 조사," Interspeech 논문집, 2015.

[24] C.Veaux, J.Yamagishi, 그리고 S.King, "The voicebank corpus: 디자인, 수집 및 대규모 지역 사투리 음성 데이터베이스의 데이터 분석," Proc.Int.Conf.OrientalCOCOSDA, 2013년 11월.

[25] J. Thiemann, N. Ito, and E. Vincent, "다양한 환경의 다채널 음향 잡음 데이터베이스: 다채널 환경 잡음 녹음의 데이터베이스," J.Acoust.Soc.Am., vol.133, no.5, pp.3591–3591, 2013.

[26] 활동적인 음성 레벨의 목적적 측정 ITU-T 권고안 P.56, ITU-T 권고안, 스위스 제네바, 1993년.

[27] H. 카와하라, I. 마스다-카츠세, 그리고 A. 쉐베이니, "음성 표현의 재구성을 위한 음높이 적응형 시간-주파수 평활화 및 순간 주파수 기반 F0 추출: 소리의 반복 구조의 가능한 역할," 음성 통신, 제27권, 187-207쪽, 1999년.

[28] 음성 신호 처리 툴킷: SPTK 3.4, 나고야 공과대학교, 2010년.

[29] D.Talkin, "음높이 추적을 위한 강력한 알고리즘," 음성 코딩 및 합성, 1995년, 495-518쪽.

[30] F. Weninger, "CURRENNT 소개: 뮌헨 오픈소스 CUDA 재귀 신경망 툴킷," 기계학습연구지, 제16권, 547-551쪽, 2015년.

[31] I. Cohen과 B. Berdugo, "비정상적인 잡음 환경에서의 음성 개선," 신호 처리, 제81권, 제11호, 2001년, 2403-2418쪽.

[32] I. Cohen, "악조건에서의 잡음 스펙트럼 추정: 개선된 최소 제어 재귀 평균," IEEE 음성 및 오디오 처리 트랜잭션, 제11권, 제5호, 466-475쪽, 2003년 9월.

[33] J. Yamagishi, T. Kobayashi, Y. Nakano, K. Ogata, and J. Iso-
gai, "HMM 기반 음성 합성을 위한 화자 적응 알고리즘 및 제한된 SMAPLR 적응 알고리즘의 분석," IEEE Trans. on Audio, Speech and Language Processing, vol.17, no.1, pp.66–83, 2009.

[34] R.Dall,C.Veaux,J.Yamagishi,andS.King,“Analysisofspeaker clustering strategies for HMM-based speech synthesis,” in Proc. Interspeech,Portland,USA,Sep.2012.

[34] R.Dall, C.Veaux, J.Yamagishi, 그리고 S.King, "HMM 기반 음성 합성을 위한 화자 클러스터링 전략 분석," Interspeech 학회 논문집, 포틀랜드, 미국, 2012년 9월.

[35] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kita-
mura, "HMM 기반 음성 합성을 위한 음성 매개 변수 생성 알고리즘," ICASSP 논문집, vol. 3. IEEE, 2000, pp. 1315-1318.

[36] 코딩 시스템의 중간 품질 수준에 대한 주관적 평가 방법, ITU 권고안 ITU-R BS.1534-1, 국제 전기통신 연합 무선통신 총회, 스위스 제네바, 2003년 3월.

[37] IEEE, "음성 품질 측정을 위한 IEEE 권장 사례," IEEE 오디오 및 전기음향학 트랜잭션, 제17권, 제3호, 225-246쪽, 1969년.

