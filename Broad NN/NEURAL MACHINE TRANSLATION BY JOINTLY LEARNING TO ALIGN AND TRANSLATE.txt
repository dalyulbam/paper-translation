ICLR 2015에서 학회 논문으로 발표되었습니다.

신경망 기계 번역은 공동으로 정렬하고 번역하는 것을 학습합니다.

드미트리 바다나우
독일 브레멘의 야곱스 대학교

경현 조 요슈아 벤지오∗
몬트리올 대학교

요약

신경망 기계 번역은 최근 제안된 기계 번역 접근 방식입니다. 전통적인 통계 기계 번역과 달리, 신경망 기계 번역은 번역 성능을 최대화하기 위해 공동으로 조정될 수 있는 단일 신경망을 구축하는 것을 목표로 합니다. 최근에 제안된 신경망 기계 번역 모델은 대부분 인코더-디코더의 가족에 속하며, 소스 문장을 고정 길이 벡터로 인코딩하고 디코더가 번역을 생성합니다. 본 논문에서는 고정 길이 벡터의 사용이 이 기본 인코더-디코더 아키텍처의 성능 향상에 병목 현상을 일으킨다고 추측하고, 이를 해결하기 위해 모델이 명시적으로 하드 세그먼트로 이러한 부분을 형성하지 않고도 대상 단어를 예측하는 데 관련된 소스 문장의 일부를 자동으로 (소프트하게) 탐색할 수 있도록 확장하는 것을 제안합니다. 이 새로운 접근 방식으로 우리는 영어-프랑스어 번역 작업에서 기존 최첨단 구문 기반 시스템과 비교 가능한 번역 성능을 달성합니다. 더 나아가, 질적 분석 결과 모델이 찾은 (소프트)정렬은 우리의 직관과 잘 일치한다는 것을 보여줍니다.

1. 소개

신경망 기계 번역은 최근에 Kalchbrenner와 Blunsom (2013), Sutskever 등 (2014) 및 Cho 등 (2014b)에 의해 제안된 기계 번역에 대한 새로운 접근 방식입니다. 예를 들어 Koehn 등 (2003)의 전통적인 구문 기반 번역 시스템과는 달리, 신경망 기계 번역은 문장을 읽고 올바른 번역을 출력하는 단일하고 큰 신경망을 구축하고 훈련시키려고 합니다.

대부분의 제안된 신경 기계 번역 모델은 인코더-디코더(숫츠케버 등, 2014; 조 등, 2014a)의 가족에 속합니다. 각 언어에 대한 인코더와 디코더가 있거나 각 문장에 적용되는 언어별 인코더를 포함하며, 이들의 출력이 비교됩니다(헤르만과 블런솜, 2014). 인코더 신경망은 소스 문장을 읽고 인코딩하여 고정 길이 벡터로 변환합니다. 그런 다음 디코더는 인코딩된 벡터에서 번역을 출력합니다. 언어 쌍을 위한 인코더와 디코더로 구성된 전체 인코더-디코더 시스템은 소스 문장이 주어졌을 때 올바른 번역의 확률을 최대화하기 위해 공동으로 훈련됩니다.

이 인코더-디코더 접근 방식의 잠재적 문제는 신경망이 소스 문장의 모든 필요한 정보를 고정 길이 벡터로 압축할 수 있어야 한다는 것입니다. 이는 특히 훈련 말뭉치의 문장보다 긴 문장, 특히 긴 문장에 대해 신경망이 대응하기 어려울 수 있습니다. Cho et al. (2014b)은 실제로 입력 문장의 길이가 증가함에 따라 기본 인코더-디코더의 성능이 빠르게 저하되는 것을 보였습니다.

이 문제를 해결하기 위해, 우리는 인코더-디코더 모델에 확장을 도입하여 정렬과 번역을 동시에 학습합니다. 제안된 모델이 번역 중에 단어를 생성할 때마다, 가장 관련 있는 정보가 집중된 소스 문장의 위치 집합을 (소프트하게) 탐색합니다. 그런 다음 모델은 이러한 소스 위치와 이전에 생성된 모든 대상 단어와 관련된 문맥 벡터를 기반으로 대상 단어를 예측합니다.

∗CIFAR 고급 연구원

1
아르Xiv: 1409.0473v7 [cs.CL] 19 May 2016
ICLR 2015에서 학회 논문으로 발표되었습니다.

이 접근 방식의 가장 중요한 차이점은 기본 인코더-디코더와 달리 전체 입력 문장을 하나의 고정 길이 벡터로 인코딩하려고 하지 않는다는 것입니다. 대신, 입력 문장을 일련의 벡터로 인코딩하고 번역하는 동안 이 벡터들의 하위 집합을 적응적으로 선택합니다. 이는 소스 문장의 모든 정보를 고정 길이 벡터로 압축해야 하는 신경 기계 번역 모델을 해방시킵니다. 우리는 이를 통해 모델이 긴 문장에 더 잘 대처할 수 있다는 것을 보여줍니다.

이 논문에서는, 우리는 정렬과 번역을 동시에 학습하는 제안된 접근 방식이 기본 인코더-디코더 방식보다 훨씬 향상된 번역 성능을 달성한다는 것을 보여줍니다. 이 개선은 더 긴 문장에서 더욱 뚜렷하게 나타나지만, 어떤 길이의 문장에서도 관찰될 수 있습니다. 영어에서 프랑스어로의 번역 작업에서, 제안된 접근 방식은 단일 모델로 전통적인 구문 기반 시스템과 비교 가능하거나 유사한 번역 성능을 달성합니다. 더 나아가, 질적 분석 결과, 제안된 모델은 소스 문장과 해당 대상 문장 사이에 언어학적으로 타당한 (부드러운) 정렬을 찾는다는 것을 보여줍니다.

2 배경: 신경망 기계 번역

확률론적인 관점에서 번역은 소스 문장 x가 주어졌을 때 대상 문장 y의 조건부 확률을 최대화하는 것과 동일합니다. 즉, argmax y입니다.

y
p(y | x). 인공신경망 기계 번역에서는, 우리는 병렬 훈련 말뭉치를 사용하여 문장 쌍의 조건부 확률을 최대화하기 위해 매개변수화된 모델을 적합시킵니다. 번역 모델에 의해 조건부 분포가 학습된 후에는 소스 문장이 주어졌을 때 해당하는 번역을 조건부 확률을 최대화하는 문장을 탐색하여 생성할 수 있습니다.

최근에는 많은 논문들이 신경망을 사용하여 이 조건부 분포를 직접 학습하는 것을 제안하였다 (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a; Sutskever et al., 2014; Cho et al., 2014b; Forcada and ˜ Neco, 1997 참조). 이 신경 기계 번역 접근 방식은 일반적으로 두 가지 구성 요소로 이루어져 있는데, 첫 번째는 소스 문장 x를 인코딩하고 두 번째는 타겟 문장 y로 디코딩한다. 예를 들어, (Cho et al., 2014a)와 (Sutskever et al., 2014)는 두 개의 순환 신경망(RNN)을 사용하여 가변 길이의 소스 문장을 고정 길이의 벡터로 인코딩하고 벡터를 가변 길이의 타겟 문장으로 디코딩하는 데 사용되었다.

신경 기계 번역은 아직 매우 새로운 접근 방식이지만 이미 유망한 결과를 보여주고 있다. Sutskever 등(2014)은 장기 단기 기억(LSTM) 유닛을 사용한 RNN을 기반으로 한 신경 기계 번역이 영어-프랑스어 번역 작업에서 기존의 구문 기반 기계 번역 시스템과 거의 동일한 성능을 달성한다고 보고했다. 신경 기계 번역 시스템에 신경 구성 요소를 추가함으로써, 예를 들어 구문 테이블에서 구문 쌍을 점수화하거나 후보 번역을 재순위화하는 데 사용함으로써(Sutskever 등, 2014), 이전의 최고 수준의 성능을 능가할 수 있게 되었다.

2.1 RNN 인코더-디코더

여기에서는 Cho et al. (2014a)와 Sutskever et al. (2014)가 제안한 RNN Encoder-Decoder라는 기본 프레임워크에 대해 간략히 설명하고, 우리가 동시에 정렬과 번역을 학습하는 새로운 아키텍처를 구축하는 데 사용합니다.

인코더-디코더 프레임워크에서, 인코더는 입력 문장인 벡터 시퀀스 x = (x1,··· ,xTx)를 벡터 c로 읽습니다. 가장 일반적인 접근 방식은 RNN을 사용하는 것입니다.

ht = f (xt,ht−1)               (1)
그리고

c = q ({h1,··· ,hTx}),
ht ∈ Rn은 시간 t에서의 숨겨진 상태이고, c는 숨겨진 상태의 시퀀스에서 생성된 벡터입니다. f와 q는 일부 비선형 함수입니다. 예를 들어, Sutskever et al. (2014)은 f로 LSTM을 사용하고 q ({h1,··· ,hT}) = hT를 사용했습니다.

1. 우리는 최첨단 성능이란, 어떠한 신경망 기반 구성요소도 사용하지 않은 기존 구문 기반 시스템의 성능을 의미합니다.
2. 이전 연구들 대부분(예: Cho et al., 2014a; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013)은 가변 길이의 입력 문장을 고정 길이 벡터로 인코딩하는 것을 사용했지만, 이는 필요하지 않을 뿐만 아니라 가변 길이 벡터를 가지는 것이 오히려 유리할 수 있습니다. 이후에 보여드리겠습니다.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

디코더는 종종 문맥 벡터 c와 이전에 예측된 모든 단어 {y1,··· ,yt(cid:48)−1}를 고려하여 다음 단어 yt(cid:48)를 예측하는 데에 훈련됩니다. 다시 말해, 디코더는 결합 확률을 순서대로 조건부 확률로 분해하여 번역 y에 대한 확률을 정의합니다.

p(y) = T ∏ t=1 p(yt | {y1,··· ,yt−1},c),    (2)
where y = y1,··· ,yTy. With an RNN, each conditional probability is modeled as

p(yt | {y1,··· ,yt−1},c) = g(yt−1,st,c), (3)
yt-1, st, c를 주어로 하는 p(yt | {y1,··· ,yt−1},c)는 g(yt−1,st,c)와 같다. (3)

g는 비선형적이고 잠재적으로 다층으로 구성된 함수로, yt의 확률을 출력합니다. st는 RNN의 숨겨진 상태입니다. RNN과 디컨볼루션 신경망의 하이브리드와 같은 다른 아키텍처도 사용될 수 있다는 점에 유의해야 합니다 (Kalchbrenner and Blunsom, 2013).

3 배우기를 위한 정렬과 번역

이 섹션에서는 신경망 기계 번역을 위한 새로운 아키텍처를 제안합니다. 새로운 아키텍처는 인코더로 양방향 RNN을 사용하고 (3.2절), 번역 중에 소스 문장을 탐색하는 디코더를 사용합니다 (3.1절).

3.1 디코더: 일반적인 설명

x
1
x
2
x
3
x
T
+
αt,1
αt,2 αt,3
αt,T
y
t-1
y
t

x
1
x
2
x
3
x
T
+
αt,1
αt,2 αt,3
αt,T
y
t-1
y
t

h 1 h 2 h 3  h T
h
1
h
2
h
3
h
T
s
t-1
s
t

h 1 h 2 h 3  h T
h
1
h
2
h
3
h
T
s
t-1
s
t

그림 1: 제안된 모델의 그래픽 일러스트
소스 문장 (x1, x2, ..., xT)이 주어졌을 때 t번째 목표 단어 yt를 생성하려고 시도하는 모델을 나타냅니다.
새로운 모델 아키텍처에서 우리는 식 (2)의 각 조건부 확률을 다음과 같이 정의합니다.

p(yi|y1,...,yi−1,x) = g(yi−1,si,ci), (4)
p(yi|y1,...,yi−1,x) = g(yi−1,si,ci), (4)

시간 i에 대한 RNN 숨겨진 상태인 si는 다음과 같이 계산됩니다.

시 = f(si−1,yi−1,ci).

기존의 인코더-디코더 접근 방식과 달리 (식 (2) 참조), 여기서는 각 대상 단어 yi마다 고유한 문맥 벡터 ci에 대해 확률이 조건화되는 것에 유의해야 합니다.

컨텍스트 벡터 ci는 입력 문장을 인코더가 매핑하는 주석(h1,··· ,hTx)의 시퀀스에 의존합니다. 각 주석 hi는 입력 시퀀스 전체에 대한 정보를 포함하며, 특히 i번째 단어 주변의 부분에 중점을 둡니다. 다음 섹션에서는 주석이 어떻게 계산되는지 자세히 설명합니다.

그러면 문맥 벡터 ci는 이러한 주석 hi의 가중합으로 계산됩니다.

ci = ci =
Tx
(cid:88) j=1αijhj. (5)
각 주석 hj의 가중치 αij는 다음과 같이 계산됩니다.

αij = exp(eij)
(cid:80)Tx
k=1
exp(eik),                (6)
where

eij = a(si−1,hj)는 j 위치 주변의 입력과 i 위치의 출력이 얼마나 일치하는지를 점수화하는 정렬 모델입니다. 이 점수는 RNN의 숨겨진 상태 si−1 (yi를 방출하기 바로 직전인 Eq. (4))와 입력 문장의 j번째 주석 hj에 기반합니다.

우리는 제안된 시스템의 다른 모든 구성 요소와 함께 학습되는 피드포워드 신경망으로 정렬 모델 a를 매개변수화합니다. 전통적인 기계 번역과 달리, 주의 모델은 사용되지 않습니다.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

정렬은 잠재 변수로 간주되지 않습니다. 대신, 정렬 모델은 직접적으로 소프트 정렬을 계산하여 비용 함수의 그래디언트를 역전파할 수 있게 합니다. 이 그래디언트는 정렬 모델과 전체 번역 모델을 동시에 훈련하는 데 사용될 수 있습니다.

우리는 모든 주석의 가중 합을 계산하는 방식을 이해할 수 있습니다. 이는 가능한 정렬에 대한 기대 주석으로 계산하는 것입니다. αij를 타겟 단어 yi가 소스 단어 xj에 정렬되거나 번역된 확률로 정의합시다. 그러면 i번째 컨텍스트 벡터 ci는 확률 αij를 가진 모든 주석에 대한 기대 주석입니다.

확률 αij 또는 그와 관련된 에너지 eij는 다음 상태 si를 결정하고 yi를 생성하는 데 이전 숨겨진 상태 si-1에 대한 주석 hj의 중요성을 반영합니다. 직관적으로, 이는 디코더에서 주의 메커니즘을 구현합니다. 디코더는 원본 문장의 일부에 주의를 기울일 부분을 결정합니다. 디코더에 주의 메커니즘을 부여함으로써, 인코더가 원본 문장의 모든 정보를 고정 길이 벡터로 인코딩하는 부담을 줄일 수 있습니다. 이 새로운 접근 방식을 통해 정보는 주석의 시퀀스 전체에 퍼져 있으며, 디코더가 필요에 따라 선택적으로 검색할 수 있습니다.

3.2 인코더: 시퀀스 주석을 위한 양방향 RNN

일반적인 RNN은 식 (1)에 설명된대로 첫 번째 기호 x1부터 마지막 기호 xTx까지 순서대로 입력 시퀀스 x를 읽습니다. 그러나 제안된 방법에서는 각 단어의 주석이 이전 단어뿐만 아니라 다음 단어들도 요약할 수 있기를 원합니다. 따라서 우리는 최근에 음성 인식에서 성공적으로 사용된 양방향 RNN (BiRNN, Schuster and Paliwal, 1997)을 사용하기를 제안합니다 (참조: Graves et al., 2013).

은 입력 시퀀스를 앞에서 뒤로 처리하는 반면, backward RNN은 입력 시퀀스를 뒤에서 앞으로 처리합니다.

− →
f는 입력 시퀀스를 순서대로 (x1부터 xTx까지) 읽고 전방향 숨겨진 상태의 시퀀스를 계산합니다.

(− → h1,···,− → hTx).
역방향 RNN

f는 역순으로 시퀀스를 읽어들이며 (xTx에서 x1까지), 역방향 숨겨진 상태의 시퀀스를 얻습니다.

(←−
h 1,···
,←−
h Tx).

(←−
h 1,···
,←−
h Tx).

우리는 각 단어 xj에 대한 주석을 얻기 위해 전방 숨겨진 상태를 연결합니다.

− →
마이너스 →
h j and the
에이치 제이 앤 더

뒤로 한 칸
←−
h j, 즉, hj =
(cid:104)− →
h (cid:62)
j
;←−
h (cid:62)
j
(cid:105)(cid:62)
. 이렇게 하면 주석 hj에는 이전 단어와 다음 단어의 요약이 포함됩니다. RNN의 최근 입력을 더 잘 나타내는 경향 때문에 주석 hj는 xj 주변의 단어에 초점을 맞출 것입니다. 이 주석의 시퀀스는 나중에 디코더와 정렬 모델에서 컨텍스트 벡터를 계산하는 데 사용됩니다 (식 (5)–(6)).

제안된 모델의 그래픽 설명은 그림 1을 참조하십시오.

4 실험 설정

우리는 영어에서 프랑스어로의 번역 작업에 제안된 접근 방식을 평가합니다. 우리는 ACL WMT '14에서 제공하는 이중 언어 병렬 말뭉치를 사용합니다. 비교를 위해 Cho et al. (2014a)에 의해 최근에 제안된 RNN Encoder-Decoder의 성능도 보고합니다. 우리는 두 모델 모두에 대해 동일한 훈련 절차와 동일한 데이터셋을 사용합니다.

4.1 데이터셋

WMT '14에는 다음과 같은 영어-프랑스어 병렬 말뭉치가 포함되어 있습니다: Europarl (61M 단어), 뉴스 코멘터리 (5.5M), UN (421M) 그리고 각각 90M 및 272.5M 단어의 크롤링된 두 개의 말뭉치, 총 850M 단어입니다. Cho et al. (2014a)에서 설명한 절차를 따라, Axelrod et al. (2011)의 데이터 선택 방법을 사용하여 병합된 말뭉치의 크기를 348M 단어로 줄입니다. 우리는 언급된 병렬 말뭉치 이외의 단일 언어 데이터를 사용하지 않습니다. 그러나 인코더를 사전 훈련하기 위해 훨씬 더 큰 단일 언어 말뭉치를 사용할 수도 있습니다. 우리는 뉴스-테스트를 연결합니다.

3 http://www.statmt.org/wmt14/translation-task.html
4 구현은 https://github.com/lisa-groundhog/GroundHog에서 사용 가능합니다.
5 http://www-lium.univ-lemans.fr/˜schwenk/cslm_joint_paper/에서 온라인으로 사용 가능합니다.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

0    10   20   30   40    50   60

영    십   이십 삼십 사십 오십 육십

문장 길이
05
10
15
20
25
30

B L
E
U s c
o r e

B L
E
U s c
o r e

RNNsearch-50
RNNsearch-30
RNNenc-50
RNNenc-30

그림 2: 생성된 번역의 BLEU 점수는 문장의 길이에 따라 테스트 세트에서 측정되었습니다. 결과는 모델에게 알려지지 않은 단어를 포함하는 전체 테스트 세트에 대한 것입니다.

2012년과 뉴스-테스트-2013년을 개발(검증) 세트로 사용하여 모델을 평가하고, WMT '14의 테스트 세트(news-test-2014)에서 모델을 평가합니다. 이 테스트 세트는 훈련 데이터에 없는 3003개의 문장으로 구성되어 있습니다.

평범한 토큰화 후에는 각 언어별로 가장 빈도가 높은 30,000개의 단어로 모델을 훈련시킵니다. 단어 목록에 포함되지 않은 단어는 특수 토큰([UNK])으로 매핑됩니다. 데이터에는 소문자 변환이나 어간 추출과 같은 다른 특수 전처리를 적용하지 않습니다.

4.2 모델들

우리는 두 종류의 모델을 훈련시킵니다. 첫 번째는 RNN 인코더-디코더 (RNNencdec, Cho et al., 2014a)이고, 다른 하나는 제안된 모델인 RNNsearch로 지칭합니다. 우리는 각 모델을 두 번 훈련시킵니다: 먼저 최대 30단어로 이루어진 문장으로 (RNNencdec-30, RNNsearch-30), 그리고 그 다음으로 최대 50단어로 이루어진 문장으로 (RNNencdec-50, RNNsearch-50).

RNNencdec의 인코더와 디코더는 각각 1000개의 은닉 유닛을 가지고 있습니다. RNNsearch의 인코더는 1000개의 은닉 유닛을 가진 전방향 및 후방향 순환 신경망(RNN)으로 구성되어 있습니다. 그리고 디코더는 1000개의 은닉 유닛을 가지고 있습니다. 두 경우 모두, 우리는 각 대상 단어의 조건부 확률을 계산하기 위해 단일 맥스아웃(Goodfellow et al., 2013) 은닉층을 가진 다중 레이어 네트워크를 사용합니다 (Pascanu et al., 2014).

우리는 각 모델을 훈련시키기 위해 미니배치 확률적 경사 하강법(SGD) 알고리즘과 Adadelta(Zeiler, 2012)를 함께 사용합니다. 각 SGD 업데이트 방향은 80개의 문장으로 구성된 미니배치를 사용하여 계산됩니다. 우리는 각 모델을 약 5일 동안 훈련시켰습니다.

모델이 훈련되면, 우리는 빔 탐색을 사용하여 조건부 확률을 대략적으로 최대화하는 번역을 찾습니다 (예: Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014)은 이 접근법을 사용하여 신경 기계 번역 모델에서 번역을 생성했습니다.

실험에서 사용된 모델의 아키텍처와 훈련 절차에 대한 자세한 내용은 부록 A와 B를 참조하십시오.

5 결과

5.1 양적 결과

표 1에서는 BLEU 점수로 측정된 번역 성능을 나열합니다. 표에서 명확하게 나타나는 것은 모든 경우에 제안된 RNNsearch가 기존의 RNNencdec보다 우수한 성능을 보인다는 것입니다. 더 중요한 것은, 알려진 단어로만 구성된 문장만을 고려할 때 RNNsearch의 성능이 기존의 구문 기반 번역 시스템(Moses)과 동등하다는 것입니다. 이는 Moses가 RNNsearch와 RNNencdec를 훈련시키기 위해 사용한 병렬 코퍼스 외에 별도의 단일 언어 코퍼스(418M 단어)를 사용한다는 것을 고려할 때, 상당한 성과입니다.

6 우리는 오픈 소스 기계 번역 패키지 Moses의 토큰화 스크립트를 사용했습니다.
7 이 논문에서 '숨겨진 유닛'이라고 하면 항상 게이트된 숨겨진 유닛을 의미합니다 (부록 A.1.1 참조).

2015년 ICLR에서 학회 논문으로 발표되었습니다.

유럽 경제 지역에 대한 협정은 1992년 8월에 서명되었습니다. <끝> L'accord

sur

라

지역

경제적인

유럽인

a

여름

서명

Please provide the sentences that you would like to have translated into Korean.

팔월

1992년

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

<end>

해양 환경은 환경 중에서 가장 알려지지 않은 것으로 알려져야 한다.

(a) 저는 한국어를 배우고 있어요.
(b) 번역을 도와주셔서 감사합니다.

équipement
signifie
que
la
Syrie
ne
peut
plus
produire
de
nouvelles
armes
chimiques.

장비
의미하다
그
시리아
더 이상
생산할 수 없다
새로운
화학 무기를
.

"이것은 나의 가족과의 미래를 바꿀 것이다," 남자가 말했다. <끝>

(c) 이 책은 내가 읽은 가장 재미있는 책이다.
(d) 나는 한국 음식을 좋아한다.

그림 3: RNNsearch-50에서 찾은 네 개의 샘플 정렬. 각 플롯의 x축과 y축은 소스 문장(영어)과 생성된 번역(프랑스어)의 단어에 해당합니다. 각 픽셀은 i번째 대상 단어에 대한 j번째 소스 단어의 주석 가중치 αij를 회색조(0: 검정색, 1: 흰색)로 표시합니다. (식 (6) 참조). (a) 임의의 문장. (b-d) 테스트 세트에서 알려지지 않은 단어가 없고 길이가 10에서 20 단어인 문장 중에서 임의로 선택된 세 개의 샘플.

제안된 접근 방식의 동기 중 하나는 기본 인코더-디코더 접근 방식에서 고정 길이의 컨텍스트 벡터를 사용하는 것이었습니다. 우리는 이 제한이 긴 문장에서 기본 인코더-디코더 접근 방식의 성능을 저하시킬 수 있다고 추측했습니다. 그림 2에서 우리는 RNNencdec의 성능이 문장의 길이가 증가함에 따라 급격히 하락하는 것을 볼 수 있습니다. 반면에, RNNsearch-30과 RNNsearch-50는 문장의 길이에 대해 더 견고합니다. 특히, RNNsearch-50은 길이가 50 이상인 문장에서도 성능 저하가 없습니다. 이 제안된 모델의 우수성은 RNNsearch-30이 심지어 RNNencdec-50보다 성능이 우수하다는 사실로 더욱 확실해집니다 (표 1 참조).

2015년 ICLR에서 학회 논문으로 발표되었습니다.

모델   전체  없음 UNK◦
RNNencdec-30 13.93 24.19
RNNsearch-30 21.50 31.44
RNNencdec-50 17.82 26.71
RNNsearch-50 26.75 34.16
RNNsearch-50(?) 28.45 36.15
Moses   33.30 35.63

표 1: 테스트 세트에서 계산된 훈련된 모델들의 BLEU 점수. 두 번째와 세 번째 열은 각각 모든 문장과, 문장 자체와 참조 번역에 알 수 없는 단어가 없는 문장들의 점수를 보여줍니다. 개발 세트에서의 성능 향상이 멈출 때까지 RNNsearch-50은 훨씬 더 오랜 시간 동안 훈련되었습니다. (◦)알 수 없는 단어가 없는 문장들만 평가할 때 [UNK] 토큰을 생성하지 않도록 했습니다 (마지막 열).

5.2 질적 분석

5.2.1 정렬

제안된 접근 방식은 생성된 번역과 원본 문장의 (소프트) 정렬을 직관적으로 검토하는 방법을 제공합니다. 이는 그림 3과 같이 방정식 (6)의 주석 가중치 αij를 시각화하여 수행됩니다. 각 플롯의 행렬의 각 행은 주석과 관련된 가중치를 나타냅니다. 이를 통해 대상 단어를 생성할 때 원본 문장의 어떤 위치가 더 중요하게 고려되었는지 확인할 수 있습니다.

우리는 그림 3의 정렬에서 영어와 프랑스어 사이의 단어 정렬이 대부분 단조롭다는 것을 알 수 있습니다. 각 행렬의 대각선에는 강한 가중치가 있습니다. 그러나 우리는 또한 몇 가지 복잡하고 단조롭지 않은 정렬도 관찰합니다. 형용사와 명사는 일반적으로 프랑스어와 영어 사이에서 다른 순서로 정렬됩니다. 그리고 우리는 그림 3 (a)에서 한 예를 볼 수 있습니다. 이 그림에서 우리는 모델이 구문 [European Economic Area]를 [zone économique européenne]로 올바르게 번역한다는 것을 알 수 있습니다. RNNsearch는 [zone]를 [Area]와 정확하게 정렬할 수 있었으며, 두 단어 ([European]과 [Economic])를 건너뛰고 한 번에 한 단어씩 뒤로 돌아가 전체 구문 [zone économique européenne]를 완성했습니다.

소프트 정렬의 강점은 하드 정렬과는 달리 분명하게 나타납니다. 예를 들어, 그림 3 (d)에서 확인할 수 있습니다. 번역된 소스 구문 [the man]을 고려해보십시오. 어떤 하드 정렬도 [the]를 [l’]로, [man]을 [homme]로 매핑할 것입니다. 이는 번역에 도움이 되지 않습니다. 왜냐하면 [the] 다음에 오는 단어를 고려하여 [le], [la], [les] 또는 [l’]로 번역해야 하는지 결정해야 하기 때문입니다. 우리의 소프트 정렬은 이 문제를 자연스럽게 해결하며, 모델이 [the]와 [man]을 모두 살펴볼 수 있도록 합니다. 이 예에서는 모델이 [the]를 올바르게 [l’]로 번역할 수 있었습니다. 그림 3의 모든 경우에서 유사한 동작을 관찰할 수 있습니다. 소프트 정렬의 추가적인 이점은 소스와 타겟 구문의 길이가 다른 경우에도 자연스럽게 처리할 수 있다는 것입니다. 이는 어떤 단어를 어디로 매핑해야 하는지에 대해 직관적이지 않은 방법([NULL]을 사용하는 방법)을 요구하지 않습니다. (예: Koehn, 2010의 4장과 5장 참조)

5.2.2 긴 문장

그림 2에서 명확하게 볼 수 있듯이 제안된 모델(RNNsearch)은 기존 모델(RNNencdec)보다 긴 문장을 번역하는 데 훨씬 우수합니다. 이는 RNNsearch가 긴 문장을 완벽하게 고정 길이 벡터로 인코딩할 필요가 없고, 특정 단어를 둘러싼 입력 문장의 부분만 정확하게 인코딩하면 되기 때문일 것으로 생각됩니다.

예시로, 테스트 세트에서 다음과 같은 원문 문장을 고려해보십시오.

입원 권한은 의사가 병원이나 의료 센터에 환자를 입원시키고 진단이나 시술을 수행할 권리입니다. 이 권리는 의사가 병원에서 의료 직종으로서의 지위를 가지고 있기 때문에 부여됩니다.

RNNencdec-50은 이 문장을 다음과 같이 번역했습니다:

입원 허가는 의사가 환자를 병원이나 의료 센터에 인정하거나 환자의 건강 상태에 따라 진단을 내리거나 진단을 받을 권리입니다.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

RNNencdec-50는 [의료 센터]까지 소스 문장을 올바르게 번역했습니다. 그러나 그 이후로 (밑줄 친 부분) 원래 소스 문장의 의미에서 벗어났습니다. 예를 들어, 소스 문장에서 [병원에서의 보건의료 종사자로서의 지위에 기초하여]를 [en fonction de son ´ etat de sant´ e]로 대체했습니다 ("건강 상태에 따라").

반면에, RNNsearch-50은 다음과 같은 올바른 번역을 생성했으며, 입력 문장의 전체 의미를 보존하고 어떤 세부 사항도 생략하지 않았다.

입원 허가는 의사가 환자를 입원시키고 진단이나 시술을 수행할 권리입니다. 이는 의료 종사자로서 병원에서의 업무 상태에 따라 이루어집니다.

테스트 세트에서 다른 문장을 고려해 봅시다.

이러한 경험은 디즈니의 노력의 일부로, "시리즈의 수명을 연장하고 디지털 플랫폼을 통해 관객과의 새로운 관계를 구축하는 것이 점점 더 중요해지고 있다"고 그는 덧붙였다.

RNNencdec-50에 의한 번역은 다음과 같습니다.

이러한 종류의 경험은 디즈니의 노력 중 하나로, "새로운 콘텐츠의 수명을 연장하고 디지털 독자와의 복잡한 관계를 발전시키는" 것입니다.

이전 예제와 마찬가지로, RNNencdec는 대략 30개 단어를 생성한 후에 원래 소스 문장의 실제 의미에서 벗어나기 시작했습니다 (밑줄 친 구절 참조). 그 이후로 번역 품질이 저하되며, 마침표가 없는 기본적인 실수와 같은 오류가 발생합니다.

다시 한 번, RNNsearch-50은 이 긴 문장을 정확하게 번역할 수 있었습니다.

이러한 경험은 디즈니의 노력의 일환으로, "시리즈의 수명을 연장하고 점점 중요해지는 디지털 플랫폼을 통해 새로운 관객과의 관계를 형성하기 위한 것"이라고 그는 덧붙였다.

이미 제시된 양적 결과와 함께, 이러한 질적 관찰은 RNNsearch 아키텍처가 표준 RNNencdec 모델보다 긴 문장의 번역을 훨씬 신뢰할 수 있게 만든다는 가설을 확인합니다.

부록 C에서는 RNNencdec-50, RNNsearch-50 및 Google 번역에 의해 생성된 긴 소스 문장의 몇 가지 추가 샘플 번역과 함께 참고 번역을 제공합니다.

6 관련 연구

6.1 정렬하는 법 배우기

최근에는 필기 합성의 맥락에서 Graves (2013)에 의해 출력 기호와 입력 기호를 정렬하는 유사한 접근 방식이 제안되었습니다. 필기 합성은 주어진 문자열 시퀀스의 필기를 생성하는 작업입니다. 그의 연구에서는 가우시안 커널의 혼합을 사용하여 주석의 가중치를 계산했는데, 각 커널의 위치, 너비 및 혼합 계수는 정렬 모델에서 예측되었습니다. 더 구체적으로, 그의 정렬은 위치를 단조롭게 증가하도록 예측하는 것으로 제한되었습니다.

우리 접근 방식과의 주요 차이점은 (Graves, 2013)에서 주석의 가중치 모드가 한 방향으로만 이동한다는 것입니다. 기계 번역의 맥락에서는 이는 심각한 제한 사항이며, (장거리) 재배열이 종종 문법적으로 올바른 번역을 생성하는 데 필요합니다 (예: 영어에서 독일어로).

우리의 접근 방식은 반면에 번역에서 대부분의 입력 및 출력 문장이 15-40 단어로만 이루어져 있기 때문에 모든 단어에 대한 주석 가중치를 계산해야 한다는 단점이 있다. 그러나 이는 제안된 방식의 적용 가능성을 다른 작업에 제한할 수도 있다.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

6.2 기계 번역을 위한 신경망

Bengio et al. (2003)이 이전 단어들의 고정된 개수에 대한 조건부 확률을 모델링하기 위해 신경망을 사용한 신경 확률 언어 모델을 소개한 이후로, 신경망은 기계 번역에서 널리 사용되어 왔습니다. 그러나 신경망의 역할은 기존의 통계 기반 기계 번역 시스템에 단순히 단일 기능을 제공하거나 기존 시스템이 제공하는 후보 번역 목록을 재정렬하는 데로 크게 제한되어 왔습니다.

예를 들어, Schwenk (2012)는 피드포워드 신경망을 사용하여 소스와 대상 구문의 점수를 계산하고, 이 점수를 구문 기반 통계 기계 번역 시스템의 추가 기능으로 사용하는 것을 제안했습니다. 최근에는 Kalchbrenner와 Blunsom (2013) 및 Devlin 등 (2014)이 기존 번역 시스템의 하위 구성 요소로서 신경망의 성공적인 사용을 보고했습니다. 전통적으로, 대상 언어 모델로 훈련된 신경망은 후보 번역 목록을 재점수화하거나 재정렬하는 데 사용되었습니다 (예: Schwenk 등, 2006).

위의 접근 방식들은 최첨단 기계 번역 시스템보다 번역 성능을 향상시키는 것으로 나타났지만, 우리는 신경망을 기반으로 완전히 새로운 번역 시스템을 설계하는 더욱 야심찬 목표에 더 관심이 있습니다. 이 논문에서 고려하는 신경 기계 번역 접근 방식은 이전 작업들과는 근본적으로 다른 것입니다. 기존 시스템의 일부로서 신경망을 사용하는 대신, 우리의 모델은 독립적으로 작동하며 소스 문장으로부터 직접 번역을 생성합니다.

7 결론

전통적인 신경 기계 번역 방법인 인코더-디코더 방식은 전체 입력 문장을 고정 길이 벡터로 인코딩하여 번역을 디코딩합니다. Cho et al. (2014b)와 Pouget-Abadie et al. (2014)의 최근 경험적 연구 결과를 기반으로, 고정 길이의 컨텍스트 벡터를 사용하는 것은 긴 문장을 번역하는 데 문제가 있을 것으로 추측했습니다.

이 논문에서는 이 문제를 해결하기 위한 새로운 아키텍처를 제안했습니다. 우리는 기본 인코더-디코더를 확장하여 모델이 각 대상 단어를 생성할 때 입력 단어의 집합 또는 인코더에 의해 계산된 주석을 (소프트하게) 탐색하도록 했습니다. 이를 통해 모델은 전체 소스 문장을 고정 길이 벡터로 인코딩할 필요가 없어지며, 또한 모델은 다음 대상 단어 생성에 관련된 정보에만 집중할 수 있습니다. 이는 신경 기계 번역 시스템이 긴 문장에서 좋은 결과를 얻는 능력에 매우 긍정적인 영향을 미칩니다. 전통적인 기계 번역 시스템과 달리, 정렬 메커니즘을 포함한 번역 시스템의 모든 구성 요소는 올바른 번역을 생성하는 로그 확률을 개선하기 위해 공동으로 훈련됩니다.

우리는 영어에서 프랑스어로의 번역 작업에서 제안된 RNNsearch 모델을 테스트했습니다. 실험 결과, 제안된 RNNsearch는 문장의 길이에 관계없이 기존의 인코더-디코더 모델 (RNNencdec)보다 훨씬 우수한 성능을 보였으며, 소스 문장의 길이에 대해 훨씬 더 견고한 모델임을 확인할 수 있었습니다. RNNsearch가 생성한 (소프트-)정렬을 조사한 정성적 분석에서, 모델이 올바른 번역을 생성하면서 각 대상 단어를 소스 문장의 관련 단어 또는 주석과 올바르게 정렬할 수 있다는 결론을 내릴 수 있었습니다.

아마도 더 중요한 것은 제안된 접근 방식이 기존 구문 기반 통계 기계 번역과 비교할 만한 번역 성능을 달성했다는 것입니다. 이는 제안된 아키텍처 또는 전체 신경 기계 번역 패밀리가 올해에 제안된 것만 고려하면 놀라운 결과입니다. 우리는 여기서 제안된 아키텍처가 더 나은 기계 번역과 일반적으로 자연 언어를 더 잘 이해하기 위한 유망한 한 걸음이라고 믿습니다.

미래에 남은 도전 중 하나는 알려지지 않은 또는 희귀한 단어를 더 잘 다루는 것입니다. 이는 모델이 보다 널리 사용되고 현재 최첨단 기계 번역 시스템과 모든 맥락에서의 성능을 맞추기 위해 필요합니다.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

감사의 말씀

저자들은 Theano의 개발자들에게 감사의 말씀을 드리고자 합니다 (Bergstra et al., 2010; Bastien et al., 2012). 우리는 다음 기관들에게 연구 자금 및 컴퓨팅 지원에 대한 지원을 인정합니다: NSERC, Calcul Québec, Compute Canada, Canada Research Chairs 및 CIFAR. Bahdanau는 Planet Intelligent Systems GmbH의 지원에 감사드립니다. 또한 Felix Hill, Bart van Merriënboer, Jean Pouget-Abadie, Coline Devin 및 Tae-Ho Kim에게도 감사드립니다.

참고문헌

Axelrod, A., He, X., 그리고 Gao, J. (2011). 유사한 도메인 데이터 선택을 통한 도메인 적응.
자연어 처리에 대한 ACL 컨퍼런스의 EMNLP 논문집에서 발표됨, 355-362쪽. Association for Computational Linguistics.

바스티앙, F., 람블랭, P., 패스카누, R., 베르스트라, J., 굿펠로우, I. J., 베르제롱, A., 부샤르, N.,
그리고 벤지오, Y. (2012). Theano: 새로운 기능과 속도 향상. 딥 러닝과 비지도 학습 기능 NIPS 2012 워크샵.

Bengio, Y., Simard, P., 그리고 Frasconi, P. (1994). 경사 하강법으로 장기 의존성을 학습하는 것은 어렵습니다. IEEE 신경망 트랜잭션, 5(2), 157-166.

Bengio, Y., Ducharme, R., Vincent, P., 그리고 Janvin, C. (2003). 신경망 확률 언어 모델. J. Mach. Learn. Res., 3, 1137–1155.

Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: CPU 및 GPU 수학 표현 컴파일러. Python for Scientific Computing Conference (SciPy) 논문집. 구두 발표.

Boulanger-Lewandowski, N., Bengio, Y., 그리고 Vincent, P. (2013). 재귀 신경망을 이용한 오디오 코드 인식. ISMIR에서.

조, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., 그리고 Bengio, Y. (2014a).
통계 기계 번역을 위한 RNN 인코더-디코더를 사용한 구문 표현 학습.
Empiricial Methods in Natural Language Processing (EMNLP 2014) 논문집에 수록 예정.

조, K., van Merriënboer, B., Bahdanau, D., 그리고 Bengio, Y. (2014b). 신경망 기계 번역의 특성에 대하여: 인코더-디코더 접근법. 제8회 구문, 의미 및 통계적 번역 구조 워크샵에 대하여. 나타날 예정.

데블린, J., 지비브, R., 황, Z., 라마르, T., 슈왈츠, R., 그리고 마쿨, J. (2014). 통계 기계 번역을 위한 빠르고 견고한 신경망 합성 모델. Association for Computational Linguistics에서 발표.

Forcada, M. L. 및 Neco, R. P. (1997). 번역을 위한 재귀적 이종 연상 기억. J. Mira, R. Moreno-D´ıaz 및 J. Cabestany 편집, 생물학적 및 인공 계산: 뇌과학에서 기술로, Lecture Notes in Computer Science의 1240권, 453-462쪽. Springer Berlin Heidelberg.

굿펠로우, 아이., 워드-팔리, 디., 미르자, 엠., 쿠르빌, 에이., 그리고 벤지오, 와이. (2013). 맥스아웃 네트워크. 제30회 국제 기계 학습 컨퍼런스 논문집, 1319-1327쪽.

Graves, A. (2012). 순환 신경망을 이용한 시퀀스 변환. 제29회 국제 기계 학습 대회 논문집(ICML 2012)에서 발표된 논문입니다.

Graves, A. (2013). 순환 신경망을 사용하여 시퀀스 생성하기. arXiv:1308.0850 [cs.NE].

Graves, A., Jaitly, N., and Mohamed, A.-R. (2013). 하이브리드 음성 인식과 깊은 양방향 LSTM. 자동 음성 인식 및 이해 (ASRU), 2013 IEEE 워크샵에서, 페이지 273-278.

2015년 ICLR에서 학회 논문으로 발표되었습니다.

헤르만, K. 및 블런솜, P. (2014). 단어 정렬 없이 다국어 분산 표현. 제2회 국제 학습 표현 대회 (ICLR 2014) 논문집에서.

호크라이터, S. (1991). 동적 신경망에 대한 연구. 학위 논문, 인포매틱스 학부, 브라우어 교수님 연구실, 뮌헨 공과대학교.

Hochreiter, S. 및 Schmidhuber, J. (1997). 장기 단기 기억. 신경 계산, 9(8), 1735-1780.

Kalchbrenner, N. and Blunsom, P. (2013). 재귀적인 연속 번역 모델. ACL 자연어 처리 기법에 대한 EMNLP 학회 논문집에서 발표된 논문, 1700-1709쪽. 계산 언어학 협회.

캠브리지 대학 출판사, 뉴욕, 미국에 위치한 코엔, P. (2010). 통계 기계 번역.

Koehn, P., Och, F. J., 그리고 Marcu, D. (2003). 통계적 구문 기반 번역. 2003년 북미 협회 컨퍼런스 논문집, NAACL '03, 48-54쪽, Stroudsburg, PA, 미국. Association for Computational Linguistics.

Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). 순환 신경망 훈련의 어려움에 대하여. ICML'2013에서.

파스카누, R., 미코로프, T., 그리고 벵지오, Y. (2013b). 순환 신경망 훈련의 어려움에 대하여. 제30회 국제 기계 학습 대회 (ICML 2013) 논문집에서.

Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2014). 깊은 순환 신경망을 구축하는 방법. 제2회 국제 학습 표현 대회 논문집(ICLR 2014)에서.

Pouget-Abadie, J., Bahdanau, D., van Merriënboer, B., Cho, K., and Bengio, Y. (2014). 자동 분할을 사용한 신경 기계 번역에서 문장 길이의 저주 극복. 통계적 번역에서 구문, 의미 및 구조에 대한 여덟 번째 워크샵. 나타날 예정.

슈스터, M. 및 파리왈, K. K. (1997). 양방향 순환 신경망. 신호 처리, IEEE 트랜잭션, 45(11), 2673-2681.

Schwenk, H. (2012). 구문 기반 통계 기계 번역을 위한 연속 공간 번역 모델. M. Kay와 C. Boitet 편집자, 24th International Conference on Computational Linguistics (COLIN) 논문집, 1071-1080쪽. 인도 공과 대학교.

Schwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). 통계 기계 번역을 위한 연속 공간 언어 모델. COLING/ACL 주요 컨퍼런스 포스터 세션 논문집, 723-730쪽. Association for Computational Linguistics.

순서대로 시퀀스 학습을 위한 신경망을 사용한 학습에 대한 연구입니다.

제일러, M. D. (2012). ADADELTA: 적응형 학습률 방법. arXiv:1212.5701 [cs.LG].

2015년 ICLR에서 학회 논문으로 발표되었습니다.

모델 아키텍처

건축 선택 사항

제안된 방식은 섹션 3에서 제시된 일반적인 프레임워크로, 여기에서는 순환 신경망(RNN)의 활성화 함수 f와 정렬 모델 a를 자유롭게 정의할 수 있습니다. 여기에서는 본 논문의 실험을 위해 우리가 선택한 방법을 설명합니다.

재발성 신경망

RNN의 활성화 함수 f로는 Cho 등이 최근에 제안한 게이트된 숨겨진 유닛을 사용합니다. 게이트된 숨겨진 유닛은 원소별 tanh와 같은 전통적인 간단한 유닛의 대안입니다. 이 게이트된 유닛은 이전에 Hochreiter와 Schmidhuber (1997)에 의해 제안된 LSTM 유닛과 유사하며, 장기 의존성을 더 잘 모델링하고 학습할 수 있는 능력을 공유합니다. 이는 미분의 곱이 1에 가까운 풀린 RNN의 계산 경로를 가지고 있기 때문에 가능합니다. 이러한 경로는 기울기가 사라지는 현상에 너무 많이 영향을 받지 않고 쉽게 역전파될 수 있도록 합니다. 따라서 Sutskever 등이 유사한 맥락에서 수행한 것처럼 여기에서 설명한 게이트된 숨겨진 유닛 대신 LSTM 유닛을 사용하는 것이 가능합니다.

n개의 게이트된 숨겨진 유닛을 사용하는 RNN의 새로운 상태 si는 다음과 같이 계산됩니다.

시 = f(si−1,yi−1,ci) = (1 − zi) ◦ si−1 + zi ◦ ˜ si,

◦는 요소별 곱셈을 나타내며, zi는 업데이트 게이트의 출력입니다. 제안된 업데이트된 상태 ˜ si는 다음과 같이 계산됩니다.

˜ si = tanh(We(yi−1) + U [ri ◦ si−1] + Cci), 

˜ si = tanh(We(yi−1) + U [ri ◦ si−1] + Cci)

e(yi−1) ∈ Rm은 단어 yi−1의 m차원 임베딩이고, ri은 리셋 게이트의 출력입니다. yi가 K차원 벡터로 표현될 때, e(yi)는 단순히 임베딩 행렬 E ∈ Rm×K의 열입니다. 가능한 경우, 식을 덜 혼잡하게 만들기 위해 편향 항을 생략합니다.

갱신 게이트 zi는 각 은닉 유닛이 이전 활성화를 유지할 수 있도록 허용하며, 리셋 게이트 ri는 이전 상태에서 얼마나 많은 정보를 리셋해야 하는지를 제어합니다. 우리는 이를 계산합니다.

zi = σ (Wze(yi−1) + Uzsi−1 + Czci),
ri = σ (Wre(yi−1) + Ursi−1 + Crci),

σ (·)은 로지스틱 시그모이드 함수입니다.

디코더의 각 단계에서, 우리는 출력 확률 (식 (4))을 다중 계층 함수 (Pascanu et al., 2014)로 계산합니다. 우리는 maxout 유닛 (Goodfellow et al., 2013)의 단일 은닉층을 사용하고 출력 확률 (각 단어에 대해 하나씩)을 소프트맥스 함수로 정규화합니다 (식 (6) 참조).

1.2 정렬 모델

모델의 정렬 모델은 Tx × Ty 번의 평가가 필요한 각 문장 쌍의 길이 Tx와 Ty를 고려하여 설계되어야 합니다. 계산을 줄이기 위해 단일층 다중 퍼셉트론을 사용합니다.

a(si−1,hj) = v
a
tanh(Wasi−1 + Uahj),

Wa ∈ Rn×n, Ua ∈ Rn×2n, va ∈ Rn은 가중치 행렬입니다. Uahj는 i에 의존하지 않으므로, 계산 비용을 최소화하기 위해 미리 계산할 수 있습니다.

8 여기에서는 디코더의 공식을 보여줍니다. 동일한 공식은 컨텍스트 벡터 ci와 관련된 항목을 무시하기만 하면 인코더에서도 사용할 수 있습니다.

12
2015년 ICLR에서 학회 논문으로 발표되었습니다.

모델에 대한 자세한 설명 2가지

2.1 인코더

이 섹션에서는 실험에서 사용된 제안된 모델 (RNNsearch)의 아키텍처에 대해 자세히 설명합니다 (4-5절 참조). 이후로는 가독성을 높이기 위해 모든 편향 용어를 생략합니다.

모델은 1-of-K로 인코딩된 단어 벡터의 소스 문장을 입력으로 사용합니다.

x = (x1,...,xTx), xi ∈ RKx
그리고 1-of-K 코드화된 단어 벡터의 번역 문장을 출력합니다.

y = (y1,...,yTy), yi ∈ RKy,
여기서 Kx와 Ky는 각각 원본 언어와 대상 언어의 어휘 크기를 나타냅니다. Tx와 Ty는 각각 원본 문장과 대상 문장의 길이를 나타냅니다.

먼저, 양방향 순환 신경망(BiRNN)의 전방 상태가 계산됩니다.

− → h i is the i-th hidden state,
− → z i is the i-th input,
◦ represents element-wise multiplication.

− →
안녕
=tanh(cid:16)−→
WExi +
− →
U
(cid:104)− →
안녕
i ◦
− →
안녕
i−1(cid:105)(cid:17)
− →
안녕
i =σ
(cid:16)−→
WzExi +
− →
U
z− →
안녕
i−1(cid:17)
− →
안녕
i =σ
(cid:16)−→
WrExi +
− →
U
r− →
안녕
i−1(cid:17)
.

E는 Rm×Kx의 단어 임베딩 행렬입니다.

−→ W,−→ Wz,−→ Wr은 Rn×m에 속하는 가중치 행렬입니다. m과 n은 단어 임베딩 차원과 은닉 유닛의 수입니다. σ(·)은 일반적으로 로지스틱 시그모이드 함수입니다.

뒤쳐진 주

(←−
h 1,···
,←−
h Tx)는 비슷하게 계산됩니다. 우리는 가중치 행렬과는 달리 단어 임베딩 행렬 E를 전방향 및 역방향 RNN 사이에서 공유합니다.

우리는 주석 (h1, h2, ···, hTx)를 얻기 위해 앞뒤 상태를 연결합니다.

안녕하세요

(7) 

Please give me a glass of water.

A.2.2 디코더

인코더로부터의 주석을 통해 디코더의 숨겨진 상태 si가 계산됩니다.

시 = (1 - 지) ◦ 시-1 + 지 ◦ ˜ 시

어디에

˜ 시 = tanh(WEyi−1 + U [ri ◦ si−1] + Cci)
zi = σ (WzEyi−1 + Uzsi−1 + Czci)
ri = σ (WrEyi−1 + Ursi−1 + Crci)
E는 대상 언어의 단어 임베딩 행렬입니다. W, Wz, Wr ∈ Rn×m, U, Uz, Ur ∈ Rn×n,
그리고 C, Cz, Cr ∈ Rn×2n은 가중치입니다. 다시 말해, m은 단어 임베딩 차원이고 n은 은닉 유닛의 수입니다. 초기 은닉 상태 s0는 s0 =
tanh(cid:16) Ws←−
h
1(cid:17)
로 계산됩니다. 여기서 Ws ∈ Rn×n입니다.

매 단계마다 정렬 모델에 의해 컨텍스트 벡터 ci가 재계산됩니다.

ci = ci
Tx = Tx
(cid:88) = (cid:88)
j=1αijhj = j=1αijhj

13
ICLR 2015에서 학회 논문으로 발표되었습니다.

모델 업데이트 (×105) 에포크 시간 GPU 훈련 NLL 개발 NLL
RNNenc-30 8.46 6.4 109 TITAN BLACK 28.1 53.0
RNNenc-50 6.00 4.5 108 Quadro K-6000 44.0 43.6
RNNsearch-30 4.71 3.6 113 TITAN BLACK 26.7 47.2
RNNsearch-50 2.88 2.2 111 Quadro K-6000 40.7 38.1
RNNsearch-50(?) 6.67 5.0 252 Quadro K-6000 36.7 35.2

표 2: 학습 통계 및 관련 정보. 각 업데이트는 단일 미니배치를 사용하여 매개변수를 한 번 업데이트하는 것과 대응합니다. 한 epoch는 훈련 세트를 한 번 통과하는 것을 의미합니다. NLL은 훈련 세트 또는 개발 세트의 문장들의 평균 조건부 로그 확률입니다. 문장의 길이가 다르다는 점에 유의하세요.

어디에

αij = exp(eij)
(cid:80)Tx
k=1
exp(eik)
eij = v(cid:62)
a
tanh(Wasi−1 + Uahj),

그리고 hj는 원본 문장에서 j번째 주석입니다 (식 (7) 참조). va ∈ Rn(cid:48), Wa ∈ Rn(cid:48)×n 및 Ua ∈ Rn(cid:48)×2n은 가중치 행렬입니다. 모델이 RNN 인코더-디코더 (Cho et al., 2014a)가 되는 것에 유의하십시오. ci를 고정한다면

− →
h Tx.
− →
h Tx.

이전 디코더 상태 i−1, 문맥 c_i, 그리고 마지막으로 생성된 단어 y_i−1을 사용하여, 우리는 대상 단어 y_i의 확률을 정의합니다.

p(yi|si,yi−1,ci) ∝ exp(시, 이전 시, 단어의 특성에 따른 가중치) × 현재 시의 단어

안녕,

어디에

ti = (2) max(8)˜ ti,2j−1,˜ ti,2j(9)(3)(62)

j=1,...,l

그리고 ˜ ti,k는 ˜ ti 벡터의 k번째 요소로 계산됩니다.

˜ ti =Uosi−1 + VoEyi−1 + Coci.
˜ ti =Uosi−1 + VoEyi−1 + Coci.

Wo ∈ RKy×l, Uo ∈ R2l×n, Vo ∈ R2l×m 그리고 Co ∈ R2l×2n은 가중치 행렬입니다. 이는 단일 맥스아웃 은닉층을 가진 깊은 출력 (Pascanu et al., 2014)을 가지고 있다고 이해할 수 있습니다 (Goodfellow et al., 2013).

A.2.3 모델 크기

이 논문에서 사용된 모든 모델의 은닉층 크기 n은 1000이고, 단어 임베딩 차원 m은 620이며, 깊은 출력의 maxout 은닉층 크기 l은 500입니다. 정렬 모델의 은닉 유닛 수 n(cid:48)은 1000입니다.

B 훈련 절차

B.1 파라미터 초기화

우리는 순환 가중치 행렬을 초기화했습니다.

U, Uz, Ur,←−
U
,←−
U
z,←−
U
r,− →
U
,− →
U z 그리고
− →
U r을 무작위 또는 직교 행렬로 초기화합니다. Wa와 Ua에 대해서는 각 요소를 평균이 0이고 분산이 0.0012인 가우시안 분포에서 샘플링하여 초기화합니다. Va의 모든 요소와 모든 편향 벡터는 0으로 초기화합니다. 다른 가중치 행렬은 평균이 0이고 분산이 0.012인 가우시안 분포에서 샘플링하여 초기화합니다.

B.2 훈련

SGD 알고리즘을 사용했습니다. Adadelta (Zeiler, 2012)는 각 매개변수의 학습률을 자동으로 조정하는 데 사용되었습니다 ((cid:15) = 10−6 및 ρ = 0.95). 우리는 명시적으로

14
2015년 ICLR에서 학회 논문으로 발표되었습니다.

비용 함수의 그래디언트의 L2-노름을 정규화하여, 노름이 미리 정의된 임계값인 1보다 큰 경우에는 최대한 1로 유지했습니다 (Pascanu et al., 2013b). 각 SGD 업데이트 방향은 80개의 문장 미니배치로 계산되었습니다.

각 업데이트마다 우리의 구현은 미니배치에서 가장 긴 문장의 길이에 비례하는 시간이 필요합니다. 따라서 계산의 낭비를 최소화하기 위해 매 20번째 업데이트 전에 1600개의 문장 쌍을 검색하여 길이에 따라 정렬하고 20개의 미니배치로 분할했습니다. 훈련 데이터는 훈련 전에 한 번 섞였으며 이러한 방식으로 순차적으로 탐색되었습니다.

표 2에서는 실험에 사용된 모든 모델의 훈련과 관련된 통계를 제시합니다.

C. 긴 문장의 번역

1. Can you please tell me how to get to the nearest subway station from here?
2. I am sorry, but I don't understand what you are saying.
3. Yesterday, I went to the park with my friends and we had a great time.
4. My favorite food is pizza because it is delicious and easy to eat.
5. In the summer, I like to go swimming at the beach and soak up the sun.
6. I have been studying Korean for two years, and I can now have basic conversations.
7. It is important to exercise regularly and eat a balanced diet for good health.
8. I am planning to visit Korea next year to experience the culture and try the local cuisine.
9. The movie I watched last night was very interesting and had a surprising ending.
10. I enjoy reading books because they allow me to escape into different worlds.

입원 권한은 의사가 병원이나 의료 센터에서 환자를 입원시키고 진단이나 시술을 수행하는 권리입니다. 이 권리는 의사가 병원에서 의료 직원으로서의 지위에 따라 부여됩니다.

이러한 경험은 디즈니의 노력의 일환으로, "시리즈의 수명을 연장하고 점점 중요해지는 디지털 플랫폼을 통해 관객과의 새로운 관계를 구축하는 것"이라고 그는 덧붙였다.

목요일 기자회견에서 블레어씨는 이 동영상에는 시장에 대한 형사고발을 이끌 수 있는 "합리적 동기"가 없다고 말했습니다.

테이블 3: RNNenc-50과 RNNsearch-50에 의해 생성된 번역은 테스트 세트에서 선택된 30 단어 이상의 긴 소스 문장입니다. 각 소스 문장에 대해 우리는 또한 골드 표준 번역을 보여줍니다. Google 번역에 의한 번역은 2014년 8월 27일에 이루어졌습니다.

15

