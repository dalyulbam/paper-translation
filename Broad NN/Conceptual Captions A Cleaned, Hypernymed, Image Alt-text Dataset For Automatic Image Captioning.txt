개념적 캡션: 정제된, 상위어로 구성된 이미지 대체 텍스트 데이터셋

자동 이미지 캡션 생성을 위해

피유시 샤르마, 난 딩, 세바스찬 굿맨, 라두 소리쿠트

구글 인공지능
베니스, CA 90291
{piyushsharma,dingnan,seabass,rsoricut}@google.com

요약

우리는 이미지 캡션 주석의 새로운 데이터셋인 Conceptual Captions을 제공합니다. 이 데이터셋은 MS-COCO 데이터셋(Lin et al., 2014)보다 이미지 수가 10배 더 많으며, 더 다양한 이미지와 이미지 캡션 스타일을 포함하고 있습니다. 우리는 이를 수억 개의 웹페이지에서 이미지 캡션 주석을 추출하고 필터링하여 달성했습니다. 또한, 우리는 여러 이미지 캡션 모델의 정량적 평가를 제시하고, Inception-ResNet-v2(Szegedy et al., 2016)를 기반으로 한 이미지 특징 추출 및 Transformer(Vaswani et al., 2017)을 기반으로 한 시퀀스 모델링 아키텍처가 Conceptual Captions 데이터셋에서 훈련시 최상의 성능을 달성한다는 것을 보여줍니다.

1 소개

자동 이미지 설명은 이미지의 시각적 내용을 정확하게 반영하는 자연어 문장(보통 문장)을 생성하는 작업입니다. 이 작업은 딥러닝 아키텍처를 기반으로 한 다양한 솔루션 제안의 폭발적인 증가를 보았습니다 (Bengio, 2009), 2015 COCO 챌린지의 우승자들 (Vinyals et al., 2015a; Fang et al., 2015)로 시작하여 다양한 개선 사항 (예 : Bernardi et al. (2016)의 검토 참조)으로 이어졌습니다. 자동 이미지 설명 시스템의 실제 응용 프로그램에는 이미지 인덱싱이나 검색을 위한 설명을 활용하거나 시각 장애인들에게 시각 신호를 텍스트 음성 변환 기술을 통해 전달할 수 있는 정보로 변환하는 데 도움이 됩니다. 과학적 도전은 컴퓨터 비전과 자연어 처리의 교차점에서 최신 개선 사항을 조화롭게 활용하고 발전시키는 것으로 간주됩니다.

알트 텍스트: 파키스탄 노동자가 2005년 11월 7일 파키스탄 발라코트의 타지마할 호텔에서 파편을 청소하는 데 도움을 주고 있습니다.

개념적 캡션: 한 명의 작업자가 잔해를 청소하는 데 도움을 줍니다.

가수 저스틴 팀버레이크는 2017년 9월 23일 테네시주 프랭클린에서 개최된 2017 피그리미지 음악 및 문화 축제에서 공연을 했다.

개념적 캡션: 팝 아티스트
도시에서 페스티벌에서 공연한다.

그림 1: 개념적 캡션 데이터셋에서의 이미지와 이미지 설명의 예시; 우리는 기존의 대체 텍스트 설명으로부터 시작하여, 청결함, 정보성, 유창성, 학습 가능성을 균형있게 처리하여 개념적 캡션을 자동으로 생성합니다.

이 작업에 대한 관심 증가에 기여한 주요한 발전은 두 가지 주요 범주로 나뉩니다. 첫 번째는 주석이 달린 대량의 데이터의 가용성입니다. 관련 데이터셋에는 ImageNet 데이터셋 (Deng et al., 2009)이 포함되며, 1400만 개 이상의 이미지와 100만 개의 경계 상자 주석이 있습니다. 또한 MS-COCO 데이터셋 (Lin et al., 2014)도 있으며, 12만 개의 이미지와 5가지 이미지 캡션 주석이 있습니다. 두 번째는 현대적인 합성곱 신경망 (예: Krizhevsky et al. (2012))과 같은 강력한 모델링 메커니즘의 가용성입니다. 이러한 모델링 메커니즘은 이미지 픽셀을 수동 특성 공학 없이 고수준 특성으로 변환할 수 있습니다.

본 논문에서는 데이터와 모델링 범주 모두에 기여합니다. 첫째로, 우리는 새로운 캡션 주석 데이터셋인 Conceptual Captions (Fig. 1)을 제시합니다. 이 데이터셋은 COCO보다 10배 이상의 이미지를 가지고 있습니다.

∗https://github.com/google-research-datasets/conceptual-captions
데이터셋. Conceptual Captions은 약 3.3M개의 (이미지, 설명) 쌍으로 구성되어 있습니다. COCO 이미지의 선별된 스타일과는 달리, Conceptual Captions 이미지와 해당 원시 설명은 웹에서 수집되어 더 다양한 스타일을 대표합니다. 원시 설명은 웹 이미지와 관련된 Alt-text HTML 속성†에서 수집되었습니다. 우리는 깨끗함, 정보성, 유창성 및 학습 가능성의 균형을 달성하기 위해 후보 이미지/설명 쌍을 추출, 필터링 및 변환하는 자동 파이프라인(Fig. 2)을 개발했습니다.
모델링 범주에 기여하기 위해, 우리는 여러 이미지 캡션 모델을 평가합니다. Huang et al. (2016)의 연구 결과를 기반으로, 이미지 특징 추출을 위해 Inception-ResNet-v2 (Szegedy et al., 2016)를 사용하며, 잔여 연결과 계산 효율적인 Inception 유닛을 통해 최적화 이점을 얻습니다. 캡션 생성에는 RNN 기반 (Hochreiter and Schmidhuber, 1997) 및 Transformer 기반 (Vaswani et al., 2017) 모델을 모두 사용합니다. 우리의 결과는 Transformer 기반 모델이 더 높은 출력 정확도를 달성한다는 것을 보여줍니다. Vaswani et al. (2017)의 보고서와 함께, RNN과 비교하여 훈련 및 서비스에 필요한 매개변수 및 FLOP 수가 줄어드는 것에 대한 보고서를 고려하면, T2T8x8 (Section 4)와 같은 모델은 이미지 캡션 작업의 성능을 끌어올리고 더욱 주목할 만합니다.

2 관련 연구

자동 이미지 캡션은 오랜 역사를 가지고 있다 (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Kiros et al., 2015). Deep Neural Networks (Bengio, 2009)의 성공과 Flickr30K (Young et al., 2014) 및 MS-COCO (Lin et al., 2014)와 같은 데이터셋에서 제공되는 주석이 가능한 데이터의 가용성으로 인해 가속화되었다.
DNN의 훈련 요구에 비해 COCO 데이터셋은 크지 않다 (약 106개의 이미지 순서). 그럼에도 불구하고, 이는 매우 인기가 있었으며, 이는 비아이코닉 뷰 또는 비표준적인 객체의 관점을 가진 이미지에 대한 주석을 제공하므로 일상적인 장면의 구성을 반영한다 (Flickr30K에 대해서도 동일하다 (Young et al., 2014)). COCO 주석 - 카테고리 라벨링, 인스턴스 스팟팅 및 인스턴스 세그멘테이션 -은 이미지의 모든 객체에 대해 수행된다.

†https://en.wikipedia.org/wiki/Alt_속성

배경에서, 혼잡한 환경에서, 또는 부분적으로 가려진 상태에서. 그 이미지는 또한 주석이 달려 있으며, 즉, 사람 주석자가 이미지의 시각적 내용을 객체와 그들의 행동 또는 관계 측면에서 반영하기 위해 작성한 문장들입니다. 이미지 캡션 생성을 위해 다양한 DNN 모델들이 COCO 캡션을 사용하여 훈련되고 평가되었습니다 (Vinyals et al., 2015a; Fang et al., 2015; Xu et al., 2015; Ranzato et al., 2015; Yang et al., 2016; Liu et al., 2017; Ding and Soricut, 2017). 이러한 모델들은 sequence-to-sequence 모델들 (Sutskever et al., 2014; Bahdanau et al., 2015)에서 영감을 받았지만, RNN 대신에 CNN 기반 인코딩을 사용합니다 (Hochreiter and Schmidhuber, 1997; Chung et al., 2014). 최근에는 Transformer 아키텍처 (Vaswani et al., 2017)가 시퀀스 모델링을 위해 RNN (및 CNN)에 대한 대안으로 입증되었습니다. 이 연구에서는 CNN, RNN 및 Transformer 레이어를 결합한 모델을 사용하여 Conceptual Captions 데이터셋이 이미지 캡션 작업에 미치는 영향을 평가합니다.
이 작업과 관련된 것은 Pinterest 이미지 및 문장 설명 데이터셋 (Mao et al., 2016)입니다. 이는 대규모 데이터셋 (약 10^8 개의 예제)이지만, 텍스트 설명은 연관된 이미지의 시각적 내용을 엄격하게 반영하지 않으므로 이미지 캡션 모델을 직접적으로 훈련시키는 데 사용할 수 없습니다.

3 개념적 캡션 데이터셋 생성

개념적 캡션 데이터셋은 Flume (Chambers et al., 2010) 파이프라인을 사용하여 프로그래밍 방식으로 생성됩니다. 이 파이프라인은 수십억 개의 인터넷 웹페이지를 병렬로 처리합니다. 이 웹페이지에서 이미지와 캡션의 후보를 추출, 필터링 및 처리합니다. 필터링 및 처리 단계에 대한 자세한 설명은 다음 섹션에서 제공됩니다.

이미지 기반 필터링은 첫 번째 필터링 단계로, 인코딩 형식, 크기, 가로 세로 비율 및 노출적인 내용을 기반으로 이미지를 제거합니다. 이 필터링은 양쪽 차원이 400 픽셀보다 큰 JPEG 이미지만 유지하며, 큰 차원 대 작은 차원의 비율이 2를 초과하지 않는 이미지만 유지합니다. 또한 음란물이나 저속한 내용을 감지하는 이미지는 제외됩니다. 이 필터링은 후보 이미지 중 65% 이상을 제거합니다.

텍스트 기반 필터링 두 번째 필터링 단계,
텍스트 기반 필터링은 HTML 웹페이지에서 Alt-텍스트를 수집합니다.
Alt-텍스트는 일반적으로 이미지와 함께 제공됩니다.
[Alt-텍스트는 처리되지 않음:
원하지 않는 이미지 형식,
가로세로 비율 또는 크기]
ALT-텍스트

"페라리 말이야"

인생의 의미

데미 로바토는 2017년 아메리칸 뮤직 어워즈에서 검은색 에스터 압너 2018 봄 드레스와 스튜어트 와이츠먼 샌들을 신고 있었다.

이미지

[대체 텍스트 무시됨]
자막

"팝 록 아티스트
검은색을 입은
가운과 샌들을 신은
시상식에서"

[대체 텍스트 삭제됨: 텍스트 없음 대 이미지-객체 중첩]
이미지
필터링
텍스트
필터링
이미지/텍스트
필터링
텍스트
변환
파이프라인

이미지 이미지 이미지

그림 2: 개념적 캡션 파이프라인 단계와 예시 및 최종 출력.

그리고 이미지의 성격이나 내용을 설명하려는 의도를 가지고 있습니다. 이러한 대체 텍스트 값들은 어떤 방식으로든 제한되거나 강제되지 않기 때문에, 그 중 많은 것들은 버려져야 합니다. 예를 들어, 검색 엔진 최적화(SEO) 용어나 트위터 해시태그 용어 등이 있습니다. 우리는 Google Cloud Natural Language API를 사용하여 후보 대체 텍스트를 분석합니다. 구체적으로, 품사(POS), 감정/극성, 음란물/저속한 어노테이션을 사용합니다. 이러한 어노테이션들에 추가로 다음과 같은 휴리스틱을 적용합니다.

잘 구성된 자막은 다양한 품사 태그를 포함한 높은 고유 단어 비율을 가져야 합니다. 한정사, 명사, 또는 전치사가 없는 후보는 제외됩니다. 또한 명사 비율이 높은 후보도 제외됩니다.

• 토큰 반복율이 높은 후보자는 제외됩니다.

• 대문자 사용은 잘 구성된 문장의 좋은 지표입니다. 첫 단어가 대문자로 시작되지 않거나 대문자 단어 비율이 너무 높은 후보자는 제외됩니다.

높은 확률로 토큰은 바람직하지 않은 텍스트의 좋은 지표입니다. 우리는 최소 5번 이상 나타나는 10억 개의 토큰 유형으로 이루어진 어휘 V W를 사용합니다.

영어 위키백과, 그리고 이 어휘에 없는 토큰을 포함하는 후보들을 제외하십시오.

• 극성 주석에서 너무 높거나 너무 낮은 점수를 받은 후보자들이나 음란물/저속한 언어 감지기를 작동시키는 후보자들은 제외됩니다.

• 미리 정의된 보일러플레이트 접두사/접미사 시퀀스는 텍스트와 일치하는 부분이 잘려나갑니다. 예: "사진을 확대하려면 클릭", "스톡 사진". 또한 "임베디드 이미지 퍼머링크", "프로필 사진"과 같은 패턴으로 시작하거나 끝나는 텍스트도 삭제합니다.

이러한 필터는 들어오는 지원자 중 약 3%만 후속 단계로 진행할 수 있도록 허용합니다.

이미지 및 텍스트 기반 필터링
이미지 및 텍스트 내용을 기반으로 한 별도의 필터링 외에도, 우리는 텍스트 토큰 중 어느 하나도 이미지의 내용과 매핑되지 않는 후보자를 걸러냅니다.
이를 위해, 우리는 Google Cloud Vision API를 통해 사용 가능한 분류기를 사용하여 이미지에 클래스 레이블을 할당합니다. 이는 많은 수의 레이블을 가진 이미지 분류기(약 10^5의 크기)를 사용합니다. 특히, 이러한 레이블은 V w 토큰 유형으로도 100% 커버됩니다.
일반적으로 이미지에는 5에서 20개의 레이블이 할당되지만, 정확한 수는 원래 대체 텍스트에 따라 달라집니다. 해리슨 포드와 칼리스타 플록하트가 '할리우드 홈사이드' 프리미어에 참석합니다.

2003년 9월 5일 프랑스 드보빌에서 개최된 제29회 미국 영화제.
Conceptual Captions 배우들이 영화제에서 개봉작에 참석했습니다.

해리슨 포드와 칼리스타 플록하트에 무슨 일이 있었나요? "배우"로 매핑된 이름, 위치, 날짜가 삭제되었습니다.

사진 설명: 착륙 장치를 내린 상태로 접근 중인 영국 항공의 에어버스 A319 비행기의 측면 모습

아래 - 스톡 이미지
착륙 장치가 내려간 상태로 착륙 접근 중인 비행기의 측면 모습을 상징적으로 표현한 캡션.

"British Airways Airbus A319 비행기"라는 문구가 "비행기"로 매핑되었습니다. 보일러플레이트는 제거되었습니다.

원래 대체 텍스트: 버려진 노리치 유니온 외부에는 예술가 던컨 맥켈러의 두 개의 조각품이 나무에 장식되어 있다.

영국 브리스톨에 위치한 사무실 - 스톡 이미지
버려진 사무실 외부에 사람이 만든 개념적 캡션 조각상이 나무에 장식되어 있다.

무슨 일이 일어났나요? 물건이 몇 개 떨어졌어요. (예: "두 개") 특정 명사구가 "사람"으로 상위어화되었어요.

명사 수식어 삭제; 위치 삭제; 표준 문구 제거.

표 1: 원래 대체 텍스트 버전에서 파생된 개념적 캡션의 예시들.

이미지. 우리는 이러한 라벨을 후보 텍스트와 비교하여 형태학 기반 어간 추출을 고려합니다. 이미지, 캡션 쌍 중에 겹치는 부분이 없는 후보들은 제거됩니다. 이 필터는 들어오는 후보들 중 약 60%를 제거합니다.

텍스트 변환 및 상위어화
현재 데이터셋 버전에서는 약 10억 개의 영어 웹페이지에서 50억 개 이상의 이미지를 고려했습니다. 위의 필터링 기준은 고정밀도를 가지도록 설계되었으며 (잠재적으로 재현율이 낮을 수 있음). 원래의 입력 후보 중에서는 0.2% (cid:104)이미지, 캡션(cid:105) 쌍만이 위에서 설명한 필터링 기준을 통과합니다.
남은 후보 캡션들은 적절한 대체 텍스트 이미지 설명(그림 1의 대체 텍스트 참조)으로 보이지만, 이러한 후보 캡션의 대부분은 사람, 장소, 위치 등의 고유명사를 포함하고 있어 이미지 캡션 작업의 일부로 학습하기가 굉장히 어렵습니다.
이러한 경우에 어떤 일이 발생할 수 있는지 알려주기 위해, 비-상위어화되지 않은 대체 텍스트 데이터로 RNN 기반의 캡션 모델(섹션 4 참조)을 훈련시키고, 그 결과 예시를 그림 3에 제시합니다. 사람 신원, 위치 등의 자동 결정이 필요한 경우, 별도의 작업으로 시도해야 하며 이미지에 대한 메타 정보(예: 위치)를 활용해야 합니다.
Google Cloud Natural Language API를 사용하여 명명된 개체 및 구문 종속성 주석을 얻습니다. 그런 다음 Google Knowledge Graph (KG) Search API를 사용하여 명명된 개체를 KG 항목과 일치시키고 관련된 상위어 용어를 활용합니다. 예를 들어, "Harrison Ford"와 "Calista Flockhart"는 모두 명명된 개체로 식별됩니다.

지미 바른스는 시드니 엔터테인먼트 센터에서 공연을 합니다.

모델 출력: 가수 저스틴 비버는 MGM에서 열리는 빌보드 뮤직 어워즈에서 무대에서 공연을 합니다.

그림 3: 깨끗하고 상위어가 없는 대체 텍스트 데이터로 훈련된 모델 출력의 예시.

그래서 우리는 그들을 해당하는 KG 항목과 일치시킵니다. 이러한 KG 항목들은 "배우"를 상위어로 가지고 있으므로 원래의 표면 토큰을 해당 상위어로 대체합니다. 텍스트 변환을 달성하기 위해 다음 단계가 적용됩니다.

• 특정 유형의 명사 수식어 (고유 명사, 숫자, 단위)는 제거됩니다.

• 날짜, 기간 및 전치사 기반 위치 (예: "로스앤젤레스에서")는 제거됩니다.

• 명명된 개체들은 식별되고, 지식 그래프 항목과 일치시킨 후, 상위어로 대체됩니다.

• 동일한 헤드를 가진 결과적인 협조 명사구 (예: "배우와 배우")는 단일 헤드, 복수형 형태로 해결됩니다 (예: "배우들").

약 20%의 샘플은 이 변환 과정에서 제외됩니다. 왜냐하면 문장이 너무 짧거나 일관성이 없을 수 있기 때문입니다.
마지막으로, 우리는 낮은 빈도의 개념을 식별하기 위해 또 다른 텍스트 분석과 개체 해결을 수행합니다. 우리는 모든 해결된 개체들을 클러스터링하고, 모든 감지된 유형의 개수가 100 이상인 후보들만 유지합니다 (예: "배우", "개", "동네" 등). 이러한 남은 (cid:104)이미지, 캡션(cid:105) 쌍은 약 16,000개의 개체 유형을 포함하며, 예제의 수로 잘 대표됩니다.
표 1은 변환 전/후 쌍의 몇 가지 예시를 포함하고 있습니다.

개념적 캡션 품질을 평가하기 위해, 우리는 개념적 캡션 데이터셋의 테스트 분할에서 추출한 4K개의 무작위 예제 샘플을 고려합니다. 우리는 이 샘플에 대해 동일한 방법론을 사용하여 인간 평가를 수행합니다. (5.4절에서 설명한 방법론과 동일한 방법을 사용하여 인간 평가를 수행합니다.)

좋음 (3점 중)
1+  2+    3
개념적 캡션 96.9% 90.3% 78.5%

표 2: 개념적 캡션 샘플에 대한 인간 평가 결과.

결과는 표 2에 제시되며, 3개의 주석 중 90% 이상의 캡션은 대다수 (2개 이상)의 좋은 판단을 받았음을 보여줍니다. 이는 개념적 캡션 파이프라인이 알고리즘적 처리를 포함하고 있음에도 불구하고 고품질의 이미지 캡션을 생성한다는 것을 나타냅니다.

예시 고유 토큰/자막
토큰 평균 표준편차 중앙값
훈련 3,318,333 51,201 10.3 4.5 9.0
검증 28,355 13,063 10.3 4.6 9.0
테스트 22,530 11,731 10.1 4.5 9.0

테이블 3: 개념적 캡션에 대한 훈련/검증/테스트 분할에 대한 통계.

표 3에서는 개념적 캡션 데이터셋의 Train/Validation/Test 분할에 대한 통계를 제시합니다. 훈련 세트에는 약 3.3백만 개의 예제가 있으며, 검증 세트에는 약 2.8천 개의 예제가 있고, 테스트 세트에는 약 2.2천 개의 예제가 있습니다. 훈련 세트 어휘 사전(고유 토큰)의 크기는 51,201입니다. 테스트 세트는 인간 판단(2+ GOOD)을 사용하여 정리되었으며, 훈련 및 검증 분할은 자동 파이프라인에서 생성된 모든 데이터를 포함하고 있음에 유의하십시오. 데이터 분할별 캡션당 토큰의 평균/표준편차/중앙값 통계는 각각 약 10.3/4.5/9.0으로 일관되어 있습니다.

4 이미지 캡션 모델

개념적 캡션 데이터셋의 영향을 평가하기 위해, 우리는 이전에 문헌에서 제안된 여러 이미지 캡션 모델을 고려합니다. 이 모델들은 그림 4의 설명을 통해 이해할 수 있으며, 주로 이러한 구성 요소들을 구체화하는 방식에서 차이가 있습니다.

인코더

<GO> 프리스비를 던지는 사람들
디코더
프리스비를 던지는 사람들이 있습니다.

이미지 임베딩

H
YZ

그림 4: 주요 모델 구성 요소.

이 아키텍처에는 세 가지 주요 구성 요소가 있습니다.

• (전처리된) 이미지를 입력으로 받아 벡터 형태의 이미지 임베딩 X = (x 1, x 2, ..., x L)을 출력하는 깊은 CNN.

이미지 임베딩을 가져와 텐서 H로 인코딩하는 인코더 모듈 = f enc(X).

• 각 단계 t에서 H와 디코더 입력 Y 1:t에 조건을 걸고 출력 z t = f dec(Y 1:t,H)를 생성하는 디코더 모델.

우리는 이 아키텍처의 두 가지 주요한 인스턴스를 탐구합니다. 하나는 LSTM 셀을 사용하여 RNN을 사용하여 f enc 및 f dec 함수를 구현하는 것이며, 이는 Show-And-Tell (Vinyals et al., 2015b) 모델에 해당합니다. 다른 하나는 Transformer self-attention 네트워크를 사용하여 f enc 및 f dec를 구현합니다. 이 논문의 모든 모델은 Inception-ResNet-v2를 CNN 구성 요소로 사용합니다 (Szegedy et al., 2016).

4.1 RNN 기반 모델

우리의 RNN 기반 모델의 구현은 Show-And-Tell (Vinyals et al., 2015b) 모델과 유사합니다.

h l (cid:44) RNNenc(x l,h l−1), 그리고 H = h L,
z t (cid:44) RNNdec(y t,z t−1), 여기서 z 0 = H.
원래의 Show-And-Tell 모델에서는 전체 이미지의 단일 이미지 임베딩이 RNN의 첫 번째 셀에 공급되며 이는 텍스트 생성에도 사용됩니다. 우리 모델에서는 단일 이미지 임베딩이 하나의 셀만 있는 RNNenc에 공급되고, 그런 다음 다른 RNNdec가 텍스트 생성에 사용됩니다. 우리는 단일 이미지 (1x1) 임베딩과 이미지의 8x8 파티션을 모두 시도해 보았습니다. 각 파티션은 고유한 임베딩을 가지고 있습니다. 8x8 경우, 이미지 임베딩은 RNNenc에 시퀀스로 공급됩니다. 두 경우 모두, 우리는 Show-And-Tell 모델과 동일하게 교차 어텐션 없이 일반적인 RNN을 적용합니다. 교차 어텐션을 사용한 RNN은 Show-Attend-Tell 모델에서 사용되었지만, 우리는 그 성능이 Show-And-Tell 모델보다 우수하지 않다고 판단합니다.

4.2 트랜스포머 모델

트랜스포머 기반 모델에서, 인코더와 디코더는 모두 N개의 레이어로 구성되어 있습니다. 우리는 인코더의 n번째 레이어를 Xn = {xn,1,...,xn,L}로 표기하며, X0 = X, H = XN으로 나타냅니다. 각 레이어는 멀티헤드 셀프 어텐션 레이어 ATTN과 위치별 피드포워드 네트워크 FFN으로 구성되어 있습니다.

x(n,j) = 주의(xn,j,Xn; 우리 q, 우리 k, 우리 v)
, 소프트맥스(행렬곱(xn,j 우리 q, Xn우리 k), Xn우리)

v

x (n+1),j = FFN(xₙⱼ; We f)

우리가 q, k, v에 대한 인코더 가중치 행렬인 We q, We k, We v를 자기 주의 하위층의 쿼리, 키, 값 변환에 사용하고, We f는 피드포워드 하위층의 인코더 가중치 행렬을 나타냅니다. RNN 기반 모델과 유사하게, 우리는 단일 이미지 임베딩 (1x1)과 8x8 이미지 임베딩 벡터를 사용하는 것을 고려합니다.
디코더에서는 n번째 레이어를 Z n = {z n,1,...,z n,T}로 표기하고, Z 0 = Y로 표기합니다. 디코더와 인코더 레이어 사이에는 두 가지 주요한 차이점이 있습니다. 첫째, 디코더의 자기 주의 하위층은 오른쪽으로 마스킹되어 있어 "미래" 위치에 주의를 기울이지 않도록 합니다 (즉, z n,j는 z n,(j+1),...,z n,T에 주의하지 않습니다). 둘째, 자기 주의 층과 피드포워드 층 사이에는 디코더가 제3의 교차 주의 층을 추가하여 z n,j를 최상위 인코더 표현 H = X N에 연결합니다.

z (n, j) = ATTN(zn, j, Zn, 1:j; Wd q, Wd k, Wd v)
z (n, j) = ATTN(z (n, j), H; Wc q, Wc k, Wc v)
z (n+1), j = FFN(z (n, j); Wd f)

어디에 있어요, 어디로 가요, 그리고 어디에요

쿼리, 키 및 값 변환을 위한 가중치 행렬은 디코더의 셀프 어텐션 서브레이어에 있습니다. Wc q, Wc k, Wc

v
are
the corresponding decoder weight matrices in the
cross-attention sub-layer; and Wd

v
는
교차 어텐션 서브레이어에서 해당하는 디코더 가중치 행렬입니다. 그리고 Wd

f는 피드포워드 서브레이어의 디코더 가중치 행렬입니다.

Transformer 기반 모델은 임베딩 레이어에서 위치 정보를 활용합니다. 8x8 경우, 64개의 임베딩 벡터는 [0,...,63]의 위치를 가진 1차원 시퀀스로 직렬화됩니다. 위치 정보는 각 위치마다 사인과 코사인 함수를 적용하여 모델링되며, 각 임베딩 차원마다 다른 주파수로 적용됩니다 (Vaswani et al., 2017). 그리고 임베딩 표현에 추가됩니다.

5 실험 결과

이 섹션에서는 이미지 캡션 모델을 훈련시키기 위해 Conceptual Captions 데이터셋(이하 'Conceptual'로 지칭)을 사용하는 것의 영향을 평가합니다. 이를 위해, 섹션 4에서 설명한 모델을 두 가지 실험 조건에서 훈련합니다: COCO 데이터셋(Lin et al., 2014)에서 제공된 훈련 및 개발 세트를 사용하는 경우와 Conceptual 데이터셋을 사용하는 경우. 우리는 세 가지 다른 테스트 세트를 사용하여 결과 모델을 정량적으로 평가합니다: blind COCO-C40 테스트 세트(COCO로 훈련된 모델에 대해 도메인 내, Conceptual로 훈련된 모델에 대해 도메인 외); Conceptual 테스트 세트(COCO로 훈련된 모델에 대해 도메인 외, Conceptual로 훈련된 모델에 대해 도메인 내); 그리고 Flickr (Young et al., 2014) 1K 테스트 세트(모두에게 도메인 외).

5.1 데이터셋 세부사항

COCO 이미지 캡션 COCO 이미지 캡션 데이터셋은 일반적으로 82,000개의 이미지로 훈련되며, 40,000개의 이미지로 검증됩니다. 각 이미지는 적어도 5개의 실제 캡션과 함께 제공됩니다. 표준적인 방법을 따라, 우리는 훈련 세트와 대부분의 검증 데이터셋을 결합하여 모델을 훈련시키고, 검증을 위해 4,000개의 이미지만 보류합니다.

개념적 캡션 개념적 캡션 데이터셋은 훈련용으로 약 3.3백만 장의 이미지, 검증용으로 2만 8천 장의 이미지, 테스트 세트로는 2만 2천 5백 장의 이미지를 포함하고 있습니다. 더 자세한 통계는 테이블 3을 참조하세요. COCO로 훈련된

RNN8x8 건물 앞에 서 있는 남자들의 그룹입니다.

몇몇 사람들이 보도를 걷고 있습니다.

테이블에 앉아 있는 아이
케이크가 올려진 상태로

인형의 가까운 촬영
테이블 위에 있는 것

T2T8x8 대학교에서 군복과 넥타이를 한 남자들이 이야기하고 있습니다.

시계와 두 개의 문이 있는 좁은 복도

파티에서 여자가 생일 케이크를 자르는 중이다.

차의 한쪽에 물고기 사진이 있다.

개념적으로 훈련된

RNN8x8 졸업생들이 졸업식을 위해 줄을 서 있다.
선배들의 행렬을 바라보는 풍경
생일 파티에서 아이의 그림
어떤 것에 대해 생각하는 만화 비즈니스맨
T2T8x8 졸업생들이 자신들의 졸업장을 받기 위해 줄을 서 있다.

대성당의 회랑

예술에 대해 배우기
공예에 대해 배우기

만화 비즈니스맨
도움을 청하는 중

그림 5: 두 가지 훈련 조건에서 모델 출력물을 나란히 비교한 결과. 개념 기반 모델(하단)은 환각 현상이 적고 표현력이 더 뛰어나며 다양한 종류의 이미지를 잘 다룹니다. 가운데의 두 이미지는 플리커에서 가져온 것이고, 나머지 두 이미지는 개념적 캡션에서 가져온 것입니다.

5.2 실험 설정

이미지 전처리 각 입력 이미지는 먼저 무작위 왜곡과 잘라내기를 통해 전처리됩니다 (50%∼100%의 무작위 비율을 사용). 이는 모델이 훈련 이미지의 개별 픽셀에 과적합되는 것을 방지합니다.

인코더-디코더 RNN 기반 모델에 대해, RNN 셀로 1층, 512차원의 LSTM을 사용합니다. Transformer 기반 모델에 대해서는 (Vaswani et al., 2017)에서 제공하는 기본 설정을 사용하며, 인코더와 디코더 레이어는 각각 6개로 구성되고, 은닉층 크기는 512이며, 어텐션 헤드는 8개입니다.

텍스트 처리 훈련 자막은 최대 15개의 토큰으로 줄여집니다. 우리는 토큰 유형의 최소 등장 횟수를 4로 설정하여 COCO 데이터셋에 약 9,000개의 토큰 유형과 Conceptual Captions 데이터셋에 약 25,000개의 토큰 유형을 얻습니다. 다른 모든 토큰은 특수 토큰 (cid:104)UNK(cid:105)로 대체됩니다. 단어 임베딩 행렬은 크기가 512이며 출력 투영 행렬과 연결되어 있습니다.

최적화 모든 모델은 MLE 손실을 사용하여 훈련되며, 학습률 0.01로 Adagrad(Duchi et al., 2011)를 사용하여 최적화됩니다. 미니 배치 크기는 25입니다. 모든 모델 파라미터는 총 5M 단계 동안 훈련되며, 배치 업데이트는 40개의 워커에 비동기적으로 분산됩니다. 최종 모델은 주어진 훈련 조건에서 개발 세트의 최고 CIDEr 점수를 기준으로 선택됩니다.

추론 중에는 이전 위치의 디코더 예측이 다음 위치의 입력으로 사용됩니다. 우리는 빔 탐색을 사용합니다.

가장 가능성 있는 출력 시퀀스를 계산하기 위해 크기 4를 사용합니다.

5.3 질적 결과

우리의 실험 결과를 제시하기 전에, 우리가 관찰한 패턴에 대해 간단히 논의합니다.
COCO로 훈련된 모델과 Conceptual로 훈련된 모델 사이의 차이점 중 하나는 이미지 내 개체에 대한 적절한 자연어 용어를 사용하는 능력입니다. 그림 5의 가장 왼쪽 이미지의 경우, COCO로 훈련된 모델은 이미지의 사람들을 "남자들의 그룹"이라고 표현합니다. 반면에 Conceptual로 훈련된 모델은 더 적절하고 정보를 제공하는 용어인 "졸업생"을 사용합니다. Flickr 테스트 세트의 두 번째 이미지는 이를 더욱 분명하게 보여줍니다. Conceptual로 훈련된 T2T8x8 모델은 "대성당의 회랑"이라는 이미지 내용을 완벽하게 묘사합니다. 다른 모델들은 이렇게 정확한 설명을 만들어내지 못합니다.
두 번째 차이점은 COCO로 훈련된 모델들이 종종 객체를 환각하는 것처럼 보인다는 것입니다. 예를 들어, 첫 번째 이미지에 대해 "건물의 앞"을 환각하고, 두 번째 이미지에 대해 "시계와 두 개의 문"을 환각하며, 세 번째 이미지에 대해 "생일 케이크"를 환각합니다. 반면에 Conceptual로 훈련된 모델들은 이러한 문제를 가지지 않는 것으로 보입니다. 우리는 COCO 기반 모델의 환각 문제가 COCO 데이터에서 존재하는 높은 상관관계에서 비롯된다고 가설을 세웁니다 (예: 테이블에 아이가 있다면 케이크도 있다). 이러한 데이터의 높은 상관관계는 캡션 모델이 올바른 수준의 세분화에서 표현을 올바르게 분리하고 학습하는 것을 허용하지 않습니다.
모델 훈련 1+ 2+ 3+
RNN8x8 COCO 0.390 0.276 0.173
T2T8x8 COCO 0.478 0.362 0.275
RNN8x8 Conceptual 0.571 0.418 0.277
T2T8x8 Conceptual 0.659 0.506 0.355

표 4: 플리커 1K 테스트에서의 인간 평가 결과.

세 번째 차이점은 다양한 이미지 유형에 대한 탄력성입니다. COCO는 자연 이미지만을 포함하고 있으므로, 네 번째 이미지와 같은 만화 이미지는 COCO로 훈련된 모델에 대해 대량의 환각 효과를 초래합니다 ("봉제 동물", "물고기", "차량의 한쪽"). 반면, 개념적으로 훈련된 모델은 이러한 이미지를 쉽게 처리합니다.

5.4 양적 결과

이 섹션에서는 여러 이미지 캡션 모델이 생성한 출력물의 품질에 대한 양적 결과를 제시합니다. 자동 평가 결과와 인간 평가 결과를 모두 제시합니다.

5.4.1 인간 평가 결과

인간 평가를 위해, 우리는 전문 평가자들의 풀(수십 명의 평가자)을 사용하며, 이중맹검 평가 조건을 사용합니다. 평가자들은 주어진 이미지, 캡션 입력에 대해 일반적인 상식 판단만을 사용하여 GOOD 또는 BAD 라벨을 할당하도록 요청됩니다. 이는 일반 사용자의 반응을 근사화한 것입니다. 일반적으로 미리 정의된 GOOD vs. BAD 개념을 수용하지 않는 일반 사용자의 반응을 모방하기 위해 이렇게 요청합니다. 우리는 각 입력 쌍에 대해 3명의 별도의 평가자들에게 평가를 요청하고, k 이상 (k+)의 GOOD 주석을 받은 쌍의 비율을 보고합니다. 표 4에서는 Flickr 1K 테스트 세트의 결과를 보고합니다. 이 평가는 훈련 조건에 대해 도메인 밖이므로, 모든 모델은 상대적으로 동등한 조건에서 평가됩니다. 결과는 개념 기반 모델이 우수함을 나타냅니다. 50.6%의 경우 (T2T8x8 모델의 경우), 대다수의 평가자 (2+)가 GOOD 라벨을 지정했습니다. 결과는 또한 Transformer 기반 모델이 COCO 및 개념적 훈련 조건에서 모두 8 포인트 이상의 좋은 차이로 RNN 기반 모델보다 우수함을 나타냅니다.

모델 훈련 CIDEr ROUGE-L METEOR
RNN1x1 COCO  1.021 0.694  0.348
RNN8x8 COCO  1.044 0.698  0.354
T2T1x1 COCO  1.032 0.700  0.358
T2T8x8 COCO  1.032 0.700  0.356
RNN1x1 개념적 0.403 0.445 0.191
RNN8x8 개념적 0.410 0.437 0.189
T2T1x1 개념적 0.348 0.403 0.171
T2T8x8 개념적 0.345 0.400 0.170

표 5: COCO C40 테스트에서의 자동 측정치.

모델 훈련 CIDEr ROUGE-L SPICE
RNN1x1 COCO  0.183 0.149 0.062
RNN8x8 COCO  0.191 0.152 0.065
T2T1x1 COCO  0.184 0.148 0.062
T2T8x8 COCO  0.190 0.151 0.064
RNN1x1 개념적 1.351 0.326 0.235
RNN8x8 개념적 1.401 0.330 0.240
T2T1x1 개념적 1.588 0.331 0.254
T2T8x8 개념적 1.676 0.336 0.257

테이블 6: 22.5K 개념적 캡션 테스트 세트의 자동 측정치.

모델 훈련 CIDEr ROUGE-L SPICE
RNN1x1 COCO 0.340 0.414 0.101
RNN8x8 COCO 0.356 0.413 0.103
T2T1x1 COCO 0.341 0.404 0.101
T2T8x8 COCO 0.359 0.416 0.103
RNN1x1 개념적 0.269 0.310 0.076
RNN8x8 개념적 0.275 0.309 0.076
T2T1x1 개념적 0.226 0.280 0.068 T2T8x8 개념적 0.227 0.277 0.066
표 7: Flickr 1K 테스트의 자동 측정

5.4.2 자동 평가 결과

이 섹션에서는 이미지 캡션 평가 메트릭을 사용하여 자동 평가 결과를 보고합니다.
COCO C40 테스트 세트 (그림 5)에 대해 COCO 온라인 평가 서버‡를 사용하여 CIDEr (Vedantam et al., 2015), ROUGE-L (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005) 메트릭의 숫자 값으로 결과를 보고합니다. Conceptual Captions (그림 6) 및 Flickr (그림 7) 테스트 세트에 대해 CIDEr, ROUGE-L 및 SPICE (Anderson et al., 2016)§의 숫자 값으로 결과를 보고합니다.
모든 메트릭에 대해 높은 숫자는 후보와 실제 캡션 간의 거리가 가까움을 의미합니다.
자동 메트릭은 도메인 내 및 도메인 외 상황을 감지하는 데 좋습니다. COCO 모델을 COCO에서 테스트한 경우, 그림 5의 결과에서 RNN 및 Transformer 기반 모델 모두 CIDEr 점수가 1.02-1.04 범위에 있으며, COCO 기준에 대해 Conceptual 기반 모델을 테스트한 경우 CIDEr 점수가 0.35-0.41 범위로 감소합니다. Conceptual 모델을 Conceptual Captions 테스트 세트에서 테스트한 경우, 그림 6의 결과에서 T2T8x8 모델의 CIDEr가 1.468로 매우 높으며, Transformer 기반 모델이 RNN 기반 모델보다 우수함을 인간 평가 결과와 일치시킵니다. Conceptual Captions 기준에 대해 COCO 기반 모델을 테스트한 경우, 모든 점수가 0.2 CIDEr 이하입니다.
자동 메트릭은 상호 일치하지 않습니다.

‡http://mscoco.org/dataset/#captions-eval.
§https://github.com/tylin/coco-caption.
인간 평가 결과. 자동 측정에 따르면, COCO 훈련된 모델은 개념 훈련된 모델보다 우수하다 (CIDEr 점수는 COCO 훈련 조건에서 중간 0.3대에 위치하며, 개념 훈련 조건에서 중간 0.2대에 위치한다). 또한, RNN 기반 모델은 Transformer 기반 모델보다 우수하다. 특히, 이러한 메트릭은 인간보다 낮은 점수를 매기는 메소드들 (Vinyals et al., 2015a; Fang et al., 2015)이 COCO 2015 챌린지에서 우승한 메소드들과 동일하다. 그럼에도 불구하고, 인간은 이 작업에서 여전히 훨씬 더 뛰어나다. 이러한 메트릭이 인간 평가 결과와 일치하지 않는 것은 이 분야의 진전을 이끌 수 있는 능력에 대한 심각한 의문을 제기한다.
이러한 메트릭의 중요한 약점은 환각 효과가 과소 처벌된다는 것이다 (참조에 해당하지 않는 토큰에 대한 작은 정밀도 벌칙), 이는 환각의 존재에서 인간 판단이 급격히 감소하는 경향을 보인다.

6 결론

우리는 새로운 이미지 캡션 데이터셋인 Conceptual Captions을 제공합니다. 이 데이터셋은 몇 가지 주요 특징을 가지고 있습니다: COCO 이미지 캡션 데이터셋보다 1개의 자릿수 크기인 약 3.3백만 개의 예시가 있습니다. 이 데이터셋은 자연 이미지, 제품 이미지, 전문 사진, 만화, 그림 등 다양한 종류의 이미지로 구성되어 있습니다. 그리고 이 데이터셋의 캡션은 원래 Alt-text 속성에서 가져온 설명을 기반으로 하며, 청결함, 정보성 및 학습 가능성 사이의 균형을 이루기 위해 자동으로 변형되었습니다.
우리는 Conceptual Captions 데이터를 사용하여 생성된 이미지/캡션 쌍의 품질뿐만 아니라 여러 이미지 캡션 모델이 Conceptual Captions 데이터를 학습한 경우의 성능도 평가합니다. 결과는 이러한 모델이 더 나은 성능을 달성하고, 객체 환각과 같은 COCO로 학습된 모델에서 발생하는 몇 가지 문제를 피할 수 있음을 나타냅니다. 우리는 Conceptual Captions 데이터셋의 제공으로 자동 이미지 캡션 작업에 대한 상당한 진전이 이루어질 것을 기대합니다.

참고문헌

피터 앤더슨, 바수라 페르난도, 마크 존슨, 그리고 스티븐 골드. 2016년. SPICE: 시맨틱 제안 이미지 캡션 평가. ECCV에서.

D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of ICLR.

D. Bahdanau, K. Cho, 그리고 Y. Bengio. 2015. 정렬과 번역을 동시에 학습하는 신경 기계 번역. ICLR 논문집에서 발표.

사탄지브 바네르지와 알론 라비. 2005. METEOR: 기계 번역 평가를 위한 자동 측정 항목으로서 인간 판단과의 상관관계 향상. ACL 기계 번역 및 요약을 위한 내재적 및 외재적 평가 척도 워크샵 논문집.

요슈아 벤지오. 2009. 인공지능을 위한 심층 아키텍처 학습. Found. Trends Mach. Learn. 2(1):1–127.

라파엘라 베르나르디, 루케트 차키치, 데즈몬드 엘리엇,
아이쿠트 에르뎀, 에르쿠트 에르뎀, 나즐리 이키즐러-친비스,
프랭크 켈러, 에이드리언 뮤스캣, 바바라 플랭크.
2016년. 이미지로부터 자동 설명 생성: 모델, 데이터셋 및 평가 척도에 대한 조사. JAIR 55.

크레이그 체임버스, 아시쉬 라니와라, 프란시스 페리,
스티븐 아담스, 로버트 헨리, 로버트 브래드쇼,
그리고 네이선. 2010년. Flumejava: 쉬운, 효율적인 데이터 병렬 파이프라인. ACM SIGPLAN 프로그래밍 언어 디자인 및 구현 (PLDI) 컨퍼런스에서 발표. 2 Penn Plaza, Suite 701 New York, NY 10121-0701, 363-375쪽. http://dl.acm.org/citation.cfm?id=1806638.

준영 청, 채글라르 굴체레, 경현 조, 그리고 요슈아 벤지오. 2014년. 순차 모델링에 대한 게이트 순환 신경망의 경험적 평가. arXiv 사전 인쇄 arXiv:1412.3555.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: 대규모 계층적 이미지 데이터베이스. CVPR에서.

난딩과 라두 소리쿠트. 2017년. 소프트맥스 정책 그래디언트를 이용한 콜드 스타트 강화학습. NIPS에서.

제프 도나휴, 리사 앤 헨드릭스, 세르지오 구아다라마, 마르커스 로어바흐, 스바시니 베누고팔란, 케이트 샨코, 그리고 트레버 다렐. 2014년. 시각 인식과 설명을 위한 장기 반복 합성곱 신경망. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 (CVPR) 논문집.

존 두치, 엘라드 하잔, 요람 싱어. 2011년.
온라인 학습과 확률적 최적화를 위한 적응적 서브그래디언트 방법. 기계 학습 연구지 12(7월): 2121-2159.

2015년. 캡션에서 시각적 개념으로, 그리고 다시 돌아오기까지. IEEE 컴퓨터 비전 및 패턴 인식 (CVPR) 컨퍼런스 논문집에서.

셉 호크라이터와 유르겐 슈미드후버. 1997년. 장단기 기억. 신경 계산 9(8):1735-1780.

마이카 호도시, 피터 영, 줄리아 호켄마이어.
2013. 이미지 설명을 순위 매기기로 프레임 설정:
데이터, 모델 및 평가 지표. JAIR.
조나단 황, 비벡 라토드, 첸 선, 멩롱 주, 아눕 코라티카라, 알리레자 파티, 이안 피셔,
즈비그네프 보이나, 양 송, 세르지오 과다라마,
그리고 케빈 머피. 2016. 현대 합성곱 객체 탐지기의 속도/정확도 트레이드오프.
CoRR abs/1611.10012.

안드레이 카르파티와 리 페이페이. 2015. 이미지 설명 생성을 위한 깊은 시각-의미적 정렬. IEEE 컴퓨터 비전 및 패턴 인식 (CVPR) 학회 논문집에서 발표.

라이언 키로스, 루슬란 살라후딘오프, 그리고 리처드 S 제멜. 2015. 시각-의미 임베딩을 다중 모달 신경 언어 모델과 통합하기. 계산언어학 협회 트랜잭션.

크리즈헤브스키, 아이구스케버, 그리고 힌턴. 2012. 딥 컨볼루션 신경망을 이용한 이미지넷 분류. NIPS에서.

Chin-Yew Lin과 Franz Josef Och. 2004. 최장 공통 부분 수열과 스킵-바이그램 통계를 사용한 기계 번역 품질의 자동 평가. ACL 논문집에서 발표된 내용입니다.

쯔옹이 린, 마이클 마이어, 세르지 J. 벨롱지,
루보미르 D. 부르데브, 로스 B. 기르시크, 제임스 헤이스,
피에트로 페로나, 데바 라마난, 피오트르 돌라르, 그리고
C. 로렌스 지트닉. 2014년. Microsoft COCO: 문맥에서의 공통 객체. CoRR abs/1405.0312.

시치 리우, 젠하이 주, 닝 예, 세르지오 과다라마, 케빈 머피. 2017. 정책 기울기 방법을 사용한 이미지 설명 메트릭스의 최적화. 컴퓨터 비전 국제 학회(ICCV)에서 발표.

준화 마오, 지아징 쉬, 유시 징, 앨런 유일.
2016. 대규모 웹 주석 이미지로 다중 모달 단어 임베딩을 훈련하고 평가하는 것. NIPS에서.

마르크 아우렐리오 란자토, 수밋 초프라, 마이클 아울리, 그리고 보이체흐 자렘바. 2015년. 순차적인 신경망을 이용한 시퀀스 레벨 훈련. CoRR abs/1511.06732.

이리야 수츠케버, 오리올 비냐르스, 그리고 쿼크 V 레. 2014년.
신경망을 이용한 시퀀스 투 시퀀스 학습.
신경정보처리시스템 발전에 대한 진보. 3104-3112쪽.

크리스찬 세게디, 세르게이 이오페, 그리고 빈센트 반하우크. 2016. 인셉션-v4, 인셉션-레스넷 그리고 잔여 연결이 학습에 미치는 영향. CoRR abs/1602.07261.

아시쉬 바스와니, 노암 샤지어, 니키 파마르, 야코브 우스코레이트, 리온 존스, 에이단 엔 고메즈, 루카시 카이저, 그리고 일리아 폴로수킨. 2017년. 주의는 당신이 필요한 모든 것이다. 신경 정보 처리 시스템에서의 진보.

라마크리슈나 베단탐, C. 로렌스 지트닉, 그리고 데비 파리크. 2015. Cider: 합의 기반 이미지 설명 평가. IEEE 컴퓨터 비전 및 패턴 인식 (CVPR) 컨퍼런스에서 발표.

오리올 비냐르스, 알렉산더 토셰브, 샘이 벤지오, 그리고 두미트루 에르한. 2015a. 보여주고 말하기: 신경망 이미지 캡션 생성기. IEEE 컴퓨터 비전 및 패턴 인식 학회 논문집. 페이지 3156-3164.

오리올 비냐르스, 알렉산더 토셰브, 샘이 벵지오, 그리고 두미트루 에르한. 2015b. 쇼 앤 텔: 신경망 이미지 캡션 생성기. IEEE 컴퓨터 비전 및 패턴 인식 (CVPR) 컨퍼런스 논문집.

켈빈 쉬우, 지미 바, 라이언 키로스, 아론 쿠르빌,
루슬란 살라후딘노프, 리처드 제멜, 그리고 요슈아
벵지오. 2015년. 보여주고 집중하며 말하기: 시각적 주의와 함께하는 신경망 이미지 캡션 생성. 제32회 국제 기계 학습 대회 (ICML) 논문집.

Z. 양, Y. 원, Y. 우, R. 살라후딘노프, 그리고 W. W. 코헨. 2016. 자막 생성을 위한 리뷰 네트워크. NIPS에서.

피터 영, 앨리스 라이, 마이카 호도시, 그리고 줄리아 호크엔마이어. 2014년. 이미지 설명에서 시각적 표현으로: 이벤트 설명에 대한 의미 추론을 위한 새로운 유사도 측정. TACL 2:67–78.

