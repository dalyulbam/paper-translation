2014년 자연어 처리에 대한 경험적 방법에 관한 컨퍼런스 (EMNLP)의 절차, 1532-1543쪽, 2014년 10월 25일-29일, 카타르 도하. c (cid:13)2014 연산 언어학 협회
GloVe: 단어 표현을 위한 글로벌 벡터

제프리 페닝턴, 리처드 소처, 크리스토퍼 D. 매닝
스탠포드 대학교 컴퓨터 과학부, 스탠포드, CA 94305
jpennin@stanford.edu, richard@socher.org, manning@stanford.edu

요약

단어의 벡터 공간 표현을 학습하기 위한 최근 방법들은 벡터 연산을 사용하여 미세한 의미론적 및 문법적 규칙을 포착하는 데 성공했지만, 이러한 규칙의 기원은 여전히 불분명하다. 우리는 단어 벡터에서 이러한 규칙이 나타나기 위해 필요한 모델 속성을 분석하고 명시화한다. 결과적으로, 우리는 문헌에서 주요한 두 가지 모델 패밀리인 전역 행렬 인수분해와 지역 문맥 윈도우 방법의 장점을 결합한 새로운 전역 로그-이중 선형 회귀 모델을 제시한다. 우리의 모델은 대규모 말뭉치에서 희소 행렬 전체나 개별 문맥 윈도우에 대한 훈련이 아닌 단어-단어 공기 행렬의 비영 요소에만 효율적으로 통계 정보를 활용한다. 이 모델은 의미 있는 하위 구조를 갖는 벡터 공간을 생성하며, 최근의 단어 유추 작업에서 75%의 성능을 보여준다. 또한 유사도 작업과 개체명 인식에서 관련 모델보다 우수한 성능을 발휘한다.

1 소개

언어의 의미 벡터 공간 모델은 각 단어를 실수 값 벡터로 표현합니다. 이러한 벡터는 정보 검색 (Manning et al., 2008), 문서 분류 (Sebastiani, 2002), 질문 응답 (Tellex et al., 2003), 명명된 개체 인식 (Turian et al., 2010) 및 구문 분석 (Socher et al., 2013)과 같은 다양한 응용 프로그램에서 기능으로 사용될 수 있습니다.
대부분의 단어 벡터 방법은 단어 벡터 쌍 간의 거리나 각도를 주요 평가 방법으로 사용하여 이러한 단어 표현의 내재적인 품질을 평가합니다. 최근에 Mikolov et al. (2013c)은 단어 유추를 기반으로 한 새로운 평가 체계를 소개했습니다.

단어 벡터 공간의 미세한 구조를 살펴보면 단어 벡터 간의 스칼라 거리가 아닌 다양한 차원의 차이를 조사합니다. 예를 들어, "왕은 여왕과 같은 관계이고, 남자는 여자와 같은 관계이다"라는 유추는 벡터 공간에서 왕 - 여왕 = 남자 - 여자라는 벡터 방정식으로 인코딩되어야 합니다. 이 평가 체계는 의미의 차원을 생성하는 모델을 선호하며, 분산 표현의 다중 클러스터링 개념을 포착합니다 (Bengio, 2009).

단어 벡터 학습을 위한 두 가지 주요 모델 패밀리는 다음과 같습니다: 1) 잠재 의미 분석(LSA)과 같은 전역 행렬 인수분해 방법, 2) Mikolov 등의 스킵-그램 모델과 같은 지역 문맥 윈도우 방법. 현재, 두 가지 패밀리 모두 중요한 단점을 가지고 있습니다. LSA와 같은 방법은 통계 정보를 효율적으로 활용하지만, 단어 유추 작업에서는 비교적 성능이 좋지 않아서 최적의 벡터 공간 구조를 나타내지 못합니다. 스킵-그램과 같은 방법은 유추 작업에서 더 좋은 성능을 낼 수 있지만, 전역 공기 통계를 활용하는 대신 별도의 지역 문맥 윈도우에서 훈련하기 때문에 말뭉치의 통계 정보를 잘 활용하지 못합니다.

이 작업에서는 선형 의미 방향을 생성하기 위해 필요한 모델 속성을 분석하고, 전역 로그-이중 회귀 모델이 이를 수행하기에 적합하다고 주장합니다. 우리는 전역 단어-단어 공기 빈도를 기반으로 하는 특정 가중 최소 제곱 모델을 제안하며, 이는 통계를 효율적으로 사용합니다. 이 모델은 의미 있는 하위 구조를 가진 단어 벡터 공간을 생성하며, 이는 단어 유추 데이터셋에서 75%의 정확도로 최고 수준의 성능을 보여줍니다. 또한, 우리의 방법이 다른 현재 방법들보다 여러 단어 유사도 작업 및 공통된 명명된 개체 인식 (NER) 벤치마크에서 우수한 성능을 보여준다는 것을 증명합니다.

우리는 모델의 소스 코드와 훈련된 단어 벡터를 http://nlp.stanford.edu/projects/glove/에서 제공합니다.

1532
2 관련 연구

행렬 분해 방법. 저차원 단어 표현을 생성하기 위한 행렬 분해 방법은 LSA까지 거슬러 올라가는 뿌리를 가지고 있다. 이러한 방법은 말뭉치에 대한 통계적 정보를 포착하는 큰 행렬을 저차원 근사치로 분해하는 것이다. 이러한 행렬이 포착하는 특정 유형의 정보는 응용에 따라 다양하다. LSA에서는 행렬이 "용어-문서" 유형이며, 행은 단어나 용어에 해당하고 열은 말뭉치의 다른 문서에 해당한다. 반면에 언어의 초월 공간 유사성(HAL) (Lund and Burgess, 1996)은 "용어-용어" 유형의 행렬을 활용한다. 즉, 행과 열은 단어에 해당하고 항목은 다른 단어의 문맥에서 주어진 단어가 발생하는 횟수에 해당한다.
HAL과 관련된 방법의 주요 문제점은 가장 빈번한 단어가 유사도 측정에 지나치게 많은 기여를 한다는 것이다. 예를 들어, "the"나 "and"와 같은 단어의 공존 횟수는 그들의 의미적 관련성에 비해 상대적으로 적은 정보를 전달하지만 그들의 유사성에 큰 영향을 미친다. HAL의 이러한 단점을 해결하기 위해 COALS 방법(Rohde et al., 2006)과 같은 여러 기술이 존재한다. 이 방법은 공존 행렬을 엔트로피나 상관관계 기반 정규화를 통해 변형하는 것이다. 이러한 유형의 변환의 장점은 상당한 크기의 말뭉치에 대해 원시 공존 횟수가 8~9개의 자릿수에 걸쳐 분포되도록 압축된다는 것이다. 이러한 접근 방식을 따르는 다양한 최신 모델도 존재하며, Bullinaria와 Levy(2007)의 연구에서는 양의 점별 상호 정보(PPMI)가 좋은 변환 방법임을 보여준다. 최근에는 Hellinger PCA(HPCA) (Lebret and Collobert, 2014)라는 제곱근 형태의 변환도 효과적인 단어 표현 학습 방법으로 제안되었다.
얕은 창 기반 방법. 또 다른 접근 방식은 지역적 문맥 창 내에서 예측을 돕기 위한 단어 표현을 학습하는 것이다. 예를 들어, Bengio et al. (2003)는 언어 모델링을 위한 간단한 신경망 구조의 일부로 단어 벡터 표현을 학습하는 모델을 소개했다. Collobert와 Weston(2008)은 단어 벡터 훈련을 하류 훈련 목표와 분리하여 도입했으며, 이는

Collobert et al. (2011)의 방법은 단어 표현을 학습하기 위해 이전 문맥뿐만 아니라 전체 문맥을 사용하는 것이다. 최근에는 유용한 단어 표현을 학습하기 위해 전체 신경망 구조의 중요성이 의문을 제기했다. Mikolov et al. (2013a)의 skip-gram과 continuous bag-of-words (CBOW) 모델은 두 단어 벡터 사이의 내적을 기반으로 한 단층 아키텍처를 제안한다. Mnih and Kavukcuoglu (2013)는 관련된 vector log-bilinear 모델인 vLBL과 ivLBL을 제안했으며, Levy et al. (2014)은 PPMI metric을 기반으로 한 명시적인 단어 임베딩을 제안했다. skip-gram과 ivLBL 모델에서는 단어 자체를 주어로 주어진 단어의 문맥을 예측하는 것이 목표이며, CBOW와 vLBL 모델에서는 주어진 문맥으로 단어를 예측하는 것이 목표이다. 단어 유추 작업을 통한 평가를 통해 이러한 모델들은 단어 벡터 간의 선형 관계로 언어적 패턴을 학습할 수 있는 능력을 보였다. 행렬 분해 방법과 달리 얕은 윈도우 기반 방법은 말뭉치의 공기 통계에 직접 작용하지 않는 단점이 있다. 대신, 이러한 모델들은 전체 말뭉치에서 문맥 윈도우를 스캔하여 데이터의 반복을 충분히 활용하지 못한다.

3  글로브 모델

단어 빈도 통계는 말뭉치에서의 단어 발생 횟수를 나타내는 주요 정보원으로, 모든 비지도 학습 방법에서 사용됩니다. 많은 방법들이 존재하지만, 여전히 이 통계로 어떻게 의미가 생성되고, 그 의미를 어떻게 단어 벡터가 나타낼 수 있는지에 대한 질문은 남아 있습니다. 이 섹션에서는 이 질문에 대해 약간의 빛을 비추고자 합니다. 우리는 이 통찰력을 사용하여 GloVe라고 불리는 새로운 단어 표현 모델을 구축합니다. 이 모델은 Global Vectors를 의미하는데, 전역 말뭉치 통계가 모델에 직접 포착됩니다. 먼저 우리는 몇 가지 표기법을 정립합니다. 단어-단어 공기 횟수 행렬을 X로 표기하며, 그 엔트리는 X입니다.

ij
단어 i의 맥락에서 단어 j가 나타나는 횟수를 정리하십시오. X를 계산하십시오.

i = P
i
k = X
i
k be the number of times any word appears
in the context of word i. Finally, let P
i

ij
= P(j|i) =
X ij/X
i
단어 j가 나타날 확률인 P(j|i)는 X ij/X i입니다.

1533
표 1: 60억 토큰 말뭉치에서 대상 단어인 "얼음"과 "증기"와 선택된 문맥 단어들과의 동시발생 확률. 비유에서만 비판적이지 않은 단어인 "물"과 "패션"과 같은 잡음이 상쇄되어, 큰 값(1보다 훨씬 큰 값)은 얼음에 특정한 속성과 잘 상관되고, 작은 값(1보다 훨씬 작은 값)은 증기에 특정한 속성과 잘 상관된다.

확률과 비율 k = 고체 k = 기체 k = 물 k = 패션

P(k|얼음) 1.9 × 10−4 6.6 × 10−5 3.0 × 10−3 1.7 × 10−5

P(k|스팀) 2.2 × 10−5 7.8 × 10−4 2.2 × 10−3 1.8 × 10−5

P(k|얼음)/P(k|증기) 8.9 8.5 × 10−2 1.36 0.96

우리는 의미의 특정 측면이 공존 확률에서 직접 추출될 수 있는 간단한 예제로 시작합니다. 관심 있는 특정 측면을 나타내는 두 단어 i와 j를 고려해 보겠습니다. 구체적으로, 우리는 열역학 상태 개념에 관심이 있을 것이며, 이를 위해 i = 얼음과 j = 증기를 선택할 수 있습니다. 이러한 단어들의 관계는 다양한 탐사 단어 k의 공존 확률 비율을 연구함으로써 조사할 수 있습니다. 얼음과 관련된 단어 k이지만 증기와는 관련이 없는 경우, 예를 들어 k = 고체라고 가정합니다. 우리는 비율 P ik/P를 기대합니다.

jk
클 것입니다. 마찬가지로,
증기와 관련된 단어 k에 대해서는 얼음과는 관련이 없지만, k = 가스라고 말할 때, 비율은 작아야 합니다. 물이나 패션과 같은 k와 같은 단어는 얼음과 증기 둘 다 관련이 있거나 아예 관련이 없는 경우, 비율은 1에 가까워야 합니다. 표 1은 이러한 확률과 비율을 큰 말뭉치에 대해 보여주며, 숫자는 이러한 기대에 부합함을 확인합니다. 원시 확률과 비교했을 때, 비율은 관련 있는 단어 (고체와 가스)와 관련 없는 단어 (물과 패션)를 더 잘 구별할 수 있으며, 또한 두 가지 관련 있는 단어 사이를 더 잘 구별할 수 있습니다.
위의 주장은 단어 벡터 학습의 적절한 시작점은 확률 자체가 아닌 공존 확률의 비율이어야 함을 시사합니다. P ik/P
jk
비율이 단어 i, j, k에 따라 달라짐을 고려하면, 가장 일반적인 모델은 다음과 같은 형태를 가집니다.

F(wi,wj, ˜ wk) = P
ik
P
jk
,      (1)

w ∈ Rd 단어 벡터이고 ˜ w ∈ Rd는 별도의 문맥 단어 벡터로, 이 역할은 4.2절에서 논의될 것입니다. 이 식에서 우변은 말뭉치에서 추출되며, F는 아직 명시되지 않은 몇 가지 매개변수에 따라 달라질 수 있습니다. F의 가능성은 매우 많지만, 몇 가지 원하는 조건을 강제로 적용하여 고유한 선택을 할 수 있습니다. 먼저, F가 인코딩되기를 원합니다.

정보는 비율 P ik/P를 나타냅니다.

jk
in the
word vector space. Since vector spaces are inher-
ently linear structures, the most natural way to do
this is with vector differences. With this aim, we
can restrict our consideration to those functions F
that depend only on the difference of the two target
words, modifying Eqn. (1) to,

jk
단어 벡터 공간에서. 벡터 공간은 본질적으로 선형 구조이므로, 이를 수행하는 가장 자연스러운 방법은 벡터 차이를 사용하는 것입니다. 이를 위해, 우리는 두 대상 단어의 차이에만 의존하는 함수 F에 대해서만 고려를 제한할 수 있습니다. 이를 위해, 식 (1)을 수정합니다.

글로브: 단어 표현을 위한 글로벌 벡터

제프리 페닝턴, 리처드 소처, 크리스토퍼 D. 매닝
스탠포드 대학교 컴퓨터 과학부, 스탠포드, CA 94305
jpennin@stanford.edu, richard@socher.org, manning@stanford.edu

요약

단어의 벡터 공간 표현을 학습하기 위한 최근 방법들은 벡터 연산을 사용하여 미세한 의미론적 및 문법적 규칙을 포착하는 데 성공했지만, 이러한 규칙의 기원은 여전히 불분명하다. 우리는 단어 벡터에서 이러한 규칙이 나타나기 위해 필요한 모델 속성을 분석하고 명시화한다. 결과적으로, 우리는 문헌에서 주요한 두 가지 모델 패밀리의 장점을 결합한 새로운 전역 로그-이중 선형 회귀 모델을 제시한다: 전역 행렬 인수분해와 지역 문맥 윈도우 방법. 우리의 모델은 대규모 말뭉치에서 희소 행렬 전체나 개별 문맥 윈도우에 대해 훈련하는 대신, 단어-단어 공기 행렬의 0이 아닌 요소들에만 훈련하여 통계적 정보를 효율적으로 활용한다. 이 모델은 의미 있는 하위 구조를 가진 벡터 공간을 생성하며, 최근의 단어 유추 작업에서 75%의 성능을 보여준다. 또한 유사도 작업과 개체명 인식 작업에서 관련 모델보다 우수한 성능을 보인다.

1 소개

언어의 의미 벡터 공간 모델은 각 단어를 실수 값 벡터로 표현합니다. 이러한 벡터는 정보 검색 (Manning et al., 2008), 문서 분류 (Sebastiani, 2002), 질문 응답 (Tellex et al., 2003), 명명된 개체 인식 (Turian et al., 2010) 및 구문 분석 (Socher et al., 2013)과 같은 다양한 응용 프로그램에서 기능으로 사용될 수 있습니다.
대부분의 단어 벡터 방법은 단어 벡터 쌍 간의 거리나 각도를 기본적인 방법으로 사용하여 이러한 단어 표현의 내재적인 품질을 평가합니다. 최근에 Mikolov et al. (2013c)은 단어 유추를 기반으로 한 새로운 평가 체계를 소개했습니다.

단어 벡터 공간의 미세한 구조를 살펴보면 단어 벡터 간의 스칼라 거리가 아닌 다양한 차원의 차이를 조사합니다. 예를 들어, "왕은 여왕과 같은 관계이고, 남자는 여자와 같은 관계이다"라는 유추는 벡터 공간에서 왕 - 여왕 = 남자 - 여자라는 벡터 방정식으로 인코딩되어야 합니다. 이 평가 체계는 의미의 차원을 생성하는 모델을 선호하며, 분산 표현의 다중 클러스터링 개념을 포착합니다 (Bengio, 2009).

단어 벡터 학습을 위한 두 가지 주요 모델 패밀리는 다음과 같습니다: 1) 잠재 의미 분석(LSA)과 같은 전역 행렬 인수분해 방법, 2) Mikolov 등의 스킵-그램 모델과 같은 지역 문맥 윈도우 방법. 현재, 두 가지 패밀리 모두 중요한 단점을 가지고 있습니다. LSA와 같은 방법은 통계 정보를 효율적으로 활용하지만, 단어 유추 작업에서는 비교적 성능이 좋지 않아서 최적의 벡터 공간 구조를 나타내지 못합니다. 스킵-그램과 같은 방법은 유추 작업에서 더 좋은 성능을 낼 수 있지만, 전역 공기 통계를 훈련하는 대신 별도의 지역 문맥 윈도우에서 훈련하기 때문에 말뭉치의 통계를 효과적으로 활용하지 못합니다.

이 작업에서는 선형 의미 방향을 생성하기 위해 필요한 모델 속성을 분석하고, 전역 로그-이중 회귀 모델이 이를 수행하기에 적합하다고 주장합니다. 우리는 전역 단어-단어 공기 빈도를 기반으로 하는 특정 가중 최소 제곱 모델을 제안하며, 이는 통계를 효율적으로 사용합니다. 이 모델은 의미 있는 하위 구조를 가진 단어 벡터 공간을 생성하며, 이는 단어 유추 데이터셋에서 75%의 정확도로 최고 수준의 성능을 보여줍니다. 또한, 우리의 방법이 다른 현재 방법들보다 여러 단어 유사도 작업 및 공통된 명명된 개체 인식 (NER) 벤치마크에서 우수한 성능을 보여준다는 것을 증명합니다.

우리는 모델의 소스 코드와 훈련된 단어 벡터를 http://nlp.stanford.edu/projects/glove/에서 제공합니다.

1532
저차원 단어 표현을 생성하기 위한 정규화 방법은 LSA와 같은 곳에서 시작되었습니다. 이러한 방법들은 말뭉치에 대한 통계적 정보를 포착하는 큰 행렬을 분해하기 위해 저차원 근사치를 활용합니다. 이러한 행렬들이 포착하는 특정 유형의 정보는 응용에 따라 다양합니다. LSA에서는 행렬이 "용어-문서" 유형이며, 행은 단어나 용어에 해당하고 열은 말뭉치의 다른 문서에 해당합니다. 반면에 언어의 초월적 유사성(HAL) (Lund and Burgess, 1996)은 "용어-용어" 유형의 행렬을 활용합니다. 즉, 행과 열은 단어에 해당하고 항목은 다른 단어의 문맥에서 주어진 단어가 발생하는 횟수에 해당합니다.
HAL과 관련된 방법들의 주요 문제점은 가장 빈번한 단어들이 유사도 측정에 지나치게 많은 기여를 한다는 것입니다. 예를 들어, "the"나 "and"와 같은 단어들이 두 단어가 함께 나타나는 횟수에 큰 영향을 미치며, 그들의 의미적 관련성에는 상대적으로 적은 정보를 전달합니다. HAL의 이러한 단점을 해결하기 위해 COALS 방법(Rohde et al., 2006)과 같은 여러 기법들이 존재합니다. 이 방법들은 공기 행렬을 엔트로피나 상관관계를 기반으로 정규화하는 것입니다. 이러한 유형의 변환의 장점은 상당한 크기의 말뭉치에 대해 원시 공기 횟수가 8~9개의 자릿수에 걸쳐 분포되도록 압축된다는 것입니다. 이러한 접근 방식을 따르는 다양한 최신 모델들도 존재합니다. 예를 들어, Bullinaria와 Levy(2007)의 연구는 양의 점별 상호 정보(PPMI)가 좋은 변환 방법임을 보여줍니다. 최근에는 Hellinger PCA(HPCA) (Lebret and Collobert, 2014)라는 제곱근 형태의 변환도 효과적인 단어 표현 학습 방법으로 제안되었습니다.
얕은 창 기반 방법. 또 다른 접근 방식은 지역 문맥 창 내에서 예측을 돕기 위한 단어 표현을 학습하는 것입니다. 예를 들어, Bengio et al. (2003)는 언어 모델링을 위한 간단한 신경망 구조의 일부로 단어 벡터 표현을 학습하는 모델을 소개했습니다. Collobert and Weston(2008)은 단어 벡터 훈련을 하류 훈련 목표와 분리하여 진행하였으며, 이는 단어 벡터 훈련의 성능을 향상시켰습니다.

Collobert et al. (2011)의 방법은 단어 표현을 학습하기 위해 이전 문맥뿐만 아니라 전체 문맥을 사용하는 것이다. 최근에는 유용한 단어 표현을 학습하기 위해 전체 신경망 구조의 중요성이 의문을 제기했다. Mikolov et al. (2013a)의 skip-gram과 continuous bag-of-words (CBOW) 모델은 두 단어 벡터 사이의 내적을 기반으로 한 단층 아키텍처를 제안한다. Mnih and Kavukcuoglu (2013)는 관련된 vector log-bilinear 모델인 vLBL과 ivLBL을 제안했으며, Levy et al. (2014)은 PPMI metric을 기반으로 한 명시적인 단어 임베딩을 제안했다. skip-gram과 ivLBL 모델에서는 단어 자체를 주어로 주어진 단어의 문맥을 예측하는 것이 목표이며, CBOW와 vLBL 모델에서는 주어진 문맥으로 단어를 예측하는 것이 목표이다. 단어 유추 작업을 통한 평가를 통해 이러한 모델들은 단어 벡터 간의 선형 관계로 언어적 패턴을 학습할 수 있는 능력을 보였다. 행렬 분해 방법과 달리 얕은 윈도우 기반 방법은 말뭉치의 공기 통계에 직접 작용하지 않는 단점이 있다. 대신, 이러한 모델들은 전체 말뭉치에서 문맥 윈도우를 스캔하여 데이터의 반복을 충분히 활용하지 못한다.

3  글로브 모델

단어 빈도 통계는 말뭉치에서의 단어 발생 횟수를 나타내는 주요 정보원으로, 모든 비지도 학습 방법에서 사용됩니다. 많은 방법들이 존재하지만, 여전히 이 통계로 어떻게 의미가 생성되고, 그 의미를 어떻게 단어 벡터가 나타낼 수 있는지에 대한 질문은 남아 있습니다. 이 섹션에서는 이 질문에 대해 약간의 빛을 비추고자 합니다. 우리는 이 통찰력을 사용하여 GloVe라고 불리는 새로운 단어 표현 모델을 구축합니다. 이 모델은 Global Vectors를 의미하는데, 전역 말뭉치 통계가 모델에 직접 포착됩니다. 먼저 우리는 몇 가지 표기법을 정립합니다. 단어-단어 공기 횟수 행렬을 X로 표기하며, 그 엔트리는 X입니다.

ij
단어 i의 맥락에서 단어 j가 나타나는 횟수를 정리하십시오. X를 계산하십시오.

i = i
P = P
k = k
X = X
ik = ik
be the number of times any word appears in the context of word i. Finally, let P = P

ij
= P(j|i) =
X ij/X
i
단어 j가 나타날 확률인 P(j|i)는 X ij/X i입니다.

1533
취소되어, 큰 값들(1보다 훨씬 큰 값들)이 얼음에 특정된 특성과 잘 상관되고,
작은 값들(1보다 훨씬 작은 값들)이 증기에 특정된 특성과 잘 상관되도록.

확률과 비율 k = 고체 k = 기체 k = 물 k = 패션

P(k|얼음) 1.9 × 10−4 6.6 × 10−5 3.0 × 10−3 1.7 × 10−5

P(k|스팀) 2.2 × 10−5 7.8 × 10−4 2.2 × 10−3 1.8 × 10−5

P(k|얼음)/P(k|증기) 8.9 8.5 × 10−2 1.36 0.96

우리는 의미의 특정 측면이 공존 확률에서 직접 추출될 수 있는 간단한 예제로 시작합니다. 관심 있는 특정 측면을 나타내는 두 단어 i와 j를 고려해 보겠습니다. 구체적으로, 우리는 열역학 상태 개념에 관심이 있을 것이며, 이를 위해 i = 얼음과 j = 증기로 가정합니다. 이러한 단어들의 관계는 다양한 탐사 단어 k의 공존 확률 비율을 연구함으로써 조사할 수 있습니다. 얼음과 관련된 단어 k이지만 증기와는 관련이 없는 경우, 예를 들어 k = 고체라고 가정합니다. 우리는 비율 P ik/P kj를 기대합니다.

jk
큰 것이 될 것입니다. 마찬가지로,
증기와 관련된 단어 k에 대해서는 얼음과는 관련이 없지만 k = 가스라고 말할 때, 비율은 작아야 합니다. 물이나 패션과 같은 단어 k에 대해서는 얼음과 증기 둘 다 관련이 있거나 아예 관련이 없는 경우, 비율은 1에 가까워야 합니다. 표 1은 이러한 확률과 비율을 큰 말뭉치에 대해 보여주며, 숫자는 이러한 기대에 부합함을 확인합니다. 원시 확률에 비해 비율은 관련 있는 단어 (고체와 가스)와 관련 없는 단어 (물과 패션)를 더 잘 구별할 수 있으며, 또한 두 가지 관련 있는 단어를 구별하는 데에도 더 우수합니다.
위의 주장은 단어 벡터 학습의 적절한 시작점은 확률 자체가 아닌 공존 확률의 비율이어야 함을 시사합니다. P ik/P
jk
비율이 단어 i, j, k에 따라 달라짐을 고려하면, 가장 일반적인 모델은 다음과 같은 형태를 가집니다.

F(wi,wj, ˜ wk) = P
ik
P
jk
,      (1)

w ∈ Rd 단어 벡터이고 ˜ w ∈ Rd는 별도의 문맥 단어 벡터로, 이 역할은 4.2절에서 논의될 것입니다. 이 식에서 우변은 말뭉치에서 추출되며, F는 아직 명시되지 않은 몇 가지 매개변수에 따라 달라질 수 있습니다. F의 가능성은 매우 많지만, 몇 가지 원하는 조건을 강제로 적용하여 고유한 선택을 할 수 있습니다. 먼저, F가 인코딩되기를 원합니다.

정보는 비율 P ik/P를 나타냅니다.

jk
in the
word vector space. Since vector spaces are inher-
ently linear structures, the most natural way to do
this is with vector differences. With this aim, we
can restrict our consideration to those functions F
that depend only on the difference of the two target
words, modifying Eqn. (1) to,

jk
단어 벡터 공간에서.
벡터 공간은 본질적으로 선형 구조이므로, 이를 수행하는 가장 자연스러운 방법은 벡터 차이를 사용하는 것입니다. 이를 위해, 우리는 두 대상 단어의 차이에만 의존하는 함수 F에 대해서만 고려를 제한할 수 있습니다. 이를 위해, 식 (1)을 수정합니다.




P
나는
P
너는
.

다음으로, 우리는 식 (2)의 F의 인자들이 벡터이고 우변이 스칼라임을 알 수 있습니다. F는 복잡한 함수로, 예를 들어 신경망에 의해 매개변수화될 수 있지만, 이렇게 하면 우리가 포착하려는 선형 구조를 혼란스럽게 만들 수 있습니다. 이 문제를 피하기 위해, 우리는 먼저 인자들의 내적을 취할 수 있습니다.

F (cid:16) (wi - wj)T ˜ wk (cid:17) = Pik Pjk, (3)

F가 벡터 차원을 원하지 않는 방식으로 혼합하는 것을 방지합니다. 다음으로, 단어-단어 공존 행렬의 경우, 단어와 문맥 단어 사이의 구분은 임의적이며 두 역할을 교환할 수 있습니다. 이를 일관되게 하기 위해서는 w ↔ ˜ w뿐만 아니라 X ↔ XT도 교환해야 합니다. 최종 모델은 이러한 라벨링에 대해 불변해야 하지만, 식 (3)은 그렇지 않습니다. 그러나 대칭성은 두 단계에서 복원될 수 있습니다. 첫째, F가 (R,+)와 (R >0, ×) 그룹 사이의 동형사상이 되도록 요구합니다.

F
(wi − wj)T ˜
wk
=
F(wTi ˜ wk)
F(wTj ˜ wk)
,   (4)

어떤 것은, 방정식 (3)에 의해 해결된다.

F(wT
i
˜ wk) = P
ik
=
X
ik
X
i
.      (5)

F(wT
i
˜ wk) = P
ik
=
X
ik
X
i
.      (5)

방정식 (4)의 해는 F = exp입니다.

wT
나는
˜ wk = log(P ik) = log(X ik) − log(X i) . (6)

1534
오른쪽. 그러나 이 용어는 k에 독립적이므로 편향 b로 흡수될 수 있습니다.

나
에 대해
wi. 마침내, 추가적인 편향 ˜ b k를 ˜ wk에 추가함으로써 대칭성을 복원합니다,
wT i ˜ wk + b i + ˜ b k = log(X ik) . (7)

방정식 (7)은 방정식 (1)에 비해 극단적으로 단순화된 것이지만, 실제로는 불명확하다. 로그함수의 인자가 0일 때 로그함수는 발산하기 때문에 이 문제를 해결하기 위해 로그함수에 가산적인 변화를 포함시킬 수 있다. log(X ik) → log(1 + X ik)로 변환하여 X의 희소성을 유지하면서 발산을 피할 수 있다. 공존 행렬의 로그를 인수분해하는 아이디어는 LSA와 밀접한 관련이 있으며, 우리는 이 결과 모델을 실험의 기준선으로 사용할 것이다. 이 모델의 주요 단점은 모든 공존을 동등하게 가중치를 부여한다는 것이다. 드물게 또는 결코 발생하지 않는 이러한 드문 공존은 노이즈이며 덜 중요한 정보를 가지고 있다. 그럼에도 불구하고, 0 항목만으로도 X의 데이터의 75-95%를 차지한다. 우리는 이러한 문제를 해결하는 새로운 가중치 최소제곱 회귀 모델을 제안한다. 방정식 (7)을 최소제곱 문제로 캐스팅하고 비용 함수에 가중치 함수 f(X ij)를 도입하면 우리는 다음과 같은 모델을 얻을 수 있다.

J = V X
i,j=1
f (cid:16) X ij(cid:17) (cid:16) wT i ˜ wj + b i + ˜ b j − log X ij(cid:17) 2 ,

J = V X
i,j=1
f (cid:16) X ij(cid:17) (cid:16) wT i ˜ wj + b i + ˜ b j − log X ij(cid:17) 2 ,

(8)
어휘 크기 V인 경우, 가중 함수는 다음 속성을 따라야 합니다.

1. f (0) = 0. 만약 f가 연속 함수로 간주된다면, x → 0일 때 f는 충분히 빠르게 사라져야 하므로 lim x→0 f (x) log2 x는 유한해야 한다.

2. f (x)는 증가하지 않아야 하므로, 희귀한 동시 발생은 과대 가중되지 않아야 합니다.

3. x의 값이 큰 경우에 f(x)는 비교적 작아야 하므로, 빈번한 동시 발생이 과도하게 가중되지 않습니다.

물론 이러한 특성을 만족하는 다양한 함수들이 있지만, 우리가 잘 작동하는 하나의 함수 클래스를 파라미터화할 수 있었습니다.

f (x) = 
(
(x/x max)α if x < x max


그렇지 않으면.

0.2 0.4 - 0.2 0.4
0.6 - 0.6
0.8 - 0.8
1.0 - 1.0

0.0

그림 1: 가중 함수 f, α = 3/4.

모델의 성능은 컷오프에 약간 의존하지만, 우리는 모든 실험에서 x max = 100으로 고정합니다. 우리는 α = 3/4가 α = 1인 선형 버전보다 약간의 개선을 제공한다는 것을 발견했습니다. 우리는 값 3/4를 선택하는 것에 대해 경험적 동기만 제시하지만, 흥미로운 점은 (Mikolov et al., 2013a)에서 최적의 성능을 제공하는 유사한 분수 거듭제곱 스케일링이 발견되었다는 것입니다.

3.1 다른 모델들과의 관계

모든 비감독 학습 방법은 단어 벡터 학습을 위해 궁극적으로 말뭉치의 발생 통계에 기반하므로, 모델들 사이에는 공통점이 있어야 합니다. 그럼에도 불구하고, 특히 최근의 윈도우 기반 방법인 skip-gram과 ivLBL과 같은 모델들은 이 측면에서 다소 불투명합니다. 따라서, 이 하위 절에서는 이러한 모델들이 우리가 제안한 모델과 어떻게 관련되어 있는지, 식 (8)에서 정의된 대로 보여줍니다. skip-gram 또는 ivLBL 방법의 시작점은 모델 Q입니다.

ij
단어 i의 맥락에서 단어 j가 나타날 확률에 대한 것입니다. 구체적으로, Q라고 가정합시다.

ij는 소프트맥스입니다.

Qij = exp(wTi ~ wj) / P(exp(wTi ~ wk))

이 모델들의 대부분 세부 사항은 우리의 목적과 관련이 없습니다. 단지 말하자면, 이 모델들은 맥락 창이 말뭉치를 스캔하는 동안 로그 확률을 최대화하려고 시도합니다. 훈련은 온라인, 확률적인 방식으로 진행되지만, 암시된 전역 목적 함수는 다음과 같이 작성할 수 있습니다.

J = 마이너스 X

나는 말뭉치에 속한다.
j는 i의 문맥에 속한다.
logQij. (11)

더하기 연산에서 소프트맥스의 정규화 요소를 평가하는 것은 비용이 많이 든다. 효율적인 훈련을 위해, 스킵-그램과 ivLBL 모델은 Q ij에 근사치를 도입한다. 그러나, 식 (11)의 합은 훨씬 빠르게 평가될 수 있다.

1535

J = 음수 V X

나는 1이다.
V X

j=1
X ij logQ ij , (12)

우리가 같은 항의 개수를 나타내는 공존 행렬 X를 사용한 곳을 기억해봅시다. X에 대한 표기법을 상기해봅시다.

나는
=
P
k
X
ik
그리고
P
ij
= X ij/X i, 우리는 J를 다음과 같이 다시 쓸 수 있습니다.

J = 음수 V X

나는 1이다.
X
나는
V X

j=1
Pij
logQij
=
V X

나는 1이다
X iH(P i,Q i) ,

(13)
H(P i,Q i)은 P
i
와 Q i의 교차 엔트로피로, 우리는 X i와 유사하게 정의한다. 교차 엔트로피 오차의 가중 합으로, 이 목적은 식 (8)의 가중 최소 제곱 목적과 형식적으로 유사하다. 사실, skip-gram 및 ivLBL 모델에서 사용되는 온라인 학습 방법 대신에 식 (13)을 직접 최적화하는 것이 가능하다. 이 목적을 "전역 skip-gram" 모델로 해석할 수 있으며, 더 자세히 조사하는 것이 흥미로울 수 있다.

반면, 식 (13)은 단어 벡터 학습 모델로 채택하기 전에 고려해야 할 몇 가지 원치 않는 특성을 보여준다. 먼저, 교차 엔트로피 오차는 확률 분포 간의 여러 가능한 거리 측정 중 하나에 불과하며, 긴 꼬리를 가진 분포는 종종 불확실한 사건에 지나치게 많은 가중치가 부여되어 모델링이 잘못될 수 있다는 불행한 특성을 가지고 있다. 또한, 이 측정이 유계가 되기 위해서는 모델 분포 Q가 적절하게 정규화되어야 한다. 이는 식 (10)의 전체 어휘에 대한 합 때문에 계산 병목 현상을 일으키며, Q의 이러한 특성을 요구하지 않는 다른 거리 측정을 고려하는 것이 바람직하다. 자연스러운 선택은 Q와 P의 정규화 요소를 버리는 최소 제곱 목적일 것이다.

J = X

나, 제이
엑스
아이
ˆ 피
아이제이
마이너스 ˆ 큐
아이제이원
이      (십사)

ij
와 ˆ Q
ij
= exp(wT
i
˜ wj)가 비정규화된 분포입니다. 이 단계에서 또 다른 문제가 발생하는데, 바로 X

ij
자주 매우 큰 값을 가지는데, 이는 최적화를 복잡하게 만들 수 있습니다. 효과적인 해결책은 최소화하는 것입니다.

로그의 제곱 오차인 ˆ P와 ˆ Q 대신에,

J = X

나, 제이
엑스
아이
로그 높이 P
아이 제이
마이너스 로그 높이 Q
아이 제이 제곱

X

나, 제이
엑스
아이
더블유티
아이
물결표 w 제이 빼기 로그 엑스
아이제이
2 . (15)

마지막으로, 우리는 스킵-그램과 ivLBL 모델에 내재된 온라인 훈련 방법에 의해 미리 정해진 가중치 요소 Xi가 최적이라는 보장이 없다는 것을 관찰합니다. 실제로, Mikolov 등(2013a)은 자주 나오는 단어의 가중치 요소의 효과적인 값을 줄이기 위해 데이터를 필터링함으로써 성능을 향상시킬 수 있다는 것을 관찰했습니다. 이를 염두에 두고, 우리는 보다 일반적인 가중치 함수를 소개합니다. 이 함수는 문맥 단어에 따라 달라질 수 있습니다. 결과는 다음과 같습니다.

J = X

나, 제이
f (X
ij)(cid:0)
wT
i
˜ wj − log X
ij(cid:1)
2 , (16)

이는 이전에 유도한 방정식 (8)의 비용 함수와 동등한 것입니다.

3.2 모델의 복잡성

식 (8)과 가중 함수 f (X)의 명시적 형태에서 알 수 있듯이, 모델의 계산 복잡도는 행렬 X의 0이 아닌 요소의 수에 따라 달라집니다. 이 수는 항상 행렬의 전체 항목 수보다 작기 때문에, 모델의 크기는 O(|V|2)보다 나쁘지 않게 확장됩니다. 처음 보면 이는 말뭉치 크기 |C|와 함께 확장되는 얕은 창 기반 접근법보다 상당한 개선으로 보일 수 있습니다. 그러나 일반적인 어휘는 수십만 개의 단어를 가지므로, |V|2는 실제로 대부분의 말뭉치보다 훨씬 큰 수십억 개가 될 수 있습니다. 이러한 이유로 X의 0이 아닌 요소의 수에 대해 더 엄격한 한계를 설정할 수 있는지 여부를 결정하는 것이 중요합니다.
X의 0이 아닌 요소의 수에 대해 구체적인 명제를 제시하기 위해서는 단어 공존의 분포에 대한 가정을 해야 합니다. 특히, 우리는 단어 쌍의 빈도 순위 r ij의 거듭제곱 법칙 함수로 단어 i와 단어 j의 공존 횟수 X ij를 모델링할 것으로 가정합니다:
X
ij
=
k
(r ij)α
.        (17)

1. 우리는 방정식 (16)에 편향 항도 포함시킬 수 있습니다.

1536
발생 행렬 X,

|C| ~ X

ij
X
ij
=
|X| X

ij
X
ij
=
|X| X

r=1
k
rα
= kH
|X|,α
,   (18)

우리가 마지막 합을 일반화된 조화수 H n,m의 용어로 다시 쓴 곳에서. 합의 상한선인 |X|는 최대 주파수 순위이며, 이는 행렬 X의 0이 아닌 요소의 수와 일치합니다. 이 수는 또한 X ij ≥ 1인 Eqn. (17)의 r의 최대값인 |X| = k1/α와 같습니다. 따라서 Eqn. (18)을 다음과 같이 쓸 수 있습니다.

|C| ∼ |X|α H |X|,α . (19)
|C| ∼ |X|의 α H |X|,α . (19)

우리는 |X|와 |C|가 모두 큰 경우에 어떻게 관련되는지에 관심이 있습니다. 따라서 우리는 |X|가 큰 경우에 방정식의 오른쪽을 확장할 수 있습니다. 이를 위해 우리는 일반화된 조화수의 확장을 사용합니다 (Apostol, 1976).

H x,s = 
x1−s
1 − s + ζ(s) + O(x−s) if s > 0, s , 1,

주는 것

|C| ∼
|X|
1 − α
+ ζ(α) |X|α + O(1) , (21)

|C| ∼
|X|
1 − α
+ ζ(α) |X|α + O(1) , (21)

ζ(s)이 리만 제타 함수인 경우. X가 큰 경우, 방정식 (21)의 오른쪽 항에서 두 항 중 하나만 관련이 있을 것이며, 어떤 항이 관련되는지는 α > 1인지에 따라 달라집니다.

|X| =
(
O(|C|) if α < 1,
O(|C|1/α) if α > 1.

(22)
(22)

이 논문에서 연구된 말뭉치에 대해, 우리는 Xij가 식 (17)에 α = 1.25로 잘 모델링된다는 것을 관찰합니다. 이 경우에는 |X| = O(|C|0.8)임을 알 수 있습니다. 따라서 우리는 모델의 복잡성이 최악의 경우인 O(V2)보다 훨씬 우수하며, 실제로는 O(|C|)와 같이 스케일링되는 온라인 창 기반 방법보다 조금 더 나은 결과를 얻을 수 있다고 결론지을 수 있습니다.

4 실험

4.1 평가 방법

우리는 Mikolov et al. (2013a)의 단어 유추 작업, (Luong et al., 2013)에서 설명한 다양한 단어 유사도 작업, 그리고 CoNLL-2003 공유 벤치마크에 대해 실험을 진행합니다.

표 2: 단어 유추 작업 결과, 정확도로 표시됨. 밑줄 친 점수는 비슷한 크기의 모델 그룹 내에서 가장 좋은 점수이고, 굵은 글씨로 표시된 점수는 전반적으로 가장 좋은 점수이다. HPCA 벡터는 공개적으로 사용 가능하다2; (i)vLBL 결과는 (Mnih et al., 2013)에서 가져왔다; skip-gram (SG)과 CBOW 결과는 (Mikolov et al., 2013a,b)에서 가져왔다; 우리는 word2vec 도구3를 사용하여 SG†와 CBOW†를 훈련시켰다. 자세한 내용과 SVD 모델에 대한 설명은 본문을 참조하십시오.

모델 크기 의미 유사성 합계
ivLBL 100  1.5B 55.9 50.1 53.2
HPCA  100  1.6B 4.2 16.4 10.8
GloVe 100  1.6B 67.5 54.3 60.3
SG   300  1B   61   61  61
CBOW  300  1.6B 16.1 52.6 36.1
vLBL  300  1.5B 54.2 64.8 60.0
ivLBL 300  1.5B 65.2 63.0 64.0
GloVe 300  1.6B 80.8 61.5 70.3
SVD   300  6B   6.3 8.1  7.3
SVD-S 300  6B   36.7 46.6 42.1
SVD-L 300  6B   56.6 63.0 60.1
CBOW†  300  6B   63.6 67.4 65.7
SG†  300  6B   73.0 66.0 69.1
GloVe 300  6B   77.4 67.0 71.7
CBOW  1000 6B   57.3 68.9 63.7
SG   1000 6B   66.1 65.1 65.6
SVD-L 300  42B  38.4 58.2 49.2
GloVe 300  42B  81.9 69.3 75.0

NER에 대한 데이터셋 (Tjong Kim Sang and De Meulder, 2003).
단어 유추. 단어 유추 작업은 "a는 b와 같은 관계이면, c는 ?와 같은 관계이다"와 같은 질문으로 구성됩니다.
이 데이터셋에는 19,544개의 이러한 질문이 포함되어 있으며, 의미적인 부분과 구문적인 부분으로 나뉩니다. 의미적인 질문은 일반적으로 사람이나 장소에 관한 유추입니다. 예를 들어 "아테네는 그리스와 같은 관계이면, 베를린은 ?와 같은 관계이다"와 같은 질문입니다. 구문적인 질문은 동사 시제나 형용사 형태에 관한 유추입니다. 예를 들어 "춤은 춤추기와 같은 관계이면, 날다는 ?와 같은 관계이다"와 같은 질문입니다. 질문에 정확히 답하기 위해서는 모델이 빠진 용어를 고유하게 식별해야 하며, 정확한 일치만이 올바른 일치로 계산됩니다. "a는 b와 같은 관계이면, c는 ?와 같은 관계이다"라는 질문에 대한 답은 코사인 유사도에 따라 wb - wa + wc에 가장 가까운 표현 wd를 찾아내는 것입니다.

2http://lebret.ch/words/
3http://code.google.com/p/word2vec/
4Levy et al. (2014)은 곱셈 유추 평가인 3COSMUL을 소개하고 68.24%의 정확도를 보고했습니다.

1537
40 50
60
70
80

벡터 차원
정확도 [ % ]

의미론적
구문론적 전체적인

대칭적인 맥락

2   4    6   8   10 40
50 55
60
65
70

45
사십오

창 크기
정확도 [%]

의미론적
구문론적 전체적인

대칭적인 맥락

2    4   6   8    10 40
50 55
60
65
70

45
사십오

창 크기
정확도 [%]

의미론적
구문론적 전체적인

비대칭적인 맥락

그림 2: 벡터 크기와 윈도우 크기/유형에 따른 유추 작업의 정확도. 모든 모델은 60억 토큰 말뭉치로 훈련되었습니다. (a)에서 윈도우 크기는 10입니다. (b)와 (c)에서 벡터 크기는 100입니다.

단어 유사성. 비유 작업은 흥미로운 벡터 공간 하위 구조를 테스트하기 때문에 주요 관심사입니다. 그러나 우리 모델은 테이블 3에서 다양한 단어 유사성 작업에 대해서도 평가됩니다. 이 작업에는 WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012) 및 RW (Luong et al., 2013)가 포함됩니다.
명명된 개체 인식. NER을 위한 CoNLL-2003 영어 벤치마크 데이터셋은 Reuters 뉴스 와이어 기사의 문서 모음이며, 사람, 위치, 조직 및 기타 개체 유형으로 주석이 달려 있습니다. 우리는 CoNLL-03 훈련 데이터를 사용하여 세 가지 데이터셋에서 모델을 훈련시키고 테스트합니다: 1) ConLL-03 테스트 데이터, 2) ACE Phase 2 (2001-02) 및 ACE-2003 데이터, 그리고 3) MUC7 공식 실행 테스트 세트. 우리는 BIO2 주석 표준을 채택하며, (Wang and Manning, 2013)에서 설명한 모든 전처리 단계를 사용합니다. 우리는 Stanford NER 모델의 표준 배포와 함께 제공되는 포괄적인 이산형 특징 집합을 사용합니다 (Finkel et al., 2005). CoNLL-2003 훈련 데이터셋에는 총 437,905개의 이산형 특징이 생성되었습니다. 또한, 다섯 단어 문맥의 각 단어에 대해 50차원 벡터를 추가하여 연속적인 특징으로 사용합니다. 이러한 특징을 입력으로 사용하여 우리는 (Wang and Manning, 2013)의 CRF 모델과 정확히 동일한 설정으로 조건부 랜덤 필드 (CRF)를 훈련시켰습니다.

4.2 코퍼스와 훈련 세부사항

우리는 다양한 크기의 다섯 개의 말뭉치에서 모델을 훈련시켰습니다: 10억 개의 토큰을 가진 2010년 위키피디아 덤프; 16억 개의 토큰을 가진 2014년 위키피디아 덤프; 43억 개의 토큰을 가진 Gigaword 5; Gigaword5 + Wikipedia2014의 조합.

유추 작업. 이 숫자는 데이터셋의 일부를 기반으로 평가되므로 표 2에 포함되지 않습니다. 3COSMUL은 우리의 실험 대부분에서 코사인 유사도보다 성능이 나쁘게 나타났습니다.

60억 개의 토큰을 가지고 있으며, 420억 개의 웹 데이터에서 가져왔습니다. 우리는 Stanford tokenizer를 사용하여 각 말뭉치를 토큰화하고 소문자로 변환하며, 가장 빈도가 높은 40만 개의 단어로 어휘를 구축한 다음, 공기 중 발생 횟수 행렬 X를 구성합니다. X를 구성할 때, 어떤 크기의 문맥 창을 선택해야 할지와 왼쪽 문맥과 오른쪽 문맥을 구분할지 여부를 결정해야 합니다. 이 선택의 영향을 아래에서 살펴봅니다. 모든 경우에 우리는 감소하는 가중치 함수를 사용하여, d 단어가 떨어져 있는 단어 쌍이 총 개수에 1/d를 기여하도록 합니다. 이는 매우 먼 단어 쌍이 서로의 관계에 대해 덜 관련성 있는 정보를 포함할 것으로 예상되기 때문에 이를 고려하는 한 가지 방법입니다.
모든 실험에서 우리는 x를 설정합니다.

최대값은 100이고, α는 3/4이며, AdaGrad를 사용하여 모델을 훈련합니다 (Duchi et al., 2011). X에서 0이 아닌 요소를 확률적으로 샘플링하며, 초기 학습률은 0.05입니다. 300 차원보다 작은 벡터에 대해서는 50번의 반복을 실행하고, 그렇지 않은 경우에는 100번의 반복을 실행합니다 (수렴 속도에 대한 자세한 내용은 섹션 4.6을 참조하십시오). 그 외에 특별히 언급되지 않는 한, 우리는 왼쪽으로 열 단어와 오른쪽으로 열 단어의 문맥을 사용합니다.
이 모델은 두 개의 단어 벡터 세트인 W와 ˜W를 생성합니다. X가 대칭일 때, W와 ˜W는 동등하며, 무작위 초기화의 결과로만 차이가 있습니다. 두 개의 벡터 세트는 동등하게 작동해야 합니다. 반면에, 특정 유형의 신경망에 대해서는 네트워크의 여러 인스턴스를 훈련한 다음 결과를 결합하는 것이 과적합과 잡음을 줄이고 일반적으로 결과를 개선하는 데 도움이 되는 증거가 있습니다 (Ciresan et al., 2012). 이를 염두에 두고, 우리는 이를 사용하기로 결정합니다.

5. 모델의 확장성을 보여주기 위해, 우리는 또한 8400억 개의 웹 데이터 토큰을 포함한 훨씬 큰 여섯 번째 말뭉치에서 모델을 훈련시켰지만, 이 경우에는 어휘를 소문자로 변환하지 않았으므로 결과는 직접 비교할 수 없습니다.
6. Common Crawl 데이터로 훈련된 모델의 경우, 약 2백만 단어로 구성된 더 큰 어휘를 사용합니다.

1538
의미 유추 과제에서 가장 큰 증가를 보였습니다.
우리는 다양한 최첨단 모델들과 발표된 결과들과 비교하였으며,
또한 word2vec 도구를 사용하여 우리 자신의 결과와 SVD를 사용한 여러 기준선과도 비교하였습니다. 
word2vec을 사용하여, 우리는 60억 개의 토큰 코퍼스(Wikipedia 2014 + Giga-word 5)에서 상위 40만 개의 가장 빈번한 단어들과 10개의 문맥 윈도우 크기로 skip-gram (SG†) 및 continuous bag-of-words (CBOW†) 모델을 훈련시켰습니다.
우리는 10개의 부정적인 샘플을 사용하였으며, 이는 이 코퍼스에 대해 좋은 선택임을 4.6절에서 보여줍니다.
SVD 기준선을 위해, 우리는 상위 1만 개의 가장 빈번한 단어들만을 사용하여 각 단어가 얼마나 자주 발생하는지에 대한 정보를 유지하는 X_trunc 행렬을 생성합니다. 이 단계는 많은 행렬 분해 기반 방법들에서 일반적으로 사용되며, 추가 열들은 0 항목의 비율이 불균형하게 많아지고, 그 외에도 계산 비용이 많이 들기 때문입니다.
이 행렬의 특이 벡터들은 기준선 "SVD"를 구성합니다. 또한 두 가지 관련 기준선을 평가합니다: "SVD-S"는 √X_trunc의 SVD를 취하는 것이고, "SVD-L"은 log(1+X_trunc)의 SVD를 취하는 것입니다. 이 두 가지 방법은 X의 큰 값 범위를 압축하는 데 도움이 됩니다.

4.3 결과

테이블 2에서 단어 유추 작업 결과를 제시합니다. GloVe 모델은 다른 기준선들보다 훨씬 우수한 성능을 보이며, 종종 더 작은 벡터 크기와 작은 말뭉치로도 작동합니다. word2vec 도구를 사용한 결과는 이전에 발표된 대부분의 결과보다 약간 더 좋습니다. 이는 여러 요인에 의해 발생하는데, 이중에는 일반적으로 계층적 소프트맥스보다 더 잘 작동하는 부정 샘플링의 선택, 부정적인 샘플의 수, 그리고 말뭉치의 선택이 포함됩니다. 우리는 이 모델이 대규모 420억 토큰 말뭉치에서 쉽게 훈련될 수 있으며, 상당한 성능 향상을 보여준다는 것을 입증합니다. 말뭉치 크기를 증가시키는 것이 다른 모델에 대한 개선된 결과를 보장하지 않는다는 것은 SVD 모델의 성능 저하로 확인할 수 있습니다.

우리는 X를 변환하기 위해 여러 가지 가중치 체계를 조사했으며, 여기에서는 가장 우수한 결과를 보고합니다. PPMI와 같은 많은 가중치 체계는 X의 희소성을 파괴하므로 대규모 어휘와 함께 사용하기에 적합하지 않습니다. 어휘가 작은 경우, 이러한 정보 이론적 변환은 단어 유사도 측정에서는 잘 작동하지만, 단어 유추 작업에서는 매우 부적합합니다.

표 3: 단어 유사도 작업에 대한 스피어만 순위 상관관계. 모든 벡터는 300차원입니다. CBOW∗ 벡터는 word2vec 웹사이트에서 가져온 것으로, 구문 벡터를 포함하고 있습니다.

모델 크기 WS353 MC RG SCWS RW
SVD 6B 35.3 35.1 42.5 38.3 25.6
SVD-S 6B 56.5 71.5 71.0 53.6 34.7
SVD-L 6B 65.7 72.7 75.1 56.5 37.0
CBOW† 6B 57.2 65.6 68.2 57.0 32.5
SG† 6B 62.8 65.2 69.7 58.1 37.2
GloVe 6B 65.8 72.7 77.8 53.9 38.1
SVD-L 42B 74.0 76.4 74.1 58.3 39.9
GloVe 42B 75.9 83.6 82.9 59.6 47.8
CBOW∗ 100B 68.4 79.6 75.4 59.4 45.5

이 더 큰 말뭉치에서 L 모델을 사용했습니다. 이 기본 SVD 모델이 큰 말뭉치에 적합하지 않다는 사실은 우리 모델에서 제안한 가중치 체계의 필요성에 대한 추가적인 증거를 제공합니다. 표 3은 다섯 가지 다른 단어 유사도 데이터셋의 결과를 보여줍니다. 유사도 점수는 단어 벡터에서 얻어지며, 우선 어휘 전체에서 각 특징을 정규화한 다음 코사인 유사도를 계산하여 얻습니다. 이 점수와 인간의 판단 사이의 스피어만 순위 상관 계수를 계산합니다. CBOW∗는 word2vec 웹사이트에서 제공되는 벡터로, 뉴스 데이터의 100B 단어에 대해 단어와 구문 벡터로 훈련된 것입니다. GloVe는 이보다 작은 말뭉치를 사용하면서도 더 좋은 성능을 보입니다. 표 4는 CRF 기반 모델을 사용한 NER 작업의 결과를 보여줍니다. L-BFGS 훈련은 개발 세트에서 25번의 반복 동안 개선이 없을 때 종료됩니다. 그 외의 모든 설정은 Wang과 Manning (2013)이 사용한 설정과 동일합니다. Discrete로 표시된 모델은 Stanford NER 모델의 표준 배포에 포함된 포괄적인 이산 특징 집합을 사용하지만 단어 벡터 특징은 사용하지 않습니다. 이전에 논의한 HPCA와 SVD 모델 외에도 Huang et al. (2012) (HSMN) 및 Collobert and Weston (2008) (CW)의 모델과 비교합니다. 우리는 word2vec 도구를 사용하여 CBOW 모델을 훈련시켰습니다. GloVe 모델은 CoNLL 테스트 세트를 제외한 모든 평가 지표에서 다른 모든 방법보다 우수한 성능을 보입니다. 우리는 GloVe 벡터가 다운스트림 NLP 작업에서 유용하다는 결론을 내립니다.

위와 같은 매개변수를 사용하지만, 이 경우에는 5개의 부정적인 샘플이 10개보다 약간 더 잘 작동하는 것을 발견했습니다.

1539
HPCA, HSMN, 그리고 CW에 대해 공개적으로 사용 가능한 벡터를 사용하십시오. 자세한 내용은 본문을 참조하십시오. 모델 Dev Test ACE MUC7
Discrete 91.0 85.4 77.4 73.4
SVD   90.8 85.7 77.3 73.7
SVD-S  91.0 85.5 77.6 74.3
SVD-L  90.5 84.8 73.6 71.5
HPCA  92.6 88.7 81.7 80.7
HSMN   90.5 85.7 78.7 74.7
CW    92.2 87.4 81.7 80.2
CBOW   93.1 88.2 82.2 81.1
GloVe 93.2 88.3 82.9 82.2

(Turian et al., 2010)에서 신경 벡터에 대해 보여준 것.

4.4 모델 분석: 벡터 길이와
컨텍스트 크기

그림 2에서는 벡터 길이와 문맥 창을 변화시킨 실험 결과를 보여줍니다. 타겟 단어의 왼쪽과 오른쪽으로 확장되는 문맥 창은 대칭적이라고 하고, 왼쪽으로만 확장되는 것은 비대칭적이라고 합니다. (a)에서는 200 차원 이상의 벡터에 대해 감소하는 성과를 관찰합니다. (b)와 (c)에서는 대칭적 및 비대칭적 문맥 창의 크기 변화의 효과를 조사합니다. 작고 비대칭적인 문맥 창에서 구문 부분 작업의 성능이 더 좋으며, 이는 구문 정보가 대부분 즉각적인 문맥에서 얻어지고 단어 순서에 강하게 의존할 수 있기 때문에 직관과 일치합니다. 반면, 의미 정보는 더 자주 비지역적이며, 더 큰 창 크기로 더 많이 포착됩니다.

4.5 모델 분석: 말뭉치 크기

3번 그림에서는 다른 말 유추 작업에 대한 300차원 벡터의 성능을 보여줍니다. 문법 부분 작업에서는 말뭉치 크기가 증가함에 따라 성능이 단조롭게 증가합니다. 이는 일반적으로 큰 말뭉치가 더 좋은 통계를 생성하기 때문에 예상되는 결과입니다. 흥미롭게도, 의미 부분 작업에서는 작은 위키피디아 말뭉치로 훈련된 모델이 큰 기가워드 말뭉치로 훈련된 모델보다 더 좋은 성능을 보입니다. 이는 유추 데이터셋에 있는 도시 및 국가 기반 유추의 수가 많고 위키피디아가 대부분의 해당 위치에 대해 상당히 포괄적인 기사를 가지고 있기 때문일 것입니다. 게다가, 위키피디아는

50 - 50
55 - 55
60 - 60
65 - 65
70 - 70
75 - 75
80 - 80
85 - 85
Overall Syntactic Semantic - 전반적인 구문 의미

위키2010 1B 토큰
정확도 [
% ]

위키2014 1.6B 토큰 기가워드5 4.3B 토큰 기가워드5 + 위키2014 6B 토큰 커먼 크롤 42B 토큰
그림 3: 다른 말뭉치에서 훈련된 300-차원 벡터의 유추 작업 정확도.

항목들은 새로운 지식을 흡수하기 위해 업데이트됩니다,
반면에 Gigaword는 오래된 정보이며 가능성 있게는 부정확한 정보를 담고 있는 고정된 뉴스 저장소입니다.

4.6 모델 분석: 실행 시간

총 실행 시간은 X를 채우는 작업과 모델을 훈련하는 작업으로 나뉩니다. 전자는 창 크기, 어휘 크기 및 말뭉치 크기를 포함한 여러 요소에 따라 달라집니다. 우리는 그렇게 하지 않았지만, 이 단계는 여러 대의 컴퓨터에서 쉽게 병렬화될 수 있습니다 (예: Lebret 및 Collobert (2014)의 일부 벤치마크 참조). 2.1GHz 인텔 제온 E5-2658 이중 쓰레드 기계의 단일 쓰레드를 사용하여 10 단어 대칭 컨텍스트 창, 40만 단어 어휘 및 60억 토큰 말뭉치로 X를 채우는 데 약 85분이 소요됩니다. X가 주어진 경우, 모델을 훈련하는 데 걸리는 시간은 벡터 크기와 반복 횟수에 따라 달라집니다. 위의 설정과 300차원 벡터를 사용하고 (위의 기계의 32개 코어를 모두 사용하여) 단일 반복에는 약 14분이 소요됩니다. 학습 곡선은 그림 4를 참조하십시오.

4.7 모델 분석: word2vec과의 비교

GloVe와 word2vec의 엄격한 양적 비교는 성능에 강한 영향을 미치는 많은 매개변수의 존재로 인해 복잡하다. 우리는 4.4절과 4.5절에서 확인한 주요 변동 요소를 제어하기 위해 이전 소절에서 언급된 구성으로 벡터 길이, 문맥 창 크기, 말뭉치 및 어휘 크기를 설정함으로써 제어한다.
제어해야 할 가장 중요한 나머지 변수는 훈련 시간이다. GloVe의 경우, 관련 매개변수는 훈련 반복 횟수이다. word2vec의 경우, 명백한 선택은 훈련 에포크의 횟수일 것이다. 불행하게도, 현재 코드는 단일 에포크만을 위해 설계되었다.

1540 - 천 오백 사십
62 - 육십 이
64 - 육십 사
66 68 - 육십 육 육십 팔
70 - 칠십
72 - 칠십 이

5 10 15 20 25

135710 15 20 25 30 40 50
정확도 [ % ]

반복 (GloVe)

부정적인 샘플 (CBOW)
훈련 시간 (시간)

글로브 CBOW

(a) GloVe 대 CBOW

3 6 9 12 15 18 21 24

60 - 육십
62 - 육십이
64 - 육십사
66 - 육십육
68 - 육십팔
70 - 칠십
72 - 칠십이

20  40  60  80 100

1 2 3 4 5 6 7 10 12 15 20

글로브
스킵-그램
정확도 [ % ]

반복 (GloVe)

부정적인 샘플 (Skip-Gram)
훈련 시간 (시간)

(b) GloVe 대 Skip-Gram

그림 4: 훈련 시간에 따른 단어 유추 작업의 전반적인 정확도, 이는 GloVe의 반복 횟수와 CBOW의 부정적인 샘플 수에 의해 결정된다 (a)와 skip-gram (b). 모든 경우에 대해, 우리는 동일한 6B 토큰 말뭉치 (위키피디아 2014 + 기가워드 5)에서 300차원 벡터를 훈련시키고, 동일한 400,000 단어 어휘를 사용하며, 대칭적인 크기 10의 문맥 창을 사용한다.

단일 데이터 통과에 특정된 학습 일정을 지정하여, 여러 번의 통과에 대한 수정을 어렵게 만듭니다. 다른 선택은 부정적인 샘플의 수를 다양화하는 것입니다. 부정적인 샘플을 추가하면 모델이 본 훈련 단어의 수가 효과적으로 증가하므로, 어떤 면에서는 추가적인 epoch와 유사합니다.
우리는 지정되지 않은 매개변수를 기본값으로 설정하고, 그들이 최적에 가까운 것으로 가정하며, 더 철저한 분석에서는 이 단순화를 완화해야 한다는 것을 인정합니다.
그림 4에서는 훈련 시간에 따른 유추 작업의 전반적인 성능을 그래프로 나타냅니다. 아래쪽의 두 개의 x축은 GloVe와 word2vec의 해당 훈련 반복 횟수와 부정적인 샘플의 수를 나타냅니다. 우리는 word2vec의 성능이 실제로 부정적인 샘플의 수가 약 10개를 초과하면 감소한다는 것을 알 수 있습니다. 아마도 이는 부정적인 샘플링 방법이 목표 확률 분포를 잘 근사하지 못하기 때문일 것입니다.
동일한 말뭉치, 어휘, 윈도우 크기 및 훈련 시간에 대해 GloVe가 일관되게 word2vec보다 우수한 성능을 발휘합니다. 더 빠르고 또한 속도와 관계없이 최상의 결과를 얻습니다.

5 결론

최근에는 분포 단어 표현이 계수 기반으로 가장 잘 배워지는지에 대한 문제에 상당한 관심이 집중되었습니다.

반면에, 노이즈-대조 추정은 음수 샘플이 더 많을수록 개선되는 근사 방법입니다. (Mnih et al., 2013)의 표 1에서 유추 작업의 정확도는 음수 샘플의 수에 비례하여 증가하는 함수입니다.

방법은 두 가지로 나뉘는데, 하나는 카운트 기반 방법이고 다른 하나는 예측 기반 방법이다. 현재로서는 예측 기반 모델이 상당한 지지를 받고 있는데, 예를 들어 Baroni et al. (2014)은 이러한 모델이 다양한 작업에서 더 좋은 성능을 보인다고 주장한다. 이 연구에서는 두 가지 방법이 근본적인 수준에서 크게 다르지 않다고 주장한다. 왜냐하면 두 방법 모두 말뭉치의 공기 통계를 조사하기 때문이다. 그러나 카운트 기반 방법이 전역 통계를 얼마나 효율적으로 포착하는지는 유리할 수 있다. 우리는 이러한 카운트 데이터의 주요 이점을 활용하면서도 word2vec과 같은 최근의 로그-이중 선형 예측 기반 방법에서 흔히 볼 수 있는 의미 있는 선형 하위 구조를 동시에 포착하는 모델을 구축한다. 그 결과로 GloVe는 단어 표현의 비지도 학습을 위한 새로운 전역 로그-이중 회귀 모델로, 단어 유추, 단어 유사도 및 개체명 인식 작업에서 다른 모델보다 우수한 성능을 보인다.

감사의 말씀

우리는 익명의 심사위원들께 소중한 의견에 감사드립니다. 스탠포드 대학은 국방 위협 감소 기관(DTRA)의 지원에 감사드립니다. 이는 공군 연구소(AFRL) 계약 번호 FA8650-10-C-7020과 국방 고급 연구 기획국(DARPA)의 DEFT 프로그램에 대한 AFRL 계약 번호 FA8750-13-2-0040을 포함합니다. 이 자료에 표현된 의견, 발견, 결론 또는 권장사항은 저자들의 것이며 DTRA, AFRL, DEFT 또는 미국 정부의 견해를 반영하는 것은 아닙니다.

1541
수론. 해석적 수론 소개.

마르코 바로니, 조지아나 디누, 그리고 게르만 크루셰프스키. 2014년. 세지 않고 예측하라! 문맥-세기 대 문맥-예측 의미 벡터의 체계적 비교. ACL에서.

요슈아 벤지오. 2009. 인공지능을 위한 딥 아키텍처 학습. 기계 학습의 기초와 동향.

요슈아 벵지오, 레장 뒤샤르무, 파스칼 빈센트, 그리고 크리스찬 잔빈. 2003년. 신경망 확률 언어 모델. JMLR, 3:1137-1155.

존 A. 불리나리아와 조셉 P. 레비. 2007. 단어 공존 통계에서 의미적 표현 추출: 컴퓨터 연구. 행동 연구 방법, 39(3):510–526.

Dan C. Ciresan, Alessandro Giusti, Luca M. Gambardella, and Jürgen Schmidhuber. 2012년. 심층 신경망은 전자 현미경 이미지에서 신경 세포막을 분할합니다. NIPS에서, 페이지 2852-2860.

Ronan Collobert와 Jason Weston. 2008. 자연어 처리를 위한 통합 아키텍처: 다중 작업 학습을 갖춘 심층 신경망. ICML 논문집, 160-167쪽.

Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. 자연어 처리 (거의) 처음부터. JMLR, 12:2493–2537.

스콧 디어웨스터, 수잔 T. 두메이스, 조지 W. 퍼나스, 토마스 K. 랜다우어, 그리고 리처드 하시만. 1990년. 잠재 의미 분석을 통한 색인화. 미국 정보과학 협회 저널, 41.

존 두치, 엘라드 하잔, 요람 싱어. 2011년.
온라인 학습과 확률적 최적화를 위한 적응적 서브그래디언트 방법. JMLR, 12.

레브 핀켈슈타인, 에브겐리 가브릴로비치, 요시 마티아스, 에후드 리블린, 자크 솔란, 가디 울프만, 그리고 에이탄 루핀. 2001년. 검색을 맥락에 배치: 개념 재방문. 제10회 월드 와이드 웹 국제 학회 논문집, 406-414쪽. ACM.

Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. 개선하기

단어 표현은 전역 문맥과 다중 단어 프로토타입을 통해 이루어집니다. ACL에서.

R´ emi Lebret과 Ronan Collobert. 2014. Hellinger PCA를 통한 단어 임베딩. EACL에서.

오머 레비, 요아브 골드버그, 이스라엘 라마트-간. 2014. 희소하고 명시적인 단어 표현에서의 언어적 규칙성. CoNLL-2014.

케빈 룬드와 커트 버지스. 1996년. 어휘 공존에서 고차원 의미 공간 생성하기. 행동 연구 방법, 기기 및 컴퓨터, 28:203-208.

민탕 루옹, 리처드 소처, 그리고 크리스토퍼 D 매닝. 2013. 형태학을 위한 재귀 신경망을 사용한 더 나은 단어 표현. CoNLL-2013.

토마스 미콜로프, 카이 첸, 그렉 코라도, 제프리 딘. 2013a. 벡터 공간에서 단어 표현의 효율적인 추정. ICLR 워크샵 논문.

토마스 미콜로프, 일리야 숫스케버, 카이 첸, 그렉 코라도, 그리고 제프리 딘. 2013b. 단어와 구의 분산 표현과 그들의 조합성. NIPS에서, 페이지 3111-3119.

토마스 미콜로프, 웬 타우 이, 그리고 제프리 즈와이그. 2013년. 연속 공간 단어 표현에서의 언어적 규칙성. HLT-NAACL에서.

조지 A. 밀러와 월터 G. 찰스. 1991년.
의미 유사성의 문맥적 상관관계.
언어와 인지 과정, 6(1):1–28.

안드리 Mnih와 코레이 카부크추올루. 2013년.
노이즈-대조적 추정을 사용하여 효율적으로 단어 임베딩 학습하기. NIPS에서.

Douglas L. T. Rohde, Laura M. Gonnerman, 그리고 David C. Plaut. 2006. 어휘 공존에 기반한 의미 유사성의 개선된 모델. ACM 통신, 8:627-633.

허버트 루벤스타인과 존 B. 굿나프.
1965년. 동의어의 문맥적 상관관계. ACM 통신, 8(10):627-633.

파브리지오 세바스티아니. 2002. 자동 텍스트 분류에서의 기계 학습. ACM 컴퓨팅 서베이, 34:1-47.

리처드 소처, 존 바우어, 크리스토퍼 D. 매닝, 그리고 앤드류 Y. 엔지. 2013. 구성 벡터 문법을 이용한 구문 분석. ACL에서.

1542
질문에 대한 패스 검색 알고리즘의 타당성 평가에 대한 연구입니다. 정보 검색에 대한 연구 및 개발을 위한 SIGIR 컨퍼런스 논문집에서 발표되었습니다.

에릭 F. 통 김 상과 피엔 데 물더. 2003. CoNLL-2003 공유 작업 소개: 언어 독립적인 명명된 개체 인식. CoNLL-2003에서.

조셉 투리안, 레브 라티노프, 그리고 요슈아 벤지오.
2010년. 단어 표현: 반지도 학습을 위한 간단하고 일반적인 방법. ACL 논문집, 384-394쪽.

Mengqiu Wang과 Christopher D. Manning.
2013. 시퀀스 라벨링에서 비선형 딥 아키텍처의 효과. 제6회 국제 공동 언어 처리 학회(IJCNLP) 논문집에서.

1543

