효율적인 단어 표현 추정

벡터 공간

토마스 미콜로프

구글 주식회사, 캘리포니아 주 마운틴뷰
tmikolov@google.com

카이 첸

구글 주식회사, 캘리포니아 주 마운틴뷰
kaichen@google.com

그렉 코라도

구글 주식회사, 캘리포니아 주 마운틴뷰
gcorrado@google.com

제프리 딘

구글 주식회사, 캘리포니아 주 마운틴뷰
jeff@google.com

요약

우리는 매우 큰 데이터 세트에서 단어의 연속 벡터 표현을 계산하기 위한 두 가지 새로운 모델 아키텍처를 제안합니다. 이러한 표현의 품질은 단어 유사성 작업에서 측정되며, 결과는 이전에 다른 유형의 신경망을 기반으로 한 최고의 성능 기술과 비교됩니다. 우리는 계산 비용이 훨씬 낮은 상황에서 정확도의 큰 향상을 관찰합니다. 즉, 16억 단어 데이터 세트에서 고품질 단어 벡터를 학습하는 데 하루 이하의 시간이 소요됩니다. 또한, 이러한 벡터가 문법 및 의미적 단어 유사성을 측정하기 위한 테스트 세트에서 최첨단 성능을 제공하는 것을 보여줍니다.

1 소개

많은 현재 NLP 시스템과 기술은 단어를 원자 단위로 취급합니다 - 단어 간의 유사성 개념이 없으며, 단어는 어휘 목록에서 인덱스로 표현됩니다. 이 선택은 몇 가지 좋은 이유가 있습니다 - 간결성, 견고성 및 대량의 데이터에 훈련된 간단한 모델이 적은 데이터에 훈련된 복잡한 시스템보다 우수한 성능을 발휘한다는 관찰 결과입니다. 대표적인 예는 통계적 언어 모델링에 사용되는 인기 있는 N-gram 모델입니다 - 오늘날 거의 모든 사용 가능한 데이터 (수조 단어 [3])에 N-gram을 훈련시킬 수 있습니다.

그러나 많은 작업에서 간단한 기술은 한계에 도달합니다. 예를 들어, 자동 음성 인식을 위한 관련 도메인 데이터의 양은 제한적입니다 - 성능은 일반적으로 고품질의 전사된 음성 데이터의 크기에 의해 지배됩니다 (일반적으로 수백만 단어). 기계 번역에서는 많은 언어에 대한 기존 말뭉치가 몇 십억 단어 이하로만 구성되어 있습니다. 따라서 기본 기술의 단순한 확장만으로는 어떠한 중요한 진전도 이루어지지 않는 상황이 있으며, 우리는 더 고급 기술에 집중해야 합니다.

최근 기계 학습 기술의 발전으로 인해, 훨씬 더 복잡한 모델을 훈련시킬 수 있는 대용량 데이터셋이 가능해졌으며, 일반적으로 간단한 모델보다 우수한 성능을 보입니다. 아마도 가장 성공적인 개념은 단어의 분산 표현을 사용하는 것입니다 [10]. 예를 들어, 신경망 기반 언어 모델은 N-gram 모델보다 훨씬 우수한 성능을 보입니다 [1, 27, 17].

1.1 논문의 목표

than one billion words.

수십억 단어 이상으로 구성된, 단어 벡터의 차원이 50-100 사이인 데이터입니다.

우리는 최근에 제안된 기술을 사용하여 결과 벡터 표현의 품질을 측정합니다. 이 기술은 비슷한 단어들이 서로 가까이 있을 뿐만 아니라 단어들이 다양한 유사도를 가질 수 있다는 기대를 가지고 있습니다 [20]. 이는 이전에 활용 언어의 맥락에서 관찰되었습니다. 예를 들어, 명사는 여러 단어 끝을 가질 수 있으며, 원래 벡터 공간의 부분 공간에서 비슷한 단어를 검색하면 비슷한 끝을 가진 단어를 찾을 수 있습니다 [13, 14].

어떤 놀랍게도, 단어 표현의 유사성은 단순한 문법 규칙을 넘어선다는 것이 발견되었습니다. 단어 벡터에 간단한 대수 연산을 수행하는 단어 오프셋 기술을 사용하여, 예를 들어 vector(”King”) - vector(”Man”) + vector(”Woman”)은 단어 Queen의 벡터 표현과 가장 가까운 벡터를 얻는 것을 보여주었습니다 [20].

이 논문에서는 단어들 사이의 선형 규칙성을 보존하는 새로운 모델 아키텍처를 개발하여 이 벡터 연산들의 정확도를 극대화하려고 시도합니다. 우리는 문법적이고 의미적인 규칙성을 측정하기 위한 새로운 포괄적인 테스트 세트를 설계하고, 이러한 규칙성들이 높은 정확도로 학습될 수 있다는 것을 보여줍니다. 게다가, 단어 벡터의 차원과 학습 데이터의 양이 학습 시간과 정확도에 어떤 영향을 미치는지에 대해 논의합니다.

1.2 이전 작업

단어를 연속 벡터로 표현하는 것은 오랜 역사를 가지고 있다 [10, 26, 8]. 신경망 언어 모델(NNLM)을 추정하기 위한 매우 인기 있는 모델 아키텍처가 [1]에서 제안되었는데, 여기서는 선형 투영 계층과 비선형 은닉 계층을 가진 피드포워드 신경망을 사용하여 단어 벡터 표현과 통계적 언어 모델을 동시에 학습하는 방법이 사용되었다. 이 작업은 많은 다른 연구들에 이어졌다.

NNLM의 또 다른 흥미로운 아키텍처는 [13, 14]에서 제시되었습니다. 이 아키텍처에서는 단일 은닉층을 가진 신경망을 사용하여 단어 벡터를 먼저 학습합니다. 그런 다음, 이러한 단어 벡터를 사용하여 NNLM을 훈련시킵니다. 따라서, 전체 NNLM을 구축하지 않고도 단어 벡터를 학습할 수 있습니다. 이 작업에서는 이 아키텍처를 직접 확장하고, 단어 벡터를 학습하는 단계에만 초점을 맞춥니다.

나중에는 단어 벡터가 많은 NLP 응용 프로그램을 크게 개선하고 단순화하는 데 사용될 수 있다는 것이 나타났다 [4, 5, 29]. 단어 벡터의 추정 자체는 다른 모델 아키텍처를 사용하고 다양한 말뭉치에서 훈련되었다 [4, 29, 23, 19, 9], 그리고 일부 결과 단어 벡터는 향후 연구와 비교를 위해 사용할 수 있도록 제공되었다2. 그러나 우리가 알기로는, 이러한 아키텍처들은 [13]에서 제안된 것과 비교하여 훈련에 대해 상당히 더 많은 계산 비용이 들었으며, 대각선 가중치 행렬이 사용되는 로그-이중 선형 모델의 특정 버전을 제외하고는 그렇지 않다.

2개의 모델 아키텍처

단어의 연속적인 표현을 추정하기 위해 여러 가지 다른 유형의 모델이 제안되었습니다. 이에는 잘 알려진 잠재 의미 분석(Latent Semantic Analysis, LSA)과 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)도 포함됩니다. 본 논문에서는 신경망에 의해 학습된 단어의 분산 표현에 초점을 맞추었는데, 이전 연구에서 LSA보다 선형적인 규칙을 보존하는 데 훨씬 우수한 성능을 보인 것으로 나타났습니다 [20, 31]. 게다가 LDA는 대규모 데이터셋에서 계산적으로 매우 비싸지게 됩니다.

[18]와 유사하게, 다른 모델 아키텍처를 비교하기 위해 먼저 모델을 완전히 훈련하기 위해 액세스해야 하는 매개변수의 수로 모델의 계산 복잡성을 정의합니다. 그 다음, 계산 복잡성을 최소화하면서 정확도를 최대화하려고 합니다.

1. 테스트 세트는 www.fit.vutbr.cz/˜imikolov/rnnlm/word-test.v1.txt에서 이용 가능합니다.
2. http://ronan.collobert.com/senna/
http://metaoptimize.com/projects/wordreprs/
http://www.fit.vutbr.cz/˜imikolov/rnnlm/
http://ai.stanford.edu/˜ehhuang/

다음 모든 모델에 대해, 훈련 복잡도는 비례한다.

O = E × T × Q,                 (1)

E는 훈련 에포크의 수이고, T는 훈련 세트의 단어 수이며, Q는 각 모델 아키텍처에 따라 더 정의됩니다. 일반적인 선택은 E = 3-50이고 T는 10억까지입니다. 모든 모델은 확률적 경사 하강법과 역전파를 사용하여 훈련됩니다 [26].

2.1 피드포워드 신경망 언어 모델 (NNLM)

확률적 전방향 신경망 언어 모델은 [1]에서 제안되었습니다. 이 모델은 입력, 투영, 은닉 및 출력 레이어로 구성됩니다. 입력 레이어에서는 이전 N개의 단어가 1-of-V 인코딩을 사용하여 인코딩됩니다. 여기서 V는 어휘 크기입니다. 그런 다음 입력 레이어는 공유 투영 행렬을 사용하여 차원이 N × D인 투영 레이어 P로 투영됩니다. 한 번에 N개의 입력만 활성화되므로 투영 레이어의 구성은 비교적 저렴한 작업입니다.

NNLM 아키텍처는 투영층과 은닉층 사이의 계산을 위해 복잡해집니다. 투영층의 값들이 밀집되어 있기 때문입니다. N = 10을 일반적으로 선택한 경우, 투영층의 크기 (P)는 500에서 2000이 될 수 있으며, 은닉층의 크기 H는 일반적으로 500에서 1000 단위입니다. 또한, 은닉층은 어휘 전체의 단어에 대한 확률 분포를 계산하는 데 사용되며, 결과적으로 차원 V를 가진 출력층이 생성됩니다. 따라서 각 훈련 예제당 계산 복잡도는 다음과 같습니다.

Q = N × D + N × D × H + H × V,       (2)

Q = N × D + N × D × H + H × V,       (2)

지배적인 용어는 H × V입니다. 그러나 이를 피하기 위해 여러 가지 실용적인 해결책이 제안되었습니다. 소프트맥스의 계층적 버전을 사용하거나 [25, 23, 18] 표준화된 모델을 완전히 피하기 위해 훈련 중에 정규화되지 않은 모델을 사용하는 것 [4, 9] 등이 있습니다. 어휘의 이진 트리 표현을 사용하면 평가해야 하는 출력 단위 수를 log2(V) 정도로 줄일 수 있습니다. 따라서 대부분의 복잡성은 N × D × H 용어에 의해 발생합니다.

우리의 모델에서는 어휘를 Huffman 이진 트리로 표현하는 계층적 소프트맥스를 사용합니다. 이는 단어의 빈도가 신경망 언어 모델에서 클래스를 얻는 데 잘 작동한다는 이전의 관찰을 따릅니다 [16]. Huffman 트리는 빈도가 높은 단어에 짧은 이진 코드를 할당하며, 이로 인해 평가해야 할 출력 단위의 수가 더욱 줄어듭니다: 균형 이진 트리는 log2(V)개의 출력을 평가해야 하지만, Huffman 트리 기반의 계층적 소프트맥스는 약 log2(Unigram perplexity(V))개의 출력만 필요합니다. 예를 들어 어휘 크기가 백만 개의 단어인 경우, 이는 약 두 배의 평가 속도 향상을 가져옵니다. 이는 신경망 언어 모델에서 계산 병목이 N×D×H 항에 있기 때문에 중요한 속도 향상은 아니지만, 나중에 소프트맥스 정규화의 효율성에 크게 의존하는 아키텍처를 제안할 것입니다.

2.2 재귀 신경망 언어 모델 (RNNLM)

순환 신경망 기반 언어 모델은 피드포워드 NNLM의 특정한 제한을 극복하기 위해 제안되었습니다. 이 모델은 모델의 순서인 context length (N)를 지정해야 하는 필요성과 이론적으로 RNN은 얕은 신경망보다 더 복잡한 패턴을 효율적으로 표현할 수 있다는 이점을 가지고 있습니다 [15, 2]. RNN 모델은 투영 레이어가 없으며, 입력, 은닉 및 출력 레이어만 있습니다. 이 모델의 특징은 은닉 레이어와 자기 자신을 연결하는 순환 행렬입니다. 이를 통해 순환 모델은 과거의 정보를 현재 입력과 이전 시간 단계의 은닉 레이어 상태에 기반하여 업데이트된 은닉 레이어 상태로 표현할 수 있는 일종의 단기 기억을 형성할 수 있습니다.

RNN 모델의 각 훈련 예제당 복잡도는

Q = H × H + H × V,              (3)
Q = H × H + H × V,              (3)

단어 표현 D는 숨겨진 레이어 H와 동일한 차원을 가지고 있습니다. 다시 말해, H × V는 계층적 소프트맥스를 사용하여 H × log2(V)로 효율적으로 줄일 수 있습니다. 그럼에도 불구하고, 대부분의 복잡성은 H × H에서 나타납니다.

3
2.3 신경망의 병렬 훈련

거대한 데이터 세트에서 모델을 훈련하기 위해, 우리는 DistBelief [6]라는 대규모 분산 프레임워크 위에 여러 모델을 구현했습니다. 이 중에는 피드포워드 NNLM과 이 논문에서 제안된 새로운 모델도 포함됩니다. 이 프레임워크를 통해 동일한 모델의 여러 복제본을 병렬로 실행할 수 있으며, 각 복제본은 모든 매개변수를 유지하는 중앙 서버를 통해 그래디언트 업데이트를 동기화합니다. 이 병렬 훈련을 위해 우리는 Adagrad [7]라는 적응 학습률 절차를 사용한 미니 배치 비동기적 그래디언트 하강법을 사용합니다. 이 프레임워크에서는 데이터 센터의 다른 기계에서 많은 CPU 코어를 사용하는 100개 이상의 모델 복제본을 사용하는 것이 일반적입니다.

3개의 새로운 로그-선형 모델

이 섹션에서는 계산 복잡성을 최소화하려는 단어의 분산 표현을 학습하기 위한 두 가지 새로운 모델 아키텍처를 제안합니다. 이전 섹션에서 주된 관찰은 모델의 비선형 은닉층에 의해 대부분의 복잡성이 발생한다는 것이었습니다. 신경망의 매력이기는 하지만, 우리는 데이터를 정확하게 표현하지 못할 수도 있지만 훨씬 효율적으로 더 많은 데이터로 훈련될 수 있는 더 간단한 모델을 탐색하기로 결정했습니다.

새로운 아키텍처는 이전 작업 [13, 14]에서 제안된 것을 직접 따르며, 여기서는 신경망 언어 모델이 두 단계로 성공적으로 훈련될 수 있다는 것을 발견했습니다. 먼저 간단한 모델을 사용하여 연속 단어 벡터를 학습하고, 그런 다음 이러한 분산된 단어 표현 위에 N-gram NNLM을 훈련합니다. 단어 벡터 학습에 중점을 둔 후속 작업이 많이 이루어졌지만, [13]에서 제안된 접근 방식을 가장 간단한 것으로 간주합니다. 관련된 모델은 이전에도 제안되었습니다 [26, 8].

3.1 연속 Bag-of-Words 모델

첫 번째 제안된 아키텍처는 피드포워드 NNLM과 유사하며, 비선형 은닉층은 제거되고 모든 단어에 대해 프로젝션 레이어가 공유됩니다 (프로젝션 행렬만이 아닌). 따라서, 모든 단어는 동일한 위치로 투영됩니다 (그들의 벡터가 평균화됩니다). 우리는 이 아키텍처를 단어 가방 모델이라고 부르며, 단어의 순서는 프로젝션에 영향을 주지 않습니다. 또한, 우리는 미래의 단어도 사용합니다. 우리는 다음 섹션에서 소개된 작업에서 가장 좋은 성능을 얻기 위해 입력에 네 개의 미래 단어와 네 개의 과거 단어를 가진 로그-선형 분류기를 구축했습니다. 훈련 기준은 현재 (가운데) 단어를 올바르게 분류하는 것입니다. 그런 다음 훈련 복잡도는 다음과 같습니다.

Q = N × D + D × log2(V ).         (4)
이 모델을 CBOW라고 더 명시하며, 표준 단어 가방 모델과 달리 연속적인 분산 표현을 사용합니다. 모델 구조는 그림 1에 나와 있습니다. 입력과 투영 계층 사이의 가중치 행렬은 NNLM과 마찬가지로 모든 단어 위치에 대해 공유됩니다.

3.2 연속 스킵-그램 모델

두 번째 아키텍처는 CBOW와 유사하지만, 현재 단어를 문맥에 기반하여 예측하는 대신, 같은 문장에서 다른 단어를 기반으로 단어의 분류를 최대화하려고 합니다. 더 정확히 말하면, 우리는 각 현재 단어를 연속 투영 계층과 함께 로그 선형 분류기의 입력으로 사용하고, 현재 단어의 이전과 이후 일정 범위 내에서 단어를 예측합니다. 우리는 범위를 증가시키면 결과 단어 벡터의 품질이 향상되지만, 계산 복잡성도 증가한다는 것을 발견했습니다. 더 먼 단어들은 일반적으로 현재 단어와 관련이 적기 때문에, 우리는 훈련 예제에서 그 단어들을 적게 샘플링하여 덜 가중치를 부여합니다.

이 아키텍처의 훈련 복잡도는 비례한다.

Q = C × (D + D × log2(V))        (5)
Q = C × (D + D × log2(V))        (5)

C는 단어들의 최대 거리입니다. 따라서, C = 5로 선택한다면, 각 훈련 단어마다 범위 <1;C>에서 무작위로 숫자 R을 선택한 후, R개의 단어를 과거에서 선택하여 사용합니다.

4
w(t-2)

w(t+1) - 다음 시간 단계의 w
w(t-1) - 이전 시간 단계의 w

w(t+2)

w(t)
합
입력 투사 출력

입력 투사 출력

w(t-2)

w(t-1)

w(t+1)

w(t+2)

CBOW: 연속 단어 벡터 모델
Skip-gram: 스킵-그램

그림 1: 새로운 모델 아키텍처. CBOW 아키텍처는 문맥을 기반으로 현재 단어를 예측하며, Skip-gram은 현재 단어를 기준으로 주변 단어를 예측합니다.

현재 단어의 미래로부터 R개의 단어를 올바른 라벨로 사용합니다. 이를 위해서는 현재 단어를 입력으로 하고 R + R개의 단어를 각각 출력으로 하는 R × 2개의 단어 분류를 수행해야 합니다. 다음 실험에서는 C = 10을 사용합니다.

4 결과

다른 버전의 단어 벡터의 품질을 비교하기 위해 이전 논문들은 일반적으로 예시 단어와 가장 유사한 단어를 보여주는 표를 사용하여 직관적으로 이해합니다. 프랑스 단어가 이탈리아와 아마도 다른 몇 개의 나라와 유사하다는 것을 보여주는 것은 쉽지만, 이러한 벡터를 더 복잡한 유사도 작업에 적용하는 것은 훨씬 더 어렵습니다. 우리는 이전 관찰을 따라 단어 사이에 많은 다른 유사성 유형이 있을 수 있다는 것을 알 수 있습니다. 예를 들어, 단어 "큰"은 "더 큰"과 같은 의미에서 "작은"과 유사하다는 것입니다. 또 다른 유형의 관계 예시는 단어 쌍 "큰 - 가장 큰"과 "작은 - 가장 작은"입니다. 우리는 두 개의 단어 쌍이 동일한 관계를 가진다고 표시하고, "가장 큰이 큰과 같은 의미에서 작은과 유사한 단어는 무엇인가?"라고 물을 수 있습니다.

어느 정도 놀랍게도, 이러한 질문들은 단순한 대수 연산을 통해 답을 얻을 수 있습니다. 단어의 벡터 표현을 사용하여, biggest가 big과 비슷한 의미로 small과 비슷한 단어를 찾기 위해 우리는 단순히 벡터 X = vector(”biggest”)−vector(”big”)+vector(”small”)을 계산할 수 있습니다. 그런 다음, 코사인 거리에 의해 측정된 X에 가장 가까운 단어를 벡터 공간에서 찾아 질문에 대한 답으로 사용합니다 (이 과정에서 입력된 질문 단어는 버립니다). 단어 벡터가 잘 훈련되었다면, 이 방법을 사용하여 올바른 답 (단어 smallest)을 찾을 수 있습니다.

마침내, 우리는 대량의 데이터에서 고차원 단어 벡터를 훈련시킬 때, 결과적으로 얻어지는 벡터들이 도시와 그에 속하는 나라와 같은 매우 미묘한 의미적 관계에 대한 질문에 대답하는 데 사용될 수 있음을 발견했습니다. 예를 들어, 프랑스는 파리와 같이 독일은 베를린과 같습니다. 이러한 의미적 관계를 가진 단어 벡터는 기계 번역, 정보 검색 및 질문 응답 시스템과 같은 많은 기존 NLP 응용 프로그램을 개선하는 데 사용될 수 있으며, 미래에 아직 발명되지 않은 다른 응용 프로그램을 가능하게 할 수도 있습니다.

5
표 1: 시맨틱-구문 단어 관계 테스트 세트에서 다섯 가지 유형의 시맨틱 질문과 아홉 가지 유형의 구문 질문의 예시.

관계 유형 단어 쌍 1 단어 쌍 2

공통 수도 도시 아테네 그리스 올슬로 노르웨이

모든 수도 도시 아스타나 카자흐스탄 하라레 짐바브웨

통화      앙골라 콴자   이란 리알

시카고 일리노이 주 스톡턴 캘리포니아

남자-여자     형제  자매  손자  손녀

형용사 부사로 명백한 명백하게 빠른 빠르게

반대 가능한 불가능한 윤리적인 비윤리적인

비교적으로   좋은   더 큰   힘든   더 힘든

최상급    쉬운    가장 쉬운    행운    가장 행운한

생각하다 생각하는 읽다 읽는

국적 형용사 스위스 스위스 캄보디아 캄보디아인

과거형: 걷기 걸었어요, 수영하기 수영했어요.

복수형 명사: 마우스, 쥐, 달러, 달러

복수 동사   일하다   일한다   말하다   말한다

4.1 작업 설명

단어 벡터의 품질을 측정하기 위해, 우리는 다섯 가지 유형의 의미 질문과 아홉 가지 유형의 문법 질문을 포함하는 포괄적인 테스트 세트를 정의합니다. 각 카테고리에서 두 가지 예시를 테이블 1에 보여줍니다. 전체적으로, 의미 질문은 8869개이고 문법 질문은 10675개입니다. 각 카테고리의 질문은 두 단계로 생성되었습니다. 먼저, 유사한 단어 쌍의 목록이 수동으로 작성되었습니다. 그런 다음, 두 단어 쌍을 연결하여 큰 질문 목록을 형성했습니다. 예를 들어, 우리는 68개의 큰 미국 도시와 그들이 속한 주의 목록을 만들고, 무작위로 두 단어 쌍을 선택하여 약 2.5K개의 질문을 만들었습니다. 우리의 테스트 세트에는 단일 토큰 단어만 포함되어 있으며, 따라서 New York와 같은 여러 단어로 이루어진 개체는 포함되지 않습니다.

우리는 모든 질문 유형에 대한 전체적인 정확도를 평가하며, 각각의 질문 유형(의미, 구문)에 대해서도 따로 평가합니다. 질문은 위의 방법을 사용하여 계산된 벡터와 가장 가까운 단어가 질문의 정답과 정확히 일치하는 경우에만 올바르게 답변된 것으로 가정합니다. 따라서 동의어는 오답으로 간주됩니다. 이는 현재 모델이 단어 형태에 대한 입력 정보를 가지고 있지 않기 때문에 100% 정확도에 도달하는 것은 불가능할 가능성이 높다는 것을 의미합니다. 그러나 우리는 특정 응용 프로그램에 대한 단어 벡터의 유용성이 이 정확도 지표와 긍정적으로 상관관계를 가질 것이라고 믿습니다. 특히 구문적인 질문에 대해서는 단어의 구조에 대한 정보를 통합함으로써 더 나은 진전이 이루어질 수 있습니다.

4.2 정확도의 극대화

우리는 단어 벡터를 훈련하기 위해 Google 뉴스 말뭉치를 사용했습니다. 이 말뭉치에는 약 60억 개의 토큰이 포함되어 있습니다. 우리는 어휘 크기를 가장 빈도가 높은 100만 개의 단어로 제한했습니다. 분명히, 더 많은 데이터와 더 높은 차원의 단어 벡터를 사용하는 것이 정확도를 향상시킬 것으로 예상되므로 시간 제약이 있는 최적화 문제에 직면하고 있습니다. 가능한 빠른 결과를 얻기 위해 가장 빈도가 높은 3만 개의 단어로 어휘를 제한한 훈련 데이터의 하위 집합에서 훈련된 모델을 먼저 평가했습니다. 단어 벡터 차원과 훈련 데이터 양을 다르게 선택한 CBOW 아키텍처를 사용한 결과는 표 2에 나와 있습니다.

일정 시점 이후에는 차원을 더 추가하거나 더 많은 훈련 데이터를 추가하는 것이 성능 향상에 제한적인 영향을 미친다는 것을 알 수 있습니다. 따라서, 벡터 차원과 훈련 데이터 양을 함께 증가시켜야 합니다. 이 관찰은 사소해 보일 수 있지만, 현재에는 상대적으로 많은 양의 데이터로 단어 벡터를 훈련시키지만 충분한 크기로 훈련시키지 않는 것이 인기가 있음을 언급해야 합니다.

6
테이블 2: CBOW 아키텍처에서 제한된 어휘력을 가진 단어 벡터를 사용하여 시맨틱-구문적 단어 관계 테스트 세트의 하위 집합에 대한 정확도. 가장 빈도가 높은 30,000개 단어만 포함된 질문만 사용됩니다.

차원성 / 훈련 단어 24M 49M 98M 196M 391M 783M

50         13.4 15.7 18.6 19.1 22.5 23.2

50         13.4 15.7 18.6 19.1 22.5 23.2

100 19.4 23.1 27.8 28.7 33.4 32.2

300 23.2 29.2 35.3 38.6 43.7 45.9 -> 300 23.2 29.2 35.3 38.6 43.7 45.9

600 24.0 30.1 36.5 40.8 46.6 50.4 -> 600 24.0 30.1 36.5 40.8 46.6 50.4

테이블 3: 동일한 데이터로 훈련된 모델을 사용한 아키텍처 비교, 640차원 단어 벡터 사용. 정확도는 우리의 의미-문법 단어 관계 테스트 세트와 [20]의 문법 관계 테스트 세트에 보고되었습니다.

모델   의미-문법적 단어 관계 테스트 세트 MSR 단어 관련성

건축물 의미 정확도 [%] 구문 정확도 [%] 테스트 세트 [20]

RNNLM 9 36 35

NNLM         23             53             47

NNLM         23             53             47

CBOW 24 64 61

Skip-gram     55             59             56

(예: 50 - 100). 방정식 4가 주어진 경우, 훈련 데이터 양을 두 배로 늘리는 것은 벡터 크기를 두 배로 늘리는 것과 거의 동일한 계산 복잡성 증가를 가져옵니다.

표 2와 4에 보고된 실험에서는 확률적 경사 하강법과 역전파를 사용하여 3개의 훈련 에포크를 사용했습니다. 시작 학습률은 0.025로 선택하고 선형적으로 감소시켜 마지막 훈련 에포크의 끝에 0에 가까워지도록 했습니다.

4.3 모델 아키텍처 비교

먼저, 우리는 동일한 훈련 데이터와 640 차원의 단어 벡터를 사용하여 단어 벡터를 도출하기 위한 다른 모델 아키텍처를 비교합니다. 추가 실험에서는, 우리는 새로운 의미-문법 단어 관계 테스트 세트의 전체 질문 세트를 사용합니다. 즉, 30,000 어휘에 제한되지 않습니다. 또한, 우리는 단어 간의 문법적 유사성에 초점을 맞춘 [20]에서 소개된 테스트 세트의 결과도 포함합니다.

훈련 데이터는 여러 LDC 코퍼스로 구성되어 있으며 [18]에서 자세히 설명되어 있습니다 (320M 단어, 82K 어휘). 우리는 이 데이터를 사용하여 이전에 훈련된 재귀 신경망 언어 모델과 비교를 제공하기 위해 사용했습니다. 이 모델은 단일 CPU에서 약 8 주 동안 훈련되었습니다. 우리는 DistBelief 병렬 훈련을 사용하여 동일한 640개의 은닉 유닛을 가진 피드포워드 NNLM을 훈련했습니다. 이 모델은 이전 8개 단어의 히스토리를 사용합니다 (따라서 NNLM은 RNNLM보다 매개 변수가 더 많습니다. 투영 레이어의 크기는 640 × 8입니다).

표 3에서는 RNN에서 사용된 단어 벡터가 구문 질문에서 대부분 잘 작동하는 것을 볼 수 있습니다. NNLM 벡터는 RNN보다 훨씬 더 잘 작동합니다. 이는 RNNLM의 단어 벡터가 비선형 은닉층과 직접 연결되기 때문에 놀라운 것이 아닙니다. CBOW 아키텍처는 구문 작업에서 NNLM보다 더 잘 작동하며, 의미 작업에서는 거의 동일한 성능을 보입니다. 마지막으로, Skip-gram 아키텍처는 CBOW 모델보다 구문 작업에서 약간 더 나쁜 성능을 보이지만 (그래도 NNLM보다는 더 잘 작동합니다), 다른 모델들보다 의미 부분에서 훨씬 더 잘 작동합니다.

다음으로, 우리는 하나의 CPU만을 사용하여 모델을 평가하고 공개적으로 사용 가능한 단어 벡터와 결과를 비교했습니다. 비교 결과는 표 4에 제시되었습니다. CBOW 모델은 하위 집합에서 훈련되었습니다.

Geoff Zweig에게 테스트 세트를 제공해 주신 것에 감사드립니다.

7
테이블 4: 시맨틱-구문적 단어 관계 테스트 세트에서 공개된 단어 벡터와 우리 모델의 단어 벡터를 비교한 결과입니다. 전체 어휘가 사용되었습니다.

모델             벡터  훈련   정확도 [%]

차원성 단어

의미론적 문법적 총합

Collobert-Weston NNLM 50 660M 9.3 12.3 11.0
콜로베르트-웨스턴 NNLM 50 660M 9.3 12.3 11.0

Turian NNLM        50      37M    1.4    2.6   2.1

투리안 NNLM 200 37M 1.4 2.2 1.8

Mnih NNLM          50      37M    1.8    9.1   5.8

Mnih NNLM          50      37M    1.8    9.1   5.8

Mnih NNLM          100     37M    3.3    13.2  8.8

Mnih NNLM          100     37M    3.3    13.2  8.8

미코로프 RNNLM      80      320M   4.9    18.4 12.7

미코로프 RNNLM      640     320M   8.6    36.5 24.6

황 NNLM         50      990M   13.3   11.6 12.3

우리의 NNLM은 20, 6B, 12.9, 26.4, 20.3입니다.

우리의 NNLM은 50, 6B, 27.9, 55.8, 43.2입니다.

우리의 NNLM은 100, 6B, 34.2, 64.5, 50.8입니다.

CBOW 300 783M 15.5 53.1 36.1

Skip-gram 300 783M 50.0 55.9 53.3

표 5: 동일한 데이터로 3 에포크 동안 훈련된 모델과 1 에포크 동안 훈련된 모델의 비교. 정확도는 전체 의미-문법 데이터셋에 대해 보고되었습니다.

모델          벡터 훈련   정확도 [%]  훈련 시간

차원성 단어                 [일]

의미론적 문법적 총합

3 에포크 CBOW 300 783M 15.5 53.1 36.1 1

3 에포크 Skip-gram 300  783M   50.0   55.9  53.3    3

1 에포크 CBOW    300    783M   13.8   49.9  33.6   0.3

1 에포크 CBOW    300    1.6B   16.1   52.6  36.1   0.6

1 에포크 CBOW    600    783M   15.4   53.3  36.2   0.7

1 에포크 Skip-gram 300  783M   45.6   52.2  49.2    1

1 에포크 Skip-gram 300  1.6B   52.2   55.1  53.8    2

1 에포크 Skip-gram 600  783M   56.7   54.5  55.5   2.5

구글 뉴스 데이터를 약 하루 동안 처리하였으며, Skip-gram 모델의 학습 시간은 약 세 일이었습니다.

추가로 보고된 실험에서는 한 번의 훈련 에포크만 사용했습니다 (다시 말해, 학습 속도를 선형적으로 감소시켜 훈련의 끝에서 0에 가까워지도록 했습니다). 한 번의 에포크로 두 배의 데이터를 사용하여 모델을 훈련하는 것이 동일한 데이터를 세 번의 에포크 동안 반복하는 것보다 비교 가능하거나 더 좋은 결과를 제공합니다. 이는 표 5에서 확인할 수 있으며, 추가적인 소량의 속도 향상을 제공합니다.

4.4 모델의 대규모 병렬 훈련

이전에 언급한 대로, 우리는 Dis-tBelief라는 분산 프레임워크에서 다양한 모델을 구현했습니다. 아래에서는 Google News 6B 데이터 세트에서 학습된 여러 모델의 결과를 보고합니다. 이 학습에는 미니 배치 비동기 경사 하강법과 AdaGrad [7]라는 적응 학습률 절차를 사용했습니다. 우리는 훈련 중에 50에서 100개의 모델 복제본을 사용했습니다. CPU 코어의 수는

8
표 6: DistBelief 분산 프레임워크를 사용하여 훈련된 모델의 비교. 1000차원 벡터를 사용한 NNLM의 훈련은 완료하는 데 너무 오랜 시간이 걸릴 것임을 유의하십시오.

모델     벡터  훈련  정확도 [%]    훈련 시간

차원성 단어               [일 x CPU 코어]

의미론적 문법적 총합

NNLM 100 6B 34.2 64.5 50.8 14 x 180

NNLM 100 6B 34.2 64.5 50.8 14 x 180

CBOW 1000 6B 57.3 68.9 63.7 2 x 140

스킵-그램 1000 6B 66.1 65.1 65.6 2.5 x 125

테이블 7: Microsoft 문장 완성 챌린지에서 모델의 비교 및 조합.

건축     정확도 [%]

4-그램 [32]          39

평균 LSA 유사도 [32] 49

로그-이중 선형 모델 [24] 54.8

RNNLMs [19] 55.4

스킵-그램           48.0

Skip-gram + RNNLMs 58.9

데이터 센터 기계들은 다른 생산 작업들과 공유되기 때문에 예측이 어렵습니다. 또한 사용량은 상당히 변동할 수 있습니다. 분산 프레임워크의 오버헤드로 인해 CBOW 모델과 Skip-gram 모델의 CPU 사용량은 단일 기계 구현과 비교하여 서로 더 가까워집니다. 결과는 표 6에 보고되었습니다.

4.5 마이크로소프트 연구 문장 완성 챌린지

마이크로소프트 문장 완성 챌린지는 최근 언어 모델링과 기타 NLP 기술을 발전시키기 위한 과제로 소개되었습니다 [32]. 이 과제는 1040개의 문장으로 구성되어 있으며, 각 문장마다 한 단어가 빠져 있으며, 목표는 다섯 가지 합리적인 선택지 중에서 나머지 문장과 가장 일관성 있는 단어를 선택하는 것입니다. 이 데이터셋에 대한 여러 기술의 성능이 이미 보고되었는데, N-gram 모델, LSA 기반 모델 [32], 로그-이중 선형 모델 [24] 및 현재 이 벤치마크에서 최고의 성능을 보이는 순환 신경망의 조합 [19]을 포함합니다. 이 조합은 현재 55.4%의 정확도를 달성하고 있습니다.

우리는 이 작업에서 Skip-gram 아키텍처의 성능을 탐색했습니다. 먼저, [32]에서 제공된 5천만 단어로 640차원 모델을 훈련시켰습니다. 그런 다음, 입력에서 알 수 없는 단어를 사용하여 테스트 세트의 각 문장의 점수를 계산하고, 문장 내의 모든 주변 단어를 예측합니다. 최종 문장 점수는 이러한 개별 예측의 합입니다. 문장 점수를 사용하여 가장 가능성 있는 문장을 선택합니다.

표 7에는 일부 이전 결과와 새로운 결과의 간단한 요약이 제시되었습니다.
Skip-gram 모델 자체는 LSA 유사도보다 이 작업에서 성능이 우수하지 않지만, 이 모델에서 얻은 점수는 RNNLM의 점수와 보완적이며, 가중 조합을 통해 새로운 최고 성과 58.9%의 정확도를 달성하였습니다 (세트의 개발 부분에서는 59.2%, 테스트 부분에서는 58.7%).

학습된 관계의 5가지 예시

표 8은 다양한 관계를 따르는 단어들을 보여줍니다. 우리는 위에서 설명한 방법을 따릅니다: 관계는 두 단어 벡터를 빼고 그 결과를 다른 단어에 더함으로써 정의됩니다. 따라서 예를 들어, 파리 - 프랑스 + 이탈리아 = 로마입니다. 보시다시피 정확도는 꽤 좋지만, 더 개선할 여지가 분명히 있습니다 (우리의 정확도 측정 기준을 사용하는 것에 유의하세요.

9
표 8: 테이블 4의 최상의 단어 벡터를 사용한 단어 쌍 관계의 예시 (300 차원으로 783M 단어를 학습한 Skip-gram 모델).

관계    예시 1    예시 2    예시 3

프랑스 - 파리 이탈리아: 로마 일본: 도쿄 플로리다: 탤라하시

큰 - 더 큰 작은: 더 큰 차가운: 더 차가운 빠른: 더 빠른

마이애미 - 플로리다
바르티모어 - 메릴랜드
달라스 - 텍사스
코나 - 하와이

아인슈타인 - 과학자 메시: 중앙 미드필더 모차르트: 바이올리니스트 피카소: 화가

사르코지 - 프랑스 베를루스코니: 이탈리아 멀켈: 독일 코이즈미: 일본

구리 - Cu    아연: Zn    금: Au    우라늄: 플루토늄

베를루스코니 - 실비오 사르코지: 니콜라 푸틴: 메드베데프 오바마: 바락

마이크로소프트 - 윈도우 구글: 안드로이드 IBM: 리눅스 애플: 아이폰

마이크로소프트 - 볼머 구글: 야후 IBM: 맥널리 애플: 잡스

일본 - 스시 독일 - 브라트부르스트 프랑스 - 타파스 미국 - 피자

정확한 일치를 가정하면, 표 8의 결과는 약 60%만 점수를 받을 것입니다. 우리는 더 큰 데이터 세트와 더 큰 차원을 가진 단어 벡터가 훨씬 더 좋은 성능을 발휘할 것이라고 믿습니다. 이는 새로운 혁신적인 응용 프로그램의 개발을 가능하게 할 것입니다. 정확도를 향상시키는 또 다른 방법은 관계의 예시를 하나 이상 제공하는 것입니다. 관계 벡터를 형성하기 위해 하나 대신 열 개의 예시를 사용함으로써 (개별 벡터를 평균화합니다), 우리는 우리의 최고 모델의 정확도를 의미적-문법적 테스트에서 약 10% 절대적으로 향상시켰습니다.

벡터 연산을 사용하여 다른 작업을 해결하는 것도 가능합니다. 예를 들어, 우리는 단어 목록의 평균 벡터를 계산하고 가장 먼 단어 벡터를 찾음으로써 목록에 없는 단어를 선택하는 데에 좋은 정확도를 관찰했습니다. 이는 특정 인간 지능 테스트에서 인기 있는 유형의 문제입니다. 분명히, 이러한 기술을 사용하여 아직 많은 발견이 이루어져야 합니다.

6 결론

이 논문에서는 다양한 모델에서 파생된 단어 벡터 표현의 품질을 구문 및 의미 언어 작업 모음에서 연구했습니다. 우리는 인기있는 신경망 모델 (피드포워드 및 순환)과 비교하여 매우 간단한 모델 아키텍처를 사용하여 고품질의 단어 벡터를 훈련시킬 수 있다는 것을 관찰했습니다. 훨씬 낮은 계산 복잡성으로 인해 훨씬 더 큰 데이터 세트에서 매우 정확한 고차원 단어 벡터를 계산할 수 있습니다. DistBelief 분산 프레임워크를 사용하면 어휘 크기에 거의 제한이 없는 1조 단어의 말뭉치에서도 CBOW 및 Skip-gram 모델을 훈련시킬 수 있습니다. 이는 이전에 유사한 모델에 대해 발표된 최고 결과보다 수십 배 큽니다.

최근에 단어 벡터가 이전의 최첨단 기술을 크게 능가하는 것으로 입증된 흥미로운 작업은 SemEval-2012 Task 2입니다. 공개된 RNN 벡터는 다른 기술과 함께 사용되어 이전 최고 결과에 비해 스피어만의 순위 상관관계에서 50% 이상의 증가를 달성했습니다. 신경망 기반의 단어 벡터는 이전에도 많은 자연어 처리 작업에 적용되었으며, 예를 들어 감성 분석 및 유사 문장 감지에 사용되었습니다. 이 논문에서 설명된 모델 아키텍처가 이러한 응용 프로그램에 도움이 될 것으로 기대됩니다.

우리의 지속적인 작업은 단어 벡터가 지식 베이스의 사실 자동 확장에 성공적으로 적용될 수 있음을 보여주고, 또한 기존 사실의 정확성 검증에도 사용될 수 있음을 보여줍니다. 기계 번역 실험 결과도 매우 유망합니다. 또한, 우리의 기술을 잠재 관계 분석 [30] 등과 비교하는 것도 흥미로울 것입니다. 우리는 포괄적인 테스트 세트가 연구 커뮤니티가 단어 벡터를 추정하는 기존 기술을 개선하는 데 도움이 될 것이라고 믿습니다. 또한, 고품질의 단어 벡터가 미래의 NLP 응용 프로그램에 중요한 구성 요소가 될 것으로 기대합니다.

10
7 후속 작업

이 논문의 초기 버전 작성 후, 우리는 연속 단어 가방 모델과 스킵-그램 아키텍처를 사용하여 단일 기계 다중 스레드 C++ 코드를 게시했습니다. 훈련 속도는 이 논문에서 이전에 보고된 것보다 상당히 높으며, 일반적인 하이퍼파라미터 선택에 따라 시간당 수십억 단어의 속도로 진행됩니다. 또한, 우리는 1000억 단어 이상으로 훈련된 명명된 개체를 나타내는 140만 개 이상의 벡터를 게시했습니다. 우리의 후속 연구 중 일부는 예정된 NIPS 2013 논문 [21]에 게시될 것입니다.

참고문헌

[1] Y. Bengio, R. Ducharme, P. Vincent. 신경망 확률 언어 모델. 기계 학습 연구 저널, 3:1137-1155, 2003.

[2] Y. Bengio, Y. LeCun. 인공지능을 향한 학습 알고리즘의 확장. In: 대규모 커널 머신, MIT Press, 2007.

[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 기계 번역에서의 대형 언어 모델. 2007년 Empirical Methods in Natural Language Processing and Computational Language Learning 합동 회의 논문집.

[4] R. Collobert와 J. Weston. 자연어 처리를 위한 통합 아키텍처: 다중 작업 학습을 하는 딥 신경망. 국제 기계 학습 회의(ICML), 2008년.

[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. 자연어 처리 (거의) 제로부터. 기계 학습 연구 저널, 12:2493-2537, 2011.

[6] J. 딘, G.S. 코라도, R. 몽가, K. 첸, M. 데빈, Q.V. 레, M.Z. 마오, M.A. 란자토, A. 세니어, P. 터커, K. 양, A. Y. 엔지., 대규모 분산 딥 네트워크, NIPS, 2012.

[7] J.C. Duchi, E. Hazan, and Y. Singer. 온라인 학습과 확률적 최적화를 위한 적응형 서브그래디언트 방법. 기계 학습 연구 저널, 2011.

[8] J. 엘만. 시간 속에서 구조를 찾기. 인지 과학, 14, 179-211, 1990.

[9] 에릭 H. 황, R. 소처, C. D. 매닝, 그리고 앤드류 Y. 엔그. 전역 문맥과 다중 단어 프로토타입을 통한 단어 표현 개선. 발표: 계산 언어학 협회, 2012.

[10] G.E. 힌턴, J.L. 맥클렐랜드, D.E. 루멜하트. 분산 표현. 병렬 분산 처리: 인지의 미세 구조 탐구. 1권: 기초, MIT 출판사, 1986년.

[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 과제 2: 관계 유사도의 정도 측정. 제6회 국제 시맨틱 평가 워크샵 논문집 (SemEval 2012)에서, 2012년.

[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. 감성 분석을 위한 단어 벡터 학습. ACL 논문집, 2011.

[13] T. Mikolov. 언어 모델링을 통한 체코어 음성 인식, 석사 학위 논문, 브르노 공과대학교, 2007년.

[14] T. Mikolov, J. Kopeck´ y, L. Burget, O. Glembek and J. ˇ Cernock´ y. 2009년 ICASSP 학회 논문집에서 발표된 고도로 변형 언어를 위한 신경망 기반 언어 모델입니다.

[15] T. Mikolov, M. Karafi´ at, L. Burget, J. ˇ Cernock´ y, S. Khudanpur. 재귀 신경망 기반 언어 모델, Interspeech 논문집, 2010.

[16] T.Mikolov, S.Kombrink, L.Burget, J. ˇ Cernock´ y, S.Khudanpur. 재귀 신경망 언어 모델의 확장, ICASSP 2011 논문집에서 발표.

[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇ Cernock´ y. Empirical Evaluation and Com- bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.
[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. ˇ Cernock´ y. 고급 언어 모델링 기술의 경험적 평가 및 결합, Interspeech 논문집, 2011년.

코드는 https://code.google.com/p/word2vec/에서 사용 가능합니다.

11
[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. ˇ Cernock´ y. 대규모 신경망 언어 모델 훈련을 위한 전략, 자동 음성 인식 및 이해 프로시딩, 2011.

[19] T. Mikolov. 신경망 기반의 통계 언어 모델. 박사학위 논문, 브르노 공과대학교, 2012.

[20] T. Mikolov, W.T. Yih, G. Zweig. 연속 공간 단어 표현에서의 언어적 규칙성. NAACL HLT 2013.

[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. 단어와 구문의 분산 표현과 그들의 합성에 대한 연구. NIPS 2013에 승인됨.

[22] A. Mnih, G. Hinton. 통계 언어 모델링을 위한 세 가지 새로운 그래픽 모델. ICML, 2007.

[23] A. Mnih, G. Hinton. 확장 가능한 계층 분산 언어 모델. 신경 정보 처리 시스템 21에서, MIT Press, 2009.

[24] A. Mnih, Y.W. Teh. 신경 확률 언어 모델을 훈련하기 위한 빠르고 간단한 알고리즘. ICML, 2012.

[25] F. Morin, Y. Bengio. 계층적 확률적 신경망 언어 모델. AISTATS, 2005.

[26] D. E. 루멜하트, G. E. 힌튼, R. J. 윌리엄스. 오류를 역전파하여 내부 표현 학습하기. 자연, 323:533-536, 1986.

[27] H. Schwenk. 연속 공간 언어 모델. 컴퓨터 음성 및 언어, 제 21권, 2007년.

[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.

[28] R. Socher, E.H. 황, J. Pennington, A.Y. Ng, 그리고 C.D. Manning. 동적 풀링 및 펼침 재귀 오토인코더를 사용한 동의어 감지. NIPS에서, 2011년.

[29] J. Turian, L. Ratinov, Y. Bengio. 단어 표현: 준지도 학습을 위한 간단하고 일반적인 방법. Proc. Association for Computational Linguistics, 2010.

[30] P. D. Turney. 잠재 관계 분석을 통한 의미 유사도 측정. In: 인공지능 국제 합동 회의 논문집, 2005.

[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. 관계 유사도 측정을 위한 이질적 모델의 결합. NAACL HLT 2013.

[32] G. Zweig, C.J.C. Burges. 마이크로소프트 연구 문장 완성 챌린지, 마이크로소프트 연구 기술 보고서 MSR-TR-2011-129, 2011.

12

