깊은 문맥화된 단어 표현

매튜 E. 피터스†, 마크 뉴만†, 모히트 이예†, 매트 가드너†,
{matthewp, markn, mohiti, mattg}@allenai.org

크리스토퍼 클락∗, 켄튼 리∗, 루크 제틀모이어†∗
{csquared,kentonl,lsz}@cs.washington.edu

†인공지능 앨런 연구소
∗워싱턴 대학교 폴 G. 앨런 컴퓨터 과학 및 공학 학부

요약

우리는 새로운 유형의 깊은 문맥화된 단어 표현을 소개합니다. 이 표현은 (1) 단어 사용의 복잡한 특성 (예: 구문과 의미)과 (2) 언어적 맥락에서 이러한 사용이 어떻게 다양한지 (즉, 다의성을 모델링하기 위해) 모델링합니다. 우리의 단어 벡터는 대규모 텍스트 말뭉치에서 사전 훈련된 깊은 양방향 언어 모델 (biLM)의 내부 상태의 학습된 함수입니다. 우리는 이러한 표현이 기존 모델에 쉽게 추가될 수 있으며, 질문 응답, 텍스트 함의 및 감성 분석을 포함한 여섯 가지 어려운 자연어 처리 문제에서 기술 수준을 크게 향상시킬 수 있다는 것을 보여줍니다. 또한, 사전 훈련된 네트워크의 깊은 내부를 노출하는 것이 중요하며, 하류 모델이 다양한 유형의 반지도 학습 신호를 혼합할 수 있도록 해준다는 분석 결과도 제시합니다.

1 소개

사전 훈련된 단어 표현 (Mikolov et al.,
2013; Pennington et al., 2014)은 많은 신경망 언어 이해 모델에서 중요한 구성 요소입니다. 그러나 고품질의 표현을 학습하는 것은 어려울 수 있습니다. 이들은 이상적으로는 (1) 단어 사용의 복잡한 특성 (예: 구문과 의미)과 (2) 이러한 사용이 언어적 맥락에서 어떻게 다양한지 (즉, 다의성을 모델링하기 위해) 모델링해야 합니다. 이 논문에서는 이러한 어려움을 직접 해결하는 새로운 유형의 깊은 문맥화된 단어 표현을 소개하며, 기존 모델에 쉽게 통합할 수 있으며, 다양한 언어 이해 문제에서 고려된 모든 경우에서 최신 기술을 크게 개선합니다.
우리의 표현은 각 토큰에 대해 전체 입력 문장의 함수로 할당되는 전통적인 단어 유형 임베딩과 다릅니다. 우리는 양방향 LSTM에서 유도된 벡터를 사용하며, 이는 결합된 언어 모델과 함께 훈련됩니다.

대규모 텍스트 코퍼스에서 언어 모델 (LM) 목표를 달성합니다. 이러한 이유로 우리는 그들을 ELMo (언어 모델로부터 임베딩) 표현이라고 부릅니다. 이전의 문맥화된 단어 벡터 학습 방법과는 달리, ELMo 표현은 깊습니다. 즉, 이는 biLM의 모든 내부 레이어의 함수입니다. 더 구체적으로 말하면, 각 입력 단어 위에 쌓인 벡터들의 선형 조합을 학습하여 각 최종 작업에 대해 성능을 현저히 향상시킵니다. 이러한 방식으로 내부 상태를 결합함으로써 매우 풍부한 단어 표현이 가능해집니다. 내재적 평가를 통해, 우리는 상위 수준의 LSTM 상태가 단어 의미의 문맥 종속적인 측면을 포착한다는 것을 보여줍니다 (예: 수정 없이 지도 학습 단어 의미 해소 작업에서 잘 수행될 수 있음). 한편, 하위 수준의 상태는 구문의 측면을 모델링합니다 (예: 품사 태깅을 수행하는 데 사용될 수 있음). 이러한 신호들을 동시에 노출시킴으로써 학습된 모델이 각 최종 작업에 가장 유용한 준지도 학습 유형을 선택할 수 있게 됩니다. 광범위한 실험 결과, ELMo 표현이 실제로 매우 잘 작동함을 보여줍니다. 우리는 먼저 텍스트 의미론, 질문 응답 및 감성 분석을 포함한 여섯 가지 다양하고 도전적인 언어 이해 문제에 기존 모델에 쉽게 추가될 수 있음을 보여줍니다. ELMo 표현만으로도 모든 경우에서 최첨단 기술을 현저히 향상시킵니다. 상호 비교가 가능한 작업에서는, ELMo는 신경 기계 번역 인코더를 사용하여 문맥화된 표현을 계산하는 CoVe (McCann et al., 2017)보다 우수한 성능을 보입니다. 마지막으로, ELMo와 CoVe의 분석 결과, 깊은 표현이 LSTM의 최상위 레이어만으로 유도된 표현보다 우수한 성능을 보입니다. 우리의 훈련된 모델과 코드는 공개적으로 사용 가능하며, ELMo가 많은 다른 NLP 문제에 유사한 이득을 제공할 것으로 기대합니다.

2 관련 연구

대규모 미분류 텍스트에서 단어의 문법 및 의미 정보를 포착할 수 있는 능력으로 인해, 사전 훈련된 단어 벡터(Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014)는 질문 응답(Liu et al., 2017), 텍스트 함의(Chen et al., 2017) 및 의미 역할 라벨링(He et al., 2017)을 포함한 대부분의 최신 NLP 아키텍처의 표준 구성 요소입니다. 그러나 이러한 단어 벡터 학습 접근 방식은 각 단어에 대해 단일 문맥-독립적 표현만 허용합니다.
이전에 제안된 방법은 전통적인 단어 벡터의 일부 한계를 극복하기 위해 부분 단어 정보로 보강하는 방법(e.g., Wieting et al., 2016; Bojanowski et al., 2017)이나 각 단어 의미에 대해 별도의 벡터를 학습하는 방법(e.g., Neelakantan et al., 2014)을 사용합니다. 우리의 접근 방식은 문자 컨볼루션을 통해 부분 단어 단위의 이점을 얻으며, 사전 정의된 의미 클래스를 예측하기 위한 명시적인 훈련 없이 다중 의미 정보를 하위 작업에 매끄럽게 통합합니다.
최근 다른 연구들은 문맥-의존적 표현 학습에도 초점을 맞추고 있습니다. context2vec(Melamud et al., 2016)는 중심 단어 주변의 문맥을 인코딩하기 위해 양방향 LSTM(Long Short Term Memory; Hochreiter and Schmidhuber, 1997)을 사용합니다. 문맥적 임베딩 학습을 위한 다른 접근 방식은 표현에 중심 단어 자체를 포함하며, 이는 지도형 신경 기계 번역(MT) 시스템(CoVe; McCann et al., 2017) 또는 비지도 언어 모델(Peters et al., 2017)의 인코더로 계산됩니다. 이러한 접근 방식은 대용량 데이터셋의 이점을 얻지만, MT 접근 방식은 병렬 말뭉치의 크기에 제한이 있습니다. 본 논문에서는 풍부한 단일 언어 데이터에 대한 완전한 이용을 하며, 약 3000만 개의 문장으로 구성된 말뭉치에서 biLM을 훈련시킵니다(Chelba et al., 2014). 또한 이러한 접근 방식을 다양한 NLP 작업에 효과적으로 적용되는 깊은 문맥적 표현으로 일반화합니다.

1. http://allennlp.org/elmo
http://allennlp.org/elmo

이전 연구에서는 깊은 biRNN의 다른 레이어가 다른 유형의 정보를 인코딩한다는 것을 보여주었습니다. 예를 들어, 깊은 LSTM의 하위 레벨에서 다중 작업 구문 감독 (예: 품사 태그)을 도입하면 상위 레벨 작업인 의존 구문 분석 (Hashimoto et al., 2017) 또는 CCG 슈퍼 태깅 (Søgaard and Goldberg, 2016)과 같은 전반적인 성능을 향상시킬 수 있습니다. RNN 기반의 인코더-디코더 기계 번역 시스템에서 Belinkov et al. (2017)은 2층 LSTM 인코더의 첫 번째 레이어에서 학습된 표현이 POS 태그를 예측하는 데 더 우수하다는 것을 보였습니다. 마지막으로, 단어 문맥을 인코딩하기 위한 LSTM의 최상위 레이어 (Melamud et al., 2016)는 단어 의미의 표현을 학습하는 것으로 나타났습니다. 우리는 ELMo 표현의 수정된 언어 모델 목적에 의해 유사한 신호가 유도되며, 이러한 다른 유형의 반지도 학습을 혼합하는 하류 작업을 위한 모델을 학습하는 데 매우 유익할 수 있다는 것을 보여줍니다.

Dai와 Le (2015) 및 Ramachandran 등 (2017)은 언어 모델과 시퀀스 오토인코더를 사용하여 인코더-디코더 쌍을 사전 훈련시키고, 그 후에 과제별 지도 학습으로 세부 조정합니다. 대조적으로, 미지의 데이터로 biLM을 사전 훈련한 후에는 가중치를 고정하고 추가적인 과제별 모델 용량을 추가하여, 하류 훈련 데이터 크기가 작은 지도 모델에 대해 크고 풍부하며 범용적인 biLM 표현을 활용할 수 있습니다.

3 ELMo: 언어 모델로부터의 임베딩

대부분의 널리 사용되는 단어 임베딩과 달리, ELMo 단어 표현은 이 섹션에서 설명한 대로 전체 입력 문장의 함수입니다. 이는 문자 컨벌루션을 사용하는 두 개의 층으로 구성된 양방향 언어 모델 (Sec. 3.1)의 내부 네트워크 상태의 선형 함수로 계산됩니다 (Sec. 3.2). 이 설정을 통해 우리는 준지도 학습을 수행할 수 있으며, 양방향 언어 모델은 대규모로 사전 훈련되고 (Sec. 3.4) 기존의 다양한 신경망 NLP 아키텍처에 쉽게 통합될 수 있습니다 (Sec. 3.3).

3.1 양방향 언어 모델

주어진 N개의 토큰(t 1,t 2,...,t N)으로 이루어진 시퀀스가 있을 때, 전방향 언어 모델은 토큰 t k가 주어진 히스토리 (t 1,...,t k−1)에 대해 발생할 확률을 모델링하여 시퀀스의 확률을 계산합니다.

p(t 1,t 2,...,t N) = t 1,t 2,...,t N에 대한 확률 분포 =

N
(cid:89)
k=1p(t k | t 1,t 2,...,t k−1).

N
(cid:89)
k=1p(t k | t 1,t 2,...,t k−1).

최근의 최첨단 신경 언어 모델(J´ ozefowicz et al., 2016; Melis et al., 2017; Merity et al., 2017)은 문맥에 독립적인 토큰 표현 xLM k(토큰 임베딩 또는 문자의 CNN을 통해)을 계산한 후 L개의 순방향 LSTM 레이어를 통과시킵니다. 각 위치 k에서 각 LSTM 레이어는 문맥에 의존하는 표현 - → hLM k,j를 출력합니다. 여기서 j = 1,...,L입니다. 최상위 레이어는

LSTM 출력, hLMk,L,은 Softmax 레이어를 사용하여 다음 토큰 tk+1을 예측하는 데 사용됩니다.
역방향 LM은 순방향 LM과 유사하지만, 시퀀스를 역순으로 실행하여 미래 문맥을 고려하여 이전 토큰을 예측합니다.

p(t 1,t 2,...,t N) = t 1,t 2,...,t N에 대한 확률 분포 =

N (cid:89)
k=1p(t
k
| t k+1,t k+2,...,t N).

N (cid:89)
k=1p(t
k
| t k+1,t k+2,...,t N).

이는 전방향 LM과 유사한 방식으로 구현될 수 있으며, L개의 층으로 이루어진 역방향 LSTM 레이어 j는 (t k+1,...,t N)가 주어졌을 때 t k의 표현인 hLM k,j를 생성합니다.
biLM은 전방향과 역방향 LM을 결합합니다. 우리의 공식은 전방향과 역방향 방향의 로그 우도를 동시에 최대화합니다.

N
k=1( logp(t k | t 1,...,t k−1;Θ

x,− →
Θ LSTM,Θ s)

+logp(t_k | t_k+1,...,t_N;Θ_x,←−Θ_LSTM,Θ_s) ).




3.2 ELMo

ELMo는 biLM의 중간 레이어 표현의 작업 특정 조합입니다.

각 토큰 t k에 대해, L-레이어 biLM은 2L + 1개의 표현을 계산합니다.

R k = {xLM k,− → hLM k,j,←− hLM k,j | j = 1,...,L} = {hLM k,j | j = 0,...,L}

어디에 hLM이 있는지
k,0
토큰 레이어인지 hLM이 있는 곳은 어디인지

k,j = [− → hLM k,j ;←− hLM k,j], 각각의 biLSTM 레이어에 대해.
하류 모델에 포함하기 위해, ELMo는 R의 모든 레이어를 하나의 벡터로 축소합니다.
ELMo k = E(R k;Θ e). 가장 간단한 경우에는,
ELMo는 단순히 최상위 레이어를 선택합니다. E(R k) = hLM

k, L,
TagLM (Peters et al., 2017)과 CoVe (Mc-
Cann et al., 2017)와 같이. 보다 일반적으로, 우리는 모든 biLM 레이어에 대한 작업 특정 가중치를 계산합니다.

ELMotask
k
= E(R k;Θtask) = γtask

L

j=0
j는 0이다.

stask
stask

j
j

hLM
hLM

k,j
k,j

.

(1)
(1)에서 stask는 소프트맥스 정규화된 가중치이며,
스칼라 파라미터 γtask는 태스크 모델이 전체 ELMo 벡터를 조정할 수 있게 합니다. γ는 최적화 과정을 돕는 데 실용적으로 중요합니다 (자세한 내용은 부록을 참조하십시오). 각 biLM 레이어의 활성화는 서로 다른 분포를 가지고 있으므로, 경우에 따라 가중치를 적용하기 전에 각 biLM 레이어에 레이어 정규화(Ba et al., 2016)를 적용하는 것도 도움이 되었습니다.

3.3 지도 학습 NLP 작업에 biLM 사용하기

사전 훈련된 biLM과 대상 NLP 작업을 위한 지도 학습 아키텍처가 주어지면, biLM을 사용하여 작업 모델을 개선하는 것은 간단한 과정입니다. 우리는 단순히 biLM을 실행하고 각 단어의 모든 레이어 표현을 기록합니다. 그런 다음, 아래에 설명된 대로 최종 작업 모델이 이러한 표현의 선형 조합을 학습하도록 합니다.
먼저 biLM이 없는 지도 모델의 가장 낮은 레이어를 고려해보십시오. 대부분의 지도 NLP 모델은 가장 낮은 레이어에서 공통 아키텍처를 공유하므로 ELMo를 일관되고 통합된 방식으로 추가할 수 있습니다. 토큰(t 1,...,t N)의 시퀀스가 주어지면, 사전 훈련된 단어 임베딩과 선택적으로 문자 기반 표현을 사용하여 각 토큰 위치에 대해 문맥 독립적인 토큰 표현 x k를 형성하는 것이 표준입니다. 그런 다음, 모델은 일반적으로 양방향 RNN, CNN 또는 피드 포워드 네트워크를 사용하여 문맥에 민감한 표현 h k를 형성합니다.
지도 모델에 ELMo를 추가하기 위해, 우리는 먼저 biLM의 가중치를 고정하고 ELMo 벡터 ELMotask를 연결합니다.

k
와 함께
x k를 사용하여 ELMo 향상된 표현을 전달하십시오.
[x k;ELMotask
k
]를 작업 RNN에 전달하십시오. 일부 작업 (예 : SNLI, SQuAD)에서는 작업 RNN의 출력에도 ELMo를 포함하여 추가 개선을 관찰합니다. 이를 위해 다른 출력 특정 선형 가중치 세트를 도입하고 h k를 [h k;ELMotask
k
]로 대체합니다. 지도 모델의 나머지 부분은 변경되지 않으므로 이러한 추가 사항은 더 복잡한 신경망 모델의 맥락에서 발생할 수 있습니다. 예를 들어, biLSTMs 다음에 bi-attention 레이어가 있는 SNLI 실험이나 biLSTMs 위에 클러스터링 모델이 있는 공지 대응 실험을 참조하십시오.
마지막으로, ELMo에 적당한 양의 드롭아웃을 추가하는 것이 유익하다는 것을 발견했습니다 (Srivastava et al., 2014) 그리고 경우에 따라 ELMo 가중치를 정규화하기 위해 손실에 λ(cid:107)w(cid:107)2 2를 추가하는 것이 유용합니다. 이렇게하면 ELMo 가중치가 모든 biLM 레이어의 평균에 가까워지도록 귀납적 편향이 부과됩니다.

3.4 사전 훈련된 양방향 언어 모델 아키텍처

이 논문에서 사용된 사전 훈련된 biLM은 J´ ozefowicz et al. (2016)와 Kim et al. (2015)의 아키텍처와 유사하지만, 양방향 학습과 LSTM 레이어 사이에 잔차 연결을 추가하여 수정되었습니다. 이 연구에서는 Peters et al. (2017)이 biLM을 사용하는 것의 중요성을 강조하였기 때문에 대규모 biLM에 초점을 맞추었습니다.
전체 언어 모델 퍼플렉서티를 균형잡기 위해 모델 크기와 계산 요구 사항을 줄이면서 순수하게 문자 기반 입력 표현을 유지하기 위해 J´ ozefowicz et al. (2016)의 단일 최상의 모델 CNN-BIG-LSTM의 임베딩 및 은닉 차원을 절반으로 줄였습니다. 최종 모델은 4096 유닛과 512 차원 투영을 가진 L = 2 biLSTM 레이어와 첫 번째 레이어에서 두 번째 레이어로의 잔차 연결을 사용합니다. 문맥에 무감각한 유형 표현은 2048 문자 n-gram 컨볼루션 필터를 사용한 후 두 개의 하이웨이 레이어 (Srivastava et al., 2015)와 512 표현으로 선형 투영을 거칩니다. 결과적으로, biLM은 순수하게 문자 입력 때문에 훈련 세트 외부의 입력 토큰에 대해 세 개의 표현 레이어를 제공합니다. 이에 반해, 전통적인 단어 임베딩 방법은 고정된 어휘에 있는 토큰에 대해 하나의 표현 레이어만 제공합니다.

1B 단어 벤치마크(Chelba et al., 2014)에서 10 에포크 동안 훈련한 후, 전방 및 후방 혼란도의 평균은 39.7이며, 전방 CNN-BIG-LSTM의 경우 30.0입니다. 일반적으로, 전방 및 후방 혼란도는 거의 동일하며, 후방 값이 약간 더 낮습니다.
미리 훈련된 양방향 언어 모델은 어떤 작업에 대한 표현을 계산할 수 있습니다. 일부 경우, 도메인 특정 데이터에서 양방향 언어 모델을 세밀하게 조정하면 혼란도가 크게 감소하고 하류 작업의 성능이 향상됩니다. 이는 양방향 언어 모델의 도메인 전이의 한 유형으로 볼 수 있습니다. 결과적으로, 대부분의 경우 하류 작업에서 세밀하게 조정된 양방향 언어 모델을 사용했습니다. 자세한 내용은 부록 자료를 참조하십시오.

4 평가

표 1은 ELMo의 성능을 다양한 6가지 벤치마크 NLP 작업에 대해 보여줍니다. 고려된 모든 작업에서 ELMo를 추가하는 것만으로도 새로운 최첨단 결과를 얻을 수 있으며, 강력한 기본 모델 대비 상대적 오차 감소는 6-20%에 이릅니다. 이는 다양한 모델 아키텍처와 언어 이해 작업에 걸쳐서 매우 일반적인 결과입니다. 이 섹션의 나머지 부분에서는 개별 작업 결과에 대한 개략적인 개요를 제공하며, 전체 실험 세부 내용은 부록 자료를 참조하십시오.

질문 응답 스탠포드 데이터셋(SQuAD) (Rajpurkar et al., 2016)은 100,000개 이상의 크라우드 소싱 질문-답변 쌍을 포함하며, 답변은 주어진 위키피디아 단락에서의 구간입니다. 우리의 기준 모델(Clark and Gardner, 2017)은 Seo et al. (BiDAF; 2017)의 양방향 어텐션 플로우 모델의 개선된 버전입니다. 양방향 어텐션 구성 요소 뒤에 self-attention 레이어를 추가하고, 일부 풀링 연산을 단순화하고, LSTMs를 게이트 순환 유닛(GRUs; Cho et al., 2014)로 대체합니다. ELMo를 기준 모델에 추가한 후, 테스트 세트 F1 점수는 81.1%에서 85.8%로 4.7% 향상되었으며, 기준 모델 대비 상대적 오차 감소는 24.9%입니다. 또한, 전체 단일 모델 최첨단 기준을 1.4% 향상시켰습니다. 11개 멤버로 구성된 앙상블은 F1을 87.4로 끌어올려, 리더보드 제출 시점의 전체 최첨단입니다. ELMo로 인한 4.7%의 증가는 CoVe를 기준 모델에 추가한 1.8%의 개선보다도 유의미하게 큽니다 (McCann et al., 2017).

2017년 11월 17일 기준.
과제 이전 최고 성과

우리

베이스라인
엘모 +

기준선
증가
(절대적인/
상대적인)
SQuAD Liu et al. (2017)   84.4 81.1   85.8     4.7 / 24.9%
SNLI  Chen et al. (2017)  88.6 88.0   88.7 ± 0.17 0.7 / 5.8%
SRL   He et al. (2017)    81.7 81.4   84.6     3.2 / 17.2%
Coref Lee et al. (2017)   67.2 67.2   70.4     3.2 / 9.8%
NER   Peters et al. (2017) 91.93 ± 0.19 90.15 92.22 ± 0.10 2.06 / 21%
SST-5 McCann et al. (2017) 53.7 51.4  54.7 ± 0.5 3.3 / 6.8%

표 1: ELMo 향상된 신경망 모델과 최첨단 단일 모델 기준의 테스트 세트 비교 결과, 여섯 가지 벤치마크 NLP 작업에서의 성능. 성능 측정 지표는 작업에 따라 다름 - SNLI와 SST-5의 경우 정확도; SQuAD, SRL 및 NER의 경우 F1; Coref의 경우 평균 F1. NER와 SST-5의 테스트 크기가 작기 때문에, 다른 무작위 시드로 다섯 번 실행한 결과의 평균과 표준 편차를 보고합니다. "증가" 열은 기준선 대비 절대적 및 상대적 개선을 나열합니다.

텍스트적 함의는 "가설"이 "전제"를 고려할 때 참인지 아닌지를 결정하는 작업입니다. 스탠포드 자연어 추론(SNLI) 코퍼스(Bowman 등, 2015)는 약 55만 개의 가설/전제 쌍을 제공합니다. 우리의 기준선인 ESIM 시퀀스 모델(Chen 등, 2017)은 biLSTM을 사용하여 전제와 가설을 인코딩한 후, 행렬 어텐션 레이어, 로컬 추론 레이어, 다른 biLSTM 추론 구성 레이어, 마지막으로 출력 레이어 전에 풀링 작업을 수행합니다. 전반적으로, ESIM 모델에 ELMo를 추가하면 5개의 무작위 시드를 기준으로 정확도가 평균 0.7% 향상됩니다. 5명의 구성원으로 이루어진 앙상블은 전체 정확도를 89.3%로 끌어올려 이전의 최고 앙상블인 88.9%(Gong 등, 2018)를 능가합니다.

시맨틱 역할 라벨링은 문장의 술어-인자 구조를 모델링하는 시스템으로, 종종 "누가 누구에게 무엇을 했는가"라는 질문에 대답하는 것으로 설명됩니다. He 등(2017)은 SRL을 BIO 태깅 문제로 모델링하고, Zhou와 Xu(2015)를 따라 전방향과 후방향이 교차된 8층 깊은 biLSTM을 사용했습니다. 표 1에 나와 있듯이, He 등(2017)의 재구현에 ELMo를 추가하면 단일 모델 테스트 세트의 F1 점수가 81.4%에서 84.6%로 3.2% 증가하며, OntoNotes 벤치마크(Pradhan 등, 2013)에서 새로운 최고 성능을 달성하며 이전 최고 앙상블 결과보다 1.2% 향상됩니다.

코레퍼런스 해결은 텍스트에서 동일한 실제 세계 개체를 참조하는 언급을 클러스터링하는 작업입니다. 우리의 기준 모델은 Lee 등의 엔드 투 엔드 스팬 기반 신경망 모델(2017)입니다. 이 모델은 biLSTM을 사용합니다.

그리고 주의 메커니즘은 먼저 스팬 표현을 계산한 다음 소프트맥스 언급 순위 모델을 적용하여 공조 체인을 찾습니다. 우리는 CoNLL 2012 공유 작업에서 OntoNotes 공조 주석과의 실험에서 ELMo를 추가함으로써 평균 F1을 67.2에서 70.4로 3.2% 향상시켰으며, 이는 이전 최고 앙상블 결과보다 1.6% F1을 더 향상시켰습니다.




감정 분석 스탠포드 감정 트리뱅크(SST-5; Socher et al., 2013)에서의 세밀한 감정 분류 작업은 영화 리뷰에서 문장을 설명하기 위해 매우 부정적인 것부터 매우 긍정적인 것까지 다섯 가지 레이블 중 하나를 선택하는 것을 포함합니다. 이 문장들은 관용구와 복잡한 문법 구조와 같은 다양한 언어 현상을 포함하고 있습니다. 작업 베이스라인 최근 마지막

모든 레이어
λ=1 λ=0.001

SQuAD 80.8 84.7 85.0 85.2
SNLI 88.1 89.1 89.3 89.5
SRL 81.6 84.1 84.6 84.8

표 2: SQuAD, SNLI 및 SRL의 개발 세트 성능, biLM의 모든 레이어를 사용하여 정규화 강도 λ의 다른 선택과 최상위 레이어만을 비교합니다.

작업
입력
오직
입력 &
출력
출력
오직
SQuAD     85.1  85.6  84.8
SNLI      88.9  89.5  88.7
SRL       84.7  84.3  80.9

테이블 3: SQuAD, SNLI 및 SRL의 개발 세트 성능, ELMo를 지도 모델의 다른 위치에 포함시킬 때.

모델이 학습하기 어려운 부정과 같은 틱 구조. 우리의 기준 모델은 McCann et al. (2017)의 이중주의 분류 네트워크 (BCN)이며, CoVe 임베딩을 추가로 사용할 때 이전 최고 성과를 유지했습니다. BCN 모델에서 CoVe를 ELMo로 대체하면 최고 성과 대비 1.0%의 절대 정확도 향상이 있습니다.

5 분석

이 섹션은 우리의 주요 주장을 검증하고 ELMo 표현의 몇 가지 흥미로운 측면을 명확히하기 위해 소작용 분석을 제공합니다. 5.1 절에서는 하위 작업에서 깊은 문맥 표현을 사용하면 이전 작업보다 성능이 향상되는 것을 보여줍니다. 이는 biLM 또는 MT 인코더에서 생성된 것과 관계없이 최상위 레이어만 사용하는 이전 작업과 비교하여 ELMo 표현이 전반적으로 가장 우수한 성능을 제공한다는 것을 보여줍니다. 5.3 절에서는 biLM에서 포착된 다양한 유형의 문맥 정보를 탐구하고, 문법 정보가 하위 레이어에서 더 잘 표현되고 의미 정보가 상위 레이어에서 포착된다는 것을 내재적 평가를 통해 보여줍니다. 또한, 우리의 biLM이 일관되게 CoVe보다 더 풍부한 표현을 제공한다는 것을 보여줍니다. 추가로, ELMo가 작업 모델에 포함되는 위치에 대한 민감도 (5.2 절), 훈련 세트 크기 (5.4 절)를 분석하고 작업별로 학습된 ELMo 가중치를 시각화합니다 (5.5 절).

5.1 대체로 가중치를 부여하는 계층적 방식

방정식 1에 대한 여러 가지 대안이 있습니다. 이전 연구에서는 문맥 표현에 대해 마지막 레이어만 사용했습니다. 이는 biLM(Peters et al., 2017)이나 MT 인코더(CoVe; McCann et al., 2017)에서 마지막 레이어일 수 있습니다. 정규화 매개변수 λ의 선택도 중요합니다. λ = 1과 같은 큰 값은 가중 함수를 단순한 평균으로 줄이는 효과가 있으며, 작은 값 (예: λ = 0.001)은 레이어 가중치를 변동시킵니다. 표 2는 SQuAD, SNLI 및 SRL에 대한 이러한 대안을 비교합니다. 모든 레이어의 표현을 포함하는 것은 마지막 레이어만 사용하는 것보다 전반적인 성능을 향상시킵니다. 또한 마지막 레이어에서 문맥 표현을 포함하는 것은 기준선보다 성능을 향상시킵니다. 예를 들어, SQuAD의 경우, 마지막 biLM 레이어만 사용하면 개발 F1이 기준선보다 3.9% 향상됩니다. 마지막 레이어 대신 모든 biLM 레이어를 평균화하면 F1이 추가로 0.3% 향상됩니다 (λ = 1 열과 "Last Only"를 비교). 또한 작업 모델이 개별 레이어 가중치를 학습하도록 허용하면 F1이 추가로 0.2% 향상됩니다 (λ = 1 대 λ = 0.001). ELMo의 대부분의 경우 작은 λ가 선호됩니다. 그러나 훈련 세트가 작은 NER의 경우 λ에 민감하지 않은 결과가 나타납니다 (표시되지 않음). 전반적인 경향은 CoVe에서도 비슷하지만 기준선보다 증가폭이 작습니다. SNLI의 경우, λ = 1로 모든 레이어를 평균화하면 개발 정확도가 88.2%에서 88.7%로 향상됩니다. SRL의 F1은 마지막 레이어만 사용하는 경우에 비해 λ = 1의 경우에 약간 0.1% 향상되어 82.2가 됩니다.

5.2 ELMo를 어디에 포함해야 할까요?

이 논문의 모든 작업 아키텍처에는 가장 낮은 레이어의 biRNN에 입력으로 단어 임베딩만 포함되어 있습니다. 그러나 우리는 일부 작업에 대해 biRNN의 출력에 ELMo를 포함하는 작업별 아키텍처가 전반적인 결과를 개선한다는 것을 발견했습니다. 표 3에 나와 있는 것처럼, SNLI와 SQuAD에 대해 입력 레이어와 출력 레이어 모두에 ELMo를 포함하는 것은 입력 레이어만 있는 것보다 결과를 개선시킵니다. 그러나 SRL의 경우 (그리고 coreference resolution은 표시되지 않음) 성능은 입력 레이어에만 포함되었을 때 가장 높습니다. 이 결과에 대한 하나의 가능한 설명은 SNLI와 SQuAD 아키텍처가 biRNN 이후에 어텐션 레이어를 사용하기 때문에, 이 레이어에서 ELMo를 도입하면 모델이 biLM의 내부 표현에 직접적으로 주의를 기울일 수 있게 됩니다. SRL의 경우,

글로브 플레이

놀기, 게임, 게임들, 놀았다, 플레이어들, 놀이, 플레이어,
놀다, 축구, 멀티플레이어

비엘엠
치코 루이즈는 알루식의 땅볼에 대한 화려한 플레이를 했다.

키퍼는 그룹에서 유일한 주니어로서, 결정적인 상황에서 타격 능력과 전반적으로 훌륭한 플레이로 인정받았다.
올리비아 데 하빌랜드는 가슴에 브로드웨이 연극을 하기 위해 가슴에 사인했다.

{...} 그들은 성공적인 연극에서 주요 역할을 맡은 배우들이었고, 충분한 재능을 가지고 있어서 역할을 능숙하게, 좋은 절제감으로 소화할 수 있었습니다.

테이블 4: GloVe와 biLM의 컨텍스트 임베딩을 사용하여 "play"에 가장 가까운 이웃들.

모델              F1
WordNet 1차 의미 베이스라인 65.9
Raganato et al. (2017a) 69.9
Iacobacci et al. (2016) 70.1
CoVe, 첫 번째 레이어 59.4
CoVe, 두 번째 레이어 64.7
biLM, 첫 번째 레이어 67.4
biLM, 두 번째 레이어 69.0

표 5: 모든 단어의 세밀한 의미 해석 F1. CoVe와 biLM에 대한 점수를 첫 번째와 두 번째 레이어 biLSTMs에 대해 보고합니다.

과제별 문맥 표현은 biLM에서 얻은 것보다 더 중요할 것으로 보입니다.

5.3 biLM의 표현에는 어떤 정보가 포착되나요?

ELMo를 추가하면 단어 벡터만 사용하는 것보다 작업 성능이 향상되므로, biLM의 문맥 표현은 단어 벡터로는 포착되지 않는 NLP 작업에 대해 일반적으로 유용한 정보를 인코딩해야 합니다. 직관적으로, biLM은 단어의 의미를 문맥을 사용하여 명확하게 하는 역할을 해야 합니다. 예를 들어, "play"라는 다의어 단어를 고려해보세요. 표 4의 상단에는 GloVe 벡터를 사용하여 "play"의 가장 가까운 이웃들이 나열되어 있습니다. 이들은 여러 품사에 걸쳐 분포되어 있지만 "play"의 스포츠 관련 의미에서 집중되어 있습니다. 반면에, 하단의 두 행은 "play"의 원문 문장에서 biLM의 문맥 표현을 사용하여 SemCor 데이터셋에서 가장 가까운 이웃 문장들을 보여줍니다. 이러한 경우에는 biLM이 원문 문장에서 품사와 단어 의미를 모두 명확하게 구분할 수 있습니다.
이러한 관찰은 아래에서 설명하는 SemCor 데이터셋을 사용하여 양적으로 측정할 수 있습니다.

모델           정확도

Collobert et al. (2011) 97.3
마와 호비 (2016) 97.6
링 등 (2015) 97.8
CoVe, 첫 번째 레이어 93.3
CoVe, 두 번째 레이어 92.8
biLM, 첫 번째 레이어 97.3
biLM, 두 번째 레이어 96.8

테이블 6: PTB에 대한 테스트 세트 POS 태깅 정확도입니다. CoVe와 biLM에 대해서는 첫 번째와 두 번째 레이어 biLSTM의 점수를 보고합니다.

문맥적 표현의 본질적 평가는 Belinkov et al. (2017)와 유사합니다. biLM에 의해 인코딩된 정보를 분리하기 위해, 표현은 세밀한 단어 의미 해소(WSD) 작업과 POS 태깅 작업에 직접적으로 예측에 사용됩니다. 이 접근 방식을 사용하여 CoVe와 각 개별 레이어 간에도 비교할 수 있습니다.

단어 의미 해소 주어진 문장에서는
우리는 biLM 표현을 사용하여
단순한 1-최근접 이웃 접근법을 사용하여 대상 단어의 의미를 예측할 수 있습니다. 이는 Melamud 등 (2016)과 유사합니다. 이를 위해 먼저 biLM을 사용하여 SemCor 3.0, 우리의 훈련 말뭉치 (Miller 등, 1994)의 모든 단어에 대한 표현을 계산한 다음 각 의미에 대한 평균 표현을 취합니다. 테스트 시에는 다시 biLM을 사용하여 주어진 대상 단어에 대한 표현을 계산하고 훈련 세트에서 가장 가까운 이웃 의미를 선택하며, 훈련 중 관찰되지 않은 레마에 대해서는 WordNet의 첫 번째 의미로 대체합니다.

표 5는 Raganato et al. (2017b)의 평가 프레임워크를 사용하여 Raganato et al. (2017a)의 네 개의 테스트 세트와 비교한 WSD 결과를 비교합니다. 전반적으로, biLM 상위 레이어 표현은 F1이 69.0으로, 첫 번째 레이어보다 WSD에서 더 우수합니다. 이는 수작업으로 제작된 특징을 사용하는 최첨단 WSD 전용 지도 모델 (Iacobacci et al., 2016) 및 보조 고정식 의미 레이블과 POS 태그와 함께 훈련된 작업 특정 biLSTM (Raganato et al., 2017a)과 경쟁력이 있습니다. CoVe biLSTM 레이어는 biLM과 유사한 패턴을 따릅니다 (두 번째 레이어가 첫 번째 레이어보다 전반적인 성능이 더 높음). 그러나 우리의 biLM은 CoVe biLSTM보다 우수하며, WordNet 첫 번째 의미 기준선보다 우수합니다.
POS 태깅 biLM이 기본 구문을 포착하는지 확인하기 위해, 우리는 context 표현을 선형 분류기의 입력으로 사용하여 Penn Treebank (PTB)의 Wall Street Journal 부분에서 POS 태그를 예측했습니다 (Marcus et al., 1993). 선형 분류기는 모델 용량을 거의 추가하지 않으므로, 이는 biLM의 표현의 직접적인 테스트입니다. WSD와 유사하게, biLM 표현은 신중하게 조정된 작업 특정 biLSTM (Ling et al., 2015; Ma and Hovy, 2016)과 경쟁력이 있습니다. 그러나 WSD와 달리, 첫 번째 biLM 레이어를 사용한 정확도가 상위 레이어보다 높으며, 다중 작업 훈련에서 깊은 biLSTM의 결과 (Søgaard and Goldberg, 2016; Hashimoto et al., 2017) 및 MT (Blinkov et al., 2017)와 일치합니다. CoVe POS 태깅 정확도는 biLM과 동일한 패턴을 따르며, WSD와 마찬가지로 biLM이 CoVe 인코더보다 더 높은 정확도를 달성합니다.
지도 학습 작업에 대한 함의 이러한 실험들은 biLM의 다른 레이어가 다른 유형의 정보를 나타내고, 하향 작업에서 가장 높은 성능을 위해 모든 biLM 레이어를 포함하는 것이 중요한 이유를 설명합니다. 게다가, biLM의 표현은 CoVe보다 WSD와 POS 태깅에 더 전이 가능하며, 이는 ELMo가 하향 작업에서 CoVe보다 우수한 성능을 내는 이유를 설명하는 데 도움이 됩니다.

5.4 샘플 효율성

ELMo를 모델에 추가하면 샘플 효율성이 상당히 향상되며, 최첨단 성능에 도달하기 위해 필요한 매개변수 업데이트 수와 전체 훈련 세트 크기 모두에서 그렇습니다. 예를 들어, SRL 모델은 ELMo 없이 훈련을 진행한 후 486번의 에포크 이후에 최대 개발 F1을 달성합니다. ELMo를 추가한 후에는 모델이 기준선 최대값을 에포크 10에서 초과하며, 도달에 필요한 업데이트 수는 98% 감소합니다.

그림 1: 훈련 세트 크기가 0.1%에서 100%로 변화함에 따라 SNLI와 SRL의 기준선 대 ELMo 성능 비교.

그림 2: 태스크와 ELMo 위치에 따른 softmax 정규화된 biLM 레이어 가중치의 시각화. 1/3보다 작은 정규화된 가중치는 수평선으로 무늬가 있고, 2/3보다 큰 가중치는 점무늬가 있다.

동일한 수준의 성능.

또한, ELMo를 강화한 모델은 ELMo가 없는 모델보다 작은 교육 세트를 더 효율적으로 사용합니다. 그림 1은 전체 교육 세트의 백분율이 0.1%에서 100%로 변할 때 기준 모델과 ELMo를 사용한 모델의 성능을 비교합니다. ELMo를 사용한 경우 작은 교육 세트에서 가장 큰 개선이 이루어지며, 성능 수준에 도달하기 위해 필요한 교육 데이터의 양을 크게 줄입니다. SRL 경우, 1%의 교육 세트를 사용한 ELMo 모델은 10%의 교육 세트를 사용한 기준 모델과 거의 동일한 F1 값을 가지고 있습니다.

5.5 학습된 가중치의 시각화

그림 2는 소프트맥스 정규화된 학습된 레이어 가중치를 시각화합니다. 입력 레이어에서 과제 모델은 첫 번째 biLSTM 레이어를 선호합니다. 공용 참조와 SQuAD의 경우, 이는 강하게 선호되지만 다른 과제에 대해서는 분포가 덜 피크되어 있습니다. 출력 레이어 가중치는 비교적 균형이 잡혀 있으며, 하위 레이어를 약간 선호합니다.
6 결론

우리는 biLMs에서 고품질의 깊은 문맥의존적 표현을 학습하기 위한 일반적인 접근법을 소개하고, ELMo를 다양한 NLP 작업에 적용할 때 큰 개선이 있음을 보였습니다. 우리는 제거 실험과 다른 통제 실험을 통해 biLM 레이어가 문맥에서 단어에 대한 구문 및 의미 정보를 효율적으로 인코딩하며, 모든 레이어를 사용하는 것이 전체 작업 성능을 향상시킨다는 것을 확인했습니다.

참고문헌

지미 바, 라이언 키로스, 그리고 제프리 E. 힌튼. 2016년.
레이어 정규화. CoRR abs/1607.06450.

요나탄 벨린코프, 나디르 두라니, 파힘 달비, 하산 사자드, 그리고 제임스 R. 글래스. 2017년. 신경 기계 번역 모델은 형태론에 대해 무엇을 배우는가? ACL에서.

피오트르 보야노프스키, 에두아르 그라브, 아르망 주랭, 토마스 미코로프. 2017. 서브워드 정보로 단어 벡터를 풍부하게 하는 방법. TACL 5:135–146.

사무엘 R. 보우먼, 가보르 안젤리, 크리스토퍼 포츠, 그리고 크리스토퍼 D. 매닝. 2015. 자연어 추론 학습을 위한 대규모 주석 말뭉치. 2015 년 자연어 처리에 대한 경험적 방법 (EMNLP) 컨퍼런스 논문집에서 발표. 계산언어학 협회.

CiprianChelba, TomasMikolov, MikeSchuster, QiGe,
Thorsten Brants, Phillipp Koehn, and Tony Robin-
son. 2014. 통계 언어 모델링의 진전을 측정하기 위한 10억 단어 벤치마크. INTERSPEECH에서.

Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, and Diana Inkpen. 2017. Enhanced lstm
for natural language inference. In ACL.

Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei,
Hui Jiang, 그리고 Diana Inkpen. 2017년. 자연어 추론을 위한 향상된 LSTM. ACL에서.

Jason Chiu와 Eric Nichols. 2016. 양방향 LSTM-CNN을 이용한 개체명 인식. TACL에서.

경현 조, 바트 반 메리엔부어, 드미트리 바단아우, 그리고 요슈아 벤지오. 2014년. 신경망 기계 번역의 특성에 대하여: 인코더-디코더 접근 방식. SSST@EMNLP에서.

크리스토퍼 클락과 매튜 가드너. 2017. 간단하고 효과적인 다단락 독해. CoRR abs/1710.10723.

케빈 클락과 크리스토퍼 D. 매닝. 2016. 언급 순위 맞추기 공동 참조 모델을 위한 심층 강화 학습. EMNLP에서.

Ronan Collobert, Jason Weston, L´ eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. 자연어 처리 (거의) 처음부터. JMLR에서.

앤드류 M. 다이와 쿼크 V. 레. 2015. 반지도 시퀀스 학습. NIPS에서.

그렉 듀렛과 단 클라인. 2013년. 핵심 참조 해결에서의 쉬운 승리와 어려운 전투. EMNLP에서.

야린 갈과 주빈 가라마니. 2016. 순환 신경망에서 드롭아웃의 이론적 기반 응용. NIPS에서.

이첸 공, 헝 루오, 그리고 지안 장. 2018. 상호작용 공간에서의 자연어 추론. ICLR에서.

카즈마 하시모토, 카이밍 씨옹, 요시마사 츠루오카, 그리고 리처드 소처. 2017년. 공동 다중 작업 모델: 다중 NLP 작업을 위한 신경망 성장. EMNLP 2017에서.

루헝 허, 켄튼 리, 마이크 루이스, 루크 S. 제틀레모이어. 2017. 깊은 의미 역할 라벨링: 무엇이 동작하고 무엇이 다음 단계인가. ACL에서.

셉 호크라이터와 유르겐 슈미드후버. 1997년. 장단기 기억. 신경 계산 9.

이그나시오 이아코바치, 모하마드 타허 필레바르, 그리고 로베르토 나비글리. 2016년. 단어 의미 모호성 해소를 위한 임베딩: 평가 연구. ACL에서.

라팔 요제포비치, 오리올 비냐르스, 마이크 슈스터, 노암 샤지어, 그리고 용희 우. 2016년. 언어 모델링의 한계 탐색. CoRR abs/1602.02410.

라팔 요제포비치, 보이체 자레바, 그리고 일리야 숫크에버. 2015년. 순환 신경망 아키텍처의 경험적 탐구. ICML에서.

윤 김, 야신 제르니트, 데이비드 손택, 알렉산더 M 러쉬. 2015. 캐릭터 인식 신경 언어 모델. AAAI 2016에서.

디에더릭 P. 킹마와 지미 바. 2015. Adam: 확률적 최적화를 위한 방법. ICLR에서.

안킷 쿠마르, 오잔 이르소이, 피터 온드루스카, 모히트 이예르, 이샨 굴라자니 제임스 브래드버리, 빅터 종, 로맹 폴루스, 그리고 리처드 소처. 2016년. 무엇이든 물어보세요: 자연어 처리를 위한 동적 메모리 네트워크. ICML에서.

존 D. 라퍼티, 앤드류 맥캘럼, 페르난도 페레이라. 2001. 조건부 랜덤 필드: 일련 데이터의 분할과 레이블링을 위한 확률적 모델. ICML에서.

Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. 명명된 개체 인식을 위한 신경 아키텍처. NAACL-HLT에서.
Kenton Lee, Luheng He, Mike Lewis, and Luke S. Zettlemoyer. 2017. 엔드 투 엔드 신경 공조 해결. EMNLP에서.

왕 링, 크리스 다이어, 앨런 W. 블랙, 이사벨 트란-
코소, 라몬 페르만데스, 실비오 아미르, 루이스 마루조,
그리고 티아고 루이스. 2015. 형태에서 기능 찾기:
개방 어휘 단어 표현을 위한 구성적인 문자 모델. EMNLP에서.

Xiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2017년. 기계 독해를 위한 확률적 답변 네트워크. arXiv 사전 인쇄 arXiv:1712.03556.

Xuezhe Ma와 Eduard H. Hovy. 2016. 양방향 LSTM-CNNs-CRF를 통한 end-to-end 시퀀스 라벨링. ACL에서.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993년. 영어의 큰 주석이 달린 말뭉치 구축: 펜 트리뱅크. 컴퓨터 언어학 19:313–330.

브라이언 맥캔, 제임스 브래드버리, 카이밍 씽, 그리고 리처드 소처. 2017년. 번역에서 배운 것: 맥락화된 단어 벡터. NIPS 2017에서.

오렌 멜라무드, 야콥 골드버거, 이도 다간.
2016년. context2vec: 양방향 lstm을 사용하여 일반적인 문맥 임베딩 학습. CoNLL에서 발표.

G´ abor Melis, Chris Dyer, 그리고 Phil Blunsom. 2017. 신경 언어 모델 평가의 최신 동향에 대하여. CoRR abs/1707.05589.

스티븐 메리티, 니티시 시리쉬 케스카르, 그리고 리처드 소처. 2017. LSTM 언어 모델의 정규화와 최적화. CoRR abs/1708.02182.

토마스 미콜로프, 일리야 숫스케버, 카이 첸, 그레그 S 코라도, 그리고 제프 딘. 2013년. 단어와 구문의 분산 표현과 그들의 조합성. NIPS에서.

조지 A. 밀러, 마틴 코도로우, 샤리 랜드스, 클라우디아 리콕, 그리고 로버트 G. 토마스. 1994년. 의미 식별을 위한 의미적 일치도 사용. HLT에서.

츠엔드수렌 문크다라이와 홍유. 2017. 텍스트 이해를 위한 신경망 트리 인덱서. EACL에서.

아르빈드 니라칸탄, 지반 샨카르, 알렉산드르 파스소스, 그리고 앤드류 맥콜럼. 2014년. 벡터 공간에서 단어당 다중 임베딩의 효율적인 비모수적 추정. EMNLP에서.

마사 파머, 폴 킹스베리, 다니엘 길데아.
2005년. Thepropositionbank: 의미 역할의 주석이 달린 말뭉치. 계산 언어학 31:71-106.

제프리 페닝턴, 리처드 소처, 그리고 크리스토퍼 D. 매닝. 2014. Glove: 단어 표현을 위한 글로벌 벡터. EMNLP에서.

매튜 E. 피터스, 왈리드 암마르, 찬드라 바가바툴라, 그리고 러셀 파워. 2017년. 양방향 언어 모델을 이용한 준지도 시퀀스 태깅. ACL에서 발표.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. 온토노트를 사용한 견고한 언어 분석을 향하여. CoNLL에서.

Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012년. Conll-2012 공유 작업: 온토노트에서의 다국어 무제한 핵심 참조 모델링. EMNLP-CoNLL 공유 작업에서.

알레산드로 라가나토, 클라우디오 델리 보비, 그리고 로베르토 나비글리. 2017a. 단어 의미 모호성을 위한 신경망 시퀀스 학습 모델. EMNLP에서.

알레산드로 라가나토, 호세 카마초-콜라도스, 그리고 로베르토 나비글리. 2017b. 단어 의미 구분: 통합 평가 프레임워크와 경험적 비교. EACL에서.

PranavRajpurkar, JianZhang, KonstantinLopyrev, 그리고 Percy Liang. 2016년. Squad: 텍스트 이해를 위한 100,000개 이상의 질문. EMNLP에서.

Prajit Ramachandran, Peter Liu, and Quoc Le. 2017년.
레이블이 없는 데이터를 사용하여 시퀀스 학습 개선하기. EMNLP에서.

에릭 F. 통 김 상과 피엔 데 뮐더.
2003년. CoNLL-2003 공유 작업에 대한 소개:
언어 독립적인 명명된 개체 인식. CoNLL에서.

민준서, 아니루다 켐바비, 알리 파르하디, 그리고 한나네 하지시르지. 2017년. 기계 이해를 위한 양방향 주의 흐름. ICLR에서.

리처드 소처, 알렉스 페렐리진, 전 제인 우, 제이슨 차앙, 크리스토퍼 D 매닝, 앤드류 Y 엔지, 그리고 크리스토퍼 포츠. 2013년. 감성 트리뱅크를 통한 의미 합성에 대한 재귀적인 깊은 모델. EMNLP에서.

안데르스 쇼가드와 요아브 골드버그. 2016년. 하위 레이어에서 낮은 수준의 작업을 지도하는 깊은 다중 작업 학습. ACL 2016에서.

니티시 스리바스타바, 제프리 E. 힌튼, 알렉스 크리즈헤브스키, 일리야 수츠케버, 그리고 루슬란 살라후트디노프. 2014년. 과적합을 방지하기 위한 간단한 방법인 드롭아웃. 기계 학습 연구 저널 15:1929-1958.

Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. 2015. 매우 깊은 신경망 훈련. NIPS에서.

Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-
gio. 2010. 단어 표현: 반지도 학습을 위한 간단하고 일반적인 방법. ACL에서.
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,
and Ming Zhou. 2017. 독해와 질문 답변을 위한 게이트 자기 일치 네트워크. ACL에서.

존 위팅, 모히트 반살, 케빈 김펠, 그리고 카렌 리브스쿠. 2016년. Charagram: 문자 n-그램을 통한 단어와 문장 임베딩. EMNLP에서 발표.

Sam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. 핵심 참조 해결을 위한 전역 특징 학습. HLT-NAACL에서.

매튜 D. 제일러. 2012. Adadelta: 적응형 학습률 방법. CoRR abs/1212.5701.

지에 주와 웨이 쉬우. 2015. 재귀 신경망을 사용한 의미 역할 라벨링의 엔드 투 엔드 학습. ACL에서.

펑 주, 젠유 치, 순종 정, 지아밍 쉬,
홍윤 바오, 그리고 보 쉬. 2016년. 이중방향 LSTM과 이차원 최대 풀링을 통합하여 개선된 텍스트 분류. COLING에서 발표.
깊은 문맥화된 단어 표현을 동반하는 부록 자료.

이 보충 자료에는 섹션 4의 최첨단 모델들의 모델 아키텍처, 훈련 루틴 및 하이퍼파라미터 선택에 대한 세부 정보가 포함되어 있습니다.
개별 모델들은 모두 하위 여러 레이어 아래에 문맥 독립 토큰 표현을 가진 공통 아키텍처를 가지고 있으며, 모든 경우에 RNN의 쌓인 레이어 - LSTM을 사용하고 있습니다. 단, SQuAD 모델은 GRU를 사용합니다.

미세 조정 biLM

3.4절에서 언급한 대로, 과제별 데이터에 대한 biLM의 세밀 조정은 일반적으로 헷갈림의 큰 하락을 초래했습니다. 특정 과제에 대해 세밀 조정하기 위해, 지도 학습 레이블은 일시적으로 무시되었으며, biLM은 훈련 세트에서 1회 에포크 동안 세밀 조정되고 개발 세트에서 평가되었습니다. 세밀 조정된 후에는 과제 훈련 중에 biLM 가중치가 고정되었습니다.
표 7은 고려된 과제에 대한 개발 세트 헷갈림을 나열합니다. CoNLL 2012를 제외한 모든 경우, 세밀 조정은 헷갈림에서 큰 개선을 가져왔습니다. 예를 들어, SNLI의 경우 72.1에서 16.8로 개선되었습니다.
세밀 조정이 지도 학습 성능에 미치는 영향은 과제에 따라 다릅니다. SNLI의 경우, biLM의 세밀 조정은 개발 정확도를 88.9%에서 89.5%로 0.6% 향상시켰습니다. 그러나 감성 분류의 경우, 세밀 조정된 biLM을 사용하든 그렇지 않든 개발 세트 정확도는 거의 동일합니다.

1번 식에서 γ의 중요성 2가지

식 (1)의 γ 매개 변수는 최적화를 돕기 위해 실용적으로 중요했으며, biLM 내부 표현과 과제 특정 표현 간의 다른 분포 때문에 그 중요성이 컸습니다. 특히 5.1절의 마지막만을 고려한 경우에는 특히 중요합니다. 이 매개 변수 없이는 마지막만을 고려한 경우 SNLI에서 성능이 저조하게 나타나며, SRL에서는 훈련이 완전히 실패합니다.

A. 3 텍스트 함의

우리의 기준 SNLI 모델은 Chen et al. (2017)의 ESIM 시퀀스 모델입니다. 원래의 구현을 따라, 모든 LSTM 및 피드 포워드 레이어에 300 차원을 사용하고 훈련 중에 고정된 300 차원 GloVe 임베딩을 사용했습니다. 정규화를 위해, 우리는

데이터셋

이전에
조율하기 전에
조율 후에
조율 후에

SNLI 72.1 16.8
CoNLL 2012 (coref/SRL) 92.3 -
CoNLL 2003 (NER) 103.2 46.3

SQuAD
맥락    99.1  43.5
질문    158.2 52.0
SST              131.5 78.6

표 7: 다양한 데이터셋에 대한 훈련 세트에서 1 epoch 동안 미세 조정 전후의 개발 세트 퍼플렉서티 (낮을수록 좋음). 보고된 값은 순방향과 역방향 퍼플렉서티의 평균입니다.

각 LSTM 레이어의 입력에 50% 변이 드롭아웃(Gal and Ghahramani, 2016)을 추가하고 최종 두 개의 완전히 연결된 레이어의 입력에 50% 드롭아웃(Srivastava et al., 2014)을 추가했습니다. 모든 피드 포워드 레이어는 ReLU 활성화 함수를 사용합니다. 매개변수는 Adam(Kingma and Ba, 2015)을 사용하여 최적화되었으며, 그래디언트 노름은 5.0으로 클리핑되었으며 초기 학습률은 0.0004이며, 개발 세트의 정확도가 이후 에포크에서 증가하지 않을 때마다 절반으로 감소했습니다. 배치 크기는 32였습니다.
최상의 ELMo 구성은 ELMo 벡터를 가장 낮은 레이어 LSTM의 입력과 출력에 모두 추가했으며, (1)을 사용하였으며, 레이어 정규화와 λ = 0.001을 사용했습니다. ELMo 모델의 매개변수 수가 증가했기 때문에, 모든 순환 및 피드 포워드 가중치 행렬에 0.0001의 정규화 계수를 가진 (cid:96)2 정규화를 추가하고, 어텐션 레이어 이후에 50% 드롭아웃을 추가했습니다.
표 8은 우리 시스템의 테스트 세트 정확도를 이전에 발표된 시스템과 비교합니다. 전반적으로, ESIM 모델에 ELMo를 추가하여 정확도를 0.7% 향상시켰으며, 새로운 단일 모델의 최고 성능인 88.7%를 달성했으며, 다섯 개의 멤버 앙상블은 전체 정확도를 89.3%로 끌어올렸습니다.

A. 4 질문에 대한 답변

우리의 QA 모델은 Clark와 Gardner (2017)의 모델의 간소화된 버전입니다. 각 토큰의 대소문자를 구분하는 300 차원의 GloVe 단어 벡터 (Pennington et al., 2014)와 합성곱 신경망을 사용하여 생성된 문자 기반 임베딩을 연결하여 토큰을 임베딩합니다. 그리고 학습된 문자 임베딩에 대해 max-pooling을 수행합니다. 토큰 임베딩은 공유된 양방향 GRU를 통과한 후 BiDAF (Seo et al., 2017)의 양방향 어텐션 메커니즘을 거칩니다. 증강된 모델의 정확도는 다음과 같습니다.

특징 기반 (Bowman et al., 2015) 78.2
DIIN (Gong et al., 2018)   88.0
BCN+Char+CoVe (McCann et al., 2017) 88.1
ESIM (Chen et al., 2017)   88.0
ESIM+TreeLSTM (Chen et al., 2017) 88.6
ESIM+ELMo                  88.7 ± 0.17
DIIN 앙상블 (Gong et al., 2018) 88.9
ESIM+ELMo 앙상블         89.3

표 8: SNLI 테스트 세트 정확도. 단일 모델 결과가 상단에 위치하며, 앙상블 결과는 하단에 있습니다.

텍스트 벡터는 선형 레이어를 통과한 후 ReLU 활성화 함수를 거치며, 잔차 self-attention 레이어가 이어지고 이는 GRU를 사용한 뒤 동일한 attention 메커니즘을 context-to-context로 적용한 뒤 다른 선형 레이어를 거칩니다. 마지막으로 결과는 선형 레이어를 통과하여 답변의 시작과 끝 토큰을 예측합니다. 입력 전에 GRU와 선형 레이어에 대해 변동 드롭아웃을 0.2의 비율로 사용합니다. GRU에는 90의 차원을 사용하고, 선형 레이어에는 180의 차원을 사용합니다. 배치 크기는 45로 하여 Adadelta를 사용하여 모델을 최적화합니다. 테스트 시에는 가중치의 지수 이동 평균을 사용하고 출력 범위를 최대 17로 제한합니다. 훈련 중에는 단어 벡터를 업데이트하지 않습니다.
ELMo를 입력과 출력 모두에 레이어 정규화 없이 추가하고, ELMo 가중치를 정규화하지 않은 경우(λ = 0)에 성능이 가장 높았습니다.
테스트 세트 결과는 2017년 11월 17일에 우리 시스템을 제출했을 때의 SQuAD 리더보드와 비교한 표 9에서 확인할 수 있습니다. 전반적으로, 우리의 제출은 단일 모델과 앙상블 결과에서 가장 높은 성능을 보여주었으며, 이전 단일 모델 결과(SAN)를 1.4% F1로 개선하고 기준선을 4.2%로 개선했습니다. 11개의 멤버로 구성된 앙상블은 F1을 87.4%로 끌어올려 이전 앙상블 최고치보다 1.0% 증가했습니다.

5. 의미 역할 라벨링

우리의 기준 SRL 모델은 (He et al., 2017)의 정확한 재구현입니다. 단어는 100 차원 벡터 표현의 연결로 표현되며, GloVe (Pennington et al., 2014)를 사용하여 초기화되고 이진 형태의 단어 별 술어 기능은 100 차원 임베딩으로 표현됩니다.

https://nlp.stanford.edu/projects/snli/에서 포괄적인 비교를 찾을 수 있습니다.

침구. 이 200차원 토큰 표현은 8층의 "교대로" 이루어진 300차원 은닉 크기의 biLSTM을 통과하며, LSTM 층의 방향은 층마다 교대로 변경됩니다. 이 깊은 LSTM은 층 사이에 Highway 연결(Srivastava et al., 2015)과 변분적 순환 드롭아웃(Gal and Ghahramani, 2016)을 사용합니다. 이 깊은 표현은 최종적으로 밀집층과 소프트맥스 활성화 함수를 거쳐 모든 가능한 태그에 대한 분포를 형성합니다. 레이블은 PropBank(Palmer et al., 2005)의 의미론적 역할과 BIO 레이블링 체계를 사용하여 인자 범위를 나타냅니다. 훈련 중에는 학습률이 1.0이고 ρ = 0.95인 Adadelta를 사용하여 태그 시퀀스의 음의 로그 우도를 최소화합니다(Zeiler, 2012). 테스트 시에는 BIO 제약 조건을 사용하여 유효한 범위를 강제하기 위해 Viterbi 디코딩을 수행합니다. 모든 LSTM 은닉층에는 10%의 변분적 드롭아웃이 추가됩니다. 값이 1.0을 초과하는 경우 그래디언트는 클리핑됩니다. 모델은 500 epoch 또는 검증 F1이 200 epoch 동안 향상되지 않을 때까지 훈련됩니다. 사전 훈련된 GloVe 벡터는 훈련 중에 세밀하게 조정됩니다. 최종 밀집층과 모든 LSTM 셀은 직교로 초기화됩니다. 모든 LSTM에 대해 잊어버리는 게이트 편향은 1로 초기화되며, 다른 게이트는 0으로 초기화됩니다(J´ ozefowicz et al., 2015). 표 10은 (He et al., 2017)의 ELMo 보강 구현과 이전 결과의 테스트 세트 F1 점수를 비교합니다. 우리의 단일 모델 점수인 84.6 F1은 CONLL 2012 의미론적 역할 레이블링 작업에서 새로운 최고 성과를 나타내며, 이전의 단일 모델 결과보다 2.9 F1, 5개 모델 앙상블보다 1.2 F1 높습니다.

A. 6 핵심 참조 해결

우리의 기준선 공용참조 모델은 Lee et al. (2017)의 엔드 투 엔드 신경망 모델입니다. 모든 하이퍼파라미터를 사용합니다.

모델                       EM  F1
BiDAF (Seo et al., 2017)    68.0 77.3
BiDAF + Self Attention      72.1 81.1
DCN+                        75.1 83.1
Reg-RaSoR                   75.8 83.3
FusionNet                   76.0 83.9
r-net (Wang et al., 2017)   76.5 84.3
SAN (Liu et al., 2017)      76.8 84.4
BiDAF + Self Attention + ELMo 78.6 85.8
DCN+ Ensemble               78.9 86.0
FusionNet Ensemble          79.0 86.0
Interactive AoA Reader+ Ensemble 79.1 86.5
BiDAF + Self Attention + ELMo Ensemble 81.0 87.4

테이블 9: SQuAD에 대한 테스트 세트 결과를 보여주는 정확도 (EM)와 F1을 모두 표시합니다. 표의 상단 절반은 단일 모델 결과이며, 하단에 앙상블 결과가 있습니다. 가능한 경우 참고 자료가 제공되었습니다.

모델            F1
Pradhan et al. (2013) 77.5
Zhou and Xu (2015) 81.3
He et al. (2017), 단일 81.7
He et al. (2017), 앙상블 83.4
He et al. (2017), 우리의 구현 81.4
He et al. (2017) + ELMo 84.6

테이블 10: SRL CoNLL 2012 테스트 세트 F1.

모델              평균 F1
더렛과 클라인 (2013) 60.3
와이즈먼 등 (2016) 64.2
클락과 매닝 (2016) 65.7
이 등 (2017) (단일) 67.2
이 등 (2017) (앙상블) 68.8
이 등 (2017) + ELMo 70.4

테이블 11: CoNLL 2012 공유 작업의 테스트 세트에서의 핵심 참조 해결 평균 F1.

원래 구현을 정확히 따르는 매개변수로 설정되었습니다.
최상의 구성은 가장 낮은 레이어 biLSTM의 입력에 ELMo를 추가하고 (1)을 사용하여 biLM 레이어에 가중치를 부여했으며 정규화나 레이어 정규화 없이 수행되었습니다. ELMo 표현에는 50%의 드롭아웃이 추가되었습니다.
표 11은 이전에 발표된 결과와 우리의 결과를 비교합니다. 전반적으로, 우리는 단일 모델의 최고 성능을 평균 F1 기준으로 3.2% 향상시켰으며, 우리의 단일 모델 결과는 이전 앙상블 최고 성능을 1.6% F1 향상시켰습니다. biLSTM의 출력에 ELMo를 biLSTM 입력에 추가하는 것은 F1을 약 0.7% 감소시켰습니다 (표시되지 않음).

7. 명명된 개체 인식

우리의 기준 NER 모델은 50 차원의 사전 훈련된 Senna 벡터 (Collobert et al., 2011)를 CNN 기반의 문자 표현과 연결합니다. 문자 표현은 16 차원의 문자 임베딩과 3개 문자 폭의 128 개의 컨볼루션 필터, ReLU 활성화 및 최대 풀링을 사용합니다. 토큰 표현은 두 개의 biLSTM 레이어를 통과한 후 최종 밀집 레이어와 소프트맥스 레이어로 전달됩니다. 훈련 중에는 CRF 손실을 사용하고 테스트 시에는 Viterbi 알고리즘을 사용하여 출력 태그 시퀀스가 유효한지 확인합니다.
양방향 LSTM 레이어의 입력에 변동 드롭아웃이 추가됩니다. 훈련 중에는 그래디언트의 (cid:96)2 노름이 5.0을 초과하는 경우에는 그래디언트를 재조정하고, 학습률이 0.001인 Adam을 사용하여 매개변수를 업데이트합니다. 사전 훈련된 Senna 임베딩은 훈련 중에 세밀하게 조정됩니다. 개발 세트에서 조기 중지를 사용하고 다른 무작위 시드로 다섯 번의 실행에서 평균 테스트 세트 점수를 보고합니다.
ELMo는 가장 낮은 레이어의 작업 biLSTM의 입력에 추가되었습니다. CoNLL 2003 NER 데이터 세트가 상대적으로 작기 때문에, 우리는 λ = 0.1로 설정하여 학습 가능한 레이어 가중치를 효과적으로 일정하게 제한하는 것이 최상의 성능을 얻을 수 있다고 발견했습니다.
표 12는 우리의 ELMo 향상된 biLSTM-CRF 태거의 테스트 세트 F1 점수를 이전 결과와 비교한 것입니다. 전체적으로, 우리 시스템의 92.22% F1은 새로운 최고 성능을 수립합니다. Peters et al. (2017)와 비교했을 때, 표현을 사용한 경우
Model             F1 ± std.
Collobert et al. (2011)♣ 89.59
Lample et al. (2016) 90.94
Ma and Hovy (2016) 91.2
Chiu and Nichols (2016)♣,♦ 91.62 ± 0.33
Peters et al. (2017)♦ 91.93 ± 0.19
biLSTM-CRF + ELMo 92.22 ± 0.10

표 12: CoNLL 2003 NER 작업의 테스트 세트 F1. 
♣가 포함된 모델은 가젯리스트를 사용하였으며, ♦가 포함된 모델은 훈련에 훈련 및 개발 세트를 모두 사용하였습니다.

모델                     정확도

DMN (Kumar et al., 2016) 52.1
LSTM-CNN (Zhou et al., 2016) 52.4
NTI (Munkhdalai and Yu, 2017) 53.1
BCN+Char+CoVe (McCann et al., 2017) 53.7
BCN+ELMo                  54.7

DMN (Kumar et al., 2016) 52.1
LSTM-CNN (Zhou et al., 2016) 52.4
NTI (Munkhdalai and Yu, 2017) 53.1
BCN+Char+CoVe (McCann et al., 2017) 53.7
BCN+ELMo                  54.7

테이블 13: SST-5에 대한 테스트 세트 정확도.

모든 레이어의 biLM은 조금씩 개선됩니다.

A. 8 감정 분류

우리는 McCann et al. (2017)에서 설명한 것과 거의 동일한 바이어텐션 분류 네트워크 아키텍처를 사용합니다. 단, 최종 맥스아웃 네트워크를 더 간단한 피드포워드 네트워크로 대체하였으며, 이는 드롭아웃을 적용한 두 개의 ReLu 레이어로 구성되어 있습니다. 배치 정규화된 맥스아웃 네트워크를 사용한 BCN 모델은 우리의 실험에서 유의미하게 낮은 검증 정확도를 달성했지만, McCann et al. (2017)의 구현과 우리의 구현 사이에는 불일치가 있을 수 있습니다. CoVe 훈련 설정과 일치하기 위해, 4개 이상의 토큰을 포함하는 구문만을 훈련합니다. 바이언스텐션 LSTM에는 300차원의 은닉 상태를 사용하며, Adam (Kingma and Ba, 2015)을 사용하여 학습률 0.0001로 모델 파라미터를 최적화합니다. 훈련 가능한 바이언스텐션 언어 모델 레이어의 가중치는 λ = 0.001로 정규화되며, 입력과 출력에 ELMo를 추가합니다. 출력 ELMo 벡터는 두 번째 바이언스텐션 LSTM에서 계산되고 입력에 연결됩니다.

