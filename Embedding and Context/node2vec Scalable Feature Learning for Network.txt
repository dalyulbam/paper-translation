노드투벡: 네트워크에 대한 확장 가능한 특징 학습

아디티야 그로버
스탠포드 대학교
adityag@cs.stanford.edu

주레 레스코베츠
스탠포드 대학교
jure@cs.stanford.edu

요약

네트워크의 노드와 엣지에 대한 예측 작업은 학습 알고리즘이 사용하는 특징 엔지니어링에 신중한 노력이 필요합니다. 표현 학습의 보다 넓은 분야에서 최근 연구는 특징 자체를 학습하여 예측을 자동화하는 데 중요한 진전을 이루었습니다. 그러나 현재의 특징 학습 접근법은 네트워크에서 관찰되는 연결성 패턴의 다양성을 충분히 포착하지 못합니다.
여기에서는 네트워크의 노드에 대한 연속적인 특징 표현을 학습하기 위한 알고리즘적인 프레임워크인 node2vec을 제안합니다. node2vec에서는 노드를 특징의 저차원 공간에 매핑하여 노드의 네트워크 이웃을 보존하는 가능성을 최대화하는 특징을 학습합니다. 우리는 노드의 네트워크 이웃에 대한 유연한 개념을 정의하고, 다양한 이웃을 효율적으로 탐색하는 편향된 랜덤 워크 절차를 설계합니다. 우리의 알고리즘은 네트워크 이웃에 대한 엄격한 개념에 기반한 이전 연구를 일반화시키며, 이웃을 탐색하는 유연성이 더 풍부한 표현을 학습하는 핵심 요소라고 주장합니다.
우리는 다양한 도메인의 실제 네트워크에서 다중 레이블 분류 및 링크 예측에 대한 기존 최첨단 기술과 node2vec의 효능을 입증합니다. 종합적으로, 우리의 작업은 복잡한 네트워크에서 최첨단 작업 독립적인 표현을 효율적으로 학습하는 새로운 방법을 대표합니다.

카테고리 및 주제 설명: H.2.8 [데이터베이스 관리]: 데이터베이스 응용 프로그램 - 데이터 마이닝; I.2.6 [인공지능]: 학습
일반 용어: 알고리즘; 실험
키워드: 정보 네트워크, 특징 학습, 노드 임베딩, 그래프 표현.

노드와 엣지에 대한 예측이 많은 네트워크 분석 작업이 있습니다. 일반적인 노드 분류 작업에서는 네트워크의 노드들의 가장 가능성 있는 레이블을 예측하는 것에 관심이 있습니다. 예를 들어, 소셜 네트워크에서는 사용자의 관심사를 예측하고, 단백질-단백질 상호작용 네트워크에서는 단백질의 기능적 레이블을 예측하는 것에 관심이 있을 수 있습니다. 마찬가지로 링크 예측에서는 엣지를 예측하고 싶습니다.

이 작업의 전체 또는 일부를 개인적이거나 교실 사용을 위해 디지털 또는 인쇄 복사본을 만들 수 있는 권한이 부여됩니다. 이 복사본은 이윤을 추구하거나 상업적 이점을 얻기 위해 복제 또는 배포되지 않아야 하며, 이 공지와 전체 인용을 포함해야 합니다. 저자 이외의 다른 사람이 소유한 이 작업의 구성 요소에 대한 저작권은 존중되어야 합니다. 출판은 출처를 밝히고 요약을 허용합니다. 그 외에 복사하거나 다른 서버에 게시하거나 목록에 배포하려면 사전에 구체적인 허가와/또는 수수료가 필요합니다. 허가를 요청하려면 permissions@acm.org로 문의하십시오.

KDD'16, 2016년 8월 13일부터 17일까지 미국 캘리포니아 주 샌프란시스코에서 개최되었습니다.

c (cid:13)2016 저작권은 소유자/저자에게 있습니다. 출판권은 ACM에게 라이선스가 부여되었습니다.
ISBN 978-1-4503-4232-2/16/08...$15.00

DOI: http://dx.doi.org/10.1145/2939672.2939754

네트워크 내의 노드 쌍이 서로 연결되어 있는지 예측하십시오 [18]. 링크 예측은 다양한 도메인에서 유용하며, 예를 들어 유전체학에서는 유전자 간의 새로운 상호작용을 발견하는 데 도움이 되며, 소셜 네트워크에서는 실제 세계의 친구를 식별할 수 있습니다 [2, 34].
어떤 지도 학습 알고리즘은 정보를 가진, 구별되는, 독립적인 특징들의 집합을 필요로 합니다. 네트워크 예측 문제에서는 노드와 엣지를 위한 특징 벡터 표현을 구성해야 합니다. 전형적인 해결책은 전문 지식에 기반한 도메인 특정 특징을 수작업으로 설계하는 것입니다. 특징 공학에 필요한 지루한 노력을 제외하더라도, 이러한 특징들은 일반적으로 특정 작업을 위해 설계되며 다른 예측 작업에는 일반화되지 않습니다.
대안적인 접근 방식은 최적화 문제를 해결하여 특징 표현을 학습하는 것입니다 [4]. 특징 학습의 도전은 계산 효율성과 예측 정확성을 균형있게 조절하는 목적 함수를 정의하는 것입니다. 스펙트럼의 한쪽에는 하류 예측 작업의 성능을 최적화하는 특징 표현을 직접 찾는 것이 가능합니다. 이 지도 절차는 정확도가 높지만, 추정해야 할 매개 변수의 수가 증가하여 훈련 시간 복잡성이 높아집니다. 다른 극단에서는 목적 함수를 하류 예측 작업과 독립적으로 정의하고 표현을 순수하게 비지도 방식으로 학습할 수 있습니다. 이렇게 하면 최적화가 계산적으로 효율적이며, 신중하게 설계된 목적에 따라 작업에 독립적인 특징을 얻을 수 있습니다 [21, 23].
그러나 현재의 기술은 네트워크에서 확장 가능한 비지도 특징 학습을 위해 합리적인 목적을 정의하고 최적화하는 데에 실패합니다. 주성분 분석, 다차원 스케일링 및 그 확장과 같은 선형 및 비선형 차원 축소 기법을 기반으로 한 고전적인 접근 방식은 데이터 표현의 분산을 최대화하는 목적을 최적화합니다. 결과적으로, 이러한 접근 방식은 대규모 실제 네트워크에 대해 비용이 많이 드는 적절한 데이터 행렬의 고유 분해를 필수로 합니다. 또한, 결과적인 잠재 표현은 네트워크 예측 작업에서 성능이 좋지 않습니다.

대안으로, 노드의 지역 이웃을 보존하려는 목적을 설계할 수 있습니다. 이 목적은 단일 은닉층 피드포워드 신경망에 대한 역전파와 유사한 확률적 경사 하강법(SGD)을 사용하여 효율적으로 최적화될 수 있습니다. 최근의 시도들 [24, 28]은 효율적인 알고리즘을 제안하지만, 네트워크 이웃에 대한 엄격한 개념에 의존하므로 네트워크에 고유한 연결 패턴에 대해 거의 민감하지 않습니다. 구체적으로, 네트워크에서 노드는 소속된 커뮤니티에 따라 조직될 수 있습니다 (즉, 동질성); 다른 경우에는 네트워크에서 노드의 구조적 역할에 기반하여 조직될 수 있습니다 (즉, 구조적 동등성) [7, 10, 36]. 예를 들어, 그림 1에서 노드 u와 s1은 동일한 긴밀한 커뮤니티에 속하는 것을 관찰할 수 있으며, 두 개의 다른 커뮤니티에서 노드 u와 s6은 동일한 허브 노드의 구조적 역할을 공유합니다. 실제 세계의 네트워크는 이러한 동등성의 혼합을 보통 나타냅니다. 따라서, 동일한 네트워크 커뮤니티에 속하는 노드를 서로 가깝게 포함하는 표현을 학습할 수 있는 능력과 유사한 역할을 공유하는 노드가 유사한 임베딩을 가지도록 표현을 학습할 수 있는 능력을 모두 갖춘 유연한 알고리즘을 허용하는 것이 중요합니다. 이를 통해 특징 학습 알고리즘은 다양한 도메인과 예측 작업에 걸쳐 일반화할 수 있습니다.

현재 작업. 우리는 확장 가능한 특징 학습을 위한 반지도 알고리즘인 node2vec을 제안합니다. 우리는 자연어 처리에 대한 이전 연구 [21]를 바탕으로 SGD를 사용하여 사용자 정의 그래프 기반 목적 함수를 최적화합니다. 직관적으로, 우리의 접근 방식은 d-차원 특징 공간에서 노드의 네트워크 이웃을 보존하는 가능성을 최대화하는 특징 표현을 반환합니다. 우리는 노드의 네트워크 이웃을 생성하기 위해 2차 무작위 이동 접근법을 사용합니다.
우리의 주요 기여는 노드의 네트워크 이웃의 유연한 개념을 정의하는 것입니다. 적절한 이웃의 개념을 선택함으로써, node2vec은 노드의 네트워크 역할이나 소속된 커뮤니티에 기반한 표현을 학습할 수 있습니다. 이를 위해 우리는 주어진 노드의 다양한 이웃을 효율적으로 탐색하는 편향된 무작위 이동의 집합을 개발합니다. 이 결과 알고리즘은 유연하며, 우리에게 튜닝 가능한 매개변수를 통해 검색 공간을 제어할 수 있습니다. 이는 이전 연구 [24, 28]의 제한된 검색 절차와 대조적입니다. 따라서 우리의 방법은 이전 연구를 일반화하고 네트워크에서 관찰되는 동등성의 전체 스펙트럼을 모델링할 수 있습니다. 우리의 검색 전략을 조정하는 매개변수는 직관적으로 해석될 수 있으며, 다른 네트워크 탐색 전략을 향해 이동합니다. 이러한 매개변수는 반지도 학습 방식을 사용하여 직접 학습할 수도 있습니다.
또한 개별 노드의 특징 표현을 노드 쌍 (즉, 엣지)로 확장하는 방법을 보여줍니다. 엣지의 특징 표현을 생성하기 위해 우리는 간단한 이진 연산자를 사용하여 개별 노드의 학습된 특징 표현을 조합합니다. 이러한 조합성은 node2vec을 노드 및 엣지를 포함한 예측 작업에 적합하게 만듭니다.
우리의 실험은 네트워크에서 두 가지 일반적인 예측 작업에 초점을 맞추고 있습니다. 하나는 모든 노드에 하나 이상의 클래스 레이블이 할당되는 다중 레이블 분류 작업이고, 다른 하나는 노드 쌍이 주어졌을 때 엣지의 존재를 예측하는 링크 예측 작업입니다. 우리는 node2vec의 성능을 최첨단 특징 학습 알고리즘 [24, 28]과 비교합니다. 우리는 소셜 네트워크, 정보 네트워크, 시스템 생물학 네트워크와 같은 다양한 도메인의 실제 네트워크를 사용하여 실험을 진행합니다. 실험 결과, node2vec이 다중 레이블 분류에서 최대 26.7% 및 링크 예측에서 최대 12.6%의 성능 향상을 보입니다. 이 알고리즘은 10%의 레이블 데이터에서도 경쟁력 있는 성능을 보이며, 노이즈나 누락된 엣지와 같은 변동에도 강건합니다. 계산적으로, node2vec의 주요 단계는 쉽게 병렬화될 수 있으며, 수백만 개의 노드를 가진 대규모 네트워크에 몇 시간 안에 확장할 수 있습니다.

총적으로, 우리의 논문은 다음과 같은 기여를 합니다:
1. 우리는 효율적인 확장 가능한 특징 학습 알고리즘인 node2vec을 제안합니다. 이 알고리즘은 SGD를 사용하여 네트워크에 대한 이웃 보존 목적을 효율적으로 최적화합니다.
2. 우리는 node2vec이 확립된 네트워크 분석 방법과 일치함을 보여줍니다.


BFS (너비 우선 탐색)

깊이 우선 탐색

그림 1: 노드 u에서 BFS와 DFS 검색 전략 (k = 3).

원칙은 네트워크 과학에서 다양한 동등성에 부합하는 표현을 발견하는 유연성을 제공합니다.
3. 우리는 노드 기반 예측 작업을 위해 노드2벡 및 다른 특징 학습 방법을 확장하여 노드 쌍에 대한 이웃 보존 목적을 기반으로 합니다.
4. 우리는 실제 데이터셋에서 노드2벡을 사용하여 다중 레이블 분류 및 링크 예측을 경험적으로 평가합니다.
나머지 논문은 다음과 같이 구성되어 있습니다. 섹션 2에서는 네트워크의 특징 학습에 대한 관련 연구를 간략히 조사합니다. 섹션 3에서는 노드2벡을 사용한 특징 학습의 기술적 세부 사항을 제시합니다. 섹션 4에서는 다양한 실제 네트워크에서 노드와 엣지에 대한 예측 작업에서 노드2벡을 경험적으로 평가하고 알고리즘의 매개 변수 민감도, 변동 분석 및 확장성 측면을 평가합니다. 섹션 5에서는 노드2벡 프레임워크에 대한 논의를 마치고 미래 연구에 대한 일부 유망한 방향을 강조합니다. 데이터셋과 노드2벡의 참조 구현은 프로젝트 페이지에서 확인할 수 있습니다: http://snap.stanford.edu/node2vec.

2. 관련 연구
기계 학습 커뮤니티에서는 다양한 제목으로 특성 공학이 광범위하게 연구되었습니다. 네트워크에서 노드의 특성을 생성하기 위한 전통적인 패러다임은 일반적으로 네트워크 속성에 기반한 일부 초기 수작업 특성을 포함하는 특성 추출 기술에 기반합니다 [8, 11]. 반면에, 우리의 목표는 특성 추출을 표현 학습 문제로 캐스팅하여 전체 과정을 자동화하는 것입니다. 이 경우에는 수작업으로 설계된 특성이 필요하지 않습니다.
비지도 특성 학습 접근 방식은 일반적으로 그래프의 다양한 행렬 표현의 스펙트럼 특성을 활용합니다. 특히 라플라시안 행렬과 인접 행렬을 사용합니다. 이 선형 대수 관점에서 이러한 방법은 차원 축소 기술로 볼 수 있습니다. 선형 (예: PCA) 및 비선형 (예: IsoMap) 차원 축소 기술이 제안되었습니다 [3, 27, 30, 35]. 이러한 방법은 계산 및 통계적 성능의 단점을 가지고 있습니다. 계산 효율성 측면에서 데이터 행렬의 고유 분해는 근사치로 해결되지 않는 한 비용이 많이 듭니다. 따라서 이러한 방법은 대규모 네트워크에 확장하기 어렵습니다. 또한, 이러한 방법은 네트워크에서 관찰되는 다양한 패턴에 대해 견고하지 않은 목적을 최적화하며 (예: 동질성 및 구조적 동등성), 기본 네트워크 구조와 예측 작업 간의 관계에 대한 가정을 만듭니다. 예를 들어, 스펙트럴 클러스터링은 그래프 컷이 분류에 유용할 것이라는 강력한 동질성 가정을 만듭니다 [29]. 이러한 가정은 많은 시나리오에서 합리적이지만, 다양한 네트워크에 걸쳐 효과적으로 일반화하기에는 부족합니다.
자연어 처리를 위한 표현 학습에 대한 최근 발전은 단어와 같은 이산 객체의 특성 학습을 위한 새로운 방법을 열었습니다. 특히 Skip-gram 모델 [21]은 이웃 보존 우도 목적을 최적화함으로써 단어의 연속적인 특성 표현을 학습하는 것을 목표로 합니다. 알고리즘은 다음과 같이 진행됩니다. 문서의 단어를 스캔하고, 각 단어에 대해 주변 단어 (즉, 일부 문맥 창 내의 단어)를 예측할 수 있도록 단어를 임베딩하려고 합니다. 단어의 특성 표현은 SGD와 부정적인 샘플링을 사용하여 우도 목적을 최적화함으로써 학습됩니다 [22]. Skip-gram 목적은 분포 가설에 기반합니다. 이 가설은 비슷한 문맥에서 나타나는 단어들은 비슷한 의미를 가지는 경향이 있다는 것을 말합니다 [9]. 즉, 비슷한 단어는 비슷한 단어 이웃에 나타납니다.
Skip-gram 모델에서 영감을 받아 최근 연구에서는 네트워크를 "문서"로 표현함으로써 네트워크에 대한 유사한 방식을 확립했습니다 [24, 28]. 문서가 단어의 순서화된 시퀀스인 것과 마찬가지로, 기본 네트워크에서 노드의 시퀀스를 샘플링하고 네트워크를 노드의 순서화된 시퀀스로 변환할 수 있습니다. 그러나 노드를 샘플링하는 다양한 전략이 있으며, 이로 인해 다른 학습된 특성 표현이 생성됩니다. 사실, 우리가 보여줄 것처럼, 모든 네트워크와 모든 예측 작업에 걸쳐 효과적인 샘플링 전략이 명확하지 않습니다. 이는 노드 샘플링에 대한 유연성을 제공하지 않는 이전 연구의 주요한 결함입니다 [24, 28]. 우리의 알고리즘 node2vec은 특정 샘플링 전략에 결합되지 않은 유연한 목적을 설계함으로써 이 제한을 극복하며, 탐색된 탐색 공간을 조정하기 위한 매개 변수를 제공합니다 (3장 참조).
마지막으로, 노드 및 엣지 기반 예측 작업에 대해 최근 연구에서는 기존 및 새로운 그래프 특화 심층 네트워크 구조를 기반으로 한 지도 특성 학습에 대한 작업이 있습니다 [15, 16, 17, 31, 39]. 이러한 구조는 비선형 변환의 여러 계층을 사용하여 하류 예측 작업의 손실 함수를 직접 최소화함으로써 높은 정확도를 제공하지만, 훈련 시간 요구 사항이 높아 확장성이 제한됩니다.

3. 특징 학습 프레임워크
네트워크에서의 특징 학습을 최대 우도 최적화 문제로 정의합니다. G = (V,E)가 주어진 네트워크라고 가정합니다. 우리의 분석은 일반적이며 (비)방향, (비)가중치 네트워크에 적용됩니다. 우리는 노드에서 특징 표현을 학습하기 위한 매핑 함수 f : V → Rd를 목표로 합니다. 여기서 d는 특징 표현의 차원 수를 지정하는 매개변수입니다. 동등하게, f는 크기가 |V| × d인 행렬입니다. 모든 소스 노드 u ∈ V에 대해, 우리는 S라는 이웃 샘플링 전략을 통해 노드 u의 네트워크 이웃 NS(u)를 V의 부분집합으로 정의합니다. 우리는 Skip-gram 아키텍처를 네트워크에 확장하여 다음 목적 함수를 최적화하려고 합니다. 이 목적 함수는 f에 의해 주어진 노드 u의 특징 표현에 대해 조건부로 노드 u의 네트워크 이웃 NS(u)를 관찰하는 로그-우도를 최대화합니다.

맥스
에프
에이치

u∈V
f(u)에 대한 NS(u)의 로그 확률. (1)

최적화 문제를 다루기 쉽게 하기 위해 우리는 두 가지 표준 가정을 합니다:
• 조건부 독립성. 우리는 우리의 특징 표현을 기준으로 다른 이웃 노드를 관찰하는 것과 관련하여 이웃 노드를 관찰할 확률이 독립적이라고 가정하여 가능도를 분해합니다.

Pr(NS(u)|f(u)) = (cid:89) ni∈NS(u)Pr(ni|f(u)).
• 특징 공간에서의 대칭성. 소스 노드와 이웃 노드는 특징 공간에서 서로에게 대칭적인 영향을 미칩니다. 따라서, 우리는 조건부 확률 Pr(ni|f(u))를 모델링합니다.

모든 소스-이웃 노드 쌍의 후드를 소프트맥스 단위로 변환하고, 이는 그들의 특징들의 내적으로 매개변수화됩니다.

Pr(ni|f(u)) = exp(f(ni) · f(u)) / ∑v∈V exp(f(v) · f(u)).

위의 가정에 따라서, 식 1의 목적은 단순화됩니다.

맥스
에프
에이치

u∈V
− logZu +
∑
ni∈NS(u)f(ni) ·
f(u)
.  (2)

노드당 파티션 함수, Zu = (cid:80)

v∈V
f(u) · f(v)의 지수,
큰 네트워크에서 계산하기 어려우며 우리는 부정적인 샘플링 [22]을 사용하여 근사화합니다. 우리는 기능 f를 정의하는 모델 매개변수에 대한 확률적 경사 상승을 사용하여 방정식 2를 최적화합니다.
Skip-gram 아키텍처를 기반으로 한 기능 학습 방법은 원래 자연어 [21]의 맥락에서 개발되었습니다.
텍스트의 선형적인 특성 때문에 이웃의 개념은 연속된 단어 위에 슬라이딩 윈도우를 사용하여 자연스럽게 정의될 수 있습니다.
그러나 네트워크는 선형적이지 않으므로 더 풍부한 이웃의 개념이 필요합니다. 이 문제를 해결하기 위해 우리는 주어진 소스 노드 u의 다양한 이웃을 샘플링하는 무작위 절차를 제안합니다. 이웃 NS(u)는 즉시 이웃에 제한되지 않고 샘플링 전략 S에 따라 크게 다른 구조를 가질 수 있습니다.

3.1 전통적인 탐색 전략
우리는 소스 노드의 이웃을 샘플링하는 문제를 지역 탐색의 한 형태로 보고 있다. 그림 1은 소스 노드 u가 주어졌을 때 이웃 NS(u)를 생성(샘플링)하는 그래프를 보여준다. 다양한 샘플링 전략 S를 공정하게 비교하기 위해 이웃 집합 NS의 크기를 k개의 노드로 제한하고, 단일 노드 u에 대해 여러 집합을 샘플링한다. 일반적으로, k개의 노드로 이루어진 이웃 집합 NS를 생성하기 위해 두 가지 극단적인 샘플링 전략이 있다:
• 너비 우선 샘플링 (BFS) 이웃 NS는 소스의 즉시 이웃 노드로 제한된다. 예를 들어, 그림 1에서 크기가 k = 3인 이웃을 샘플링할 때 BFS는 노드 s1, s2, s3를 샘플링한다.
• 깊이 우선 샘플링 (DFS) 이웃은 소스 노드로부터 거리가 증가하는 순서로 순차적으로 샘플링된 노드로 구성된다. 그림 1에서 DFS는 노드 s4, s5, s6를 샘플링한다.
너비 우선과 깊이 우선 샘플링은 학습된 표현에 대한 탐색 공간에서 극단적인 시나리오를 나타내며 흥미로운 영향을 미친다.

특히, 네트워크의 노드에 대한 예측 작업은 종종 동질성과 구조적 동등성이라는 두 가지 유사성 사이를 오가게 된다 [12]. 동질성 가설 [7, 36]에 따르면, 서로 높은 상호 연결성을 가지고 비슷한 네트워크 클러스터나 커뮤니티에 속하는 노드들은 서로 가깝게 임베딩되어야 한다 (예: 그림 1의 노드 s1과 u는 동일한 네트워크 커뮤니티에 속한다). 반면에 구조적 동등성 가설 [10]에 따르면, 네트워크에서 유사한 구조적 역할을 가진 노드들은 서로 가깝게 임베딩되어야 한다 (예: 그림 1의 노드 u와 s6는 각각 해당 커뮤니티의 허브 역할을 한다). 중요한 점은 동질성과 달리 구조적 동등성은 연결성을 강조하지 않는다는 것이다. 노드들은 네트워크에서 멀리 떨어져 있을 수 있지만 여전히 동일한 구조적 역할을 가질 수 있다. 실제로, 이러한 동등성 개념은 상호 배타적이지 않다. 네트워크는 종종 동질성을 나타내는 노드와 구조적 동등성을 반영하는 노드 모두를 가지고 있다.
우리는 BFS와 DFS 전략이 위의 동등성 중 하나를 반영하는 표현을 생성하는 데 중요한 역할을 한다는 것을 관찰한다. 특히, BFS에 의해 샘플링된 이웃은 구조적 동등성과 밀접하게 일치하는 임베딩을 이끌어낸다. 직관적으로, 우리는 구조적 동등성을 확인하기 위해서는 지역 이웃을 정확하게 특성화하는 것만으로도 충분하다는 것을 알 수 있다. 예를 들어, 다리와 허브와 같은 네트워크 역할에 기반한 구조적 동등성은 각 노드의 즉시 이웃을 관찰함으로써 추론할 수 있다. BFS는 이러한 특성화를 달성하고 모든 노드의 이웃에 대한 미세한 관점을 얻는다. 또한, BFS에서 샘플링된 이웃 노드들은 많은 횟수로 반복되는 경향이 있다. 이는 소스 노드와의 1-hop 노드의 분포를 특성화하는 데 분산을 줄이는 데 중요하다. 그러나 주어진 k에 대해 그래프의 매우 작은 부분만 탐색된다.
DFS의 경우에는 소스 노드 u로부터 더 멀리 이동할 수 있기 때문에 네트워크의 더 큰 부분을 탐색할 수 있다 (샘플 크기 k가 고정된 경우). DFS에서 샘플링된 노드들은 동질성에 기반한 커뮤니티를 추론하는 데 필수적인 매크로적인 이웃의 특징을 더 정확하게 반영한다. 그러나 DFS의 문제는 네트워크에서 노드 간 종속성뿐만 아니라 이러한 종속성의 정확한 성격을 특성화하는 것이 중요하다는 것이다. 이는 샘플 크기에 제한이 있고 탐색해야 할 큰 이웃이 있는 경우에 어렵다. 또한, 더 깊은 수준으로 이동하는 것은 샘플링된 노드가 소스로부터 멀리 떨어져 있을 수 있으므로 복잡한 종속성을 야기한다.

3.2 노드투벡
위의 관찰을 기반으로, 우리는 유연한 이웃 샘플링 전략을 설계하여 BFS와 DFS 사이를 부드럽게 보간할 수 있습니다. 이를 위해 BFS 및 DFS 방식으로 이웃을 탐색할 수 있는 유연한 편향 랜덤 워크 절차를 개발합니다.

3.2.1 랜덤 워크
형식적으로, 주어진 출발 노드 u를 기반으로 고정된 길이 l의 랜덤 워크를 시뮬레이션합니다. ci는 워크에서 i번째 노드를 나타내며, c0는 u로 시작합니다. 노드 ci는 다음 분포에 따라 생성됩니다.

P(ci = x | ci−1 = v) = P(ci = x, ci−1 = v) / P(ci−1 = v)

(cid:40)
πvx
Z
만약 (v,x) ∈ E 이면
0 그렇지 않으면

노드 v와 x 사이의 비정규화 전이 확률인 whereπvx이고, Z는 정규화 상수입니다.

3.2.2 검색 편향 α
우리의 무작위 이동을 편향시키는 가장 간단한 방법은 정적인 엣지 가중치 wvx를 기반으로 다음 노드를 샘플링하는 것입니다. 즉, πvx = wvx입니다. (가중치가 없는 그래프의 경우 wvx = 1입니다.) 그러나 이렇게 하면 네트워크 구조를 고려하고 다양한 유형의 네트워크 이웃을 탐색하기 위한 검색 절차를 안내할 수 없습니다. 또한, 구조적 동등성과 동질성에 적합한 극단적인 샘플링 패러다임인 BFS와 DFS와 달리, 우리의 무작위 이동은 이러한 동등성 개념이 경쟁적이거나 배타적이지 않으며, 실제 세계의 네트워크는 일반적으로 둘 다 혼합되어 나타납니다. 우리는 두 개의 매개변수 p와 q를 가진 2차 무작위 이동을 정의합니다. 이 무작위 이동은 걷기를 안내하기 위해 다음 단계를 결정해야 합니다: (t,v) 엣지를 따라 이동한 무작위 이동이 이제 노드 v에 머물고 있습니다 (그림 2). 이제 이 무작위 이동은 v에서 시작하는 엣지 (v,x)에 대한 전이 확률 πvx를 평가해야 합니다. 우리는 정규화되지 않은 전이 확률을 πvx = αpq(t,x) · wvx로 설정합니다.


그림 2: 노드2벡스에서의 랜덤 워크 절차의 설명.
워크는 방금 t에서 v로 전환되었으며, 이제 v 노드에서 다음 단계를 평가하고 있습니다. 엣지 레이블은 탐색 편향 α를 나타냅니다.

그리고 dtx는 노드 t와 x 사이의 최단 경로 거리를 나타냅니다.
dtx는 {0,1,2} 중 하나이어야 하므로, 두 매개변수는 도보를 안내하는 데 필요하고 충분합니다.
직관적으로 말하면, 매개변수 p와 q는 도보가 시작 노드 u의 이웃을 탐색하고 떠나는 속도를 제어합니다. 특히, 이 매개변수들은 검색 절차가 BFS와 DFS 사이를 (대략적으로) 보간하고, 따라서 다른 노드 동등성 개념에 대한 친화성을 반영할 수 있도록 합니다.

반환 매개변수, p. 매개변수 p는 걷기에서 노드를 즉시 다시 방문할 가능성을 제어합니다. 높은 값으로 설정하면 (max(q,1)보다 큰 값), 다음 노드가 다른 이웃이 없는 경우를 제외하고 이미 방문한 노드를 샘플링할 가능성이 적어집니다. 이 전략은 중간 탐색을 장려하고 샘플링에서 2-hop 중복을 피하는 데 도움이 됩니다. 반면에, p가 낮은 값이면 (< min(q,1)), 걷기가 한 단계 되돌아가게 될 것이고 이는 걷기를 시작 노드 u에 "지역적"으로 유지시킵니다.

인-아웃 매개변수, q. 매개변수 q는 "안쪽"과 "바깥쪽" 노드를 구분할 수 있게 합니다. 그림 2로 돌아가면, q > 1이면 무작위로 이동하는 것은 노드 t에 가까운 노드를 향해 편향됩니다. 이러한 이동은 시작 노드에서의 작은 지역 내의 노드로 구성된 샘플을 가지므로 기본 그래프에 대한 지역적인 뷰를 얻을 수 있고, 근사적인 BFS 동작을 합니다. 반대로, q < 1이면, 이동은 노드 t로부터 더 멀리 떨어진 노드를 방문하는 경향이 더 큽니다. 이러한 동작은 DFS의 특징으로, 바깥쪽 탐색을 장려합니다. 그러나 여기서 중요한 차이점은 우리가 무작위로 이동하는 프레임워크 내에서 DFS와 유사한 탐색을 달성한다는 것입니다. 따라서 샘플된 노드는 주어진 출발 노드 u로부터 엄격히 증가하는 거리에 있지 않지만, 우리는 처리 가능한 전처리와 우수한 샘플링 효율성을 얻을 수 있습니다. 무작위로 이동하는 경우 πv,x를 이동의 이전 노드인 t의 함수로 설정함으로써, 이동은 2차 마르코프적입니다.

랜덤 워크의 이점. 순수한 BFS/DFS 접근 방식에 비해 랜덤 워크에는 여러 가지 이점이 있습니다. 랜덤 워크는 공간 및 시간 요구 사항 측면에서 계산적으로 효율적입니다. 그래프의 모든 노드의 즉시 이웃을 저장하기 위한 공간 복잡도는 O(|E|)입니다. 2차 랜덤 워크의 경우, 각 노드의 이웃들 간의 상호 연결을 저장하는 것이 도움이 되며, 이는 실제 네트워크에서는 일반적으로 작은 평균 차수인 a에 대한 공간 복잡도 O(a2|V|)를 유발합니다. 랜덤 워크의 다른 주요 이점은 클래식한 탐색 기반 샘플링 전략에 비해 시간 복잡도입니다. 특히, 샘플 생성 과정에서 그래프 연결성을 강제함으로써, 랜덤 워크는 다른 소스 노드 간에 샘플을 재사용하여 효과적인 샘플링 속도를 증가시키는 편리한 메커니즘을 제공합니다. 길이 l > k의 랜덤 워크를 시뮬레이션함으로써 한 번에 l - k 노드에 대해 k개의 샘플을 생성할 수 있습니다. 이는 Marko-Operator 심볼을 통해 가능합니다.
평균 (cid:1) [f(u) (cid:1) f(v)]i = fi(u)+fi(v)
2 Hadamard (cid:0) [f(u) (cid:0) f(v)]i = fi(u) ∗ fi(v)
가중치-L1 (cid:107) · (cid:107)¯1 (cid:107)f(u) · f(v)(cid:107)¯1i = |fi(u) − fi(v)|
가중치-L2 (cid:107) · (cid:107)¯2 (cid:107)f(u) · f(v)(cid:107)¯2i = |fi(u) − fi(v)|2

표 1: 엣지 특징 학습을 위한 이진 연산자 ◦의 선택.
이 정의들은 g(u,v)의 i번째 구성 요소에 해당합니다.

무작위로 움직이는 경로의 본질적인 복잡성 때문에, 우리의 효율적인 복잡성은 샘플 당 O(lk(l−k))입니다. 예를 들어, 그림 1에서 우리는 길이가 6인 무작위로 움직이는 경로 {u,s4,s5,s6,s8,s9}를 샘플링합니다. 이로 인해 NS(u) = {s4,s5,s6}, NS(s4) = {s5,s6,s8} 및 NS(s5) = {s6,s8,s9}가 됩니다. 샘플 재사용은 전체 절차에 약간의 편향을 도입할 수 있습니다. 그러나 우리는 이것이 효율성을 크게 향상시킨다는 것을 관찰합니다.

3.2.3 노드투벡 알고리즘

알고리즘 1 노드투벡스 알고리즘.

LearnFeatures (그래프 G = (V,E,W), 차원 d, 노드 당 Walks r, Walk 길이 l, Context 크기 k, Return p, In-out q)
π = PreprocessModifiedWeights(G,p,q)
G(cid:48) = (V,E,π)
Walk 초기화
iter = 1부터 r까지 반복
모든 노드 u ∈ V에 대해
walk = node2vecWalk(G(cid:48),u,l)
walks에 walk 추가
f = StochasticGradientDescent(k, d, walks)
f 반환

노드2벡스워크 (그래프 G = (V, E, π), 시작 노드 u, 길이 l)
걷기를 [u]로 초기화
걷기_반복 = 1부터 l까지 반복
현재 = 걷기[-1]
V현재 = 이웃노드얻기(현재, G)
s = AliasSample(V현재, π)
s를 걷기에 추가
걷기 반환

node2vec의 의사 코드는 알고리즘 1에 제시되어 있습니다. 임의의 랜덤 워크에서는 시작 노드 u의 선택으로 인해 암묵적인 편향이 있습니다. 모든 노드에 대한 표현을 학습하기 때문에, 우리는 모든 노드에서 시작하는 길이가 고정된 l의 r개의 랜덤 워크를 시뮬레이션하여 이 편향을 보상합니다. 워크의 각 단계에서 샘플링은 전이 확률 πvx에 기반하여 수행됩니다. 2차 마르코프 체인의 전이 확률 πvx는 사전에 계산될 수 있으며, 따라서 랜덤 워크를 시뮬레이션하는 동안 노드의 샘플링은 O(1) 시간 내에 효율적으로 수행될 수 있습니다. node2vec의 세 가지 단계인 전이 확률 계산을 위한 전처리, 랜덤 워크 시뮬레이션 및 SGD를 사용한 최적화는 순차적으로 실행됩니다. 각 단계는 병렬화되어 비동기적으로 실행되며, node2vec의 전반적인 확장성에 기여합니다. node2vec은 다음에서 사용할 수 있습니다: http://snap.stanford.edu/node2vec.

3.3 엣지 특징 학습
노드2벡 알고리즘은 네트워크의 노드들에 대한 풍부한 특징 표현을 학습하기 위한 준지도 학습 방법을 제공합니다. 그러나 우리는 종종 개별 노드 대신 노드 쌍을 포함하는 예측 작업에 관심이 있습니다. 예를 들어 링크 예측에서는 네트워크의 두 노드 사이에 링크가 존재하는지 예측합니다. 우리의 랜덤 워크는 자연스럽게 연결 구조에 기반하기 때문에...

기본 네트워크의 노드들 사이에서, 우리는 개별 노드들의 특징 표현을 이용하여 노드들의 쌍으로 확장합니다.
두 개의 노드 u와 v가 주어졌을 때, 우리는 해당하는 특징 벡터 f(u)와 f(v)에 대해 이항 연산자 ◦를 정의하여 표현 g(u,v)를 생성합니다. 이때 g : V × V → Rd(cid:48)이며, 여기서 d(cid:48)는 쌍 (u,v)의 표현 크기입니다. 우리는 연산자가 어떤 노드 쌍에 대해서도 일반적으로 정의되도록 원합니다. 실제로는 해당 쌍 사이에 엣지가 존재하지 않더라도, 이는 우리의 표현을 링크 예측에 유용하게 만들기 때문입니다. 우리의 테스트 세트에는 참과 거짓 엣지(즉, 존재하지 않는 엣지)가 모두 포함되어 있습니다. 우리는 표 1에 요약된 d(cid:48) = d인 여러 선택지를 고려합니다.

4. 실험
Eq.2에서의 목표는 어떠한 하류 작업과도 독립적이며, node2vec에 의해 제공되는 탐색의 유연성은 아래에서 논의되는 다양한 네트워크 분석 설정에 학습된 특징 표현을 제공합니다.

4.1 사례 연구: 레 미제라블 네트워크
3.1 절에서 우리는 BFS와 DFS 전략이 동질성 (즉, 네트워크 커뮤니티)과 구조적 동등성 (즉, 노드의 구조적 역할)의 원칙에 기반하여 노드를 포함하는 스펙트럼의 극단적인 끝을 나타낸다는 것을 관찰했습니다. 이제 우리는 이 사실을 경험적으로 입증하고 node2vec이 실제로 이러한 원칙을 따르는 임베딩을 발견할 수 있다는 것을 보여주려고 합니다.
우리는 노드가 소설 레 미제라블 [13]의 캐릭터에 해당하고 엣지가 공동으로 출현하는 캐릭터를 연결하는 네트워크를 사용합니다. 이 네트워크는 77개의 노드와 254개의 엣지로 구성되어 있습니다. 우리는 d = 16로 설정하고 네트워크의 모든 노드에 대한 특성 표현을 학습하기 위해 node2vec을 실행합니다. 특성 표현은 k-평균을 사용하여 클러스터링됩니다. 그런 다음 노드가 클러스터에 할당된 색상을 기반으로 원래 네트워크를 두 차원으로 시각화합니다.
그림 3 (상단)은 p = 1, q = 0.5로 설정한 경우의 예를 보여줍니다. 네트워크의 영역 (즉, 네트워크 커뮤니티)이 동일한 색상을 사용하여 색칠되는 것을 주목하세요. 이 설정에서 node2vec은 소설의 주요 부분에서 서로 자주 상호 작용하는 캐릭터들의 클러스터/커뮤니티를 발견합니다. 캐릭터 간의 엣지가 공동 출현에 기반하기 때문에 이 특성화는 동질성과 밀접한 관련이 있다고 결론짓을 수 있습니다.
같은 구조적 역할을 가진 노드를 발견하기 위해 동일한 네트워크를 사용하지만 p = 1, q = 2로 설정하고 node2vec을 사용하여 노드 특성을 얻은 다음 얻은 특성을 기반으로 노드를 클러스터링합니다. 여기서 node2vec은 색상이 구조적 동등성에 해당하는 클러스터에 보완적으로 노드를 할당합니다. 예를 들어, node2vec은 파란색으로 색칠된 노드를 서로 가깝게 임베딩합니다. 이러한 노드는 소설의 다른 부분 사이에서 다리 역할을 하는 캐릭터를 나타냅니다. 마찬가지로, 노란색 노드는 대부분 주변에 있고 상호 작용이 제한된 캐릭터를 나타냅니다. 이러한 노드 클러스터에 대해 대체 의미를 할당할 수 있지만, 핵심은 node2vec이 특정한 동등성 개념에 결합되어 있지 않다는 것입니다. 우리의 실험을 통해 보여주듯이, 이러한 동등성 개념은 대부분의 실제 네트워크에서 흔히 나타나며 학습된 표현의 성능에 중요한 영향을 미칩니다.

4.2 실험 설정
우리의 실험은 노드2벡스를 통해 얻은 특징 표현을 표준 지도 학습 작업인 노드의 다중 레이블 분류와 엣지의 링크 예측에 대해 평가합니다. 두 작업 모두에서 우리는 다음의 특징 학습 알고리즘에 대한 노드2벡스의 성능을 평가합니다:
그림 3: 노드2벡스에 의해 생성된 레 미제라블 공동출현 네트워크의 보완적 시각화. 레이블 색상은 동질성 (상단)과 구조적 동등성 (하단)을 반영합니다.

• 스펙트럴 클러스터링 [29]: 이는 그래프 G의 정규화된 라플라시안 행렬의 상위 d개의 고유벡터를 노드의 특징 벡터 표현으로 사용하는 행렬 분해 방법입니다.
• 딥워크 [24]: 이 방법은 균일한 무작위 워크를 시뮬레이션하여 d차원의 특징 벡터 표현을 학습합니다. 딥워크의 샘플링 전략은 node2vec의 p = 1 및 q = 1인 특수한 경우로 볼 수 있습니다.
• LINE [28]: 이 방법은 두 개의 별도 단계에서 d차원의 특징 벡터 표현을 학습합니다. 첫 번째 단계에서는 노드의 즉시 이웃들에 대한 BFS 스타일 시뮬레이션을 통해 d/2 차원을 학습합니다. 두 번째 단계에서는 소스 노드로부터 2-hop 거리에 있는 노드를 엄격하게 샘플링하여 다음 d/2 차원을 학습합니다.
DeepWalk [24]보다 성능이 낮은 다른 행렬 분해 방법은 제외합니다. 또한 GraRep [6]라는 최근의 접근 방식은 2-hop 이상의 네트워크 이웃 정보를 포함하지만 대규모 네트워크에 효율적으로 확장할 수 없습니다.
샘플링 기반 특징 학습 알고리즘을 평가하기 위해 이전 연구에서 사용된 설정과는 달리, 우리는 각 방법에 대해 동일한 수의 샘플을 생성한 다음 예측 작업에서 얻은 특징의 품질을 평가합니다. 이를 통해 알고리즘에 비해 구현 언어 (C/C++/Python)로 인한 성능 향상을 할인합니다. 따라서 샘플링 단계에서 DeepWalk, LINE 및 node2vec의 매개변수는 런타임에 동일한 수의 샘플을 생성하도록 설정됩니다. 예를 들어, K가 전체 샘플링 예산이라면, node2vec 매개변수는 K = r · l · |V |를 만족합니다. 최적화 단계에서 이러한 벤치마크는 모두 SGD를 사용하여 최적화하지만 두 가지 주요 차이점을 보정합니다. 첫째, DeepWalk는 softmax 확률을 근사하기 위해 계층적 샘플링을 사용하며 node2vec와 유사한 목적을 가지고 있습니다. 그러나 계층적 softmax는 부정 샘플링 [22]과 비교했을 때 비효율적입니다. 따라서 다른 모든 것을 동일하게 유지하면 DeepWalk에서는 부정 샘플링으로 전환합니다. 이는 node2vec와 LINE에서도 사실상의 근사치입니다. 둘째, node2vec과 DeepWalk는 hierarchical softmax 대신에 negative sampling을 사용합니다.

알고리즘        데이터셋
BlogCatalog PPI Wikipedia
스펙트럴 클러스터링 0.0405 0.0681 0.0395
딥워크      0.2110 0.1768 0.1274
LINE          0.0784 0.1447 0.1164
노드투벡      0.2581 0.1791 0.1552
노드투벡 설정 (p,q) 0.25, 0.25 4, 1 4, 0.5
노드투벡의 이득 [%] 22.3 1.3 21.8

표 2: 블로그 카탈로그, PPI (Homo sapiens) 및 위키피디아 단어 공존 네트워크에서 다중 레이블 분류에 대한 매크로-F1 점수, 노드의 50%가 훈련용으로 레이블이 지정되었습니다.

최적화를 위한 문맥 이웃 노드 수에 대한 매개변수가 있으며, 숫자가 클수록 최적화에 필요한 라운드가 더 많이 필요합니다. 이 매개변수는 LINE에서는 1로 설정되어 있지만, LINE은 다른 접근법보다 단일 에포크를 빠르게 완료하기 때문에 k 에포크 동안 실행합니다.
node2vec에 사용된 매개변수 설정은 DeepWalk와 LINE에서 일반적으로 사용되는 값과 일치합니다. 특히, d = 128, r = 10, l = 80, k = 10으로 설정하고 최적화는 단일 에포크에 대해 실행됩니다. 실험을 10개의 무작위 시드 초기화로 반복하며, 결과는 p-value가 0.01보다 작아 통계적으로 유의미합니다.
최적의 in-out 및 return 하이퍼파라미터는 10% 레이블 데이터에 대한 10-fold 교차 검증을 통해 학습되었으며, p,q ∈ {0.25,0.50,1,2,4}에 대한 그리드 서치로 수행되었습니다.

4.3 다중 레이블 분류
다중 레이블 분류 설정에서는 각 노드에 유한한 레이블 집합 L 중 하나 이상의 레이블이 할당됩니다. 훈련 단계에서는 일부 노드와 그들의 모든 레이블을 관찰합니다. 나머지 노드에 대한 레이블을 예측하는 것이 과제입니다. 특히 L이 큰 경우에는 이는 도전적인 과제입니다. 우리는 다음 데이터셋을 활용합니다:
• BlogCatalog [38]: 이는 BlogCatalog 웹사이트에 등록된 블로거들의 사회적 관계 네트워크입니다. 레이블은 블로거들이 제공한 메타데이터를 통해 추론된 블로거의 관심사를 나타냅니다. 이 네트워크는 10,312개의 노드, 333,983개의 엣지, 그리고 39개의 다른 레이블을 가지고 있습니다.
• 단백질-단백질 상호작용 (PPI) [5]: 우리는 인간의 PPI 네트워크의 부분 그래프를 사용합니다. 이 부분 그래프는 우리가 hallmark 유전자 세트 [19]로부터 레이블을 얻을 수 있는 노드들에 의해 유도된 그래프에 해당합니다. 이 네트워크는 3,890개의 노드, 76,584개의 엣지, 그리고 50개의 다른 레이블을 가지고 있습니다.
• Wikipedia [20]: 이는 Wikipedia 덤프의 첫 백만 바이트에 나타나는 단어들의 공존 네트워크입니다. 레이블은 Stanford POS-Tagger [32]를 사용하여 추론된 품사 (POS) 태그를 나타냅니다. 이 네트워크는 4,777개의 노드, 184,812개의 엣지, 그리고 40개의 다른 레이블을 가지고 있습니다.
이러한 네트워크들은 동질성과 구조적 동등성의 공정한 혼합을 보입니다. 예를 들어, 우리는 블로거들의 사회적 네트워크가 동질성을 기반으로 한 강한 관계를 보일 것으로 예상합니다. 그러나 "익숙한 이방인"이라고 할 수 있는 일부 블로거들도 있을 수 있습니다. 즉, 상호작용하지는 않지만 관심사를 공유하고 따라서 구조적으로 동등한 노드입니다. 단백질-단백질 상호작용 네트워크의 생물학적 상태들도 두 종류의 동등성을 모두 보입니다. 예를 들어, 이들은 이웃 단백질들과 보완적인 기능을 수행할 때 구조적 동등성을 나타내며, 다른 시간에는 유사한 기능을 수행하는 이웃 단백질들을 돕기 위한 동질성에 기반하여 조직화됩니다. 단어 공존 네트워크는 Wikipedia 말뭉치에서 2 길이 창에 공존하는 단어들 사이에 엣지가 존재하기 때문에 상당히 밀집되어 있습니다. 따라서, 같은 POS 태그를 가진 단어들을 찾는 것은 어렵지 않으며, 이는 높은 정도의 동질성을 나타냅니다. 동시에, 우리는 일부 구조적 동등성도 예상합니다.

나는
크로
마이크로 -
에프 1
에스코어
블로그카탈로그

0.0 0.2 0.4 0.6 0.8 1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30

매크로 - F1 스코어

0.0 0.2 0.4 0.6 0.8 1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
PPI (Homo Sapiens)

0.0 0.2 0.4 0.6 0.8 1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30
PPI (Homo Sapiens)

0.0 0.2 0.4 0.6 0.8 1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30

0.0 0.2 0.4 0.6 0.8 1.0
0.35
0.40
0.45
0.50
0.55
0.60
위키백과

0.0 0.2 0.4 0.6 0.8 1.0
0.00
0.05
0.10
0.15
0.20
0.25
0.30

스펙트럴 클러스터링 딥워크 라인 노드투벡

그림 4: 훈련에 사용된 레이블 데이터 양을 변화시킬 때 다른 벤치마크의 성능 평가. x축은 레이블 데이터의 비율을 나타내며, 상단과 하단 행의 y축은 각각 Micro-F1 및 Macro-F1 점수를 나타냅니다. DeepWalk와 node2vec는 PPI에서 비슷한 성능을 제공합니다. 다른 모든 네트워크에서는 모든 레이블 데이터 비율에 걸쳐 node2vec가 가장 우수한 성능을 발휘합니다.

명사에 이어지는 한정사, 명사 뒤에 오는 구두점 등과 같은 문법 패턴으로 인해 POS 태그가 결정됩니다.

실험 결과. 노드 특성 표현은 L2 정규화를 사용한 one-vs-rest 로지스틱 회귀 분류기에 입력됩니다. 훈련 및 테스트 데이터는 10개의 무작위 인스턴스로 균등하게 분할됩니다. 표 2에서 성능을 비교하기 위해 Macro-F1 점수를 사용하며, 상대적인 성능 향상은 가장 가까운 벤치마크 대비로 나타납니다. Micro-F1 및 정확도에 대해서도 유사한 추세가 나타나지만 표시되지 않습니다. 결과에서는 이웃 탐색의 추가적인 유연성으로 인해 node2vec이 다른 벤치마크 알고리즘을 능가하는 것을 명확히 볼 수 있습니다. BlogCatalog에서는 매개변수 p와 q를 낮은 값으로 설정하여 동질성과 구조적 동등성의 적절한 조합을 발견할 수 있으며, Macro-F1 점수에서 DeepWalk 대비 22.3%의 이득과 LINE 대비 229.2%의 이득을 얻을 수 있습니다. LINE은 예상보다 성능이 나쁘게 나타났는데, 이는 샘플 재사용 능력이 없어서 랜덤 워크 방법을 사용하여 쉽게 수행할 수 있는 기능 때문입니다. 동등성이 혼합된 다른 두 개의 네트워크에서도 node2vec의 반지도 학습 특성을 통해 특성 학습에 필요한 적절한 탐색 정도를 추론하는 데 도움을 받을 수 있습니다. PPI 네트워크의 경우, 최적의 탐색 전략 (p = 4, q = 1)은 DeepWalk의 균일한 (p = 1, q = 1) 탐색과 거의 구별할 수 없으며, 이미 방문한 노드에서의 중복을 피하기 위해 높은 p 값을 사용하여 DeepWalk보다 약간 우위를 차지하지만 Macro-F1 점수에서 LINE 대비 23.8%의 이득을 얻을 수 있습니다. 그러나 일반적으로 균일한 랜덤 워크는 node2vec이 학습한 탐색 전략보다 훨씬 나쁠 수 있습니다. 위키피디아 단어 공존 네트워크에서는 균일한 워크가 최상의 샘플로의 탐색 절차를 안내할 수 없으므로, DeepWalk 대비 21.8%의 이득과 LINE 대비 33.2%의 이득을 얻을 수 있습니다.
더 세부적인 분석을 위해, 훈련-테스트 분할을 10%에서 90%까지 변화시키면서 이전과 같이 데이터의 10%에서 매개변수 p와 q를 학습하는 동안 성능을 비교합니다. 간결함을 위해,

우리는 Micro-F1 및 Macro-F1 점수에 대한 결과를 요약하여 Figure 4에 그래픽으로 나타냅니다. 여기서 우리는 유사한 관찰을 합니다. Spectral clustering보다 모든 방법이 유의하게 우수한 성능을 보이며, DeepWalk는 LINE보다 우수한 성능을 보입니다. node2vec은 LINE보다 일관되게 우수한 성능을 보이며, 도메인 간에 DeepWalk에 비해 큰 개선을 이루어냅니다. 예를 들어, 우리는 BlogCatalog에서 70% 레이블 데이터에서 DeepWalk에 대해 가장 큰 개선을 26.7% 달성합니다. 최악의 경우, 학습된 표현에 검색 단계가 거의 영향을 미치지 않으므로 node2vec은 DeepWalk와 동등합니다. 마찬가지로, LINE과 비교했을 때 개선은 더욱 두드러지며, BlogCatalog에서 200% 이상의 drastical한 향상을 관찰할 뿐만 아니라, PPI와 같은 다른 데이터셋에서도 10% 레이블 데이터로 훈련할 때 41.1%까지 높은 개선을 관찰합니다.

4.4 매개변수 민감도
node2vec 알고리즘은 여러 매개변수를 포함하며, 그림 5a에서는 매개변수의 다른 선택이 node2vec의 성능에 어떤 영향을 미치는지 조사합니다. 이를 위해 BlogCatalog 데이터셋에서 레이블된 데이터와 레이블되지 않은 데이터를 50-50로 분할하여 사용합니다. 테스트 중인 매개변수를 제외한 모든 매개변수는 기본값을 가정합니다. p와 q의 기본값은 1로 설정됩니다.
p와 q의 매개변수에 대한 Macro-F1 점수를 측정합니다. node2vec의 성능은 in-out 매개변수 p와 return 매개변수 q가 감소함에 따라 향상됩니다. 이러한 성능 향상은 BlogCatalog에서 기대하는 동질성과 구조적 동등성에 기반할 수 있습니다. 낮은 q는 외부 탐색을 장려하지만, 시작 노드에서 너무 멀리 가지 않도록 낮은 p와 균형을 이룹니다.
또한, 특징의 수인 d와 노드의 이웃 매개변수(워크 수 r, 워크 길이 l, 이웃 크기 k)가 성능에 어떤 영향을 미치는지 조사합니다. 표현의 차원이 약 100 정도가 되면 성능이 포화되는 경향을 관찰합니다. 마찬가지로, 소스 당 워크 수와 길이를 증가시키면 성능이 향상되는 것을 관찰합니다.

매크로 -
F1
스코어

3 2 1 0 1 2 3
로그 2q
0.16
0.18 0.20
0.22
0.24
0.26
0.28

3 4 5 6 7 8 9
로그 2d
0.16
0.18 0.20
0.22
0.24
0.26
0.28

6 8 10 12 14 16 18 20
노드 당 워크 수, r
0.16
0.18
0.20
0.22
0.24
0.26
0.28

매크로 - F1 스코어

30 40 50 60 70 80 90 100110
걷기 길이, l
0.16
0.18
0.20
0.22
0.24
0.26
0.28

8 10 12 14 16 18 20
문맥 크기, k
0.16
0.18
0.20
0.22
0.24
0.26
0.28

3 2 1 0 1 2 3
로그 2p
0.16
0.18 0.20
0.22
0.24
0.26
0.28

매크로 -
F1
스코어

3 2 1 0 1 2 3
로그 2q
0.16
0.18 0.20
0.22
0.24
0.26
0.28

0.0 0.1 0.2 0.3 0.4 0.5 0.6
빠진 엣지의 비율
0.00
0.05
0.10
0.15
0.20
0.25
0.30

매크로 -
F1
스코어

6 8 10 12 14 16 18 20
노드 당 워크 수, r
0.16
0.18
0.20
0.22
0.24
0.26
0.28

매크로 - F1 스코어

30 40 50 60 70 80 90 100110
걷기 길이, l
0.16
0.18
0.20
0.22
0.24
0.26
0.28

추가 엣지의 분수
0.00
0.05
0.10
0.15
0.20
0.25
0.30

매크로 - F1 스코어

(b)

그림 5: (a). 매개변수 민감도 (b). BlogCatalog 네트워크에서의 다중 레이블 분류에 대한 격동 분석.

우리가 더 큰 전체 샘플링 예산 K를 가지고 표현을 학습하기 때문에 이는 놀랍지 않습니다. 이 두 매개변수는 방법의 성능에 상대적으로 높은 영향을 미칩니다. 흥미롭게도, 문맥 크기 k도 성능을 향상시키지만 최적화 시간이 증가하는 대가가 있습니다. 그러나 이 경우 성능 차이는 그렇게 크지 않습니다.

4.5 동요 분석
실제 세계의 많은 네트워크에서는 네트워크 구조에 대한 정확한 정보를 얻을 수 없습니다. 우리는 블로그 카탈로그 네트워크의 엣지 구조와 관련된 두 가지 불완전한 정보 시나리오에 대해 노드투벡의 성능을 분석하는 동요 연구를 수행했습니다. 첫 번째 시나리오에서는 빠진 엣지의 비율에 따른 성능을 측정합니다 (전체 네트워크 대비). 빠진 엣지는 무작위로 선택되며, 네트워크의 연결 구성 요소 수가 일정하도록 제약이 있습니다. Figure 5b (상단)에서 볼 수 있듯이, 빠진 엣지의 비율이 증가함에 따라 Macro-F1 점수의 감소는 대략적으로 작은 기울기로 선형적으로 감소합니다. 네트워크에서 빠진 엣지에 대한 견고성은 그래프가 시간에 따라 변화하는 경우 (예 : 인용 네트워크) 또는 네트워크 구축 비용이 비싼 경우 (예 : 생물학적 네트워크)에 특히 중요합니다.
두 번째 동요 설정에서는 네트워크 내에서 임의로 선택된 노드 쌍 사이에 잡음이 있는 엣지가 있습니다. Figure 5b (하단)에서 볼 수 있듯이, 노드투벡의 성능은 빠진 엣지 설정과 비교했을 때 초기에는 약간 더 빠르게 감소하지만, 시간이 지남에 따라 Macro-F1 점수의 감소 속도가 점차 느려집니다. 다시 말해, 노드투벡의 잘못된 엣지에 대한 견고성은 측정에 사용되는 측정값이 잡음이 있는 센서 네트워크와 같은 여러 상황에서 유용합니다.

4.6 확장성
확장성을 테스트하기 위해, 우리는 노드2벡을 사용하여 노드 표현을 학습합니다.
노드2벡은 에르도스-레니 그래프에 대해 기본 매개 변수 값을 사용하며,
노드 수를 100에서 1,000,000으로 증가시키고 평균 차수를 일정하게 유지합니다.

1   2   3   4    5   6   7
로그10 노드
01234
l o g 1 0 t i
m e
( i
n
s
e
c
o
n
d s )
샘플링 + 최적화 시간

샘플링 시간

그림 6: 평균 차수가 10인 에르드시-레니 그래프에서 노드2벡의 확장성.

of10. InFigure6,weempiricallyobservethatnode2vec scaleslin-
early with increase in number of nodes generating representations
for one million nodes in less than four hours. The sampling pro-
cedure comprises of preprocessing for computing transition proba-
bilities for our walk (negligibly small) and simulation of random
walks. The optimization phase is made efficient using negative
sampling [22] and asynchronous SGD [26].
Many ideas from prior work serve as useful pointers in mak-
ing the sampling procedure computationally efficient. We showed
how random walks, also used in DeepWalk [24], allow the sampled
nodes to be reused as neighborhoods for different source nodes ap-
pearing in the walk. Alias sampling allows our walks to general-
ize to weighted networks, with little preprocessing [28]. Though
we are free to set the search parameters based on the underlying
task and domain at no additional cost, learning the best settings of
Score            Definition
Common Neighbors | N(u) ∩ N(v) |
Jaccard’s Coefficient |N(u)∩N(v)|
|N(u)∪N(v)| Adamic-Adar Score (cid:80)

of10. Figure 6에서는 노드2벡이 노드 수의 증가에 따라 선형적으로 확장되며, 100만 개의 노드에 대한 표현을 4시간 이내에 생성한다는 것을 경험적으로 관찰했습니다. 샘플링 절차는 우리의 워크에 대한 전이 확률을 계산하기 위한 전처리(무시할 만큼 작음)와 무작위 워크의 시뮬레이션으로 구성됩니다. 최적화 단계는 부정적 샘플링 [22]과 비동기 SGD [26]를 사용하여 효율적으로 수행됩니다.
이전 작업에서 많은 아이디어들은 샘플링 절차를 계산적으로 효율적으로 만드는 데 유용한 지표로 작용합니다. 우리는 DeepWalk [24]에서도 사용되는 무작위 워크가 샘플링된 노드를 워크에 나타나는 다른 소스 노드의 이웃으로 재사용할 수 있도록 보였습니다. Alias 샘플링은 가중 네트워크에 일반화되는 우리의 워크를 가능하게 하며, 전처리가 거의 필요하지 않습니다 [28]. 우리는 추가 비용 없이 기본 작업과 도메인에 기반하여 검색 매개변수를 설정할 수 있지만, 최적의 설정을 학습하는 것이 가장 좋습니다.
점수            정의
공통 이웃 | N(u) ∩ N(v) |
자카드 계수 |N(u)∩N(v)|
|N(u)∪N(v)| 아다믹-아다르 점수 (cid:80)

t∈N(u)∩N(v)
1
log|N(t)|
선호적 연결 | N(u) | · | N(v) |

테이블 3: 노드 쌍 (u, v)에 대한 링크 예측 휴리스틱 점수, 각각의 즉시 이웃 집합 N(u)와 N(v)입니다.

우리의 검색 매개변수는 오버헤드를 추가합니다. 그러나 우리의 실험에서 확인된 바와 같이, 이 오버헤드는 노드투벡스가 반지도 학습이기 때문에 매우 적은 레이블 데이터로 이러한 매개변수를 효율적으로 학습할 수 있기 때문에 최소한입니다.

4.7 링크 예측
링크 예측에서는 일부 엣지가 제거된 네트워크가 주어지며, 이러한 누락된 엣지를 예측하고자 합니다. 우리는 다음과 같이 엣지의 레이블이 지정된 데이터셋을 생성합니다: 긍정적인 예를 얻기 위해, 네트워크에서 무작위로 선택된 엣지의 50%를 제거하면서, 엣지 제거 후 얻은 잔여 네트워크가 연결되어 있는지 확인합니다. 부정적인 예를 생성하기 위해, 서로 연결된 엣지가 없는 노드 쌍을 네트워크에서 무작위로 샘플링합니다.
기능 학습 알고리즘 중 어느 것도 링크 예측에 이전에 사용된 적이 없기 때문에, 우리는 추가적으로 node2vec을 일부 인기 있는 휴리스틱 점수와 비교하여 평가합니다. 우리가 고려하는 점수는 쌍을 구성하는 노드들의 이웃 집합에 기반하여 정의됩니다 (테이블 3 참조).
우리는 다음 데이터셋에서 벤치마크를 테스트합니다:
• Facebook [14]: Facebook 네트워크에서 노드는 사용자를 나타내고, 엣지는 두 사용자 간의 친구 관계를 나타냅니다. 이 네트워크에는 4,039개의 노드와 88,234개의 엣지가 있습니다.
• 단백질-단백질 상호작용(PPI) [5]: 인간을 위한 PPI 네트워크에서 노드는 단백질을 나타내고, 엣지는 두 개의 단백질 간의 생물학적 상호작용을 나타냅니다. 이 네트워크에는 19,706개의 노드와 390,633개의 엣지가 있습니다.
• arXiv ASTRO-PH [14]: 이는 논문이 제출된 e-print arXiv로부터 생성된 공동 연구 네트워크입니다. 노드는 과학자를 나타내고, 엣지는 두 과학자가 논문에서 공동 연구한 경우에 존재합니다. 이 네트워크에는 18,722개의 노드와 198,110개의 엣지가 있습니다.

실험 결과. 우리는 링크 예측을 위한 결과를 표 4에 요약합니다. 각 node2vec 항목에 대한 최적의 p와 q 매개변수 설정은 발표의 편의를 위해 생략되었습니다. 결과로부터 얻을 수 있는 일반적인 관찰은, 노드 쌍에 대한 학습된 특성 표현이 휴리스틱 벤치마크 점수보다 현저하게 우수한 성능을 보인다는 것입니다. arXiv 데이터셋에서 node2vec은 최고의 베이스라인(Adamic-Adar [1]) 대비 12.6%의 AUC 개선을 달성하여 가장 우수한 성능을 보입니다.
특성 학습 알고리즘 중에서, node2vec은 DeepWalk와 LINE을 모든 네트워크에서 능가하며, 각 알고리즘에 대해 이진 연산자의 최적 선택에 따라 AUC 점수에서 최대 3.8%와 6.5%의 향상을 보입니다. 개별적으로 연산자를 살펴보면(Table 1), node2vec은 DeepWalk와 LINE을 능가하며, Weighted-L1과 Weighted-L2 연산자를 사용하는 경우를 제외하고는 LINE이 더 우수한 성능을 보입니다. 전반적으로, node2vec과 함께 사용할 때 Hadamard 연산자는 매우 안정적이며 모든 네트워크에서 평균적으로 가장 우수한 성능을 제공합니다.

네트워크에서의 특징 학습을 검색 기반 최적화 문제로 연구했습니다. 이 관점은 여러 가지 이점을 제공합니다. 이는 고전적인 검색 전략을 설명할 수 있습니다.

알고리즘    데이터셋

페이스북 PPI arXiv
공통 이웃 0.8100 0.7142 0.8153
자카드 계수 0.8880 0.7018 0.8067
아다믹-아다르 0.8289 0.7126 0.8315
선호 부착 0.7137 0.6670 0.6996
스펙트럴 클러스터링 0.5960 0.6588 0.5812
(a) DeepWalk 0.7238 0.6923 0.7066
LINE 0.7029 0.6330 0.6516
node2vec 0.7266 0.7543 0.7221
스펙트럴 클러스터링 0.6192 0.4920 0.5740
(b) DeepWalk 0.9680 0.7441 0.9340
LINE 0.9490 0.7249 0.8902
node2vec 0.9680 0.7719 0.9366
스펙트럴 클러스터링 0.7200 0.6356 0.7099
(c) DeepWalk 0.9574 0.6026 0.8282
LINE 0.9483 0.7024 0.8809
node2vec 0.9602 0.6292 0.8468
스펙트럴 클러스터링 0.7107 0.6026 0.6765
(d) DeepWalk 0.9584 0.6118 0.8305
LINE 0.9460 0.7106 0.8862
node2vec 0.9606 0.6236 0.8477

테이블 4: 링크 예측을 위한 영역 아래 곡선 (AUC) 점수. 인기 있는 기준선과 임베딩 기반 방법과의 비교를 이진 연산자를 사용하여 부트스트랩한 결과: (a) 평균, (b) 하다마드, (c) 가중치-L1, 그리고 (d) 가중치-L2 (정의는 테이블 1 참조).

탐사-개발 트레이드오프. 또한, 예측 작업에 적용할 때 학습된 표현에 해석 가능성을 제공합니다. 예를 들어, 우리는 BFS가 제한된 이웃만 탐색할 수 있다는 것을 관찰했습니다. 이는 BFS가 노드의 즉각적인 지역 구조에 의존하는 네트워크의 구조적 동등성을 특징화하는 데 적합하다는 것을 의미합니다. 반면, DFS는 높은 분산 비용을 감수하면서 중요한 동질적인 커뮤니티를 발견하기 위해 네트워크 이웃을 자유롭게 탐색할 수 있습니다. DeepWalk와 LINE은 네트워크에서의 강직한 탐색 전략으로 볼 수 있습니다. DeepWalk [24]는 균일한 무작위 워크를 사용한 탐색을 제안합니다. 이러한 전략의 명백한 제한은 탐색된 이웃에 대해 어떠한 제어도 제공하지 않는다는 것입니다. LINE [28]은 주로 너비 우선 전략을 제안하며, 노드를 샘플링하고 1-hop 및 2-hop 이웃에 대해 독립적으로 가능도를 최적화합니다. 이러한 탐사의 효과는 쉽게 특징화할 수 있지만, 제한적이며 노드를 더 깊이 탐색하는 유연성을 제공하지 않습니다. 반면, node2vec의 탐색 전략은 매개변수 p와 q를 통해 네트워크 이웃을 유연하게 탐색하고 제어할 수 있습니다. 이러한 탐색 매개변수는 직관적인 해석을 가지고 있지만, 복잡한 네트워크에서는 데이터로부터 직접 학습할 때 최상의 결과를 얻을 수 있습니다. 실용적인 측면에서, node2vec은 확장 가능하며 변동에 강합니다.
우리는 노드 임베딩의 확장이 링크 예측을 위해 특별히 설계된 인기 있는 휴리스틱 점수보다 우수한 성능을 보여주었습니다. 우리의 방법은 표 1에 나열된 이외의 추가적인 이항 연산자를 허용합니다. 향후 연구에서는 Hadamard 연산자의 성공 이유와 탐색 매개변수에 기반한 엣지에 대한 해석 가능한 동등성 개념을 탐구하고자 합니다. node2vec의 향후 확장은 이질적인 정보 네트워크, 노드와 엣지에 대한 명시적 도메인 특징을 가진 네트워크, 그리고 부호화된 엣지 네트워크와 같은 특별한 구조를 가진 네트워크를 포함할 수 있습니다. 연속적인 특징 표현은 많은 딥러닝 알고리즘의 기반이 되며, 그래프에 대한 end-to-end 딥러닝에서 node2vec 표현을 구성 요소로 사용하는 것은 흥미로울 것입니다. 감사의 말씀을 드립니다. Austin Benson, Will Hamilton, Rok Sosiˇ c, Marinka Žitnik 및 익명의 심사위원들께 도움을 받았습니다. 이 연구는 NSF CNS-1010921, IIS-1149837, NIH BD2K, ARO MURI, DARPA XDATA, DARPA SIMPLEX, Stanford Data Science Initiative, Boeing, Lightspeed, SAP 및 Volkswagen의 일부로 지원되었습니다.

6. 참고문헌

[1] L. A. Adamic과 E. Adar. 웹상의 친구와 이웃. 사회 네트워크, 25(3):211–230, 2003.
[2] L. Backstrom과 J. Leskovec. 지도된 랜덤 워크: 사회 네트워크에서 링크 예측과 추천. WSDM, 2011.
[3] M. Belkin과 P. Niyogi. 라플라시안 이젠맵과 임베딩 및 클러스터링을 위한 스펙트럴 기법. NIPS, 2001.
[4] Y. Bengio, A. Courville, and P. Vincent. 표현 학습: 리뷰와 새로운 관점. IEEE TPAMI, 35(8):1798–1828, 2013.
[5] B.-J. Breitkreutz, C. Stark, T. Reguly, L. Boucher, A. Breitkreutz, M. Livstone, R. Oughtred, D. H. Lackner, J. Bähler, V. Wood 등. BioGRID 상호작용 데이터베이스. Nucleic acids research, 36:D637–D640, 2008.
[6] S. Cao, W. Lu, and Q. Xu. GraRep: 전역 구조 정보를 이용한 그래프 표현 학습. CIKM, 2015.
[7] S. Fortunato. 그래프에서의 커뮤니티 탐지. Physics Reports, 486(3-5):75 – 174, 2010.
[8] B. Gallagher과 T. Eliassi-Rad. 희소하게 레이블된 네트워크에서 레이블에 독립적인 특징을 활용한 분류: 경험적 연구. Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis. Springer, 2009.
[9] Z. S. Harris. 단어. 분포 구조, 10(23):146–162, 1954.
[10] K. Henderson, B. Gallagher, T. Eliassi-Rad, H. Tong, S. Basu, L. Akoglu, D. Koutra, C. Faloutsos, and L. Li. RolX: 대규모 그래프에서 구조적 역할 추출 및 마이닝. KDD, 2012.
[11] K. Henderson, B. Gallagher, L. Li, L. Akoglu, T. Eliassi-Rad, H. Tong, and C. Faloutsos. 당신이 아는 사람: 재귀적 구조적 특징을 이용한 그래프 마이닝. KDD, 2011.
[12] P. D. Hoff, A. E. Raftery, and M. S. Handcock. 사회 네트워크 분석을 위한 잠재 공간 접근 방법. J. of the American Statistical Association, 2002.
[13] D. E. Knuth. The Stanford GraphBase: 조합 계산을 위한 플랫폼, 37권. Addison-Wesley Reading, 1993.
[14] J. Leskovec과 A. Krevl. SNAP 데이터셋: Stanford 대형 네트워크 데이터셋 컬렉션. http://snap.stanford.edu/data, 2014년 6월.
[15] K. Li, J. Gao, S. Guo, N. Du, X. Li, and A. Zhang. LRBM: 링크된 데이터에 대한 제한된 볼츠만 머신 기반 접근 방법을 통한 표현 학습. ICDM, 2014.
[16] X. Li, N. Du, H. Li, K. Li, J. Gao, and A. Zhang. 동적 네트워크에서의 링크 예측을 위한 심층 학습 접근 방법. ICDM, 2014.
[17] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. 게이트 그래프 시퀀스 신경망. ICLR, 2016.
[18] D. Liben-Nowell and J. Kleinberg. 사회 네트워크에서의 링크 예측 문제. J. of the American society for information science and technology, 58(7):1019–1031, 2007.

[19] A. Liberzon, A. Subramanian, R. Pinchback, H. Thorvaldsdóttir, P. Tamayo, and J. P. Mesirov. 분자 서명 데이터베이스 (MSigDB) 3.0. Bioinformatics, 27(12):1739–1740, 2011.
[20] M. Mahoney. 대용량 텍스트 압축 벤치마크. www.mattmahoney.net/dc/textdata, 2011.
[21] T. Mikolov, K. Chen, G. Corrado, and J. Dean. 벡터 공간에서 단어 표현의 효율적인 추정. ICLR, 2013.
[22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 단어와 구문의 분산 표현과 그들의 합성. NIPS, 2013.
[23] J. Pennington, R. Socher, and C. D. Manning. GloVe: 단어 표현을 위한 전역 벡터. EMNLP, 2014.
[24] B. Perozzi, R. Al-Rfou, and S. Skiena. DeepWalk: 온라인 학습을 통한 사회적 표현. KDD, 2014.
[25] P. Radivojac, W. T. Clark, T. R. Oron, A. M. Schnoes, T. Wittkop, A. Sokolov, K. Graim, C. Funk, Verspoor 등. 단백질 기능 예측의 대규모 평가. Nature methods, 10(3):221–227, 2013.
[26] B. Recht, C. Re, S. Wright, and F. Niu. Hogwild!: 병렬 확률적 경사 하강법을 위한 락프리 접근 방식. NIPS, 2011.
[27] S. T. Roweis and L. K. Saul. 지역적 선형 임베딩에 의한 비선형 차원 축소. Science, 290(5500):2323–2326, 2000.
[28] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei. LINE: 대규모 정보 네트워크 임베딩. WWW, 2015.
[29] L. Tang and H. Liu. 분류를 위한 소셜 미디어 네트워크 활용. Data Mining and Knowledge Discovery, 23(3):447–478, 2011.
[30] J. B. Tenenbaum, V. De Silva, and J. C. Langford. 비선형 차원 축소를 위한 전역 기하학적 프레임워크. Science, 290(5500):2319–2323, 2000.
[31] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. 그래프 클러스터링을 위한 깊은 표현 학습. AAAI, 2014.
[32] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 순환 종속성 네트워크를 이용한 특성 풍부한 품사 태깅. NAACL, 2003.
[33] G. Tsoumakas and I. Katakis. 다중 레이블 분류: 개요. Thessaloniki 아리스토텔레스 대학교 정보학과, 2006.
[34] A. Vazquez, A. Flammini, A. Maritan, and A. Vespignani. 단백질-단백질 상호작용 네트워크로부터의 전역 단백질 기능 예측. Nature biotechnology, 21(6):697–700, 2003.
[35] S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. 그래프 임베딩과 확장: 차원 축소를 위한 일반적인 프레임워크. IEEE TPAMI, 29(1):40–51, 2007.
[36] J. Yang and J. Leskovec. 중첩 커뮤니티는 네트워크의 핵심-주변 구조를 설명한다. Proceedings of the IEEE, 102(12):1892–1902, 2014.
[37] S.-H. Yang, B. Long, A. Smola, N. Sadagopan, Z. Zheng, and H. Zha. 좋아요 같은 것은 서로 유사하다: 소셜 네트워크에서의 공동 친구 및 관심사 전파. WWW, 2011.
[38] R. Zafarani and H. Liu. ASU의 소셜 컴퓨팅 데이터 저장소, 2009.
[39] S. Zhai and Z. Zhang. 희소 그래프에서의 링크 예측을 위한 매트릭스 인수분해와 오토인코더의 드롭아웃 훈련. SDM, 2015.

