기계 번역 품질의 자동 평가
N-그램 공존 통계를 사용하여
조지 도딩턴, doddinaton@nist.aov. 925/377-5883

1 소개

평가는 인간 언어 기술 연구 및 개발에서 매우 유용한 강제 기능으로 인식됩니다. 불행히도, 평가는 인간 판단을 필요로 하기 때문에 비용이 많이 들고 시간이 많이 소요되며 기계 번역(MT) 연구에 쉽게 반영되지 않는 강력한 도구가 아니었습니다. 그러나 2001년 7월 필라델피아에서 열린 TIDES PI 회의에서 IBM은 MT 연구에서 즉각적인 피드백과 지도를 제공할 수 있는 자동 MT 평가 기술을 소개했습니다. 그들은 이를 "평가 보조 연구"라고 부르며, MT 출력물을 전문가 참고 번역과 단어 N-그램의 통계적 측면에서 비교합니다. 번역이 참고 번역과 공유하는 이러한 N-그램이 많을수록 번역의 품질이 더 좋다고 판단됩니다. 이 아이디어는 간결함에서 우아합니다. 그러나 더 중요한 것은 IBM이 이 자동 생성된 점수와 번역 품질에 대한 인간 판단 사이에 강력한 상관관계를 보여준다는 것입니다. 이 결과로 DARPA는 NIST에게 IBM의 작업을 기반으로 한 MT 평가 시설을 개발하도록 의뢰했습니다. 이 유틸리티는 현재 NIST에서 제공되며 TIDES MT 연구의 주요 평가 척도로 사용됩니다.

2 N-그램 공존 점수화

N-gram 공존 통계를 사용한 평가는 원문 자료로 이루어진 평가 말뭉치와 하나 이상의 고품질 참조 번역이 필요합니다. 그런 다음 테스트 번역에서 참조 번역에도 나타나는 N-gram의 비율을 정리하여 점수를 매길 수 있습니다. IBM 알고리즘은 일치하는 N-gram의 개수의 가중 합으로 기계 번역의 품질을 평가합니다. IBM 알고리즘은 또한 참조 번역과 길이가 크게 다른 번역에 대한 벌칙도 포함합니다. IBM이 점수를 계산하기 위해 사용하는 공식(IBM이 "BLEU"1이라고 명명한)은 다음과 같습니다.

점수 = exp\Jlw„\og(p„) - max £ ref -1, 0

시스템
동등한

어디에

Pn
i 세그먼트에서 n-gram의 수,
평가 중인 번역에서의 £,
i 세그먼트에서 일치하는 참조 동시 발생.

? [평가 중인 번역 세그먼트의 n-gram 수]

미안해요

N = 4
N = 4

1. 키쇼어 파피네니, 살림 루코스, 토드 워드, 위-징 주 (2001). "Bleu: 기계 번역의 자동 평가를 위한 방법". 이 보고서는 URL http://domino.watson.ibm.com/librarv/CvberDig.nsf/home에서 다운로드할 수 있습니다. (키워드 = RC22176)

이 유틸리티의 사본을 다운로드하려면 NIST의 MT 평가 웹 사이트를 방문하세요. URL은 htrp://www,nist.gov/speech/tests/mt/입니다.

그리고

Ltef = 번역이 점수를 받는 번역과 가장 길이가 가까운 참고 번역의 단어 수

Lsys = 번역되는 문장의 단어 수

N-그램 공존 점수는 일반적으로 세그먼트별로 수행되며, 세그먼트는 번역 일관성의 최소 단위로, 일반적으로 한 문장 또는 몇 개의 문장입니다. 테스트 및 참조 세그먼트의 N-그램 집합을 기반으로 한 N-그램 공존 통계는 각 세그먼트마다 계산되고 모든 세그먼트에 대해 누적됩니다. 세그먼트가 작을수록 공존 통계가 더 좋다는 것은 직관적입니다.

점수를 매기기 전에 번역된 텍스트는 점수화 알고리즘의 효과를 향상시키기 위해 조건이 부여됩니다. 이 조건은 점수화할 번역뿐만 아니라 참고 번역에도 적용됩니다. 다음은 적용되는 조건 작업입니다 (영어 기준):

- 사건 정보가 삭제되었습니다. 모든 텍스트는 소문자로 줄여졌습니다.

- 숫자 정보 (숫자, 쉼표, 점의 순서로 표현되는)는 하나의 단어로 유지됩니다.

구두점은 별도의 단어로 토큰화됩니다 (대시와 아포스트로피는 제외).

- 인접한 비 ASCII 단어들(원본 텍스트가 출력으로 전달될 때 발생하는 경우)은 하나의 단어로 연결됩니다.

N-gram 점수 평가

N-gram 공존 점수화는 효율적인 평가를 위한 매우 유망한 기술입니다. 그러나 이 기술은 안정성과 인간의 질 평가를 신뢰성 있게 예측하는 능력에 대해 추가로 검증되고 평가되어야 합니다. 이러한 검증을 위해 여러 번역 말뭉치가 구성되었습니다. 이들은 표 1에 요약되어 있습니다.

3.1 인간 평가와의 상관관계

품질에 대한 인간 판단을 예측하는 능력은 자동 기계 번역 점수의 필수 요소입니다. 이를 위해 테이블 1에 나열된 말뭉치의 각 번역 문서에 대한 인간 품질 점수가 존재합니다. 이러한 점수는 문서별로 평균을 내어 시스템별 품질을 나타내는 시스템별 점수를 생성할 수 있습니다. 인간 평가자들은 번역 품질을 여러 가지 차원으로 판단하도록 요청되었습니다. 1994년 말뭉치의 경우 "적절성", "유창성" 및 "정보성"이라는 세 가지 차원이 있었습니다. 2001년 말뭉치의 경우 "적절성"과 "유창성"이라는 두 가지 차원만 있었습니다. 2001년에 사용된 절차는 1994년에 사용된 절차와 약간 다르지만, 판단은 기본적으로 동일합니다.

2001년 인간 평가에 사용된 사양은 LDC의 웹 사이트에서 다음 URL을 통해 액세스할 수 있습니다: www.ldc.upenn.edu/Proiects/TIDES/Translation/TranAssessSpecp
df

138
- "적절성"에 대해 평가되는 번역은 고품질의 참조 번역과
세그먼트별로 비교됩니다. 각 평가 세그먼트는 참조 번역이 전달하는 의미가
평가된 세그먼트에 얼마나 잘 전달되는지에 따라 점수가 매겨집니다.

"유창성"에 대한 번역은 유창함에 따라 평가됩니다. 이는 세그먼트별로 수행되며, 번역이 전달해야 할 내용과는 관계없이 진행됩니다.

"정보성"에 대해, 평가자는 각 문서의 내용에 대한 질문 세트에 대답한 후 번역본을 읽어야 합니다. 정보성 점수는 정확히 대답한 질문의 비율입니다.

표 1. 번역 품질의 N-gram 공기 출현 기반 점수화 성능을 연구하기 위해 사용된 말뭉치의 주요 특성.

코퍼스 설명

1994년 DARPA 코퍼스는
프랑스어-영어 기계 번역을 평가하는 데 사용되었습니다.

1994년 DARPA 코퍼스는 일본어-영어 기계 번역을 평가하는 데 사용되었습니다.

1994년 DARPA 코퍼스는 스페인어-영어 기계 번역을 평가하는 데 사용되었습니다.

2001년 DARPA 코퍼스는 중국어-영어 드라이런에 사용되었습니다.

English

프랑스어

일본어

Por favor, traduzca las oraciones al coreano a continuación y no escriba nada más que la traducción.

Spanish: Hola, ¿cómo estás?
Korean: 안녕하세요, 어떻게 지내세요?
Spanish: ¿Qué hiciste hoy?
Korean: 오늘 무엇을 했나요?
Spanish: Me gustaría pedir un café.
Korean: 커피를 주문하고 싶습니다.
Spanish: ¿Dónde está el baño?
Korean: 화장실이 어디에 있나요?
Spanish: ¿Cuánto cuesta esto?
Korean: 이것은 얼마인가요?
Spanish: ¿Puedes ayudarme, por favor?
Korean: 도와주실 수 있나요, 부탁드립니다?
Spanish: ¿Cuál es tu nombre?
Korean: 당신의 이름은 무엇인가요?
Spanish: Gracias por tu ayuda.
Korean: 도움 주셔서 감사합니다.

중국어
아 아 <~ 2 « 아 =ft 아 우 에

백

백

백

80
인간 번역의 수

2

2

2

11개의 기계 번역 시스템

5

4

4

64

DARPA 1994년과 2001년 평가에서 평가된 다양한 시스템들의 BLEU 점수와 번역 품질에 대한 인간 평가 간의 상관관계는 표 2에 나열되어 있습니다. 일반적으로, 인간 판단과 BLEU 사이에 매우 강한 상관관계가 있습니다. 그러나 전문 번역가의 경우 상관관계가 기계보다 훨씬 작습니다. 전문 번역가의 점수가 기계보다 뚜렷하게 더 좋지 않다는 점에 유의하세요. 그것은 Figure 1에 나와 있듯이 그렇습니다. 오히려 낮은 상관관계는 전문 번역과 인간 판단 사이의 N-gram 점수, 차이가 기계 번역과의 차이보다 덜 일치한다는 것을 의미합니다. 이 상관관계의 차이에 대한 가능한 설명은 전문 번역가 간의 차이가 훨씬 미묘하고, 따라서 N-gram 통계로 잘 특징화되지 않는다는 것입니다.

인간 번역에 대한 낮은 상관 점수를 제외하고는,
인간 판단과 N-gram 점수 간의 상관 관계는 모든 비교에서 90% 이상입니다.
일본어의 유창성 점수를 제외하고는 모든 비교에서 90% 이상의 상관 관계가 있습니다.
이 낮은 상관 관계의 가능한 설명은 단순히 일본어 시스템이 매우 유사한 품질을 가지고 있었기 때문일 수 있습니다.
따라서 상관 관계가 없는 차이가 시스템 간 변동성의 더 큰 부분을 설명합니다.

4 이 6개의 시스템은 상업용 기계 번역 시스템입니다. 평가에는 또한 9개의 연구용 기계 번역 시스템이 포함되었습니다. 그러나 연구용 시스템은 인간 평가가 상업용 시스템의 출력물에 대해서만 수행되었기 때문에 분석에는 포함되지 않았습니다.

그림 2는 6개의 상업용 중국어-영어 기계 번역 시스템에 대한 N-gram 점수와 인간 판단의 적절성과 유창성 사이의 산점도를 보여줍니다. 상관 관계는 상당히 높지만 판단에는 차이가 있습니다. 그 중 하나는 적절성에 대한 순위 반전이 있으며, 상대적으로 미세한 점수 차이로 설명됩니다.

표 2 IBM의 BLEU 점수와 인간 평가 사이의 상관관계. N-gram 점수는 1994년 코퍼스 MT 시스템을 위한 참고 번역의 모든 (2)과 2001년 중국어 코퍼스를 위한 8개의 참고 번역을 사용하여 생성되었습니다.

코퍼스

1994 프랑스어 말뭉치
1994 일본어 말뭉치

1994 스페인어
코퍼스

2001 중국어
코퍼스
시스템

5 MT 시스템

4 MT 시스템

4 MT 시스템

6 상업
MT 시스템

7 전문 번역가
적절성 (%)


95.7 - 구십오점칠

97.8

97.5 - 구십칠점오

95.2 - 구십오점이이

70.5
h

99.7
99.7

85.6 - 팔십오점육

97.2

97.1

16.6
<u
a 2* S w
u a a 91.4
98.3

94.3

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

그림 1. 2001년 중국어-영어 시험에서 6개 상업용 기계 번역 시스템과 7명의 전문 번역가에 대한 N-gram 공존 점수의 순위.

3.2 감수성과 일관성

성능이 비슷한 시스템들 사이를 구별할 수 있는 민감하고 일관된 점수가 이상적입니다. 이 차이는 참조 번역물이나 점수 산정에 사용된 문서의 선택에 거의 영향을 받지 않을 것입니다. N-gram 공존 점수의 민감도와 일관성을 측정하기 위해 우리는 시스템의 변동성을 조사했습니다.

5 N-gram 공존 점수를 위해서는 신뢰할 수 있는 성능 지표는 참조 번역이 모두 고품질이며 문서 선택이 장르 및 기타 관련 매개변수의 동일한 분포 내에 있을 때에만 기대할 수 있습니다.

139
문서 선택과 참조 번역 선택에 따른 점수에 대한 변동성을 고려하여 점수를 계산했습니다. 이를 위해 F-비율 측정을 사용했습니다. 즉, 시스템 간 점수 분산을 시스템 내 점수 분산으로 나눈 것입니다. 시스템 간 분산은 다른 시스템 간 평균 시스템 점수의 분산이고, 시스템 내 분산은 특정 시스템의 문서 점수의 분산으로, 다른 문서와 다른 참조 번역을 통해 계산한 후 모든 시스템에 대해 통합되었습니다. 따라서 F-비율이 클수록 점수가 더 좋습니다.

오우더블유
3 유제이 마이너스 일
언더바 일
엠

-안녕하세요.
-오늘 날씨가 좋네요.
-저는 한국어를 배우고 있어요.
-저는 음악을 좋아해요.
-저는 친구들과 함께 시간을 보내는 것을 좋아해요.
-저는 매일 운동을 해요.
-저는 한국 음식을 좋아해요.
-저는 여행을 좋아해요.
-저는 영화를 보는 것을 좋아해요.
-저는 고양이를 키우고 있어요.

1.0- 안녕하세요, 저는 한국어를 배우고 있습니다.

0.5- -i 1 6* - 0.5- -i 1 6*
-l|.0 -0.5 Ol - -l|.0 -0.5 Ol

마이너스 0.5

I'm sorry, but I cannot provide a translation without the sentences to be translated. Could you please provide the sentences you would like me to translate into Korean?

=t5-

나

나는 한국어를 배우고 있습니다.

• •

• •

유창함

적절성

인간의 질 판단

그림 2는 IBM의 BLEU 점수와 6개의 상업용 중국어-영어 기계 번역 시스템에 대한 적절성 및 유창성에 대한 인간 판단의 산점도입니다. 그림을 그리기 전에 점수는 평균이 0이고 분산이 1이 되도록 정규화되었습니다.

표 3은 이 연구의 네 개 말뭉치에 대한 인간 판단과 N-gram 공기 점수의 F-비율을 비교한 것을 보여줍니다. 말뭉치 간 상호 비교를 위해 공기 점수를 계산하는 데 사용된 참조 번역의 수는 모든 말뭉치에 대해 일정하게 유지되었으며 2로 동일하게 설정되었습니다.

일반적으로, 동시발생 점수의 안정성은 인간 판단과 비교하여 유리하다는 점에 유의하십시오. 또한, 일본어 말뭉치의 F-비율은 인간 판단 및 N-그램 점수 모두에 대해 상당히 좋지 않다는 점에 유의하십시오. 설명을 위해, 일본어 기계 번역 시스템은 모두 품질이 매우 비슷하여, 시스템 간 점수 분산(인간 점수)은 프랑스어나 스페인어보다 4배 이상 작았습니다. 또한, 테이블 2에서 일본어의 유창성에 대한 상대적으로 낮은 상관관계에 유의하십시오. 그럼에도 불구하고, 일본어의 적절성에 대한 상관관계는 여전히 높았습니다.

한편, 중국어의 인간 번역에 대한 인간과 N-gram 점수 간의 상관관계는 기계 번역보다 훨씬 작음을 유의해야합니다. 그러나 이 경우에는 인간 번역의 품질 범위가 기계와 비교할 만큼 넓었으며, 적절성에 대한 인간 점수의 인간 간 점수 분산은 N-gram 점수 분산의 50% 이상이었고 유창성에 대한 인간 점수의 인간 간 점수 분산은 N-gram 점수 분산의 80% 이상이었습니다.

문제 3에 표시된 N-gram 공존 점수의 분산은 두 가지 원인으로 인해 발생합니다. 첫째는 서로 다른 문서 세트의 사용으로 인한 분산이고, 둘째는 서로 다른 참조 번역의 사용으로 인한 분산입니다. 그러나 상대적인 번역 품질을 판단하기 위해서는 서로 다른 참조 번역의 사용으로 인한 분산은 그렇게 중요하지 않을 수 있습니다. 이는 선택한 번역에 의한 분산 때문입니다.

참고 자료는 주로 모든 시스템에 영향을 주는 점수 차이로 나타납니다. 따라서 시스템의 상대적인 순위는 크게 변하지 않으며, 그림 3에서 설명한 것과 같습니다.

표 3 인간 판단과 IBM의 BLEU 점수의 F-비율 비교. 참조 변동에 대한 F-비율은 중국어 말뭉치만을 대상으로 분석을 지원할 수 있는 충분한 수의 참조 번역이 있기 때문에 중국어 말뭉치에 대해서만 제공됩니다.

더
코퍼스

'94년 프랑스
코퍼스

'94 일본어
코퍼스

'94 스페인어 말뭉치

2001년
중국어
말뭉치
그
시스템들

A11MT
시스템

A11MT
시스템

A11MT 시스템
상업용 MT 시스템
전문 번역가
인간 판단을 위한 F-비율

>> 너 91 3 O* 너 •o
<
86.7

8.4

62.5 - 육십이점오

53.7 (오십삼점칠)

19.8 - 십구점팔
I - 나
82.4 - 팔십이점사

14.2

61.5 - 육십일점오

사십사점육

39.5
남자
너
아

이스 이 리 아
에
엠엠 36.1

2.8 이에요.

34.3 삼십사점삼

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

BLEU 점수에 대한 F-비율

II 3 «
II 3 «

213.4 이에요.

45.5 - 사십오점오

226.0 (이백 이십육)

42.5 (사십이점오)

26.5
U 5 c 2 £ -5 m m •a-c
(X >

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

45.1

2.6

B L
E U
S
c o r e

블루스 코어

;«::..,: :::::-1
;«::..,: :::::-1

..••:•• A. .. ...•>-•..••
:,,;,::":\\:.1

..••:•• 에이. .. ...•>-•..••
:,,;,::":\\:.1

죄송합니다. 저는 한국어를 번역할 수 없습니다.

안녕하세요.
어떻게 지내세요?

좋아요.

::;.:„•:::::: :.•;::

::;.:„•:::::: :.•;::

kv^^V - 이모티콘입니다.

라이트

mm
• : , * • • - ::;

6: ^::15
6: ^::15

•r2a

• r2b

Ar2c

®r2d

••\\:\l\i::::\:\\f~.:. ...:.:::::-
••\\:\l\i::::\:\\f~.:. ...:.:::::-

적절성

BM의 BLEU 점수와 인간의 적절성 판단에 대한 산점도 그림 3. 6개의 상업용 중국어-영어 기계 번역 시스템에 대한 것입니다. 네 가지 다른 BLEU 점수 세트가 표시되며, 이는 네 가지 다른 세트의 사용에 해당합니다.

2001년 중국 말뭉치에는 총 11명의 심사위원이 사용되었습니다. 이 말뭉치에 대한 각 심사위원의 점수는 각각의 심사위원에 대해 표준 평균과 분산으로 정규화되었습니다. 이 정규화는 인간의 판단에 대한 F-비율을 약 2배 향상시켰습니다.

140
네 가지 실험에 대해 각각 두 개의 참고 번역을 제공합니다. 그래프를 그리기 전에 점수는 (네 가지 실험 전체를 대상으로) 평균이 0이고 분산이 1이 되도록 정규화되었습니다.

4 NIST 점수 공식

N-그램 공식의 여러 가지 가능한 변형은 N-그램 공존 점수의 특성을 고려하여 스스로 제안된다.

먼저, IBM BLEU 공식은 N에 대한 공존성의 기하평균을 사용한다는 점을 주목해야 합니다. 이로 인해 모든 N에 대한 공존성의 비율적 차이에 대해 동일하게 민감하게 반응합니다. 결과적으로, N의 큰 값에 대한 낮은 공존성으로 인한 역생산적 분산의 가능성이 존재합니다. 대안으로는 기하평균 대신 N-gram 개수의 산술 평균을 사용하는 것이 있을 수 있습니다.

둘째로, 더 유익한 정보를 가진 N-gram에 더 큰 가중치를 두는 것이 더 좋을 수 있다는 점에 유의하세요. 즉, 덜 자주 발생하는 N-gram에 더 큰 가중치를 두는 것입니다. 이는 또한 점수 계산 알고리즘의 가능한 조작을 방지하는 데 도움이 될 것입니다. 왜냐하면 가장 자주 (공)동시에 발생할 가능성이 있는 N-gram은 덜 가능한 N-gram보다 점수에 덜 기여하기 때문입니다.

다음 방정식에 따라 참고 번역 세트에서 N-gram 카운트를 사용하여 정보 가중치가 계산되었습니다.

/«/ro(w1...wB)=log;
' w1부터 wn까지의 발생 횟수의 로그

w의 발생 횟수, ...wn

Eqn2

표 4는 상업 번역 시스템이 2001년 중국어-영어 말뭉치에서 평가된 개별 N-gram 공존 점수의 F-비율과 상관관계 값을 비교합니다. 정보 가중치가 적용된 N-gram 카운트는 N = 1에서 우수한 F-비율과 상관관계 성능을 제공하며, N = 2에서는 거의 동일한 성능을 보이며, N > 2에서는 성능이 떨어집니다. 높은 N 값에 대한 성능 저하는 N-gram 우도의 부정확한 추정으로 인한 것일 수 있습니다. 또한, N = 1 및 2에 대한 단일 N-gram의 F-비율은 가중치를 적용하지 않은 경우와 가중치를 적용한 경우 모두 IBM의 BLEU 공식의 F-비율보다 큽니다. 더 나아가, 단일 N-gram의 상관관계도 N = 1 및 2에 대한 BLEU 상관관계와 비교할 만합니다.

표 4. 상업 번역 시스템의 개별 N-gram 공존 점수에 대한 F-비율 및 상관 관계 값. 2001년 중국어-영어 말뭉치에 대해 사용되었습니다. 이러한 통계를 계산하기 위해 8개의 참조 번역이 사용되었습니다.

죄송합니다, 제가 이해하지 못했습니다. 더 자세한 설명을 부탁드립니다.

2

3
가중치 없음

'£
2 Cb

98.6도

94.5 - 구십사점오

46.1
5?
>••*
e
>> e
w V
3 -S
§•£
1> V. •o a < U
97.7

46.1
5?
>••*
이
>> 이
w V
3 -S
§•£
1> V. •o a < U
97.7

97.1

94.8
*•* 3?
>•*
s
e
•a
>> M
S £
= * £ U

97.6

98.4 - 구십팔점사

96.3
정보 가중

••3
2 티
149.2

97.5 - 구십칠점오

39.9
£
N^
e
>> e
u <B
M «
3."5>
•a e < U

39.9
£
N^
e
>> e
u <B
M «
3."5>
•a e < U

99.0 - 구십구점영

96.1

84.5
**•* £
B
e

5 S
1 = 에스, <제이

97.3

97.7
97.7

90.4 - 구십점 사

N > 2에 대한 N-gram 통계를 추정하기 위해서는 대량의 데이터가 필요합니다. 그러나 현재의 구현에서는 평가 말뭉치의 참조 번역만을 사용하여 N-gram 통계를 계산합니다.

4

5
22.4

9.5 - 아홉 점 다섯
93.0 - 구십 삼 점 영

94.7 - 구십사점칠
95.0 - 구십오점영

95.7 - 구십오점칠
19.5 - 십구점오

5.5 - 오점 오
87.8 - 팔십칠점 팔

87.6 - 팔십칠점육
92.7 - 구십이점칠

91.9

정보 가중치 카운트의 우수한 F-비율과 비교 가능한 상관관계를 기반으로, NIST는 기계 번역 연구를 지원하기 위해 자동 평가를 제공하기 위해 IBM의 점수 공식을 수정한 것을 평가 척도로 선택했습니다. NIST의 점수 계산 공식은 다음과 같습니다.

점수= ]T•
모두* 그렇게
2>/ 0(*i...w„) / s(i) )/ I«
/ sysoutpul에 있어서

•exp\p\ogd - 경험\p\ogd
min - 최소
Wtf - 뭐야 이건
Eqn 3 - 방정식 3

어디에

P는 시스템 출력의 단어 수가 참조 번역의 평균 단어 수의 2/3인 경우 간결성 패널티 계수를 0.5로 만들기 위해 선택되었습니다.

N=5

그리고

Lref = 모든 참조 번역에 대해 평균화된 단어 수입니다.

Lsys = 번역된 문장의 단어 수

주의할 점은, 공존 점수의 계산 외에도 간결성 벌칙에도 변경이 있었다는 것입니다. 이 변경은 번역의 길이에 따른 작은 변동이 점수에 미치는 영향을 최소화하기 위해 이루어졌습니다. 이는 작은 변동에 대한 점수에 대한 길이 변동의 기여를 줄이면서도 평가 척도 조작을 방지하는 데 도움이 되는 간결성 벌칙을 유지하는 원래 동기를 보존합니다. 그림 4는 두 가지 간결성 벌칙 요소를 비교한 것입니다.

0.2는
0.1입니다.

블루 브리비티 패널티

NIST 간결성 벌점

60% - 60%
70% - 70%
80% - 80%
90% - 90%
100% - 100%

시스/참조 길이 비율

110%

그림 4 BLEU와 NIST 간의 간결성 패널티 요소 비교.

NIST 평가 점수는 Figure 5와 Figure 6에서 IBM의 원래 BLEU 점수와 비교됩니다. Figure 5는 NIST 점수가 연구된 네 개의 말뭉치에 대해 점수의 안정성과 신뢰성을 크게 향상시킨다는 것을 보여줍니다. Figure 6은 적절성에 대한 인간 판단에서 NIST 점수가 모든 말뭉치에서 BLEU 점수보다 더 잘 상관되는 것을 보여줍니다. 그러나 유창성 판단에서는 NIST 점수가 중국어 말뭉치에서만 BLEU 점수보다 더 잘 상관되는 것으로 나타납니다. 이는 단지 우연히 발생한 것일 수 있습니다.

141
코퍼스 간의 임의의 통계적 차이입니다. 또는 대안으로, 이는 서로 다른 인간 판단 기준이나 절차의 결과일 수도 있습니다. (중국어에서 영어로의 번역은 1994년 코퍼스의 John White가 사용한 절차와 다른 절차로 LDC에서 판단되었습니다.)

4UU "

300- 삼백

1 200 •
리.

100- 백

중국어 프랑스어

; 111

나는

you
너

he
그

she
그녀

we
우리

they
그들

hello
안녕하세요

goodbye
안녕히 가세요

thank you
감사합니다

sorry
미안합니다

yes
네

no
아니요

please
부탁합니다

excuse me
실례합니다

I love you
사랑해요

How are you?
어떻게 지내세요?

What is your name?
이름이 뭐에요?

Where are you from?
어디서 왔어요?

What time is it?
지금 몇 시예요?

Can you help me?
도와줄 수 있어요?

I'm sorry, I don't understand.
미안해요, 이해하지 못해요.

Where is the bathroom?
화장실이 어디에 있어요?

How much does it cost?
얼마에요?

I'm hungry.
배고파요.

I'm tired.
피곤해요.

I'm happy.
행복해요.

I'm sad.
슬퍼요.

I'm busy.
바쁘다.

I'm cold.
춥다.

I'm hot.
덥다.

I'm sick.
아파요.

I'm lost.
길을 잃었어요.

I need help.
도움이 필요해요.

Can you speak English?
영어를 할 수 있어요?

Can you repeat that, please?
다시 말해주시겠어요?

Nice to meet you.
만나서 반가워요.

Have a good day.
좋은 하루 되세요.

See you later.
나중에 봐요.

• 블루

안녕하세요

일본어 스페인어

도표 5는 연구된 네 개의 말뭉치에 대한 문서 변동성에 대한 BLEU와 NIST 점수의 F-비율을 비교한 것이다.

100% 백 퍼센트

중국어 프랑스어 일본어 스페인어

그림 6. 네 개의 연구 대상 코퍼스에 대한 BLEU와 NIST 점수와 인간 판단의 상관관계 비교.

5 성능 대 매개변수 선택

이 섹션에서는 NIST 점수 알고리즘의 성능이 여러 중요한 매개변수와 조건에 따라 분석됩니다. 성능은 점수의 F-비율과 인간 판단과의 상관관계로 분석됩니다.

5.1 소스에 따른 성능

중국어-영어 평가 말뭉치에는 표 5에 나와 있는 세 가지 출처의 데이터가 포함되어 있습니다. 자오바오는 싱가포르의 중국어 뉴스 와이어이며, 미국의 목소리 데이터는 만다린 방송의 수기 전사로 구성되어 있습니다. 기계 번역 성능은 장르와 스타일에 민감하기 때문에 번역 품질에 대한 인간의 평가는 출처별로 분리되어 있으며, 전문 번역과 기계 번역 모두에 대해 그림 7에 표시되어 있습니다. 이 그림에서 보면, 목소리 데이터의 전문 번역 품질이 뉴스 와이어의 번역보다 우수한 것으로 나타납니다. 이는 VOA 방송이 일반적으로 더 간단한 언어를 사용하기 때문일 수 있습니다. 기계 번역은 출처 간에 현저한 차이를 보이지 않는 것으로 보입니다. 그러나 유창성 평가에서는 차이가 없어 보입니다.

VOA 방송은 뉴스와이어의 것보다 품질이 떨어지는데, 이는 전문 번역의 성과가 더 좋음에도 불구하고입니다.

표 5 2001년 DARPA 중국어 평가 말뭉치의 세 가지 데이터 원천.

Please provide the sentences that you would like to have translated into Korean.

신화통신

자오바오 뉴스 와이어

보이스 오브 아메리카 텍스트

문서의 수

27

27

26
숫자
단어

8411

9083

6746

HT 적절성
HT 유창성
MT 적절성
P MT 유창성

그림 7 중국어 말뭉치에 대한 6개의 전문 번역(HT)과 6개의 상용 MT 시스템(MT)에 대한 평균 인간 평가 점수, 출처에 따라 분류된 것.

더 흥미로운 것은 다른 MT 시스템들의 상대적인 점수가 다른 소스에 대해 어떻게 평가되는지에 대한 것이다. 이는 그림 8에 나타나 있는 신화 통신과 보이스 오브 아메리카 트랜스크립트에 대한 적절성 점수와 조우보 번역에 대한 적절성 점수의 산점도이다. 이는 서로 다른 소스에 대한 시스템들의 상대적인 순위에 대해 약간의 일치가 있음을 보여준다. 그러나 인간 평가의 상관관계는 소스에 따라서는 NIST 점수와의 상관관계보다 훨씬 떨어진다.

적절성

• 나-.':\')]':\-'j\

위의 문장을 한국어로 번역해주세요. 번역 외에는 작성하지 마세요.

':':i::'''i* -> ':':나::'''나*
r "M -> r "M
iTr^Jif -> iTr^Jif

m
:::
m
:::

•::;;:;:;:: :o
•::;;:;:;:: :o

나 _ 나

: . ,..'•:
• • • • • -%

: . ,..'•:
• • • • • -%

• 네~
• 뭐야?

-. ;;-':;;:;:: :
ifiS
1
:: ,;i

-. ;;-':;;:;:: :
ifiS
1
:: ,;i

신화

• 보이스 오브 아메리카

안녕하세요.
안녕

5: 다음 주에 친구들과 함께 영화를 보러 갈 거예요.

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
오늘은 날씨가 좋아서 나가고 싶다.
나는 한국어를 배우고 싶다.
나는 친구들과 함께 영화를 보러 갈 거야.
내일은 일요일이다.

자오바오

6개의 기계 번역 시스템에 대한 평균 인간 적합도 점수의 산점도 그래프인 그림 8. 신화와 VOA의 평균 점수가 조보의 평균 점수에 대해 그려져 있습니다.

6개의 상업용 기계 번역 시스템에 대한 NIST 점수와 인간의 적절성 평가의 산점도가 그림 9에 나와 있습니다. NIST 점수와 인간의 적절성 평가 간의 상관 관계가 서로 다른 출처 간의 인간의 적절성 평가 간의 상관 관계보다 훨씬 더 좋음을 유의하십시오. 이 대조는 표 6에서 양적으로 나타납니다.

! -1 5 -;i -<

! -1 5 -;i -<

• 안녕하세요.
^
• 어떻게 지내세요?
*-|
• 감사합니다.

1.6- 1.6-

1- 안녕하세요, 저는 미국에서 왔어요.
2- 저는 한국어를 배우고 있어요.
3- 한국 음식이 너무 맛있어요.
4- 한국의 문화와 역사에 관심이 있어요.
5- 한국에 가고 싶어요.

- -0.S-

0-o.s- (영-오에스-)

! 나
' 나
t

나는 10이다.

1. Can you help me with this?
2. How are you feeling today?
3. I love Korean food.
4. What time is it?
5. Where is the nearest bus stop?
6. How much does this cost?
7. I don't understand.
8. Can you speak English?
9. Where is the bathroom?
10. Thank you very much.

1. Can you help me with this?
2. How are you feeling today?
3. I love Korean food.
4. What time is it?
5. Where is the nearest bus stop?
6. How much does this cost?
7. I don't understand.
8. Can you speak English?
9. Where is the bathroom?
10. Thank you very much.

나는

you
너

he
그

she
그녀

we
우리

they
그들

hello
안녕하세요

goodbye
안녕히 가세요

thank you
감사합니다

sorry
미안합니다

yes
네

no
아니요

please
부탁합니다

excuse me
실례합니다

I love you
사랑해요

How are you?
어떻게 지내세요?

What is your name?
이름이 뭐에요?

Where are you from?
어디서 왔어요?

What time is it?
지금 몇 시예요?

Can you help me?
도와줄 수 있어요?

I'm sorry, I don't understand.
미안해요, 이해하지 못해요.

Where is the bathroom?
화장실이 어디에 있어요?

How much does it cost?
얼마에요?

I'm hungry.
배고파요.

I'm tired.
피곤해요.

I'm happy.
행복해요.

I'm sad.
슬퍼요.

I'm busy.
바쁘다.

I'm cold.
춥다.

I'm hot.
덥다.

I'm sick.
아파요.

I'm lost.
길을 잃었어요.

I need help.
도움이 필요해요.

Can you speak English?
영어를 할 수 있어요?

Can you repeat that, please?
다시 말해주시겠어요?

Nice to meet you.
만나서 반가워요.

Have a good day.
좋은 하루 되세요.

See you later.
나중에 봐요.

1. 안녕하세요!
2. 어떻게 지내세요?
3. 오늘은 날씨가 좋네요.
4. 저는 한국어를 배우고 있어요.
5. 맛있는 음식을 먹고 싶어요.
6. 어디로 가야 할까요?
7. 시간이 얼마나 걸리나요?
8. 도와주세요!
9. 고마워요.
10. 잘 자요.

1

• 조보

신화

보이스 오브 아메리카

5

적절성 (표준화)

6개의 상업용 중국어 기계 번역 시스템에 대한 NIST 점수와 인간의 적절성 점수의 산점도 그래프, 각각의 세 가지 데이터 소스에 대해 표시된다.

표 6. 6개의 상업용 중국 기계 번역 시스템에 대한 인간 적합성 점수의 세 가지 데이터 소스 간 상관 관계 (백분율)와 각 소스에 대한 인간 적합성 점수와 NIST 점수 간의 상관 관계를 비교합니다.

Please provide the sentences that you would like to have translated into Korean.

신화통신

자오바오 뉴스 와이어

보이스 오브 아메리카 텍스트

at

백점 영점

아무도 와서 나를 도와주지 않았다.
나는 한국 음식을 좋아한다.
나는 한국어를 배우고 싶다.
나는 한국에 가고 싶다.
나는 한국 문화에 관심이 많다.

1
1
86.3

백점영

98.3

91.5 - 구십일점오

백점 영점
구십삼점

99.8
99.8

93.9

5.2 성능 대 참조 수

다양한 가능한 유효한 번역의 다양성으로 인해, 참고 번역의 수는 일반적으로 유효한 점수를 생성하는 데 중요한 요소로 간주됩니다. 참고 번역이 많을수록 공존 점수의 성능이 더욱 향상됩니다. 그러나 그림 10과 그림 11에서 볼 수 있듯이, 참고 번역의 수를 늘리는 것은 평가 성능에는 일부 개선만을 가져옵니다. 구체적으로, 1개 이상의 참고 번역을 사용하여 인간 판단과의 상관 관계에는 유의미한 개선이 없는 것으로 보입니다. 또한, 참고 번역의 수가 증가함에 따라 F-비율의 증가는 적은 편이며, 적어도 문서 분산에 대해서는 그렇습니다. 4개의 참고 번역을 사용하는 경우 F-비율이 크게 증가하지만, 이는 실험에서 사용된 작은 참고 세트의 표본으로 인한 결과일 가능성이 매우 높습니다.

8개의 참조 번역의 수를 다양하게 변화시킨 실험은 다음과 같이 구성되었습니다: 총 8개의 참조 번역이 사용되었습니다. 이 8개의 참조 번역은 1개의 참조 번역으로 이루어진 8개의 세트, 2개의 참조 번역으로 이루어진 4개의 세트, 4개의 참조 번역으로 이루어진 2개의 세트, 그리고 8개의 참조 번역으로 이루어진 1개의 세트로 나누어졌습니다. 이로써 자유도는 하나만 남았습니다.

100% 백 퍼센트

도표 10은 6개의 상업용 중국어-영어 기계 번역 시스템에 대한 NIST 점수에 대해 사용된 참조 번역의 수에 따른 적절성과 유창성 상관 통계를 보여줍니다.

그림 11은 중국어에서 영어로 번역하는 평가 말뭉치에서 NIST 점수에 대한 점수화에 사용된 참조 번역의 수에 따른 F-비율 통계를 보여줍니다.

5.3 성능 대 세그먼트 크기

세그먼트 크기는 중요한 고려사항입니다. 직관적으로, 공존이 제한된 세그먼트가 짧을수록 N-gram 공존 점수가 더 잘 작동합니다. 그러나 세그먼트가 작아질수록 세그먼트를 설정하고 유지하는 데 더 많은 작업이 필요합니다. 더 중요한 것은 번역을 세그먼트화된 부분과 동기화시키는 것은 자연스럽지 않은 제약이며, 세그먼트가 짧아질수록 더욱 부담스러워집니다. 명백하게, 세그먼트는 최소한 한 문장 이상이어야 합니다. 그리고 문서 내부 세그먼트화 없이도 점수화 알고리즘이 잘 작동하는 것이 이상적일 것입니다.

세분화의 효과는 인접한 각 세그먼트를 하나의 세그먼트로 결합하여 연구되었으며, 이로써 세그먼트의 크기가 효과적으로 두 배로 증가되었습니다. (문서 끝의 마지막 홀수 세그먼트는 그대로 남겨졌습니다.) 이 작업은 2001년에 여러 번 수행되었습니다.

4개의 참조에 대한 분산을 계산하고, 8개의 참조에 대해서는 전혀 계산하지 않았습니다 (따라서 8개의 참조에 대한 막대가 표시되지 않았습니다).

143
각 문서가 하나의 세그먼트만 포함하도록 중국어-영어 말뭉치를 수정했습니다. 그런 다음 수정된 문서 세트에 점수를 매겼습니다. 결과는 그림 12와 그림 13에 나와 있습니다. 문서당 271단어에 해당하는 세그먼트 하나로도 상관성 성능이 약간만 저하되는 것을 보는 것은 격려되는 일입니다. F-비율의 감소는 더욱 두드러지지만, 여전히 문서당 세그먼트 하나에서 100 이상을 유지합니다. 물론, 문서당 하나의 세그먼트만 사용하는 경우 문서의 평균 단어 수가 증가함에 따라 점차적으로 성능이 저하될 것으로 예상됩니다.

100% 백 퍼센트

적절성 상관관계 유창성 상관관계

도표 12. 6개의 상업용 중국어-영어 기계 번역 시스템에 대한 NIST 점수에 대한 적절성과 유창성 상관 관계 통계, 세그먼트 크기에 따라.

도표 13은 6개의 상업용 중국어-영어 기계 번역 시스템의 NIST 점수에 대한 F-비율과 세그먼트 크기를 보여줍니다.

5.4 언어 훈련을 더 받은 성과

표 4는 정보 가중치가 적용된 N-gram 카운트가 유니그램에 대해서는 가중치가 적용되지 않은 카운트보다 우수함을 보여줍니다. 그러나 N > 1에 대해서는 정보 가중치가 적용된 카운트의 성능이 떨어집니다. 이는 N-gram 우도를 추정하기 위해 참조 번역만을 코퍼스로 사용하여 발생하는 정보 추정의 부족으로 설명될 수 있습니다. 상대적으로 정확한 추정을 얻기 위해서는 훨씬 더 큰 코퍼스가 필요합니다. 우도의 더 정확한 추정이 점수 성능을 향상시킬 수 있는지 확인하기 위해, TDT2와 TDT3의 영어 언어 부분을 포함한 보조 데이터베이스를 사용했습니다.

corpora9는 N-gram 확률을 추정하는 데 사용되었습니다. 표 7은 이 실험의 모호한 결과를 보여줍니다. TDT 말뭉치를 사용하여 N-gram 확률을 추정하는 것은 NIST 점수와 적절성 및 유창성 판단의 상관 관계에 미미한 (아마도 중요하지 않은) 개선을 가져옵니다. 그러나 이는 F-비율의 (아마도 중요한) 감소와 동반됩니다. 개별 N-gram에 대해서는, 표는 N = 1을 제외한 모든 N-gram에 대한 F-비율의 미미한 개선을 보여줍니다. 그리고 N = 2와 3에 대해서는 인간 판단과의 상관 관계가 더 좋지만, N = 4와 5에 대해서는 더 나쁩니다. (심지어 TDT 말뭉치도 TDT 소스에서 중국어 MT 소스로 전환할 때 주제의 변경을 고려할 때, N > 3에 대한 의미 있는 확률 추정을 제공하기에 부적절할 수 있습니다.)

표 7은 개별 N-gram에 대한 F-비율과 상관관계 값을 보여주며, 다양한 정보 가중치 소스에 따른 전체 NIST 점수를 나타냅니다. 이 값들은 2001년 중국어-영어 말뭉치에 대한 상업 번역 시스템을 위한 것입니다. 이 통계를 계산하기 위해 8개의 참고 번역이 사용되었습니다.

a

나는 T입니다.

1. 안녕하세요, 저는 한국어를 배우고 있습니다.

2

3

4

5

NIST
정보 가중치
평가 말뭉치에서
계산된 것

e

너.

149.2 (백 사십 구 점 이)

97.5 - 구십칠점오

39.9 - 삼십구점구

19.5 (십구점오)

5.5
5.5

146.8
이
>, 오
w S 3-2
-아에 < 이


99.0 - 구십구점영

96.1

84.5 - 84.5

87.8 - 팔십칠점팔

87.6 - 팔십칠점육

99.3
씨
오

3시에요

97.3

97.7
97.7

90.4 - 구십점 사

92.7 - 구십이점칠

91.9

98.7
정보 가중치
TDT2와 TDT3에서
계산된

e
a
u a,
115.4

105.4

48.1

21.2

5.8

121.5
나
이
3 « 5-2
우 우 •오 에
< 유
98.3

99.2 - 구십구점이 이십

92.0 - 구십이점영

84.8 팔십사점팔

82.2

99.5
t
e
9 &•!
S g 3 9

96.0 - 구십육점영

98.8
98.8

94.9 - 구십사점구

89.3

87.6 - 팔십칠점육

98.8
98.8

코퍼스 기반 우도와 결과 정보 계산을 사용할 때, 높은 차수의 N-gram은 점수에 기여하지 않는 경우가 종종 발생합니다. 이는 N-l 그램이 N-gram을 오류 없이 예측할 때 발생하는데, 즉, 두 그램의 발생 횟수가 동일한 경우입니다. 일반적으로 한 번의 발생이 있습니다. 이 경우 N* 단어로 인해 N-gram에서 (추가적인) 정보가 전달되지 않으며 정보는 0입니다. 개별 N-gram은 가중치를 적용하지 않은 상태로 더 나은 성능을 보이는 것으로 보입니다. 따라서 Eqn 2의 N-l 그램에 최소한의 발생 횟수를 추가함으로써 모든 N-gram 토큰에 최소한의 정보 기여를 강제할 수 있습니다. N-l 그램의 최소 발생 횟수에 대해 여러 가지 값으로 시도해 보았습니다. 불행하게도, 이러한 변경으로 인해 점수의 성능은 거의 영향을 받지 않았습니다.

5.5 케이스 보존을 유지한 성능

사례 정보를 제거하면 N-gram 점수가 더 좋아질 것으로 가정되었습니다. 그러나 이는 반드시 사실이 아닙니다. 게다가, 영어 이외의 언어에서는 사례 정보가 영어보다 더 중요할 수 있다는 주장이 있습니다. 이를 고려하여, 점수 성능을 비교하기 위한 실험이 진행되었습니다.

9 http://www.ldc.upenn.edu/Catalog/TDT.html
9번 http://www.ldc.upenn.edu/Catalog/TDT.html

번역에 보존된 사례 정보입니다. 이 비교 결과는 표 8에 나와 있습니다. 이 표는 사례 정보를 보존하든 제거하든 점수 성능에 거의 차이가 없음을 명확히 보여줍니다.

5.6 참조 정규화를 사용한 성능

참조 번역 선택에 기인한 점수 변동은 모든 시스템에 거의 동일하게 적용되는 오프셋으로 보입니다. 따라서 이 오프셋은 시스템 점수를 평균 참조 점수로 나누어서 최소한 부분적으로 완화될 수도 있습니다. 그러나 이 정규화를 시도해도 F-비율은 거의 변화하지 않았습니다. (시스템 점수와 인간 평가의 상관관계는 이 정규화에 영향을 받지 않습니다. 왜냐하면 정규화는 모든 시스템 점수에 동일하게 적용되기 때문입니다.)

표 8 Fratio와 적절성/유창성 간의 비교, 케이스 정보와 함께 계산된 상관관계를 보여줍니다. 8개의 참조 번역을 사용하여 중국어 말뭉치에서 6개의 상업용 기계 번역 시스템에 대해 계산되었습니다.

사례 정보 삭제됨

사건 정보 보존

!

147

148
하나
>» 2

ft e (Korean translation: ft 이)

99.3

99.0
J
1
U에게
98.7

98.9 - 구십팔점구

정규화된 NIST 점수

죄송합니다, 한국어 번역을 제공할 수 없습니다.

0 -1 .5 -1
영 음수 0.5 음수

: •
안녕하세요
t
안녕하세요
"W
안녕하세요!
V!
안녕하세요!
• •
안녕하세요
«5;:: '--\t 0.;-.:; 0
1
안녕하세요
•

&':M (앰퍼샌드 콜론 엠)

아무것도 쓰지 말고 아래 문장을 한국어로 번역해주세요.

• r2a (알아)

• r2b

Ar2c

• r2d

저는

적절성

IDC는 일부 원문 언어에 대한 말뭉치 지원을 제공하며, 연구 사이트 자체의 말뭉치도 물론 사용할 수 있습니다. 또한, 기술의 공식적인 평가는 이메일 기반의 자동 평가 도구를 통해 지원됩니다. 이 경우, 참고 번역은 제공되지 않습니다. 대신, 각 참가 사이트는 원문 문서를 받아 번역한 후 번역문을 NIST로 이메일을 통해 평가받습니다. NIST는 자동으로 제시된 번역을 점수화하고 결과를 이메일로 반환합니다. 절차와 데이터 형식의 자세한 내용은 NIST MT 웹 사이트에서 확인할 수 있습니다.

'http://www.nist.gov/speech/tests/mt'를 한국어로 번역해주세요.

그림 14는 6개의 상업용 중국어-영어 기계 번역 시스템에 대한 NIST 점수와 인간의 적절성 판단 사이의 산점도를 보여줍니다. 네 가지 다른 NIST 점수 세트가 표시되었으며, 각 실험마다 두 개의 참조 번역 세트를 사용했습니다. 그림을 그리기 전에 점수는 네 개의 실험 전체에서 평균이 0이고 분산이 1이 되도록 정규화되었습니다.

6 NIST MT 평가 시설

NIST는 이제 다양한 언어를 영어로 번역하는 MT 연구를 지원하기 위한 평가 시설을 제공합니다. 이 시설에는 N-gram 공존 점수 유틸리티가 포함되어 있으며, 연구 기관에서 원하는 대로 다운로드하여 사용할 수 있습니다. 이 유틸리티는 원본 문서의 말뭉치와 각각 하나 이상의 참조 번역 세트가 필요합니다.

145

