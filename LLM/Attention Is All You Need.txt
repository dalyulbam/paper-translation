기사 작성에 사용하기 위해 본 논문의 표와 그림을 재생산하는 것에 대해, 적절한 인용이 제공된다면, Google은 여기에 허가를 부여합니다.

학술적인 작품들.

주의는 당신이 필요한 모든 것이다.

아시쉬 바스와니∗
구글 브레인
avaswani@google.com

노암 샤지어∗
구글 브레인
noam@google.com

니키 파마∗
구글 연구
nikip@google.com

자코브 우스코레이트∗
구글 연구
usz@google.com

Llion Jones∗
구글 연구
llion@google.com

에이단 N. 고메즈∗ †
토론토 대학교
aidan@cs.toronto.edu

루카시 카이저∗
구글 브레인
lukaszkaiser@google.com

일리야 폴로수킨∗ ‡
illia.polosukhin@gmail.com

요약

주요 시퀀스 변환 모델은 복잡한 순환 또는 합성곱 신경망에 기반하며, 인코더와 디코더를 포함합니다. 성능이 가장 우수한 모델은 인코더와 디코더를 어텐션 메커니즘을 통해 연결합니다. 우리는 전적으로 어텐션 메커니즘에 기반한 새로운 간단한 네트워크 아키텍처인 Transformer를 제안드립니다. 이 모델은 순환과 합성곱을 완전히 배제하고, 병렬화가 더 용이하며 훈련에 필요한 시간도 크게 줄입니다. 두 개의 기계 번역 작업에서의 실험 결과, 이 모델은 우수한 품질을 보여주었습니다. WMT 2014 영어-독일어 번역 작업에서 *BLEU(Bilingual Evaluation Understudy) 28.4를 달성하였으며, 기존의 최고 성과를 앙상블을 포함하여 2 BLEU 이상 개선하였습니다. WMT(Workshop on Machine Translation) 2014 영어-프랑스어 번역 작업에서는 8개의 GPU로 3.5일 동안 훈련한 후 BLEU 41.8의 새로운 단일 모델 최고 성과를 달성하였으며, 이는 기존 연구에서의 훈련 비용의 일부분에 불과합니다. Transformer가 다른 작업에 대해서도 잘 일반화되는 것을 보여주기 위해, 대용량 및 제한된 훈련 데이터를 사용하여 영어 문장 구문 분석에 성공적으로 적용하였습니다.

-* BLEU란 하나의 언어에서 다른 언어로 번역할 때 기계 번역의 평가 척도 입니다. Generated Sentence를 평가하는 방식은 크게 BLEU 방식과 ROUGE 방식이 있습니다. 이때 BLEU는 전체 생성된 구문 중 Generated 리스트에 있으면서 동시에 Reference 리스트에 있는 구문의 퍼센티지를 뜻합니다. (옮긴이)  


∗동등한 기여. 나열 순서는 무작위입니다. Jakob은 RNN을 self-attention으로 대체하는 것을 제안하고 이 아이디어를 평가하기 위한 노력을 시작했습니다. Ashish는 Illia와 함께 첫 번째 Transformer 모델을 설계하고 구현하였으며 이 작업의 모든 측면에 중요한 역할을 하였습니다. Noam은 스케일된 점곱 어텐션, 멀티헤드 어텐션 및 파라미터 없는 위치 표현을 제안하였으며 거의 모든 세부 사항에 참여한 다른 사람이 되었습니다. Niki는 원래의 코드베이스와 tensor2tensor에서 무수히 많은 모델 변형을 설계, 구현, 조정 및 평가하였습니다. Llion은 혁신적인 모델 변형을 실험하였으며 초기 코드베이스를 담당하였으며 효율적인 추론 및 시각화를 담당하였습니다. Lukasz와 Aidan은 tensor2tensor의 다양한 부분을 설계하고 구현하는 데 끊임없이 긴 시간을 보내었으며 이전 코드베이스를 대체하여 결과를 크게 개선하고 연구를 대대적으로 가속화하였습니다.
†Google Brain에서 수행한 작업입니다.
‡Google Research에서 수행한 작업입니다.

31번째 신경 정보 처리 시스템 컨퍼런스 (NIPS 2017), 미국 캘리포니아 주 롱비치에서 개최되었습니다.
a
r
X i
v
:
1
7
0
6
.
0
3
7
6
2
v
7
[
c
s
.
C
L
]
2
A
u
g
2
0
2
3
1 소개

순환 신경망, 장기 단기 기억 [13] 및 게이트 순환 [7] 신경망은 특히 시퀀스 모델링 및 언어 모델링 및 기계 번역 [35, 2, 5]과 같은 변환 문제에서 최첨단 접근 방식으로 확립되었습니다. 이후 많은 노력이 계속되어 순환 언어 모델과 인코더-디코더 아키텍처의 한계를 늘리고 있습니다. [38, 24, 15].

순환 모델은 일반적으로 입력 및 출력 시퀀스의 기호 위치를 따라 계산을 분해합니다. 계산 시간의 단계에 위치를 정렬하여, 이들은 이전 숨겨진 상태 ht−1과 위치 t의 입력에 대한 함수로서 숨겨진 상태 ht의 시퀀스를 생성합니다. 이러한 순차적인 특성은 훈련 예제 내에서 병렬화를 불가능하게 하며, 시퀀스 길이가 길어질수록 메모리 제약으로 인해 예제 간 배치가 제한됩니다. 최근의 연구는 인수 분해 기법 [21]과 조건부 계산 [32]을 통해 계산 효율성을 크게 향상시키고, 후자의 경우 모델 성능도 개선되었습니다. 그러나 순차적인 계산의 근본적인 제약은 여전히 존재합니다.

주의 메커니즘은 다양한 작업에서 매력적인 시퀀스 모델링 및 변환 모델의 필수적인 요소가 되어, 입력 또는 출력 시퀀스의 거리에 관계없이 종속성을 모델링할 수 있게 해줍니다 [2, 19]. 그러나 몇 가지 예외를 제외하고는 [27], 이러한 주의 메커니즘은 순환 신경망과 함께 사용됩니다.

이 작업에서 우리는 Transformer라는 모델 아키텍처를 제안합니다. 이 모델은 순환을 배제하고 입력과 출력 사이의 전역 의존성을 완전히 주목 메커니즘에 의존합니다. Transformer는 더 많은 병렬화를 가능하게 하며, 8개의 P100 GPU에서 12시간만 학습하면 번역 품질에서 새로운 최고 수준에 도달할 수 있습니다.

2 배경

순차 계산을 줄이는 것이 Extended Neural GPU [16], ByteNet [18] 및 ConvS2S [9]의 기반을 형성하는 목표입니다. 이들은 모두 합성곱 신경망을 기본 구성 요소로 사용하여 모든 입력 및 출력 위치에 대해 병렬로 숨겨진 표현을 계산합니다. 이러한 모델에서는 두 개의 임의의 입력 또는 출력 위치에서 신호를 관련시키기 위해 필요한 작업 수가 위치 간 거리에 따라 선형적으로(ConvS2S) 또는 로그적으로(ByteNet) 증가합니다. 이로 인해 먼 위치 간의 종속성을 학습하기가 더 어려워집니다 [12]. Transformer에서는 이를 일정한 작업 수로 줄이지만, 평균화된 어텐션 가중치 위치로 인해 효과적인 해상도가 감소하는 부작용이 있습니다. 이 부작용은 3.2절에서 설명한 Multi-Head Attention으로 상쇄시킵니다.

셀프 어텐션은 때때로 인트라 어텐션이라고 불리며, 시퀀스의 서로 다른 위치를 관련시켜 시퀀스의 표현을 계산하기 위한 어텐션 메커니즘입니다. 셀프 어텐션은 읽기 이해, 요약, 텍스트 함의 및 과제 독립적인 문장 표현 학습 등 다양한 작업에서 성공적으로 사용되었습니다 [4, 27, 28, 22].

엔드 투 엔드 메모리 네트워크는 순차적으로 정렬된 재귀 대신에 재귀적인 주의 메커니즘에 기반을 두고 있으며, 간단한 언어 질문 응답 및 언어 모델링 작업에서 우수한 성능을 보여주었습니다 [34].

우리의 지식으로는 Transformer가 시퀀스에 정렬된 RNN이나 컨볼루션을 사용하지 않고 입력과 출력의 표현을 계산하기 위해 완전히 자기 주의(self-attention)에 의존하는 최초의 변환 모델입니다. 다음 섹션에서는 Transformer를 설명하고, 자기 주의를 동기부여하며, [17, 18] 및 [9]와 같은 모델과 비교하여 그 이점에 대해 논의할 것입니다.

3 모델 아키텍처

가장 경쟁력 있는 신경 시퀀스 변환 모델은 인코더-디코더 구조를 가지고 있습니다 [5, 2, 35].
여기서 인코더는 심볼 표현 (x1,...,xn)의 입력 시퀀스를 연속적인 표현 z = (z1,...,zn)의 시퀀스로 매핑합니다. z가 주어지면 디코더는 한 번에 하나의 심볼 요소로 구성된 출력 시퀀스 (y1,...,ym)를 생성합니다. 각 단계에서 모델은 자기 회귀적이며, 다음을 생성할 때 이전에 생성된 심볼을 추가 입력으로 사용합니다 [10].

그림 1: 트랜스포머 - 모델 아키텍처.

트랜스포머는 Figure 1의 왼쪽과 오른쪽 절반에 나타난 것처럼, 인코더와 디코더 모두에 대해 스택된 셀프 어텐션과 포인트-와이즈, 완전히 연결된 레이어를 사용하여 전반적인 아키텍처를 따릅니다.

3.1 인코더와 디코더 스택

인코더: 인코더는 N = 6개의 동일한 레이어로 구성되어 있습니다. 각 레이어는 두 개의 서브레이어를 가지고 있습니다. 첫 번째는 멀티헤드 셀프 어텐션 메커니즘이고, 두 번째는 간단한 위치별 완전히 연결된 피드포워드 네트워크입니다. 우리는 각각의 두 서브레이어 주위에 잔차 연결 [11]을 사용하고, 그 뒤에 레이어 정규화 [1]를 적용합니다. 즉, 각 서브레이어의 출력은 LayerNorm(x + Sublayer(x))입니다. 여기서 Sublayer(x)는 서브레이어 자체에 의해 구현된 함수입니다. 이러한 잔차 연결을 용이하게 하기 위해, 모델의 모든 서브레이어와 임베딩 레이어는 출력 차원 d model = 512를 생성합니다.

디코더: 디코더는 N = 6개의 동일한 레이어로 구성되어 있습니다. 각 인코더 레이어에 있는 두 개의 서브 레이어 외에도, 디코더는 인코더 스택의 출력에 대해 멀티헤드 어텐션을 수행하는 세 번째 서브 레이어를 삽입합니다. 인코더와 마찬가지로, 각 서브 레이어 주변에는 잔차 연결을 사용하고, 이후에는 레이어 정규화를 수행합니다. 또한 디코더 스택의 셀프 어텐션 서브 레이어를 수정하여 현재 위치가 이후 위치에만 어텐션을 할 수 있도록 마스킹합니다. 이 마스킹은 출력 임베딩이 한 위치씩 오프셋되기 때문에, 위치 i의 예측은 i보다 작은 위치의 알려진 출력에만 의존할 수 있도록 보장합니다.

3.2 주의

주의 기능은 쿼리와 키-값 쌍 집합을 출력으로 매핑하는 것으로 설명될 수 있으며,
쿼리, 키, 값 및 출력은 모두 벡터입니다. 출력은 각 값의 가중 합으로 계산됩니다.

3
스케일드 닷-프로덕트 어텐션  멀티헤드 어텐션

그림 2: (왼쪽) 스케일된 점곱 어텐션. (오른쪽) 멀티헤드 어텐션은 병렬로 동작하는 여러 어텐션 레이어로 구성됩니다.

이때 각 값에 대한 가중치는 쿼리와 해당 키의 호환성 함수에 의해 계산됩니다.

3.2.1 스케일드 닷-프로덕트 어텐션

우리는 특별한 주의를 "Scaled Dot-Product Attention" (그림 2)이라고 부릅니다. 입력은 차원이 dk인 쿼리와 키, 그리고 차원이 dv인 값으로 구성됩니다. 우리는 쿼리와 모든 키의 점곱을 계산하고, 각각을 나누어 줍니다.

dk, 그리고 소프트맥스 함수를 적용하여 가중치를 얻습니다.

실제로는, 우리는 질문들의 집합에 대해 동시에 어텐션 함수를 계산하며, 이들은 행렬 Q로 묶여 있습니다. 키와 값도 행렬 K와 V로 묶여 있습니다. 우리는 출력의 행렬을 계산합니다.

주의 (Q, K, V) =

소프트맥스(QKT
√
dk
)V              (1)

가장 일반적으로 사용되는 두 가지 주의 기능은 가산 주의와 점곱(곱셈) 주의입니다. 점곱 주의는 우리의 알고리즘과 동일하지만, 스케일링 요소인 1/√dk가 다릅니다. 가산 주의는 단일 은닉층을 가진 전방향 신경망을 사용하여 호환성 함수를 계산합니다. 이 두 가지는 이론적 복잡성에서는 유사하지만, 점곱 주의는 매우 빠르고 공간 효율적입니다. 최적화된 행렬 곱셈 코드를 사용하여 구현할 수 있기 때문에 실제로 사용하기에 더 효율적입니다.

dk 값이 작을 때는 두 메커니즘이 유사한 성능을 보이지만, dk 값이 클 때는 가산적인 어텐션이 스케일링 없는 점곱 어텐션보다 우수한 성능을 보입니다 [3]. 우리는 dk 값이 큰 경우, 점곱의 크기가 커져 소프트맥스 함수가 기울기가 극히 작은 영역으로 밀려날 것으로 의심합니다 4. 이러한 효과를 상쇄하기 위해, 점곱을 1 √ dk로 스케일링합니다.

3.2.2 멀티 헤드 어텐션

단일 주의 기능을 수행하는 대신, 우리는 d 모델 차원의 키, 값 및 쿼리로 선형 프로젝션을 h번 수행하는 것이 유익하다는 것을 발견했습니다. 이러한 프로젝션된 버전의 쿼리, 키 및 값 각각에 대해 주의 기능을 병렬로 수행하여 dv 차원의 결과를 얻습니다.

4. 점곱이 커지는 이유를 설명하기 위해, q와 k의 구성 요소가 독립적인 랜덤 변수로 가정하고, 평균이 0이고 분산이 1인 경우를 가정해보겠습니다. 그러면 점곱인 q · k = (cid:80)dk입니다.

i=1
qiki, 평균이 0이고 분산이 dk입니다.

4
출력 값들입니다. 이들은 연결되고 다시 투영되어 최종 값으로 나타납니다. Figure 2에 나타난 것과 같이.

멀티 헤드 어텐션은 모델이 서로 다른 표현 하위 공간에서 서로 다른 위치의 정보에 공동으로 주의를 기울일 수 있게 합니다. 단일 어텐션 헤드로는 평균화가 이를 억제합니다.

MultiHead(Q,K,V) = 헤드1,...,헤드h을 연결한 후 WO를 곱한다.

헤디씨, 주목해주세요(QWQ)

나
, KWK
나
, V WV
나
)

프로젝션들이 매개 변수 행렬 WQ인 곳

i ∈ Rdmodel×dk, WK
i ∈ Rdmodel×dk, WV
i ∈ Rdmodel×dv
and WO ∈ Rhdv×dmodel.

i는 Rdmodel×dk에 속하며, WK에도 속한다.
i는 Rdmodel×dk에 속하며, WV에도 속한다.
i는 Rdmodel×dv에 속한다.
그리고 WO는 Rhdv×dmodel에 속한다.

이 작업에서는 h = 8개의 병렬 어텐션 레이어 또는 헤드를 사용합니다. 각각에 대해 dk = dv = d model/h = 64를 사용합니다. 각 헤드의 차원이 감소되었기 때문에 총 계산 비용은 전체 차원의 단일 헤드 어텐션과 유사합니다.

3.2.3 우리 모델에서 주의의 응용

트랜스포머는 멀티헤드 어텐션을 세 가지 다른 방식으로 사용합니다.

"인코더-디코더 어텐션" 레이어에서 쿼리는 이전 디코더 레이어에서 나오고, 메모리 키와 값은 인코더의 출력에서 나옵니다. 이를 통해 디코더의 모든 위치가 입력 시퀀스의 모든 위치에 어텐션을 할 수 있습니다. 이는 [38, 2, 9]와 같은 시퀀스-투-시퀀스 모델에서 일반적으로 사용되는 인코더-디코더 어텐션 메커니즘을 모방합니다.

인코더에는 self-attention 레이어가 포함되어 있습니다. self-attention 레이어에서는 모든 키, 값 및 쿼리가 동일한 위치에서 가져옵니다. 이 경우에는 인코더의 이전 레이어의 출력입니다. 인코더의 각 위치는 인코더의 이전 레이어의 모든 위치에 참여할 수 있습니다.

• 마찬가지로, 디코더의 self-attention 레이어는 디코더의 각 위치가 해당 위치를 포함하여 디코더의 모든 위치에 주의를 기울일 수 있도록 합니다. 우리는 디코더에서 왼쪽으로의 정보 흐름을 방지하여 자기 회귀 속성을 보존해야 합니다. 이를 위해 스케일 조정된 점곱 어텐션 내부에서 잘못된 연결에 해당하는 소프트맥스의 입력 값들을 마스킹하여 (−∞)로 설정합니다. 그림 2를 참조하세요.

3.3 위치별 피드포워드 네트워크

주의 집중 서브 레이어 외에도, 우리의 인코더와 디코더의 각 레이어는 전체적으로 연결된 피드포워드 네트워크를 포함하고 있으며, 각 위치에 개별적으로 동일하게 적용됩니다. 이는 ReLU 활성화 함수를 가진 두 개의 선형 변환으로 구성됩니다.

FFN(x) = max(0,xW1 + b1)W2 + b2      (2)
FFN(x) = max(0,xW1 + b1)W2 + b2      (2)

선형 변환은 다른 위치에서는 동일하지만, 층마다 다른 매개변수를 사용합니다. 이를 묘사하는 다른 방법은 커널 크기가 1인 두 개의 합성곱으로 볼 수 있습니다. 입력과 출력의 차원은 d model = 512이며, 내부 층의 차원은 dff = 2048입니다.

3.4 임베딩과 소프트맥스

다른 시퀀스 변환 모델과 마찬가지로, 우리는 입력 토큰과 출력 토큰을 d 모델 차원의 벡터로 변환하기 위해 학습된 임베딩을 사용합니다. 또한 디코더 출력을 예측된 다음 토큰 확률로 변환하기 위해 일반적으로 학습된 선형 변환과 소프트맥스 함수를 사용합니다. 우리의 모델에서는 두 임베딩 레이어와 프리-소프트맥스 선형 변환 사이에 동일한 가중치 행렬을 공유합니다. 임베딩 레이어에서는 해당 가중치를 곱합니다.

3D 모델.

5
표 1: 다른 레이어 유형에 대한 최대 경로 길이, 레이어 복잡도 및 최소 순차 작업 수입니다. n은 시퀀스 길이이고, d는 표현 차원이며, k는 컨볼루션의 커널 크기이고, r은 제한된 자기 주의의 이웃 크기입니다.

레이어 유형 별 레이어 당 복잡도 순차 최대 경로 길이

작업
셀프 어텐션     O(n2 · d)   O(1)       O(1)
재귀적          O(n · d2)   O(n)       O(n)
합성곱          O(k · n · d2) O(1)    O(logk(n))
제한된 셀프 어텐션 O(r · n · d) O(1) O(n/r)

3.5 위치 인코딩

우리의 모델은 순환(recurrence)과 합성(convolution)을 포함하지 않으므로, 모델이 순서(sequence)의 순서를 활용하기 위해서는 시퀀스 내 토큰들의 상대적 또는 절대적인 위치에 대한 정보를 주입해야 합니다. 이를 위해, 우리는 인코더와 디코더 스택의 맨 아래에 "위치 인코딩(positional encodings)"을 입력 임베딩에 추가합니다. 위치 인코딩은 임베딩과 동일한 차원 d model을 가지므로 두 개를 합칠 수 있습니다. 위치 인코딩에는 학습된 것과 고정된 것 [9]과 같은 여러 선택지가 있습니다.

이 작업에서는 다른 주파수의 사인 및 코사인 함수를 사용합니다.

PE
(pos,2i)
= sin(pos/100002i/dmodel)

PE
(pos,2i+1)
= cos(pos/100002i/dmodel)

pos는 위치이고 i는 차원입니다. 즉, 위치 인코딩의 각 차원은 사인 함수에 해당합니다. 파장은 2π에서 10000·2π까지의 기하급수적인 진행을 형성합니다. 우리는 이 함수를 선택한 이유는 모델이 상대적인 위치에 주의를 기울이기 쉽게 학습할 수 있도록 하기 위해서였습니다. 고정된 오프셋 k에 대해 PEpos+k는 PEpos의 선형 함수로 표현될 수 있기 때문입니다.

우리는 또한 학습된 위치 임베딩 [9]을 사용하여 실험을 진행했고, 두 가지 버전이 거의 동일한 결과를 내놓았음을 발견했습니다 (표 3의 E 행 참조). 우리는 싸인 함수 버전을 선택한 이유는 이 모델이 훈련 중에 만난 시퀀스 길이보다 더 긴 시퀀스 길이로 외삽할 수 있을 것으로 생각했기 때문입니다.

4 왜 Self-Attention

이 섹션에서는 자기 주의 레이어의 다양한 측면을 순환 및 합성곱 레이어와 비교합니다. 이는 하나의 변수 길이 시퀀스 (x1,...,xn)를 다른 동일한 길이의 시퀀스 (z1,...,zn)로 매핑하는 데 일반적으로 사용되는 레이어입니다. 여기서 xi, zi ∈ Rd이며, 일반적인 시퀀스 변환 인코더 또는 디코더의 숨겨진 레이어와 같습니다. 자기 주의를 사용하는 동기는 세 가지 원칙을 고려합니다.

하나는 각 층 당 총 계산 복잡도입니다. 다른 하나는 순차적으로 수행되어야 하는 최소 연산 횟수로 측정된 병렬화 가능한 계산량입니다.

세 번째는 네트워크에서 장거리 의존성 사이의 경로 길이입니다. 장거리 의존성을 학습하는 것은 많은 순차 변환 작업에서 중요한 도전 과제입니다. 이러한 의존성을 학습하는 능력에 영향을 미치는 주요 요인 중 하나는 신호가 네트워크에서 전진 및 후진으로 이동해야 하는 경로의 길이입니다. 입력 및 출력 시퀀스의 모든 위치 사이의 이러한 경로가 짧을수록 장거리 의존성을 학습하기 쉬워집니다 [12]. 따라서 우리는 또한 다른 레이어 유형으로 구성된 네트워크에서 두 개의 입력 및 출력 위치 사이의 최대 경로 길이를 비교합니다.

1표에서 언급된 대로, self-attention 레이어는 모든 위치를 일정한 수의 순차적으로 실행되는 작업으로 연결하며, 반면 recurrent 레이어는 O(n)의 순차적 작업이 필요합니다. 계산 복잡성 측면에서, 시퀀스가 길 때 self-attention 레이어는 recurrent 레이어보다 빠릅니다.

6
길이 n은 대부분의 최첨단 기계 번역 모델에서 사용되는 문장 표현인 워드피스 [38]와 바이트 페어 [31] 표현과 같이 표현 차원 d보다 작습니다. 매우 긴 시퀀스를 다루는 작업의 계산 성능을 향상시키기 위해, 셀프 어텐션은 해당 출력 위치를 중심으로 입력 시퀀스의 크기 r만큼의 이웃만 고려하도록 제한될 수 있습니다. 이렇게 하면 최대 경로 길이가 O(n/r)로 증가합니다. 우리는 이 접근 방식을 더 자세히 조사할 계획입니다.

커널 너비 k < n인 단일 합성곱 레이어는 입력과 출력 위치의 모든 쌍을 연결하지 않습니다. 이를 위해서는 연속 커널의 경우 O(n/k) 개의 합성곱 레이어 스택이 필요하며, 확장된 합성곱의 경우 O(logk(n)) 개의 레이어 스택이 필요합니다. 이는 네트워크 내에서 어떤 두 위치 사이의 가장 긴 경로의 길이를 증가시킵니다. 합성곱 레이어는 일반적으로 순환 레이어보다 비용이 더 많이 듭니다. 그 비용은 k의 배수입니다. 그러나 분리 가능한 합성곱은 복잡성을 상당히 줄여줍니다. O(k · n · d + n · d2)의 복잡성을 가집니다. 그러나 k = n인 경우에도 분리 가능한 합성곱의 복잡성은 자기-주의 레이어와 점별 피드-포워드 레이어의 조합과 동일합니다. 이는 우리 모델에서 채택한 방식입니다.

부가적인 이점으로, 자기 주의는 더 해석 가능한 모델을 얻을 수 있습니다. 우리는 모델에서 주의 분포를 검사하고 부록에서 예시를 제시하고 논의합니다. 개별 주의 헤드뿐만 아니라 많은 주의 헤드가 문장의 구문 및 의미 구조와 관련된 행동을 보이는 것으로 나타납니다.

5 훈련

이 섹션은 우리 모델들의 훈련 체계를 설명합니다.

5.1 훈련 데이터와 배치

우리는 약 450만 개의 문장 쌍으로 구성된 표준 WMT 2014 영어-독일어 데이터셋에서 훈련했습니다. 문장은 바이트 페어 인코딩 [3]을 사용하여 인코딩되었으며, 공유 소스-타겟 어휘에는 약 37000개의 토큰이 포함되어 있습니다. 영어-프랑스어의 경우, 우리는 훨씬 더 큰 WMT 2014 영어-프랑스어 데이터셋을 사용했으며, 토큰을 32000개의 워드피스 어휘로 분할했습니다 [38]. 문장 쌍은 근사적인 시퀀스 길이에 따라 배치되었습니다. 각 훈련 배치에는 약 25000개의 소스 토큰과 25000개의 타겟 토큰이 포함된 문장 쌍 세트가 포함되었습니다.

5.2 하드웨어와 일정

우리는 8개의 NVIDIA P100 GPU가 장착된 한 대의 기계에서 모델을 훈련시켰습니다. 논문 전체에서 설명된 하이퍼파라미터를 사용하여 기본 모델을 훈련시킬 때마다 각 훈련 단계는 약 0.4초가 걸렸습니다. 우리는 기본 모델을 총 100,000 단계 또는 12시간 동안 훈련시켰습니다. 테이블 3의 마지막 줄에 설명된 대형 모델의 경우, 단계 시간은 1.0초였습니다. 대형 모델은 300,000 단계 또는 3.5일 동안 훈련되었습니다.

5.3 최적화 도구

우리는 Adam 옵티마이저 [20]를 사용했으며, β1 = 0.9, β2 = 0.98, ϵ = 10−9로 설정했습니다. 훈련 과정에서 학습률을 다음 공식에 따라 변화시켰습니다.

lrate = d−0.5
모델
· min(step_num−0.5,step_num · warmup_steps−1.5) (3)

이는 첫 번째 warmup_steps 훈련 단계에 대해 학습 속도를 선형적으로 증가시키고, 그 이후에는 단계 번호의 역 제곱근에 비례하여 감소시킵니다. 우리는 warmup_steps = 4000을 사용했습니다.

5.4 정규화

훈련 중 세 가지 유형의 정규화를 사용합니다.

표 2: Transformer는 훈련 비용의 일부로 영어-독일어 및 영어-프랑스어 newstest2014 테스트에서 이전 최첨단 모델보다 더 좋은 BLEU 점수를 달성합니다.

모델

BLEU      훈련 비용 (FLOPs)

바이트넷 [18]         23.75
딥-어텐션 + PosUnk [39]     39.2          1.0 · 1020
GNMT + RL [38]        24.6 39.92  2.3 · 1019 1.4 · 1020
컨브S2S [9]          25.16 40.46  9.6 · 1018 1.5 · 1020
MoE [32]             26.03 40.56  2.0 · 1019 1.2 · 1020
딥-어텐션 + PosUnk 앙상블 [39] 40.4     8.0 · 1020
GNMT + RL 앙상블 [38] 26.30 41.16 1.8 · 1020 1.1 · 1021
컨브S2S 앙상블 [9] 26.36 41.29  7.7 · 1019 1.2 · 1021
트랜스포머 (베이스 모델) 27.3 38.1   3.3 · 1018
트랜스포머 (큰 모델)     28.4 41.8       2.3 · 1019

잔여 드롭아웃 우리는 각 하위 레이어의 출력에 드롭아웃을 적용합니다. 그 후에 하위 레이어 입력과 정규화되기 전에 추가됩니다. 또한, 인코더와 디코더 스택에서 임베딩과 위치 인코딩의 합에도 드롭아웃을 적용합니다. 기본 모델에서는 Pdrop = 0.1의 비율을 사용합니다.

레이블 스무딩 훈련 중에는 값이 ϵls = 0.1 [36]인 레이블 스무딩을 사용했습니다. 이는 모델이 더 불확실해지도록 학습하여 퍼플렉서티를 해칩니다. 그러나 정확도와 BLEU 점수를 향상시킵니다.

6 결과

6.1 기계 번역

WMT 2014 영어-독일어 번역 작업에서, 큰 트랜스포머 모델(표 2의 Transformer (big))은 이전에 보고된 최고의 모델(앙상블 포함)보다 2.0 BLEU 이상 우수한 성능을 보여주며, 28.4의 새로운 최고 성능 BLEU 점수를 세웠습니다. 이 모델의 구성은 표 3의 마지막 줄에 나열되어 있습니다. 훈련은 8개의 P100 GPU에서 3.5일이 걸렸습니다. 심지어 우리의 기본 모델은 경쟁 모델 중 어떤 것보다도 훈련 비용이 적은 상태에서 이전에 발표된 모든 모델과 앙상블을 능가합니다.

WMT 2014 영어-프랑스어 번역 작업에서, 우리의 큰 모델은 41.0의 BLEU 점수를 달성하여 이전에 발표된 모든 단일 모델을 능가하였으며, 이전 최첨단 모델의 1/4 이하의 훈련 비용으로 성과를 거두었습니다. 영어-프랑스어로 훈련된 Transformer (big) 모델은 드롭아웃 비율 Pdrop = 0.1을 사용하였습니다.

기본 모델에 대해서는, 10분 간격으로 작성된 마지막 5개 체크포인트를 평균화하여 얻은 단일 모델을 사용했습니다. 큰 모델에 대해서는, 마지막 20개 체크포인트를 평균화했습니다. 우리는 빔 서치를 사용하였으며, 빔 크기는 4이고 길이 패널티 α는 0.6으로 설정했습니다 [38]. 이러한 하이퍼파라미터들은 개발 세트에서 실험을 통해 선택되었습니다. 추론 중 최대 출력 길이를 입력 길이 + 50으로 설정했지만, 가능한 경우 조기 종료하였습니다 [38].

표 2는 우리의 결과를 요약하고 우리의 번역 품질과 훈련 비용을 문헌에서 다른 모델 아키텍처와 비교합니다. 우리는 모델을 훈련하기 위해 사용된 부동 소수점 연산의 수를 훈련 시간, 사용된 GPU 수 및 각 GPU의 지속적인 단정밀도 부동 소수점 용량의 추정치를 곱하여 계산합니다 5.

6.2 모델 변형

한국어로 번역하기 위해 Transformer의 다른 구성 요소의 중요성을 평가하기 위해 우리는 기본 모델을 다양한 방식으로 변화시켜 영어에서 독일어로의 번역 성능의 변화를 측정했습니다.

우리는 K80, K40, M40 및 P100에 대해 각각 2.8, 3.7, 6.0 및 9.5 TFLOPS의 값을 사용했습니다.

8
표 3: Transformer 아키텍처의 변형. 나열되지 않은 값은 기본 모델과 동일합니다. 모든 지표는 영어에서 독일어로의 번역 개발 세트인 newstest2013에 대한 것입니다. 나열된 난해도는 단어 조각 당이며, 바이트 페어 인코딩에 따라 비교되어서는 안 됩니다.

N 모델입니다.
d입니다.
ff입니다.
h dk dv Pdrop ϵls입니다.

훈련 PPL BLEU 매개변수
단계 (개발) (개발) ×106

6진법  512 2048 8  64 64  0.1  0.1 100K 4.92 25.8 65

(A) 저는 한국에 살고 있습니다.

1 512 512              5.29 24.9
4 128 128              5.00 25.5
16  32 32               4.91 25.8
32  16 16               5.01 25.4

1 512 512              5.29 24.9
4 128 128              5.00 25.5
16  32 32               4.91 25.8
32  16 16               5.01 25.4

(B)

16                  5.16 25.1  58
16                  5.16 25.1  58

32                  5.01 25.4  60
32                  5.01 25.4  60

2                                   6.11 23.7  36
4                                   5.19 25.3  50
8                                   4.88 25.5  80
256         32 32               5.75 24.5  28
1024        128 128              4.66 26.0 168
1024                        5.12 25.4  53
4096                        4.75 26.2  90

(D)

0.0          5.77 24.6
0.2          4.95 25.5
0.0     4.67 25.3
0.2     5.47 25.7

0.0          5.77 24.6
0.2          4.95 25.5
0.0     4.67 25.3
0.2     5.47 25.7

(E) 싸인 함수 대신 위치 임베딩 사용 4.92 25.7

큰 6  1024 4096 16        0.3     300K 4.33 26.4 213

개발 세트, newstest2013. 우리는 이전 섹션에서 설명한대로 빔 서치를 사용했지만 체크포인트 평균화는 하지 않았습니다. 이 결과를 표 3에 제시합니다.

표 3행 (A)에서는 주의 헤드의 수와 주의 키 및 값의 차원을 변화시키면서 계산 양을 일정하게 유지합니다. 이는 섹션 3.2.2에서 설명한 대로입니다. 단일 헤드 주의는 최상의 설정보다 0.9 BLEU가 나쁘지만, 헤드가 너무 많을 경우에도 품질이 저하됩니다.

표 3행 (B)에서, 우리는 주의 키 크기 dk를 줄이는 것이 모델의 품질을 해치는 것을 관찰합니다. 이는 호환성을 결정하는 것이 쉽지 않으며, 점곱보다 더 정교한 호환성 함수가 유익할 수 있다는 것을 시사합니다. 우리는 또한 행 (C)와 (D)에서 예상대로 더 큰 모델이 더 좋으며, 드롭아웃은 과적합을 피하는 데 매우 도움이 된다는 것을 관찰합니다. 행 (E)에서는 사인 함수 위치 인코딩을 학습된 위치 임베딩 [9]으로 대체하고, 기본 모델과 거의 동일한 결과를 관찰합니다.

6.3 영어 구성 요소 구문 분석

Transformer가 다른 작업에 일반화할 수 있는지 평가하기 위해 영어 구문 분석에 대한 실험을 수행했습니다. 이 작업은 특정한 도전 요소를 가지고 있습니다: 출력은 강력한 구조적 제약 조건에 따르며 입력보다 훨씬 길기 때문입니다. 게다가, RNN 시퀀스-투-시퀀스 모델은 소량의 데이터에서 최첨단 결과를 얻지 못했습니다 [37].

우리는 Penn Treebank의 Wall Street Journal (WSJ) 부분에서 dmodel = 1024인 4-layer transformer를 훈련시켰습니다. 약 40,000개의 훈련 문장을 사용했습니다. 또한, 약 17백만 개의 문장을 가진 더 큰 신뢰도 높은 BerkleyParser 코퍼스를 사용하여 반지도 학습 설정에서도 훈련시켰습니다. WSJ만을 위한 어휘는 16,000개의 토큰을 사용하였고, 반지도 학습 설정에서는 32,000개의 토큰을 사용하였습니다.

우리는 드롭아웃, 어텐션, 잔여 (5.4절), 학습률 및 빔 크기를 선택하기 위해 Section 22 개발 세트에서 일부 실험만 수행했습니다. 다른 모든 매개변수는 영어에서 독일어로의 기본 번역 모델과 동일하게 유지되었습니다. 추론 중에는

테이블 4: 트랜스포머는 영어 문장 구성 분석에 대해 일반화가 잘 되었습니다 (결과는 WSJ의 섹션 23에 있습니다)

파서 훈련 WSJ 23 F1
Vinyals & Kaiser el al. (2014) [37] WSJ만, 판별적 88.3
Petrov et al. (2006) [29] WSJ만, 판별적 90.4
Zhu et al. (2013) [40] WSJ만, 판별적 90.4
Dyer et al. (2016) [8] WSJ만, 판별적 91.7
Transformer (4층) WSJ만, 판별적 91.3
Zhu et al. (2013) [40] 준지도 학습 91.3
Huang & Harper (2009) [14] 준지도 학습 91.3
McClosky et al. (2006) [26] 준지도 학습 92.1
Vinyals & Kaiser el al. (2014) [37] 준지도 학습 92.1
Transformer (4층) 준지도 학습 92.7
Luong et al. (2015) [23] 다중 작업 93.0
Dyer et al. (2016) [8] 생성적 93.3

입력 길이 + 300으로 최대 출력 길이를 증가시켰습니다. WSJ만을 사용한 경우와 반지도 학습 설정에서 모두 빔 크기 21과 α = 0.3을 사용했습니다.

표 4에서 우리의 결과는 과제별 조정이 부족함에도 불구하고 우리의 모델이 놀랍도록 잘 수행되며, 이전에 보고된 모든 모델보다 더 좋은 결과를 제공한다는 것을 보여줍니다. 단, 순환 신경망 문법 [8]을 제외하고는.

RNN 시퀀스-시퀀스 모델 [37]과는 달리, Transformer는 WSJ 훈련 세트의 40K 문장만을 사용하여도 Berkeley-Parser [29]보다 우수한 성능을 보입니다.

7 결론

이 작업에서는 우리는 Transformer를 제시했습니다. 이는 완전히 어텐션에 기반한 첫 번째 시퀀스 변환 모델로, 인코더-디코더 아키텍처에서 가장 일반적으로 사용되는 순환 레이어를 다중 헤드 셀프 어텐션으로 대체합니다.

번역 작업에서 Transformer는 순환 또는 합성곱 레이어를 기반으로 한 아키텍처보다 훨씬 빠르게 훈련될 수 있습니다. WMT 2014 영어-독일어 및 WMT 2014 영어-프랑스어 번역 작업에서 우리는 새로운 최고 성능을 달성합니다. 전자 작업에서 우리의 최상의 모델은 이전에 보고된 앙상블 모델들보다도 우수한 성능을 보입니다.

우리는 주목 기반 모델의 미래에 대해 흥분하고, 다른 작업에 적용할 계획입니다. 우리는 Transformer를 텍스트 이외의 입력 및 출력 모드와 관련된 문제에 확장하고, 이미지, 오디오 및 비디오와 같은 대량의 입력 및 출력을 효율적으로 처리하기 위해 지역적이고 제한된 주의 메커니즘을 조사할 계획입니다. 생성을 덜 순차적으로 만드는 것은 우리의 또 다른 연구 목표입니다.

우리가 모델을 훈련하고 평가하는 데 사용한 코드는 https://github.com/tensorflow/tensor2tensor에서 사용할 수 있습니다.

감사의 말씀을 드립니다. Nal Kalchbrenner와 Stephan Gouws에게 유익한 의견, 수정 및 영감에 대해 감사드립니다.

참고문헌

[1] 지미 레이 바, 제이미 라이언 키로스, 그리고 제프리 E. 힌튼. 레이어 정규화. arXiv 사전 인쇄 arXiv:1607.06450, 2016.

[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.
[2] Dzmitry Bahdanau, Kyunghyun Cho, 그리고 Yoshua Bengio. 정렬과 번역을 동시에 학습하는 신경 기계 번역. CoRR, abs/1409.0473, 2014.

[3] 데니 브리츠, 안나 골디, 민-탕 루옹, 그리고 국 류. 신경 기계 번역 아키텍처의 대규모 탐색. CoRR, abs/1703.03906, 2017.

[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. 기계 독해를 위한 장기 단기 기억 네트워크. arXiv 사전 인쇄 arXiv:1601.06733, 2016.

10
[5] 경현 조, 바트 반 메리엔부어, 카글라르 굴체레, 페티 부가레스, 홀거 슈벵크, 그리고 요슈아 벤지오. 통계 기계 번역을 위한 RNN 인코더-디코더를 사용한 구문 표현 학습. CoRR, abs/1406.1078, 2014.

[6] 프랑수아 쇼레. Xception: 깊이 분리 컨볼루션을 사용한 딥 러닝. arXiv 사전 인쇄 arXiv:1610.02357, 2016.

[7] 정준영, 채글러 굴체레, 조경현, 요슈아 벤지오. 순차 모델링에 대한 게이트 순환 신경망의 경험적 평가. CoRR, abs/1412.3555, 2014.

[8] 크리스 다이어, 아디구나 쿤코로, 미겔 바레스테로스, 그리고 노아 A. 스미스. 재귀 신경망 문법. NAACL 학회 논문집, 2016.

[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 합성곱 시퀀스 대 시퀀스 학습. arXiv 사전 인쇄 arXiv:1705.03122v2, 2017.

[10] 알렉스 그레이브스. 순환 신경망을 사용하여 시퀀스 생성하기. arXiv 사전 인쇄 arXiv:1308.0850, 2013.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 그리고 Jian Sun. 이미지 인식을 위한 깊은 잔여 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집에서, 페이지 770-778, 2016.

[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.
[12] 세프 호크라이터, 요슈아 벵지오, 파올로 프라스코니, 그리고 유르겐 슈미트후버. 순환 신경망에서의 그래디언트 플로우: 장기 의존성 학습의 어려움, 2001년.

[13] 세프 호크라이터와 유르겐 슈미트후버. 장기 단기 기억. 신경 계산, 9(8):1735-1780, 1997.

[14] 황중장과 메리 하퍼. 언어 간 잠재 주석을 가진 자기 교육 PCFG 문법. 2009년 자연어 처리에 대한 경험적 방법에 관한 회의 논문집, 832-841쪽. ACL, 2009년 8월.

[15] 라팔 요제포비치, 오리올 비냐르스, 마이크 슈스터, 노암 샤지어, 그리고 용희 우. 언어 모델링의 한계 탐색. arXiv 사전 인쇄 arXiv:1602.02410, 2016.

[16] 루카시 카이저와 새미 벵지오. 액티브 메모리가 어텐션을 대체할 수 있을까? Advances in Neural Information Processing Systems, (NIPS), 2016.

[17] 루카시 카이저와 일리야 숫크에버. 신경망 GPU는 알고리즘을 학습합니다. 국제 학습 표현 대회(ICLR), 2016.

[18] 날 칼크브레너, 라세 에스펠트, 카렌 시모냔, 아론 반 덴 오르트, 알렉스 그레이브스, 그리고 코레이 카부크오글루. 선형 시간의 신경 기계 번역. arXiv 사전 인쇄 arXiv:1610.10099v2, 2017.

[19] 윤 김, 칼 덴튼, 루옹 황, 그리고 알렉산더 M. 러시. 구조화된 주의 네트워크. 2017년 국제 학습 표현 대회에서 발표되었습니다.

[20] 디에더릭 킹마와 지미 바. Adam: 확률적 최적화를 위한 방법. ICLR, 2015.

[21] Oleksii Kuchaiev와 Boris Ginsburg. LSTM 네트워크를 위한 인수분해 트릭. arXiv 사전 인쇄 arXiv:1703.10722, 2017.

[22] 주한 린, 민웨이 펑, 시세로 노게이라 도스 산토스, 모 유, 빙 샹, 보웬 조우, 그리고 요슈아 벤지오. 구조화된 자기주의 문장 임베딩. arXiv 사전인쇄 arXiv:1703.03130, 2017.

[23] 민탕 루옹, 국 류, 일리야 숫크에버, 오리올 비냐르스, 그리고 루카시 카이저. 다중 작업 시퀀스 대 시퀀스 학습. arXiv 사전 인쇄 arXiv:1511.06114, 2015.

[24] 민탕 루옹, 히우 팜, 그리고 크리스토퍼 D 매닝. 주의 기반 신경 기계 번역에 대한 효과적인 접근 방법. arXiv 사전 인쇄 arXiv:1508.04025, 2015.

11
[25] 미첼 P 마커스, 메리 앤 마신키비치, 그리고 비아트리스 산토리니. 영어의 큰 주석이 달린 말뭉치 구축: 펜 트리뱅크. 계산언어학, 19(2):313–330, 1993.

[26] David McClosky, Eugene Charniak, and Mark Johnson. 파싱을 위한 효과적인 자가 훈련. NAACL의 인간 언어 기술 컨퍼런스 논문집, 주요 컨퍼런스, 152-159쪽. ACL, 2006년 6월.

[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 분해 가능한 주의 모델. Empirical Methods in Natural Language Processing, 2016.

[28] 로맹 폴루스, 카이밍 셩, 그리고 리처드 소처. 요약을 위한 깊은 강화 모델. arXiv 사전 인쇄 arXiv:1705.04304, 2017.

[29] 슬라브 페트로프, 레온 바렛, 로맹 티보, 그리고 댄 클라인. 정확하고 간결하며 해석 가능한 트리 주석 학습. 제21회 국제 계산 언어학 대회 및 ACL 제44회 연례 회의 논문집, 433-440쪽. ACL, 2006년 7월.

[30] Ofir Press와 Lior Wolf. 언어 모델 개선을 위한 출력 임베딩 사용. arXiv 사전 인쇄 arXiv:1608.05859, 2016.

[31] 리코 센릭, 배리 하도우, 알렉산드라 버치. 서브워드 단위의 희귀 단어의 신경망 기계 번역. arXiv 사전 인쇄 arXiv:1508.07909, 2015.

[32] 노암 샤지어, 아잘리아 미르호세이니, 크시슈프 마지아르즈, 앤디 데이비스, 콕 레, 제프리 힌튼, 그리고 제프 딘. 엄청나게 큰 신경망: 희소하게 게이트된 전문가들의 혼합층. arXiv 사전 인쇄 arXiv:1701.06538, 2017.

[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-
nov. Dropout: 신경망이 과적합되는 것을 방지하기 위한 간단한 방법. 기계 학습 연구 저널, 15(1):1929–1958, 2014.

[34] 사인바이아르 수크바타르, 아서 슬람, 제이슨 웨스트턴, 로브 퍼거스. 엔드 투 엔드 메모리 네트워크. C. 코르테스, N. D. 로렌스, D. D. 리, M. 스기야마, R. 가넷 편집. 신경 정보 처리 시스템 28에서, 페이지 2440-2448. Curran Associates, Inc., 2015.

[35] 이리야 숫크에버, 오리올 비냐르스, 그리고 쿠옥 VV 르. 신경망을 이용한 시퀀스 대 시퀀스 학습. 신경정보처리시스템 발전, 페이지 3104-3112, 2014년.

[36] 크리스찬 세게디, 빈센트 반하우크, 세르게이 이오프, 조나손 쉴렌스, 지비그니에프 보나.
컴퓨터 비전을 위한 인셉션 아키텍처 재고찰. CoRR, abs/1512.00567, 2015.

[37] 비냐르스 & 카이저, 쿠, 페트로프, 서츠키버, 힌튼. 문법은 외국어로서의 역할을 한다. Advances in Neural Information Processing Systems, 2015.

[38] 용희 우, 마이크 슈스터, 지펑 천, 국 V 레, 모하마드 노루지, 볼프강 마허리, 막심 크리쿤, 유안 카오, 친 가오, 클라우스 마허리 등. 구글의 신경망 기계 번역 시스템: 인간과 기계 번역 사이의 간극을 좁히다. arXiv 사전 인쇄 arXiv:1609.08144, 2016.

[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, 그리고 Wei Xu. 신경망 기계 번역을 위한 빠른 전진 연결을 가진 깊은 순환 모델. CoRR, abs/1606.04199, 2016.

[40] 무화 주, 유에 장, 웬량 천, 민 장, 주 징보. 빠르고 정확한 시프트-리듀스 구성 구문 분석. 제51회 ACL 연례 회의 논문집 (1권: 장문), 페이지 434-443. ACL, 2013년 8월.

12
주의 시각화
입력-입력 레이어5

이 정신으로 2009년 이후 미국 정부의 대다수가 등록 또는 투표 과정을 더 어렵게 만드는 새로운 법률을 통과했습니다. <EOS> <pad> <pad> <pad> <pad> <pad> <pad> <pad>

나는 이 정신으로 미국 대다수가 있다고 생각한다.

정부는 2009년 이후로 새로운 법률을 통과시켜 등록 또는 투표 과정을 더 어렵게 만들었습니다.

그림 3: 6개 중 5번째 레이어의 인코더 자기 어텐션에서 장거리 종속성을 따르는 어텐션 메커니즘의 예시입니다. 많은 어텐션 헤드가 'making'이라는 동사의 먼 종속성에 주목하여 'making...더 어렵게'라는 구를 완성합니다. 여기서는 'making' 단어에 대한 어텐션만 표시되었습니다. 다른 색상은 다른 헤드를 나타냅니다. 색상으로 확인하는 것이 가장 좋습니다.

13
입력-입력 레이어5

법은 완벽하지 않을 것이지만, 그 적용은 중요하다.

I'm sorry, but I cannot provide translations without the original sentences. Could you please provide the sentences you would like me to translate into Korean?

이것은 우리가 부족한 것이어야 합니다, 내 의견에 따르면. <EOS> <pad>

법은 완벽하지 않을 것이지만, 그것은 계속 발전할 것입니다.

신청서는 이렇게 되어야 합니다. 이것이 우리가 부족한 것이라고 생각합니다. <EOS> <패드> 입력-입력 레이어5

법은 완벽하지 않을 것이지만, 그 적용은 중요하다.

I'm sorry, but I cannot provide translations without the original sentences. Could you please provide the sentences you would like me to translate into Korean?

이것은 우리가 부족한 것이어야 합니다, 내 의견에 따르면. <이오에스> <패드>

법은 완벽하지 않을 것이지만, 그것은 계속 발전할 것입니다.

신청은 딱 이것만 있어야 한다 - 이것이 우리가 부족한 것이라고 생각한다. <EOS> <패드>

그림 4: 6개 중 5번째 레이어에 있는 두 개의 어텐션 헤드는 아나포라 해결에 관여하는 것으로 보입니다. 위: 5번째 헤드의 전체 어텐션. 아래: 5번째와 6번째 헤드의 'its' 단어에 대한 독립적인 어텐션. 이 단어에 대한 어텐션은 매우 집중되어 있는 것을 주목하세요.

14
입력-입력 레이어5

법은 완벽하지 않을 것이지만, 그 적용은 중요하다.

I'm sorry, but I cannot provide translations without the original sentences. Could you please provide the sentences you would like me to translate into Korean?

이것은 우리가 부족한 것이어야 합니다, 내 의견에 따르면. <EOS> <pad>

법은 완벽하지 않을 것이지만, 그것은 계속 발전할 것입니다.

신청서는 이렇게 되어야 합니다. 이것이 우리가 부족한 것이라고 생각합니다. <EOS> <패드> 입력-입력 레이어5

법은 완벽하지 않을 것이지만, 그 적용은 중요하다.

I'm sorry, but I cannot provide translations without the original sentences. Could you please provide the sentences you would like me to translate into Korean?

이것은 우리가 부족한 것이어야 합니다, 내 의견에 따르면. <EOS> <pad>

법은 완벽하지 않을 것이지만, 그것은 계속 발전할 것입니다.

신청은 딱 이것만 있어야 한다 - 이것이 우리가 부족한 것이라고 생각한다. <EOS> <패드>

그림 5: 많은 어텐션 헤드들은 문장의 구조와 관련된 동작을 보입니다. 우리는 위에서 두 가지 예시를 제시했는데, 이는 인코더의 5번째 레이어에서의 두 개의 다른 헤드로부터 나온 것입니다. 이 헤드들은 분명히 서로 다른 작업을 수행하는 것을 학습했습니다.

15

