단일 음성 개선을 위한 마스크 없는 신경망

량 리우1, 하이신 관12, 진롱 마1, 웨이 다이1, 광용 왕1, 소웨이 딩1

1. 중국 유니사운드 AI 기술 주식회사
2. 중국과학기술대학교, 중국
liuliang, guanhaixin, majinlong, daiwei, wangguangyong, dingshaowei@unisound.com

요약

음성 개선에서는 대상 음성 단계의 명확한 구조적 특성의 부족으로 인해 보수적이고 번거로운 네트워크 프레임워크를 사용해야 합니다. 직접적인 방법과 간단한 네트워크 아키텍처를 사용하여 경쟁력 있는 성능을 달성하는 것은 어렵게 보입니다. 그러나 우리는 MFNet을 제안합니다. 이는 음성을 매핑할 뿐만 아니라 역방향 잡음도 매핑할 수 있는 직접적이고 간단한 네트워크입니다. 이 네트워크는 전역 처리를 위한 Mobileblock의 장점과 지역 상호작용을 위한 Metaformer 아키텍처를 결합한 전역 지역 전용 블록(GLFB)을 쌓아서 구성됩니다. 우리의 실험 결과는 매핑 방법을 사용한 네트워크가 마스킹 방법보다 우수한 성능을 보여주며, 역방향 잡음의 직접 매핑이 강한 잡음 환경에서 최적의 해결책임을 보여줍니다. 우리의 지식으로는 2020년 Deep Noise Suppression(DNS) 챌린지 테스트 세트에서 반향이 없는 상태에서 가로 비교를 한 결과, MFNet은 현재 최첨단 매핑 모델(SOTA)입니다.
색인어: 단일 음성 개선, 딥 러닝, 마스크 없음

1. 소개

딥러닝의 발전으로 음성 개선(SE) 기술은 상당한 진전을 이루었습니다. 일반적으로, 이러한 기술은 시간 영역 방법 [1, 2]과 T-F 영역 방법 [3, 4]으로 나눌 수 있습니다. 특히, 후자는 SE 분야에서 가장 영향력 있는 대회 중 하나인 DNS Challenge [5, 6, 7, 8]에서 더 좋은 성능을 얻었습니다. 따라서, 이 연구의 목표는 단일 채널 음성 개선을 위한 효과적인 T-F 영역 시스템을 설계하는 것입니다.
T-F 영역 음성 개선 방법에서는 T-F 스펙트럼 값의 직접 학습 (매핑 방법 [9, 10])과 T-F 마스킹의 학습 (마스킹 방법 [4, 11])이라는 두 가지 고전적인 방법이 있습니다. 크기 [9] 또는 실수 및 허수 부분을 매핑하는 것은 직접적이고 근본적인 접근 방식이지만, 어려운 문제로 보입니다. 따라서 GCRN [10]은 실수와 허수 부분을 별도로 매핑하기 위해 두 개의 디코더가 필요합니다. 마스킹 방법은 노이즈가 있는 음성 구성 요소의 사전으로부터 문제를 단순화합니다. 이는 직사각형 좌표계 DPCRN [12] FullSubnet [13] 또는 극 좌표계 DCCRN [4] DCUNet [14]에서 추정됩니다. 이를 기반으로, 근처 필터링과 합산을 사용하는 DeepFilterNet [15, 16]은 마스킹 방법의 이론적인 결함을 약간 보상할 수 있습니다.
발전이 진행됨에 따라 두 가지 방법을 결합하는 작업 (decoupling 방법 [17, 18, 19, 20]이라고 함)이 점점 인기를 얻고 있는 것 같습니다. 예를 들어, PHASEN [17]은 크기 마스킹과 위상 매핑으로 작업을 분리하고, TaylorS-ENet [18]은 이러한 분리 방법을 더 일반화시켰습니다.

부분: 크기 추정 및 복잡한 추정. CTSNet
[19]은 매핑 방법을 분리하려고 시도했습니다. 즉, 먼저 크기 스펙트럼을 매핑하고 그 다음 복잡한 스펙트럼을 매핑합니다. 또한, 연구자들은 복잡성의 한계를 넘어서 복잡한 접근 방식의 여러 단계를 포함시키는 것으로 연구를 진행했습니다. 각 단계는 큰 연쇄 네트워크를 활용하며 (연쇄 네트워크라고도 함 [21, 22, 23, 24]), 이로 인해 네트워크의 총 계산 및 매개 변수 수가 지수적으로 증가합니다. 이 접근 방식은 성능 향상과 더 복잡한 특징을 학습할 수 있는 네트워크를 가능하게 할 수 있지만, 성능과 계산 비용 사이의 균형을 고려하는 것이 중요합니다. 위의 관찰을 통해 몇 가지 불확실성을 확인하고 가설을 제시했습니다:

• 현재의 단일 채널 SE의 최신 트렌드로 인해 직접적인 기술과 기본적인 네트워크 구조를 사용하여 경쟁력 있는 성능을 달성하는 것은 어려워 보입니다.
• 마스킹과 매핑 중 어떤 방법이 더 우수한 성능을 보이는지에 대한 연구 결과와 이전 연구 [14, 25] 사이에 모순이 있습니다. 네트워크의 합리적인 최적화로 보아, 매핑 방법이 더 직접적이고 공격적이지 않게 보입니다.
• 분리 방법은 위상 추정 문제를 해결하기 위해 다단계 추정 전략을 채택하여 전체 과정을 더 복잡하게 만듭니다. 위상 추정 문제를 해결할 수 있다면, 네트워크의 구조가 크게 단순화될 것입니다.

26번 리뷰에 따르면, 현재의 모든 훈련 목표는 SA 방법으로 통칭될 수 있다. 예를 들어, 마스킹 방법은 LSA-마스킹 = ||S - ˆ M · Y ||로 표현될 수 있으며, 매핑 방법은 LSA-매핑 = ||Sr - ˆ Sr||+||Si - ˆ Si||로 표현될 수 있으며, 분리 방법은 LSA-분리 = |||S| - ˆ M · |Y ||| + ||Sr,i - ˆ Sr,i||로 표현될 수 있다. 연쇄 방법은 LSA-연쇄 = Lstage1 + Lstage2로 표현될 수 있다. 위의 방정식에서, L은 손실 함수를 나타내며, S는 목표 음성 신호를 나타내고, ˆ S는 예측된 음성 신호를 나타내며, ˆ M은 예측된 마스크를 나타내고, Y는 잡음이 섞인 음성 신호를 나타낸다. 첨자 r과 i는 각각 실수부와 허수부를 나타낸다. stage1과 stage2는 마스킹, 매핑 또는 연쇄를 사용하여 나타낼 수 있다. 관찰 결과, 우리는 위의 표현들을 직관적인 방식으로 통합할 수 있다고 믿는다. LSA-직관적 = ||S - ˆ S||. 이 전제에 기반하여, 우리는 마스크를 필요로 하지 않고, 짧은 시간 이산 코사인 변환 (STDCT) [27] 특징을 활용하는 음성 개선을 위한 간단한 단일 단계 신경망을 제안한다. 이 네트워크는 다음과 같은 특징을 가지고 있다.

우리는 효율적이고 가벼운 모듈을 설계했습니다.

그림 1: 제안된 MFNet의 아키텍처

GLFB라고 불리는 네트워크는 MetaFormerarchitecture[28], MobileNetblock[29], NAFNet [30]의 구조적 특징을 기반으로 하고 있습니다. 이 모듈 프로토타입은 MetaFormer를 기반으로 하며, 전역 모델링은 깊이별 분리 합성곱, 게이팅 메커니즘, 채널 어텐션 메커니즘을 사용하여 수행됩니다. 지역 모델링은 포인트 합성곱으로 수행됩니다.
• 우리의 네트워크 구조는 세 가지 모듈로 구성되어 있으며, 각각은 GLFB로 구성되어 있습니다. 인코더는 다운샘플링을 위해 작은 크기의 컨볼루션 커널을 사용하고, 디코더는 업샘플링을 위해 픽셀-셔플 방법을 사용합니다. 우리는 직접적인 합으로 점프 레이어 연결을 구축합니다.
• 우리가 제안한 네트워크는 실수값 STDCT 스펙트럼을 입력 특징으로 사용합니다. STFT 특징과 달리 복소값을 필요로하지 않고, STDCT 특징은 오로지 실수값만으로 표현되어 더 균일한 표현을 제공합니다. 이 네트워크는 마스크를 학습하지 않고 음성 개선을 수행하도록 설계되어 있으며, 음성과 역방향 노이즈를 모두 매핑할 수 있습니다. 우리는 이 네트워크를 MFNet이라고 명명했습니다.

우리의 실험 결과는 우리가 제안한 네트워크가 매핑 방법을 사용할 때 마스킹 접근법보다 우수한 성능을 보여준다는 것을 입증한다. 또한, 우리는 음성을 매핑하는 대신에 직접적으로 역 잡음을 학습할 때 우리의 네트워크가 강한 잡음 환경에서 더 나은 성능을 달성한다는 흥미로운 결과를 발견했다. DNS 2020 테스트 세트에서 간섭 없이, 우리가 제안한 모델은 꽤 경쟁력 있는 성능을 달성한다. 우리의 현재 이해에 따르면, 매핑 방법에서는 이 모델이 주어진 테스트 세트에서 가장 우수한 성능을 발휘한다.
이 논문의 나머지는 다음과 같이 구성되어 있다. 섹션 2에서는 제안한 방법을 소개한다. 섹션 3에서는 실험과 결과를 설명한다. 섹션 4는 종합적인 결론이다.

제안된 방법

2.1. STDCT 입력 특성

우리는 입력 특징으로 STDCT 스펙트럼을 활용했습니다. 이는 신호에 있는 모든 정보를 보존하고 암묵적인 위상 정보를 포함하는 실수값 변환입니다[31, 32]. 이로써 신호의 명시적인 위상을 추정하기 위해 복잡한 신경망을 설계할 필요가 없어졌으며, 이는 도전적이고 계산 비용이 많이 들 수 있는 작업입니다. 또한, STDCT 스펙트럼을 사용함으로써 다른 방법에서 필요한 복잡한 마스크를 추정할 필요가 없어졌습니다.

오디오 처리 기술.

2.2. 모델 구조

T-F bin 수준에서 집중적인 예측 작업에 적합한 UNet 모양의 네트워크 구조를 채택했습니다. 우리 모델의 네트워크 구조는 가능한 한 간결하게 설계되었으며 모듈은 재사용 가능하도록 설계되었으며 NAFNet의 디자인 컨셉에서 영감을 받았습니다. 모델은 인코더, 병목층 및 디코더로 구성됩니다. 포함된 기본 모듈의 관점에서 전체 네트워크는 프로젝션 레이어, GLFB 및 샘플링 (down 또는 up) 모듈 세 개만 포함합니다. 인코더에는 프로젝션 레이어, GLFB 및 다운샘플링 모듈이 포함되어 있습니다. 병목층에는 GLFB만 포함되어 있습니다. 디코더에는 프로젝션 레이어, GLFB 및 업샘플링 모듈이 포함되어 있습니다. 입력 측의 프로젝션 레이어는 STDCT 특징을 고차원 공간으로 투영하여 특징 맵의 크기는 유지되지만 채널 수는 모델에서 설정한 채널 수 n으로 증가합니다. 특징 맵의 채널 수는 각 다운샘플링 층마다 두 배로 증가하고 각 업샘플링 층마다 절반으로 감소하여 모델의 전체 채널 수는 [n,2n,4n,8n,16n,8n,4n,2n,n]입니다. 특히, 전체 네트워크에서 활성화 함수는 사용되지 않습니다. 네트워크의 성능은 주로 GLFB의 적층에 의해 결정됩니다. 인코더 단계에서 추출된 특징은 일반적인 연결 연산이 아닌 디코더 단계에 직접 추가되어 디코더 단계의 매개 변수 수를 줄이는데 기여합니다. 인코더, 병목층 및 디코더는 각각 [d1,d2,d3,d4],[m],[u1,u2,u3,u4]로 표시되는 여러 블록을 가지고 있습니다. 전체 네트워크 구조는 그림 1에 나와 있으며 텍스트 길이는 유사합니다.

2.3. 다운샘플링, 업샘플링 및 투영 레이어

UNet 모양의 네트워크에서 다운샘플링과 업샘플링은 일반적으로 각각 컨볼루션과 전치 컨볼루션을 통해 수행됩니다. 그러나 일부 연구자들은 모델 성능을 향상시키기 위해 더 큰 컨볼루션 커널을 사용하여 모델 파라미터를 늘리고 계산 비용을 증가시키는 방법을 사용해왔습니다. 이에 반해, MFNet 접근 방식은 다운샘플링을 위해 2의 컨볼루션 커널 크기와 2의 스트라이드를 사용하며, 업샘플링은 전치 컨볼루션에서 발생할 수 있는 체크보드 그리드 효과를 피하기 위해 픽셀-셔플 연산을 사용합니다. MFNet의 투영 레이어는 3 × 3 컨볼루션, 포인트 컨브, 레이어 정규화를 사용합니다.

포인트 컨브
활성화

(a) 메타포머 블록

포인트 컨브

DWConv

포인트 컨브
채널 어텐션
배치 정규화와 스위시 활성화

(b) 모바일넷 블록

포인트 컨브

DWConv

포인트 컨브
채널 어텐션

(c) GLFB (우리들의)
레이어 노름

레이어 정규화

포인트 컨브

포인트 컨브
게이트

게이트
배치 정규화
그리고 스위시 활성화

토큰 혼합
레이어 정규화

그림 2: GLFB의 구조

이 모델은 솔루션을 제공하며, 단일 채널 입력에서 특징을 추출하고, 입력 투영을 통해 고차원 특징으로 변환한 다음, 출력 투영을 통해 특징을 다시 단일 채널 출력으로 변환합니다.

2.4. 글로벌 로컬 전 블록

GLFB는 MFNet에서 중요한 모듈입니다. 이는 transformer 아키텍처 [33]에서 디자인 영감을 얻었으며, 멀티 헤드 어텐션 모듈과 피드포워드 네트워크 모듈을 포함합니다. 그러나 기본적인 self-attention 메커니즘은 특성 맵의 크기에 따라 이차적인 계산 복잡도를 가지므로, 모바일이나 자원 제한적인 장치에는 적합하지 않습니다. 이 문제를 해결하기 위해, 우리는 Metaformer 블록에 대한 연구에서 영감을 받아 멀티 헤드 어텐션 모듈 대신 수정된 MobileNet 블록을 채택합니다. 이 접근 방식은 O(n2)에 대한 토큰 길이 종속성의 복잡성 문제를 해결합니다. 동시에, 이 모듈은 transformer와 유사한 전역 및 지역 모델링 능력을 가지고 있습니다. 전역 모델링 부분은 깊이별 분리 컨볼루션, 간단한 게이팅 메커니즘 및 채널 어텐션 메커니즘을 통해 수행되며, 지역 모델링 부분은 포인트 컨볼루션을 통해 수행됩니다. 피드포워드 네트워크 모듈은 활성화 레이어를 게이트 레이어로 대체함으로써 약간 수정되었습니다. 자세한 내용은 그림 2에 나와 있습니다.
우리 모델에서 사용된 간단한 채널 어텐션 모듈은 MobileNet 블록에서 사용된 것과 동일합니다. DWConv는 깊이별 분리 컨볼루션을 나타내며, Point Conv는 포인트 컨볼루션을 나타냅니다. 2(c)에서는 이 모듈에 네 개의 Point Conv가 포함되어 있습니다. 첫 번째와 세 번째 Point Conv는 크기를 두 배로 늘립니다.

테이블 1: 마스크 없는 방법의 제거 연구

모델            PESQ STOI SNR

DCTCRN [32]       2.80 0.863 11.55
카스케이드 DCTCRN    2.83 0.867 11.59
TaylorSENet [18]  2.92 0.877 11.79
우리들(마스킹)     3.02 0.902 13.62
우리들(음성 매핑) 3.02 0.902 13.72
우리들(역 노이즈 매핑) 3.05 0.904 13.93

입력 채널의 수는 첫 번째와 네 번째 포인트 Conv에서 동일한 채널 수를 유지합니다. 게이트 메커니즘은 채널 수를 절반으로 줄입니다.

2.5. 손실 함수

우리는 MFNet을 위한 손실 함수를 제안합니다. 이 손실 함수는 두 가지 구성 요소를 포함하고 있습니다. 첫 번째는 STDCT의 절대값에 대한 평균 제곱 오차(MSE) 손실입니다. 이 부분은 다음과 같이 작성됩니다.

손실 절대값 = || |SSTDCT| - |ˆ SSTDCT| ||2 2, (1)

두 번째는 극값에 대한 MSE 손실입니다. 이 부분은 다음과 같이 작성됩니다.

Losspolar = ||SSTDCT − ˆ SSTDCT||2 2. (2)
손실극좌표 = ||SSTDCT − ˆ SSTDCT||2 2. (2)

하이퍼파라미터 γ는 절대 MSE와 극좌표 MSE 기여도의 가중치를 조정하는 역할을 합니다. MFNet의 손실 함수는 다음과 같이 작성됩니다.

LossMFNet = γ · Lossabs + (1 − γ) · Losspolar. (3)

LossMFNet = γ · Lossabs + (1 − γ) · Losspolar. (3)

수식에서 S는 목표 음성 신호를 나타내고, ˆ S는 네트워크에 의해 예측된 음성 신호를 나타냅니다.

3. 실험과 결과

3.1. 데이터셋

실험에서는 두 개의 데이터셋에서 데이터를 사용했습니다.
DNS-Challenge. Interspeech 2020 DNS-Challenge
코퍼스 [5]는 2150명의 화자에 의해 500시간 이상의 깨끗한 클립과 180시간 이상의 잡음 클립을 포함하고 있습니다. 모델 평가를 위해, 무음 검증 세트를 제공하며, 이는 반향이 있는 경우와 없는 경우로 구성되어 있으며, 각각 150개의 잡음-깨끗한 쌍을 포함합니다. 주최자가 제공한 스크립트를 따라, 우리는 훈련을 위해 3000시간의 데이터를 생성하고 SNR은 -3dB에서 15dB까지 무작위로 범위를 설정했습니다. 실험에서 공정성을 보장하기 위해, 우리는 공식 스크립트를 사용하여 데이터를 생성하고 어떠한 데이터 증강 기술도 사용하지 않았습니다.
TIMIT과 NOISEX-92. TIMIT [34] 코퍼스는 다른 테스트 깨끗한 음성으로 선택되었으며, NOISEX-92 [35]와 실제 녹음된 잡음 데이터셋은 테스트 잡음으로 사용되었습니다. 우리는 이미지 소스 방법을 사용하여 시뮬레이션된 RIR을 테스트 RIR 세트로 생성합니다. 방의 크기는 5m×4m×3.5m로 설정되었으며, T60 범위는 0.1:0.1:0.5입니다. 마이크와 스피커의 위치는 방 안에서 무작위로 설정되며, 높이 범위는 1m에서 1.5m입니다. 마이크와 스피커의 거리는 0.2m에서 3m로 제한합니다. SNR은 -9dB, -6dB, -3dB, 0dB, 3dB, 6dB, 9dB, 15dB입니다. 이 테스트 세트는 DNS 2020 테스트 세트에 비해 훨씬 크며, SNR의 범위도 더 넓습니다. 이는 우리 모델의 일반화 성능과 낮은 SNR에서의 성능을 테스트하고, 더 나아가 마스크 없는 접근법에서 음성 또는 역 잡음 매핑이 필요한지를 결정하기 위한 것입니다.
표 2: 반향이 없는 DNS 2020 테스트 세트의 실험 결과

모델              방법    MACs(G/s) WB-PESQ NB-PESQ STOI SI-SDR(dB)

시끄러운
DCCRN(2020) [4]    마스킹     11.13   -      3.27  -      -
FullSubNet(2021) [13] 마스킹  31.35  2.78    3.31 96.11 17.29
CTSNet(2021) [19] 분리 기반 매핑 5.57 2.94 3.42 96.21 16.69
TaylorSENet(2022) [18] 분리 기반 마스킹 6.14 3.22 3.59 97.36 19.15
FRCRN(2022) [23]   연쇄  241.981 3.23    3.60 97.69 19.78

MFNet 매핑 6.09 3.43 3.74 97.98 20.31

3.2. 모델 설정

모든 파형은 16kHz로 샘플링됩니다. 우리는 크기가 320이고 hop 시간이 10ms인 Hanning 창의 제곱근을 사용합니다. 옵티마이저는 AdamW입니다. 초기 학습률은 0.0034로 설정됩니다. 우리는 코사인 앤닐링과 웜업을 결합한 학습 전략을 사용하여 처음 5개의 에포크에서 학습률의 최대치에 도달합니다. 네트워크의 채널 수는 16입니다. 하이퍼파라미터 γ는 0.5입니다. 인코더, 병목, 디코더의 블록 수는 [d1 = 1, d2 = 1, d3 = 8, d4 = 4], [m = 6], [u1 = 1, u2 = 1, u3 = 1, u4 = 1]입니다. GLFB를 효과적으로 쌓음으로써 우리의 네트워크는 비대칭 구조입니다. 공간 제한으로 인해 실험 결과는 인코더가 디코더보다 중요하다는 결론을 내립니다. 따라서 인코더 단계에서는 더 많은 GLFB를 쌓습니다.

3.3. 평가 지표

다중 목적 지표가 채택되었으며, 이에는 음질을 위한 좁은 대역폭(NB) 및 넓은 대역폭(WB) 인지 평가 음성 품질(PESQ), 명료성을 위한 단기 객관적 명료도(STOI), 그리고 음성 왜곡을 위한 SI-SDR이 포함됩니다.

3.4. 마스크와 마스크 없이의 제거 연구

이 연구에서는 마스크 기반 및 마스크 없는 방법의 성능을 조사했습니다. DNS 2020 훈련 세트를 사용하고, 합성된 TIMIT 테스트 세트를 사용하여 저 SNR 조건과 보지 못한 화자에서 모델의 일반화 성능을 평가했습니다. DCTCRN [32], Cascade DCTCRN 및 TaylorSENet과 접근 방식을 비교했습니다. DCTCRN은 STDCT 특징을 사용하는 마스킹 음성 개선 네트워크로, DNS 대회에서 2위를 차지했습니다. TaylorSENet은 강력한 분리 마스킹 모델입니다. 또한, DCTCRN 모델을 연결하여 우리 모델과 다단계 모델을 비교할 수 있도록 했습니다. 공정성을 보장하기 위해 모든 모델은 동일한 훈련 구성으로 훈련되었습니다. 결과는 표 1에 제시되었습니다.

마스크 방법은 시그모이드 함수를 네트워크 출력에 연결하고, 노이지 STDCT 특징과 시그모이드 활성화된 특징의 Hadamard 곱을 취하는 것을 포함합니다. 음성 매핑은 대상 음성을 네트워크 출력의 학습 대상으로 직접 처리하는 것을 의미합니다. 반대로, 매핑 역 노이즈 방법은 네트워크 출력 특징을 노이지 STDCT 특징에 추가하고 대상 음성을 학습 대상으로 취급합니다. 실험 결과는 매핑 방법을 사용할 때 특히 매핑 역 노이즈에서 우리 네트워크가 마스킹 방법보다 더 좋은 결과를 달성한다는 것을 나타냅니다. 또한, 우리 네트워크는 PESQ, STOI 및 SNR 지표에서 DCTCRN, CascadeDCTCRN 및 TaylorSENet보다 우수한 성능을 보입니다.

모델이 합리적으로 훈련되고 이해한 후에는

계산이 충분하지 않으면, 마스킹 접근법은 너무 조심스러워져서 모델의 능력을 완전히 활용하지 못합니다. 반면에 매핑 방법은 덜 공격적이며 이 특정 모델에 더 적합해 보입니다. 흥미롭게도, 우리는 매우 노이즈가 있는 환경에서 모델이 노이즈를 직접적으로 반전시켜 배우면 더 좋은 성능을 발휘한다는 것을 관찰했습니다.

3.5. 최신 기술 방법과의 비교

우리는 제안된 SE 시스템을 Interspeech 2020 DNS-Challenge 데이터셋에서 평가하여 다른 모델과 비교하였으며, 결과는 표 2에 제시되었습니다. 우리의 MFNet 모델은 단지 6.09 GMACs/s의 계산 용량으로 뛰어난 성능을 달성하였습니다. 또한 Intel Xeon E5-2680 CPU에서 실시간 요인(RTF) 테스트를 수행하였고, 결과는 0.236이었습니다. 매핑 접근 방식을 사용하는 모델은 이 데이터셋에서 몇 개만 테스트되었기 때문에, 최고의 매핑 모델은 CTSNet이라는 디커플 기반 매핑 모델이었으며, TSCN [21]의 개선 버전인 2021 ICASSP DNS Challenge 우승자입니다. CTSNet은 비교를 위한 강력한 경쟁 모델로 간주될 수 있습니다. 우리의 모델의 성능을 보여주기 위해, DCCRN, Full-SubNet, TaylorSENet 및 FRCRN과 같은 예측을 위한 합리적인 범위 내의 다른 모든 방법의 모델과 수평 비교를 수행하였습니다. FRCRN 모델의 계산 복잡도는 https://modelscope.cn/models/damo/speech_frcrn_ans_cirm_16k/summary 웹사이트의 분석을 기반으로 계산되었습니다. 우리의 제안된 모델은 최근 제안된 모델들 중에서 매우 경쟁력이 있습니다. 우리의 MFNet은 현재 최첨단 매핑 네트워크인 CTSNet보다 큰 차이로 우수한 성능을 발휘합니다. 우리는 처리된 샘플을 https://github.com/ioyy900205/MFNet에서 제공합니다.

결론

우리는 음성 개선을 위한 혁신적인 신경망인 MFNet을 제안합니다. 이는 직접적으로 진짜 값 STDCT 스펙트럼 매핑을 학습하는데, 이는 직관적인 SA의 정의에서 영감을 받았습니다. 우리의 네트워크 아키텍처는 글로벌 및 로컬 정보를 모델링할 수 있는 간단하면서도 효과적인 단일 단계 구조를 만들기 위해 새롭게 설계된 경량 GLFB 모듈들을 쌓아올린 것으로 구성되어 있습니다. 매핑 방법을 사용하여, 우리의 제안된 프레임워크는 반향이 없는 DNS 2020 테스트 세트에서 현재 SOTA 매핑 모델보다 우수한 성능을 보입니다. 전반적으로, 우리의 실험 결과는 MFNet이 다양한 대안적인 접근 방식을 가진 다른 SOTA 모델보다 우수한 성능을 보여준다는 것을 보여줍니다. 이로써 MFNet은 음성 개선의 실용적인 응용 분야에서 유망한 후보가 됩니다. 앞으로 우리는 시스템을 인과 모델로 변환하여 실제 세계 배치를 용이하게 할 계획입니다.

[1] D. Stoller, S. Ewert, and S. Dixon, "Wave-u-net: A multi-scale neural network for end-to-end audio source separation," arXiv preprintarXiv:1806.03185,2018.
[1] D. 스톨러, S. 에워트, 그리고 S. 딕슨, "Wave-u-net: 오디오 소스 분리를 위한 멀티-스케일 신경망," arXiv 사전인쇄arXiv:1806.03185,2018.

[2] Y. Luo와 N. Mesgarani, "Conv-tasnet: 음성 분리를 위한 이상적인 시간-주파수-크기 마스킹을 능가하는 기술," IEEE/ACM 음성 및 언어 처리 트랜잭션, 제 27권, 제 8호, 1256-1266쪽, 2019년.

[3] D. S. Williamson, Y. Wang, and D. Wang, "단일음성분리를 위한 복잡한 비율 마스킹," IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 제24권, 제3호, 483-492쪽, 2015년.

[4] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu,
B. Zhang, and L. Xie, "Dccrn: Deep complex convolution re-
current network for phase-aware speech enhancement," arXiv
preprintarXiv:2008.00264,2020.

[4] Y. 휴, Y. 리우, S. 루브, M. 싱, S. 장, Y. 푸, J. 우,
B. 장, 그리고 L. 시에, "Dccrn: 위상 인식 음성 개선을 위한 딥 복소 합성곱 순환 신경망," arXiv
프리프린트arXiv:2008.00264,2020.

[5] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng,
H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun
et al., "The interspeech 2020 deep noise suppression challenge:
Datasets, subjective testing framework, and challenge results,"
arXivpreprintarXiv:2005.13981,2020.

[5] C. K. 레디, V. 고팔, R. 커틀러, E. 베이라미, R. 청,
H. 두베이, S. 마투세비치, R. 아이크너, A. 아자미, S. 브라운
외, "인터스피치 2020 딥 노이즈 억제 챌린지:
데이터셋, 주관적 테스트 프레임워크 및 챌린지 결과,"
arXivpreprintarXiv:2005.13981,2020.

[6] C.K.Reddy, H.Dubey, V.Gopal, R.Cutler, S.Braun, H.Gamper,
R. Aichner, and S. Srinivasan, "Icassp 2021 deep noise suppres-
sionchallenge," in ICASSP, 2021.

[6] C.K. Reddy, H. Dubey, V. Gopal, R. Cutler, S. Braun, H. Gamper,
R. Aichner, 그리고 S. Srinivasan, "Icassp 2021 딥 노이즈 억제 도전," ICASSP, 2021에서.

[7] C. K. Reddy, H. Dubey, K. Koishida, A. Nair, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, "인터스피치 2021 딥 노이즈 억제 챌린지," INTERSPEECH, 2021.

[8] H. Dubey, V. Gopal, R. Cutler, S. Matusevych, S. Braun, E. S. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, and R. Aichner, "Icassp2022deepnoisesuppressionchallenge," in ICASSP, 2022.

[8] H. Dubey, V. Gopal, R. Cutler, S. Matusevych, S. Braun, E. S. Eskimez, M. Thakker, T. Yoshioka, H. Gamper, 그리고 R. Aichner, "Icassp2022deepnoisesuppressionchallenge," ICASSP에서, 2022년에.

[9] Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, "깊은 신경망을 기반으로 한 음성 개선을 위한 회귀 접근 방법," IEEE/ACM Transactions on Audio, Speech, and Language Processing, 제23권, 제1호, 2014년, 7-19쪽.

[10] K. Tan과 D. Wang, "단일 음성 개선을 위한 게이트 컨볼루션 순환 네트워크를 사용한 복잡한 스펙트럼 매핑 학습," IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 제28권, 380-390쪽, 2019년.

[11] Y. Wang, A. Narayanan, and D. Wang, "지도된 음성 분리를 위한 훈련 목표에 대하여," IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 제22권, 제12호, 1849-1858쪽, 2014년.

[12] X. Le, H. Chen, K. Chen, and J. Lu, "Dpcrn: 단일 채널 음성 개선을 위한 듀얼 패스 컨볼루션 재귀 네트워크," arXiv 사전 인쇄 arXiv:2107.05429, 2021.

[13] X. Hao, X. Su, R. Horaud, 그리고 X. Li, "실시간 단일 채널 음성 개선을 위한 풀 밴드 및 서브 밴드 퓨전 모델인 Fullsubnet," ICASSP 2021-2021 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP)에서, IEEE, 2021, pp. 6633-6637.

[14] H.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, and K. Lee,
"Phase-aware speech enhancement with deep complex u-net," in
InternationalConferenceonLearningRepresentations,2019.

[14] H.-S. Choi, J.-H. Kim, J. Huh, A. Kim, J.-W. Ha, 그리고 K. Lee,
"깊은 복소 U-Net을 이용한 위상 인식 음성 개선," InternationalConferenceonLearningRepresentations,2019.

[15] H. Schroter, A. N. Escalante-B, T. Rosenkranz, and A. Maier,
"Deepfilternet: 딥 필터링을 기반으로 한 전체 대역 오디오의 저 복잡도 음성 개선 프레임워크," ICASSP 2022-2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP). IEEE, 2022, pp. 7407-7411.

[16] H. Schröter, A. Maier, A. Escalante-B, and T. Rosenkranz, "Deep-filternet2: 임베디드 장치에서 전체 대역 오디오에 대한 실시간 음성 개선을 위한," 2022년 국제 음향 신호 개선 워크샵(IWAENC)에서, IEEE, 2022, 1-5쪽.

[17] D. Yin, C. Luo, Z. Xiong, and W. Zeng, "Phasen: 음성 개선을 위한 상하반파 및 고조파 인식 네트워크," 인공지능 AAAI 학회 논문집, 제34권, 제05호, 2020년, 9458-9465쪽.

[18] A. Li, S. You, G. Yu, C. Zheng, and X. Li, "테일러, 지금 내 목소리 들리니? 단일음성 개선을 위한 테일러 펼침 프레임워크," arXiv 사전인쇄 arXiv:2205.00206, 2022.

[19] A. Li, W. Liu, C. Zheng, C. Fan, and X. Li, "두 개의 머리가 하나보다 낫다: 단일 음성 개선을 위한 이중 단계 복잡한 스펙트럼 매핑 접근법," IEEE/ACM Transactions on Audio, Speech, and Language Processing, 제 29권, pp. 1829-1843, 2021.

[20] T. Wang, W. Zhu, Y. Gao, Y. Chen, J. Feng, and S. Zhang, "Harmonic gated compensation network plus for ICASSP 2022 DNS challenge," in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9286-9290.

[20] T. 왕, W. 주, Y. 고, Y. 천, J. 펑, 그리고 S. 장, "ICASSP 2022 DNS 도전을 위한 하모닉 게이트 보상 네트워크 플러스," ICASSP 2022-2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP)에서, IEEE, 2022, pp. 9286-9290.

[21] A. Li, W. Liu, X. Luo, C. Zheng, and X. Li, "Icassp 2021 deep noise suppression challenge: Decoupling magnitude and phase optimization with a two-stage deep network," in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6628–6632.

[21] A. 리, W. 류, X. 루오, C. 정, 그리고 X. 리, "Icassp 2021 딥 노이즈 억제 챌린지: 두 단계 딥 네트워크를 사용한 크기와 위상 최적화의 분리," ICASSP 2021-2021 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP)에서, IEEE, 2021, pp. 6628–6632.

[22] A. Li, W. Liu, X. Luo, G. Yu, C. Zheng, and X. Li, "목표 분리를 통한 동시 노이즈 제거 및 반향 제거 프레임워크," arXiv 사전 인쇄 arXiv:2106.12743, 2021.

[23] S. Zhao, B. Ma, K. N. Watcharasupat, and W.-S. Gan, "Frcrn: Boosting feature representation using frequency recurrence for monaural speech enhancement," in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9281-9285.

[23] S. Zhao, B. Ma, K. N. Watcharasupat, 그리고 W.-S. Gan, "Frcrn: 단일 음성 개선을 위한 주파수 재발생을 사용한 특징 표현 강화," ICASSP 2022-2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP)에서, IEEE, 2022, pp. 9281-9285.

[24] H. Wang and D. Wang, "음성 개선을 위한 트리플 도메인 손실을 갖춘 신경 연쇄 아키텍처," IEEE/ACM Transactions on Audio, Speech, and Language Processing, 제30권, pp. 734-743, 2021.

[25] H. Wu, K. Tan, B. Xu, A. Kumar, and D. Wong, "단일 음성 개선을 위한 복소값 심층 신경망 재고찰," arXiv 사전인쇄 arXiv:2301.04320, 2023.

[26] D. 왕과 J. 천, "심층 학습을 기반으로 한 감독형 음성 분리: 개요," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1702-1726, 2018.

[27] K. R. Rao와 P. Yip, 이산 코사인 변환: 알고리즘, 장점, 응용. Academicpress, 2014.

[28] W. 유, M. 루오, P. 주우, C. 시이, Y. 주우, X. 왕, J. 펑, 그리고 S. 얀, "메타포머는 실제로 비전에 필요한 것입니다," in IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2022, pp. 10819-10829.

[29] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
"Mobilenetv2: 역 이득과 선형 병목," in 컴퓨터 비전 및 패턴 인식에 대한 IEEE 컨퍼런스 논문집, 2018, pp. 4510-4520.

[30] L. Chen, X. Chu, X. Zhang, and J. Sun, "이미지 복원을 위한 간단한 기준선," arXiv 사전 인쇄 arXiv:2204.04676, 2022.

[31] F. Gao와 H. Guan, "단일 음성 개선을 위한 저전력 합성 순환 신경망," 2021 아시아-태평양 신호 및 정보 처리 협회 연차 정상회의 및 컨퍼런스 (APSIPAASC), IEEE, 2021, 559-563쪽.

[32] Q. Li, F. Gao, H. Guan, and K. Ma, "실시간 단일 채널 음성 개선을 위한 단시간 이산 코사인 변환," arXiv 사전인쇄 arXiv:2102.04629, 2021.

[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "주의는 당신이 필요한 모든 것입니다," 신경 정보 처리 시스템의 발전, 30권, 2017년.

[34] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett, "Darpa timit 음향-음성 연속 말 코퍼스 cd-rom.nistspeechdisc1-1.1," NASASTI/Recontechnicalreport n,vol.93,p.27403,1993.

[35] A. Varga와 H. J. Steeneken, "자동 음성 인식을 위한 평가: II. noisex-92: 음성 인식 시스템에 첨가된 잡음의 영향을 연구하기 위한 데이터베이스 및 실험," Speechcommunication, vol.12, no.3, pp.247-251, 1993.

